/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/DynamicMaze/StochAgent/FreeSpace/DQN_FixedTarget/DQN_CNNDynMazeStochAgentCPU.py
episode index:0
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.067333, 7.485413, 3.664972], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.766600222911088}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9801
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9181967, 5.074237 , 3.7442672], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.921192842257538}
episode index:1
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  9.       ,  2.4867506], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.49005
{'scaleFactor': 1.0, 'currentTarget': array([22.56119039, 23.45607595]), 'previousTarget': array([22.56119039, 23.45607595]), 'currentState': array([29.454443 , 30.700596 ,  2.8919814], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:2
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.76808352, 18.33937665]), 'previousTarget': array([19.92893219, 19.92893219]), 'currentState': array([25.950083 , 25.297745 ,  4.1437693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3267
{'scaleFactor': 1.0, 'currentTarget': array([19.59550643,  2.64263055]), 'previousTarget': array([19.59550643,  2.64263055]), 'currentState': array([29.467571 ,  1.0481601,  3.9966261], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:3
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.      ,  6.      ,  2.971826], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.245025
{'scaleFactor': 1.0, 'currentTarget': array([19.50868931,  1.99799472]), 'previousTarget': array([19.50868931,  1.99799472]), 'currentState': array([ 2.9301266e+01, -2.8195620e-02,  4.9052978e+00], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:4
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 3.       , 2.5172098], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.35796557364425163
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5237894 , 4.84789   , 0.83421826], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.484026684672488}
episode index:5
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.0973876 , 12.28424463]), 'previousTarget': array([ 9.0973876 , 12.28424463]), 'currentState': array([14.        , 21.        ,  0.24380448], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29830464470354306
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.61711967, 0.7067443 , 4.244309  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.135281931561845}
episode index:6
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.70660816, 18.08399589]), 'previousTarget': array([ 6.70660816, 18.08399589]), 'currentState': array([ 8.       , 28.       ,  3.3429904], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2556896954601798
{'scaleFactor': 1.0, 'currentTarget': array([9.4667259, 3.8319436]), 'previousTarget': array([9.4667259, 3.8319436]), 'currentState': array([19.141403 ,  1.3019991,  2.4709604], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:7
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.63116353,  3.68803924]), 'previousTarget': array([13.06116265,  4.10431526]), 'currentState': array([21.44101  ,  1.7471833,  3.9832942], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.24614294397699613
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.06946   , 4.6091385 , 0.25806212], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0092955813106943}
episode index:8
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.594073 , 3.9097323, 1.5983438], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.813875860019327}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.3266047279795521
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.760807 , 5.727882 , 2.4673748], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9053224124525674}
episode index:9
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.10360009, 21.79032749]), 'previousTarget': array([12.10360009, 21.79032749]), 'currentState': array([16.       , 31.       ,  1.1011081], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.324184698748499
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7669463, 6.763152 , 5.163947 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9227355664088956}
episode index:10
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.58869508,  3.74754641]), 'previousTarget': array([15.19419324,  2.96116135]), 'currentState': array([23.484035 ,  2.3045502,  2.8105116], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2947133624986354
{'scaleFactor': 1.0, 'currentTarget': array([21.12243802, 16.63202506]), 'previousTarget': array([21.12243802, 16.63202506]), 'currentState': array([29.232084 , 22.482977 ,  4.8767505], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:11
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.62487811, 14.49087366]), 'previousTarget': array([13.70462796, 13.16058871]), 'currentState': array([20.350245 , 21.891512 ,  1.4166555], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2701539156237491
{'scaleFactor': 1.0, 'currentTarget': array([5.25467709, 4.87283821]), 'previousTarget': array([5., 5.]), 'currentState': array([14.201431  ,  0.40567026,  1.2900032 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:12
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.00943842,  4.43437224]), 'previousTarget': array([18.00943842,  4.43437224]), 'currentState': array([28.       ,  4.       ,  3.2890213], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24937284519115296
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.42079  , 8.044893 , 3.6488757], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.367937847085342}
episode index:13
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.      , 9.      , 5.653576], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23156049910607063
{'scaleFactor': 1.0, 'currentTarget': array([ 4.91401435, 18.36514759]), 'previousTarget': array([ 6.03689711, 18.91611754]), 'currentState': array([ 4.84968 , 28.36494 ,  3.293014], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:14
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.93989519, 14.21450215]), 'previousTarget': array([ 6.93989519, 14.21450215]), 'currentState': array([ 9.       , 24.       ,  0.7645069], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21612313249899925
{'scaleFactor': 1.0, 'currentTarget': array([18.79002082, 23.28440848]), 'previousTarget': array([18.79002082, 23.28440848]), 'currentState': array([24.811438 , 31.268301 ,  2.9160657], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:15
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.50693743, 5.35754597]), 'previousTarget': array([7., 5.]), 'currentState': array([18.455366  ,  6.371827  ,  0.27984607], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.23417080476700364
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1725564, 4.1829247, 3.8273616], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4291607607907926}
episode index:16
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.       , 10.       ,  5.7617717], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2203960515454152
{'scaleFactor': 1.0, 'currentTarget': array([18.57715171, 22.22139058]), 'previousTarget': array([18.57715171, 22.22139058]), 'currentState': array([24.768345 , 30.07436  ,  2.2771125], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:17
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.07755479, 16.9951469 ]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.1422086, 26.994938 ,  1.9754934], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2081518264595588
{'scaleFactor': 1.0, 'currentTarget': array([ 7.68545874, 13.82358509]), 'previousTarget': array([ 7.68545874, 13.82358509]), 'currentState': array([10.597095  , 23.390318  ,  0.07271069], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:18
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.25900177, 9.195289  ]), 'previousTarget': array([7.25900177, 9.195289  ]), 'currentState': array([12.       , 18.       ,  3.6676579], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1971964671722136
{'scaleFactor': 1.0, 'currentTarget': array([14.14761416, 22.19761136]), 'previousTarget': array([14.14761416, 22.19761136]), 'currentState': array([18.843723 , 31.026346 ,  1.6723276], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:19
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13461185, 15.00425519]), 'previousTarget': array([16.74047316, 16.12255352]), 'currentState': array([22.2513   , 22.029406 ,  3.8471243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1873366438136029
{'scaleFactor': 1.0, 'currentTarget': array([10.71221321,  4.0685989 ]), 'previousTarget': array([11.29150938,  3.21548605]), 'currentState': array([20.581873 ,  2.4593081,  1.7424316], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:20
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.29339184, 18.08399589]), 'previousTarget': array([ 3.29339184, 18.08399589]), 'currentState': array([ 2.       , 28.       ,  1.5538807], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1784158512510504
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.4093914, 5.464896 , 4.4434166], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.614088833967709}
episode index:21
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.136367 ,  7.2741413,  3.4047756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.544212462294495}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.21441053983054809
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8680005, 4.560665 , 4.4135184], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9728514831285445}
episode index:22
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.02945514, 4.76696499]), 'previousTarget': array([8.02945514, 4.76696499]), 'currentState': array([18.       ,  4.       ,  1.2373594], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.23112990794513996
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.28768 , 6.164814, 4.299306], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3653538785994768}
episode index:23
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 1.88847394, 21.17993553]), 'previousTarget': array([ 1.88847394, 21.17993553]), 'currentState': array([ 0.       , 31.       ,  3.0470965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22149949511409248
{'scaleFactor': 1.0, 'currentTarget': array([4.75108143, 5.53270859]), 'previousTarget': array([4.75108143, 5.53270859]), 'currentState': array([ 0.5177412, 14.592445 ,  2.2722397], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:24
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.17418478, 11.24607936]), 'previousTarget': array([14.89320772, 12.14509446]), 'currentState': array([21.119999 , 17.317656 ,  3.6730535], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21263951530952876
{'scaleFactor': 1.0, 'currentTarget': array([14.46492807, 19.63115373]), 'previousTarget': array([15.84046857, 19.5001144 ]), 'currentState': array([19.896515, 28.027454,  2.849094], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:25
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.92893219, 20.92893219]), 'previousTarget': array([20.92893219, 20.92893219]), 'currentState': array([28.       , 28.       ,  1.0386637], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2244741694380202
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5721207, 5.335573 , 3.4785297], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6075362391555195}
episode index:26
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.01588364, 11.04615253]), 'previousTarget': array([ 9.54057759, 10.36613715]), 'currentState': array([14.54868  , 19.376108 ,  3.4070678], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21616031131068614
{'scaleFactor': 1.0, 'currentTarget': array([ 2.14681123, 20.89696281]), 'previousTarget': array([ 2.14681123, 20.89696281]), 'currentState': array([ 0.38023794, 30.739687  ,  3.596513  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:27
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.76891701, 5.81761339]), 'previousTarget': array([4.99503719, 5.0496281 ]), 'currentState': array([ 2.0491471, 15.440651 ,  2.8369858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.23407353349952859
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.047119, 6.182398, 5.423998], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2829387288862395}
episode index:28
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.14227319,  6.99631554]), 'previousTarget': array([14.21450215,  6.93989519]), 'currentState': array([25.985535,  8.759892,  5.759444], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.23572136667335394
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.903577  , 4.0416956 , 0.23500794], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9631431652322363}
episode index:29
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.596799 ,  8.958936 ,  1.7531319], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.464572016016708}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22786398778424213
{'scaleFactor': 1.0, 'currentTarget': array([19.51788284,  2.71403527]), 'previousTarget': array([19.51788284,  2.71403527]), 'currentState': array([29.396175 ,  1.1586137,  6.075769 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:30
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.24729189, 12.26482453]), 'previousTarget': array([10.21719897, 10.65196555]), 'currentState': array([17.76741   , 19.846912  ,  0.45534593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.24743333532185005
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0817204, 6.2821975, 2.9715767], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6775426332324948}
episode index:31
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.16336881, 5.83173528]), 'previousTarget': array([9.58258088, 6.63663603]), 'currentState': array([17.834663,  8.37458 ,  3.553404], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2674005708341713
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8033237, 4.667755 , 2.6925738], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8336747769358492}
episode index:32
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.32919421, 20.36412233]), 'previousTarget': array([ 3.14624108, 21.06591064]), 'currentState': array([ 3.8930044, 30.354605 ,  6.247868 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2592975232331358
{'scaleFactor': 1.0, 'currentTarget': array([20.83800338, 14.97788999]), 'previousTarget': array([20.83800338, 14.97788999]), 'currentState': array([29.298931 , 20.308247 ,  4.2243204], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:33
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.8363468 ,  6.47731784]), 'previousTarget': array([13.3648209 ,  7.32356136]), 'currentState': array([23.699453,  8.126296,  4.830345], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25167112549098475
{'scaleFactor': 1.0, 'currentTarget': array([20.83953602, 17.38612757]), 'previousTarget': array([19.94832039, 18.38094019]), 'currentState': array([28.717007 , 23.546116 ,  5.7238855], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:34
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.85913455, 14.00182649]), 'previousTarget': array([ 6.26214666, 12.15216441]), 'currentState': array([ 8.881735 , 23.795145 ,  1.3301356], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24448052190552805
{'scaleFactor': 1.0, 'currentTarget': array([ 2.21519086, 20.90206884]), 'previousTarget': array([ 2.21519086, 20.90206884]), 'currentState': array([ 0.49021733, 30.752169  ,  2.4901037 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:35
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.1251689 , 10.71595345]), 'previousTarget': array([ 7.01725007, 11.45520022]), 'currentState': array([ 8.056576, 20.527664,  3.021006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.25951378909724165
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1358948, 5.384984 , 4.466353 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9034445043955588}
episode index:36
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([17.       , 14.       ,  5.7502995], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.26521846078611755
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3688679, 3.115826 , 3.2340155], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4921284242903985}
episode index:37
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.10369858, 4.82318372]), 'previousTarget': array([11.,  5.]), 'currentState': array([19.094429 ,  4.3927126,  3.3493283], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25823902760753553
{'scaleFactor': 1.0, 'currentTarget': array([20.97039734, 15.77954124]), 'previousTarget': array([20.97039734, 15.77954124]), 'currentState': array([29.258999  , 21.3741    ,  0.35775912], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:38
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.000977 , 12.06248  ,  3.8497198], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.11704606494452}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.26728718688767217
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.46139 , 3.566899, 5.533655], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5309732241263976}
episode index:39
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.97261386, 20.84684968]), 'previousTarget': array([11.97261386, 20.84684968]), 'currentState': array([16.      , 30.      ,  2.053939], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26060500721548036
{'scaleFactor': 1.0, 'currentTarget': array([5.1475323 , 4.98186721]), 'previousTarget': array([5.1475323 , 4.98186721]), 'currentState': array([15.072846  ,  3.7619739 ,  0.35479492], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:40
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.94944163, 13.06800962]), 'previousTarget': array([15.94944163, 13.06800962]), 'currentState': array([24.      , 19.      ,  5.758803], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.257582933989654
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8097515, 5.4748383, 4.0332003], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8710082576907}
episode index:41
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.7123137, 10.868853 ,  1.2656972], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.1135463573717885}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2514500069899003
{'scaleFactor': 1.0, 'currentTarget': array([8.05522849, 3.79614564]), 'previousTarget': array([8.05522849, 3.79614564]), 'currentState': array([17.359022  ,  0.13016367,  6.15018   ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:42
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.61556384, 12.55011649]), 'previousTarget': array([16.61556384, 12.55011649]), 'currentState': array([25.       , 18.       ,  2.5190506], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2456023324087398
{'scaleFactor': 1.0, 'currentTarget': array([18.47313001, 14.57198152]), 'previousTarget': array([18.47313001, 14.57198152]), 'currentState': array([26.625229, 20.363638,  3.287139], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:43
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.55865  ,  5.079607 ,  4.4970407], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.558981505338357}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24002046121763207
{'scaleFactor': 1.0, 'currentTarget': array([14.24901604, 22.11128164]), 'previousTarget': array([14.24901604, 22.11128164]), 'currentState': array([19.004057, 30.908417,  2.614701], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:44
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.43919477,  3.83233186]), 'previousTarget': array([18.00943842,  4.43437224]), 'currentState': array([26.3875   ,  2.8168478,  3.421065 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23468667319057357
{'scaleFactor': 1.0, 'currentTarget': array([ 3.69609855, 10.8384984 ]), 'previousTarget': array([ 3.69609855, 10.8384984 ]), 'currentState': array([ 1.516509, 20.598078,  5.356007], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:45
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.       , 31.       ,  2.8370264], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2295847889907785
{'scaleFactor': 1.0, 'currentTarget': array([12.36301193,  8.57660665]), 'previousTarget': array([11.61286992,  9.1589001 ]), 'currentState': array([21.357956, 12.94593 ,  5.759526], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:46
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.05572809, 4.47213595]), 'previousTarget': array([6.05572809, 4.47213595]), 'currentState': array([15.       ,  0.       ,  4.3134627], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.23512320354425567
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.426556 , 3.5399177, 2.680713 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0412992522181215}
episode index:47
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.43821285, 20.45394478]), 'previousTarget': array([18.43821285, 20.45394478]), 'currentState': array([25.      , 28.      ,  4.555082], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.230224803470417
{'scaleFactor': 1.0, 'currentTarget': array([5.15058371, 4.97821761]), 'previousTarget': array([6.85025446, 4.91185554]), 'currentState': array([15.047575 ,  3.546588 ,  3.3853726], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:48
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.42181602, 9.1324901 ]), 'previousTarget': array([ 5.75965265, 11.07722123]), 'currentState': array([ 6.4372706, 19.0808   ,  4.2058077], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22552633809346972
{'scaleFactor': 1.0, 'currentTarget': array([19.6275889 , 12.22632608]), 'previousTarget': array([19.6275889 , 12.22632608]), 'currentState': array([28.593203 , 16.655521 ,  0.2744121], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:49
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.882836 , 3.6744843, 0.5563202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5926051488985926}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.24101581133160033
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.882836 , 3.6744843, 0.5563202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5926051488985926}
episode index:50
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.4452998 , 13.01539647]), 'previousTarget': array([ 5.4452998 , 13.01539647]), 'currentState': array([ 6.      , 23.      ,  5.491935], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23629001110941208
{'scaleFactor': 1.0, 'currentTarget': array([21.90962613, 20.71121859]), 'previousTarget': array([21.39473733, 19.03070921]), 'currentState': array([29.235525 , 27.517921 ,  0.9574306], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:51
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.55503203,  9.71479375]), 'previousTarget': array([18.55503203,  9.71479375]), 'currentState': array([2.800000e+01, 1.300000e+01, 5.137861e-03], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23174597243423106
{'scaleFactor': 1.0, 'currentTarget': array([8.5910766 , 3.52822794]), 'previousTarget': array([6.74515777, 4.234619  ]), 'currentState': array([17.84411   , -0.26404893,  0.17657965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:52
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.838243 ,  1.0065521,  4.122399 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.69856469598509}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22737340691660407
{'scaleFactor': 1.0, 'currentTarget': array([16.43384663,  9.17850695]), 'previousTarget': array([14.5267021 ,  8.62541732]), 'currentState': array([25.826296  , 12.610984  ,  0.16121218], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:53
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.0028451 , 6.26640835]), 'previousTarget': array([7.31035268, 7.56705854]), 'currentState': array([12.210909 , 14.106048 ,  2.8363144], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.22622691041193985
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.499878, 5.266177, 2.860788], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5233135244419767}
episode index:54
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.64886795, 17.09169832]), 'previousTarget': array([ 6.64886795, 17.09169832]), 'currentState': array([ 8.      , 27.      ,  4.192979], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22211369385899546
{'scaleFactor': 1.0, 'currentTarget': array([ 2.3012434, 14.6132579]), 'previousTarget': array([ 2.3012434, 14.6132579]), 'currentState': array([-0.40159738, 24.241064  ,  0.9399817 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:55
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.82536221, 15.55338206]), 'previousTarget': array([13.38378363, 14.50162145]), 'currentState': array([18.78159  , 23.586025 ,  2.2766252], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.22753302946018414
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.588413 , 3.8229856, 1.9992988], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.315899829136398}
episode index:56
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25298532, 18.02724334]), 'previousTarget': array([18.27327206, 19.60059927]), 'currentState': array([25.38453  , 25.037313 ,  5.0967493], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.22758570996647084
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.186711 , 4.1170807, 2.1358414], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4791312028985937}
episode index:57
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.77146803, 6.12191663]), 'previousTarget': array([4.71390676, 5.71523309]), 'currentState': array([ 2.7754786 , 15.920693  ,  0.27137244], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2236618184153248
{'scaleFactor': 1.0, 'currentTarget': array([ 5.98658915, 19.8661548 ]), 'previousTarget': array([ 5.98658915, 19.8661548 ]), 'currentState': array([ 6.6487803, 29.844206 ,  2.0639536], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:58
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.54592339, 17.0103146 ]), 'previousTarget': array([ 5.54592339, 17.0103146 ]), 'currentState': array([ 6.       , 27.       ,  3.6340945], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21987094013709896
{'scaleFactor': 1.0, 'currentTarget': array([4.26642946, 6.49616496]), 'previousTarget': array([4.26642946, 6.49616496]), 'currentState': array([-0.13589916, 15.475001  ,  1.7979429 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:59
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.96876603, 14.25375628]), 'previousTarget': array([ 6.93989519, 14.21450215]), 'currentState': array([ 7.0099654, 24.199404 ,  3.4755933], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.22428961397271668
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5192866, 6.0726414, 5.24785  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.828406845034054}
episode index:60
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00277547, 22.33814414]), 'previousTarget': array([15.00277547, 22.33814414]), 'currentState': array([20.       , 31.       ,  1.9075062], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22061273505513115
{'scaleFactor': 1.0, 'currentTarget': array([ 8.96934878, 17.90677236]), 'previousTarget': array([ 7.93930062, 18.48027831]), 'currentState': array([11.908877 , 27.464972 ,  5.8443913], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:61
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.327818  , 8.61116334]), 'previousTarget': array([ 8.03861062, 10.31756858]), 'currentState': array([12.745857 , 17.016212 ,  5.3259063], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.22081034356953236
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.1811724, 5.6635528, 4.1189504], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6878413296069548}
episode index:62
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.72907437,  4.69646811]), 'previousTarget': array([19.00866927,  5.58369455]), 'currentState': array([27.726233 ,  4.4580803,  4.865071 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21730541748112708
{'scaleFactor': 1.0, 'currentTarget': array([19.43673756,  4.31935602]), 'previousTarget': array([19.43673756,  4.31935602]), 'currentState': array([29.425642 ,  3.8484125,  0.1076042], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:63
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.      ,  3.      ,  4.846693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21391002033298448
{'scaleFactor': 1.0, 'currentTarget': array([21.08464018, 17.78199434]), 'previousTarget': array([21.08464018, 17.78199434]), 'currentState': array([28.91364 , 24.003471,  3.943041], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:64
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.5528723 , 17.75779887]), 'previousTarget': array([14.18761806, 17.86266529]), 'currentState': array([21.926647, 25.463316,  5.367895], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.2217726405733671
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.329236 , 3.0742218, 2.8228362], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9537189790242528}
episode index:65
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  4.       ,  1.3914697], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.22598573738133423
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.472969 , 6.0941525, 2.3414607], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.834886215157556}
episode index:66
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.02945514, 5.23303501]), 'previousTarget': array([8.02945514, 5.23303501]), 'currentState': array([18.       ,  6.       ,  4.5739007], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.22551324277687843
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6426754, 3.7386954, 2.0211062], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8528948720976544}
episode index:67
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.91962009,  6.73641515]), 'previousTarget': array([12.06849396,  5.83158752]), 'currentState': array([22.68759  ,  8.87809  ,  1.3847415], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.22940114028022146
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.119216, 5.100948, 2.647122], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1237592476731}
episode index:68
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.       , 14.       ,  0.5369016], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22607648607326172
{'scaleFactor': 1.0, 'currentTarget': array([11.30998115, 19.01849491]), 'previousTarget': array([12.51197285, 18.73084424]), 'currentState': array([15.414525 , 28.137306 ,  2.7521133], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:69
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.23716851, 20.20708033]), 'previousTarget': array([13.23716851, 20.20708033]), 'currentState': array([18.     , 29.     ,  5.18657], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22284682198650085
{'scaleFactor': 1.0, 'currentTarget': array([20.19321621, 23.4937863 ]), 'previousTarget': array([20.19321621, 23.4937863 ]), 'currentState': array([26.541082 , 31.220661 ,  1.4307361], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:70
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.26537656, 6.80768079]), 'previousTarget': array([6.26537656, 6.80768079]), 'currentState': array([12.      , 15.      ,  2.497939], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21970813435288816
{'scaleFactor': 1.0, 'currentTarget': array([14.21307723, 21.0606778 ]), 'previousTarget': array([15.43645034, 22.61162063]), 'currentState': array([19.188932 , 29.734827 ,  4.1126137], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:71
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.67373797,  9.58607883]), 'previousTarget': array([13.67995328,  8.37553739]), 'currentState': array([23.70975  , 13.869827 ,  0.5730003], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21665663248687583
{'scaleFactor': 1.0, 'currentTarget': array([17.3427748 , 22.83963829]), 'previousTarget': array([17.3427748 , 22.83963829]), 'currentState': array([23.032461  , 31.06323   ,  0.97372794], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:72
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.63670822,  7.48876558]), 'previousTarget': array([11.63670822,  7.48876558]), 'currentState': array([21.      , 11.      ,  6.065372], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.22434382052676996
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7213736, 5.174367 , 4.2094393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7421479932215715}
episode index:73
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.16709083, 11.78881618]), 'previousTarget': array([ 6.42337349, 10.3376506 ]), 'currentState': array([ 7.861374 , 21.644241 ,  2.2554994], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22131214727640822
{'scaleFactor': 1.0, 'currentTarget': array([4.67500413, 5.6159103 ]), 'previousTarget': array([4.67500413, 5.6159103 ]), 'currentState': array([8.180499e-03, 1.446016e+01, 3.740453e+00], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:74
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.       , 10.       ,  4.2109885], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.22994459614964247
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6430874, 4.662219 , 5.3966146], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3983232140418713}
episode index:75
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.6460047 , 20.51439891]), 'previousTarget': array([ 6.85375892, 21.06591064]), 'currentState': array([ 6.0620346, 30.505741 ,  3.3702753], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2269190093581998
{'scaleFactor': 1.0, 'currentTarget': array([21.34006282, 18.56974111]), 'previousTarget': array([21.34006282, 18.56974111]), 'currentState': array([29.033142  , 24.958523  ,  0.30793202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:76
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.97774401, 19.49651365]), 'previousTarget': array([10.86197056, 17.89633523]), 'currentState': array([16.314861  , 28.50703   ,  0.97083706], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22397200923666474
{'scaleFactor': 1.0, 'currentTarget': array([4.37093806, 8.94037979]), 'previousTarget': array([4.37093806, 8.94037979]), 'currentState': array([ 2.7944512, 18.815332 ,  5.464892 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:77
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.20272434, 7.0566775 ]), 'previousTarget': array([5.2875295, 9.025413 ]), 'currentState': array([ 6.183659 , 17.00845  ,  4.8769836], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22110057322081006
{'scaleFactor': 1.0, 'currentTarget': array([14.98085653, 22.50995692]), 'previousTarget': array([14.98085653, 22.50995692]), 'currentState': array([19.932955 , 31.197691 ,  1.5321589], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:78
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.22823636,  2.12429644]), 'previousTarget': array([18.22823636,  2.12429644]), 'currentState': array([28.        ,  0.        ,  0.12925655], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21830183178763526
{'scaleFactor': 1.0, 'currentTarget': array([18.614763  ,  1.80005462]), 'previousTarget': array([18.614763  ,  1.80005462]), 'currentState': array([28.349497  , -0.48794824,  4.711248  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:79
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.13481672,  3.53475357]), 'previousTarget': array([10.3376506 ,  3.57662651]), 'currentState': array([21.930386  ,  1.52308   ,  0.22573915], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.22368684174489487
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.328432 , 5.6754704, 2.9316804], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4902992975081604}
episode index:80
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.66748887,  9.9855718 ]), 'previousTarget': array([12.95181298,  8.74202964]), 'currentState': array([22.335789, 14.97161 ,  1.217121], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22092527579742702
{'scaleFactor': 1.0, 'currentTarget': array([19.02621739, 10.20300849]), 'previousTarget': array([19.02621739, 10.20300849]), 'currentState': array([28.401937, 13.680921,  4.894772], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:81
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.55255882, 7.34944287]), 'previousTarget': array([5.45299804, 5.67949706]), 'currentState': array([12.065745 , 15.69239  ,  1.1253515], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2182310651169706
{'scaleFactor': 1.0, 'currentTarget': array([18.68163988,  4.59889795]), 'previousTarget': array([18.68163988,  4.59889795]), 'currentState': array([28.677345 ,  4.3058558,  2.7676232], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:82
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.28424463,  9.0973876 ]), 'previousTarget': array([12.28424463,  9.0973876 ]), 'currentState': array([21.       , 14.       ,  2.1115234], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21560177517580228
{'scaleFactor': 1.0, 'currentTarget': array([ 8.39004364, 20.19462979]), 'previousTarget': array([ 8.39004364, 20.19462979]), 'currentState': array([10.567586 , 29.954666 ,  1.4028311], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:83
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.79745222, 20.03184721]), 'previousTarget': array([ 3.79745222, 20.03184721]), 'currentState': array([ 3.       , 30.       ,  5.4775996], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21303508737609034
{'scaleFactor': 1.0, 'currentTarget': array([10.88724949, 16.34441113]), 'previousTarget': array([10.88724949, 16.34441113]), 'currentState': array([15.493481 , 25.22037  ,  4.2406006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:84
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([3.46752165, 9.20182901]), 'previousTarget': array([4.09529089, 9.22197586]), 'currentState': array([ 0.04112625, 18.596498  ,  3.8446789 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.2154856654323101
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.375969 , 5.030633 , 5.6008835], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.624782483322816}
episode index:85
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.91527403, 12.51341402]), 'previousTarget': array([ 3.06010481, 14.21450215]), 'currentState': array([ 0.2416138, 22.149364 ,  4.427903 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.21656774456256986
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.541629  , 4.3909187 , 0.81333816], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5804511202070277}
episode index:86
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.05494547,  4.04684785]), 'previousTarget': array([14.05494547,  4.04684785]), 'currentState': array([24.       ,  3.       ,  2.7320423], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.22266664513462764
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4435835, 5.04145  , 2.398673 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4441784490247722}
episode index:87
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.75902574, 22.48341683]), 'previousTarget': array([15.75902574, 22.48341683]), 'currentState': array([21.      , 31.      ,  0.689004], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22013634234900686
{'scaleFactor': 1.0, 'currentTarget': array([21.79813358, 20.32441686]), 'previousTarget': array([21.75325081, 21.91861735]), 'currentState': array([29.185837 , 27.06399  ,  5.2971244], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:88
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.0600758, 11.765373 ,  1.320117 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.765639957746972}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21766290030014163
{'scaleFactor': 1.0, 'currentTarget': array([ 2.23104182, 15.52353953]), 'previousTarget': array([ 2.23104182, 15.52353953]), 'currentState': array([-0.31355226, 25.194374  ,  0.5144898 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:89
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.01247661,  5.50062383]), 'previousTarget': array([15.01247661,  5.50062383]), 'currentState': array([25.       ,  6.       ,  0.6286532], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.2189225690559291
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.672181 , 3.2206726, 3.128683 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8092736614077751}
episode index:90
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2468505, 3.9625313, 1.841451 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.62202878852337}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.22750583752784195
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2468505, 3.9625313, 1.841451 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.62202878852337}
episode index:91
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.025641 ,  2.9998355,  1.6989088], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.304813188897354}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.23208841134132632
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.837756 , 4.5297923, 3.0178018], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9606927975770361}
episode index:92
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10050506,  3.41421356]), 'previousTarget': array([16.10050506,  3.41421356]), 'currentState': array([26.      ,  2.      ,  5.192355], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.23657243518032717
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.1459293, 3.787589 , 2.993517 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2211615891171133}
episode index:93
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.003455, 8.117515, 1.808593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.705769816342219}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2340557071464939
{'scaleFactor': 1.0, 'currentTarget': array([ 3.63501831, 10.24299595]), 'previousTarget': array([ 3.63501831, 10.24299595]), 'currentState': array([ 1.1155632, 19.92041  ,  3.6846995], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:94
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.178793  ,  2.6156874 ,  0.94478494], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.48341640967038}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23159196286074132
{'scaleFactor': 1.0, 'currentTarget': array([ 1.87605607, 18.02630374]), 'previousTarget': array([ 1.87605607, 18.02630374]), 'currentState': array([-0.45600128, 27.750578  ,  3.0830479 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:95
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.68661082, 18.62411165]), 'previousTarget': array([12.91270688, 18.39073472]), 'currentState': array([16.092499 , 27.601202 ,  2.9628549], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22917954658094195
{'scaleFactor': 1.0, 'currentTarget': array([4.174792  , 6.56656705]), 'previousTarget': array([4.174792  , 6.56656705]), 'currentState': array([-0.48576355, 15.414122  ,  4.5021853 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:96
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.51316702,  8.83772234]), 'previousTarget': array([16.51316702,  8.83772234]), 'currentState': array([26.       , 12.       ,  3.4167383], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22681687084299407
{'scaleFactor': 1.0, 'currentTarget': array([ 9.10324263, 21.04323533]), 'previousTarget': array([ 9.10324263, 21.04323533]), 'currentState': array([11.581099, 30.731384,  5.854927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:97
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.146845  ,  0.75220734,  2.9237864 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.471776513869406}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.23049258422150928
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.995983 , 5.409851 , 2.6211722], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0770145244570424}
episode index:98
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.       , 6.       , 1.0699311], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.23729954877491632
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2263975, 4.118143 , 3.50261  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5105371520100275}
episode index:99
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.23324944, 11.44464635]), 'previousTarget': array([ 2.67643864, 13.3648209 ]), 'currentState': array([ 0.58937514, 21.088812  ,  4.5132747 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.23813856403281505
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0767817, 6.1006823, 4.4608035], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.215912875678877}
episode index:100
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.45299804, 5.67949706]), 'currentState': array([ 9.840103 , 12.370693 ,  3.9232078], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.817806804730163}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.24519649953645054
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0281663, 6.4082785, 4.353842 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4230923216992046}
episode index:101
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.      , 2.      , 5.339107], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.2507311207980663
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.049711 , 3.1876616, 1.3990469], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0943886849943856}
episode index:102
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.19548901, 11.67206508]), 'previousTarget': array([11.19548901, 11.67206508]), 'currentState': array([18.      , 19.      ,  4.768803], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2482968380718715
{'scaleFactor': 1.0, 'currentTarget': array([10.27362419, 21.84139419]), 'previousTarget': array([10.27362419, 21.84139419]), 'currentState': array([13.261891, 31.384468,  2.059277], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:103
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.98296215, 17.50063195]), 'previousTarget': array([ 7.68379473, 16.27193785]), 'currentState': array([ 8.549663 , 27.377142 ,  1.5603149], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.2474072506797065
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3877087, 6.715814 , 5.1306086], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8217899858931905}
episode index:104
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.23886   , 11.69407375]), 'previousTarget': array([13.23886   , 11.69407375]), 'currentState': array([21.       , 18.       ,  2.6416469], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.252998741258472
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0835037, 3.6885   , 3.856089 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3141557411098852}
episode index:105
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.20254778, 20.03184721]), 'previousTarget': array([ 6.20254778, 20.03184721]), 'currentState': array([ 7.       , 30.       ,  4.1031914], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25061196068056185
{'scaleFactor': 1.0, 'currentTarget': array([19.63225985, 11.44097426]), 'previousTarget': array([19.63225985, 11.44097426]), 'currentState': array([28.78477  , 15.469817 ,  0.6493925], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:106
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.21367154, 17.74453044]), 'previousTarget': array([10.21367154, 17.74453044]), 'currentState': array([14.      , 27.      ,  2.062131], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24826979282373415
{'scaleFactor': 1.0, 'currentTarget': array([15.08675282, 21.00566928]), 'previousTarget': array([15.08675282, 21.00566928]), 'currentState': array([20.418333 , 29.465826 ,  3.2363236], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:107
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.     , 0.     , 5.35437], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.25198120796766627
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6190863, 4.6725235, 1.9976636], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6518720272313832}
episode index:108
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.       , 1.       , 2.5246835], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.25212859795802595
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5798516, 4.363183 , 5.2401743], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5563923876673558}
episode index:109
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([16.     ,  5.     ,  2.44412], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.25805817502212397
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.26721  , 5.6317034, 3.4264429], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4159344466953279}
episode index:110
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.52786405, 16.05572809]), 'previousTarget': array([10.52786405, 16.05572809]), 'currentState': array([15.      , 25.      ,  5.287403], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.2588379408628933
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.7116275, 5.9124756, 5.277741 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9396598507863132}
episode index:111
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.8304548 , 7.03454242]), 'previousTarget': array([4.8304548 , 7.03454242]), 'currentState': array([ 4.       , 17.       ,  3.6288166], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2565268878194746
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.0440083, 14.67467  ,  4.882019 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.721788110787946}
episode index:112
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.39859456, 11.15176255]), 'previousTarget': array([5.90470911, 9.22197586]), 'currentState': array([ 8.615509 , 20.902931 ,  2.2918317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2542567383697447
{'scaleFactor': 1.0, 'currentTarget': array([16.35890293, 11.7465396 ]), 'previousTarget': array([18.18598075, 12.28803339]), 'currentState': array([24.956722, 16.853153,  2.716393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:113
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.74988455, 10.62073882]), 'previousTarget': array([ 6.42337349, 10.3376506 ]), 'currentState': array([ 7.072306 , 20.532913 ,  2.7690094], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2520264161033434
{'scaleFactor': 1.0, 'currentTarget': array([ 9.30438839, 21.20843078]), 'previousTarget': array([ 9.30438839, 21.20843078]), 'currentState': array([11.871071, 30.873426,  3.000101], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:114
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.76353416, 19.19289271]), 'previousTarget': array([ 5.58369455, 19.00866927]), 'currentState': array([ 7.996599 , 29.11658  ,  5.9945207], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24983488205027085
{'scaleFactor': 1.0, 'currentTarget': array([ 3.92237692, 21.12576408]), 'previousTarget': array([ 3.92237692, 21.12576408]), 'currentState': array([ 3.2556024, 31.10351  ,  1.5798347], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:115
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.78885438, 17.1613009 ]), 'previousTarget': array([ 2.78885438, 17.1613009 ]), 'currentState': array([ 1.       , 27.       ,  3.3155751], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.2521219702862496
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4023738, 6.170785 , 5.5059657], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9806935412269477}
episode index:116
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.62389403, 14.53732219]), 'previousTarget': array([19.62389403, 14.53732219]), 'currentState': array([28.      , 20.      ,  6.054799], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2499670816513244
{'scaleFactor': 1.0, 'currentTarget': array([19.31299184,  5.22363766]), 'previousTarget': array([19.31299184,  5.22363766]), 'currentState': array([29.311771 ,  5.3798666,  0.2737528], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:117
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.27243207, 15.38418025]), 'previousTarget': array([ 6.93989519, 14.21450215]), 'currentState': array([ 7.4886913, 25.30994  ,  2.5994856], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24784871655258436
{'scaleFactor': 1.0, 'currentTarget': array([ 5.86418518, 19.58155708]), 'previousTarget': array([ 5.86418518, 19.58155708]), 'currentState': array([ 6.4558034, 29.564041 ,  3.2741659], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:118
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.29411765, 11.17647059]), 'previousTarget': array([ 8.29411765, 11.17647059]), 'currentState': array([13.       , 20.       ,  2.7487466], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.24768385725496045
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0630794, 3.4907334, 1.3050089], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5105842314124331}
episode index:119
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.84223948, 5.98143749]), 'previousTarget': array([10.02214842,  5.33480989]), 'currentState': array([19.642958,  7.967872,  1.272798], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.24934918522586755
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.840716 , 4.5944295, 2.0615761], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8848666812476325}
episode index:120
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 0.       , 3.2532933], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.2544681656187858
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7066898, 6.5014076, 5.5935216], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9816346876296327}
episode index:121
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.0973876 , 12.28424463]), 'previousTarget': array([ 9.0973876 , 12.28424463]), 'currentState': array([14.       , 21.       ,  1.6680427], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2523823609825663
{'scaleFactor': 1.0, 'currentTarget': array([ 9.35458409, 21.78092111]), 'previousTarget': array([ 9.35458409, 21.78092111]), 'currentState': array([11.866354 , 31.460333 ,  5.9181542], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:122
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.36001236, 4.34314481]), 'previousTarget': array([10.3376506 ,  3.57662651]), 'currentState': array([19.248425 ,  2.8534117,  1.8786756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2571836685630652
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.225964, 6.776244, 2.487845], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1582472582612513}
episode index:123
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.54493299, 14.32925463]), 'previousTarget': array([ 2.54493299, 14.32925463]), 'currentState': array([ 0.       , 24.       ,  3.8621378], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25510960671981464
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.7218957, 9.930463 , 0.4572165], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.631889555220717}
episode index:124
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.       , 6.       , 2.5853984], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2606005710612641
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.312187 , 6.8490357, 2.9647317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8752050586628866}
episode index:125
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47423305, 12.41495392]), 'previousTarget': array([13.47423305, 12.41495392]), 'currentState': array([21.       , 19.       ,  3.0709398], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2585323125607779
{'scaleFactor': 1.0, 'currentTarget': array([17.96054328, 23.24383644]), 'previousTarget': array([17.96054328, 23.24383644]), 'currentState': array([23.751966 , 31.396101 ,  1.6624974], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:126
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71800007, 14.14635301]), 'previousTarget': array([14.71800007, 14.14635301]), 'currentState': array([22.        , 21.        ,  0.27716807], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.2582757443952126
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.947922 , 3.5293024, 1.8875113], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7497165787707776}
episode index:127
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.01725007, 11.45520022]), 'previousTarget': array([ 7.01725007, 11.45520022]), 'currentState': array([10.       , 21.       ,  3.2132454], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.260202386148274
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.336124 , 5.697217 , 0.5343731], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9627268285619749}
episode index:128
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.1230702 ,  7.56120503]), 'previousTarget': array([12.26582832,  6.70960666]), 'currentState': array([21.533249 , 10.944774 ,  1.8735731], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25818531338743467
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.456873 , 1.877833 , 5.7616253], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.026837712663805}
episode index:129
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.1577735,  2.8196439]), 'previousTarget': array([19.13606076,  2.64398987]), 'currentState': array([27.00074  ,  1.0544217,  2.8633306], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2561992725152236
{'scaleFactor': 1.0, 'currentTarget': array([11.43920823, 22.09056835]), 'previousTarget': array([11.43920823, 22.09056835]), 'currentState': array([14.964957 , 31.448404 ,  1.5151134], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:130
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.47423305, 5.41495392]), 'previousTarget': array([5.47423305, 5.41495392]), 'currentState': array([13.       , 12.       ,  2.9403903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.260181120506704
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5870621, 6.925535 , 3.8797927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.388321432668759}
episode index:131
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.51659547, 15.11063647]), 'previousTarget': array([ 6.51659547, 15.11063647]), 'currentState': array([ 8.      , 25.      ,  5.114447], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25821005141195624
{'scaleFactor': 1.0, 'currentTarget': array([ 1.77212803, 20.3673925 ]), 'previousTarget': array([ 1.77212803, 20.3673925 ]), 'currentState': array([-0.28348318, 30.153835  ,  2.765378  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:132
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.24867144,  2.21621104]), 'previousTarget': array([17.24867144,  2.21621104]), 'currentState': array([27.       ,  0.       ,  5.6861725], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2562686224539716
{'scaleFactor': 1.0, 'currentTarget': array([6.98773782, 9.35818162]), 'previousTarget': array([6.98773782, 9.35818162]), 'currentState': array([11.137436, 18.456533,  4.596489], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:133
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.     , 7.     , 2.70895], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2615972073610315
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2969255, 5.9514117, 3.3640845], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9508067225894883}
episode index:134
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.632499 ,  6.4594316,  2.9325771], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.791169287190652}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.2637534019521526
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.3838797, 4.792627 , 2.9277704], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.4363109151545087}
episode index:135
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.10747283, 15.6349022 ]), 'previousTarget': array([ 8.25278872, 16.38476052]), 'currentState': array([12.710346, 24.963316,  5.680139], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2618140387025044
{'scaleFactor': 1.0, 'currentTarget': array([20.60880561, 23.17526974]), 'previousTarget': array([20.60880561, 23.17526974]), 'currentState': array([27.123936  , 30.761644  ,  0.26600024], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:136
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.98766829, 14.5697204 ]), 'previousTarget': array([10.56748722, 13.60429843]), 'currentState': array([14.609519 , 23.437555 ,  1.1411778], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.26276949118174014
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.479134, 5.164302, 4.31983 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5297150856758424}
episode index:137
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.0496281 , 4.99503719]), 'previousTarget': array([5.0496281 , 4.99503719]), 'currentState': array([15.      ,  4.      ,  4.094243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2672884432145981
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.7855444, 6.50595  , 3.3701682], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3358197945224473}
episode index:138
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.68598362, 9.02816633]), 'previousTarget': array([4.8304548 , 7.03454242]), 'currentState': array([ 3.9087899, 18.99792  ,  1.7254047], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.2703756718105331
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.74646  , 6.4178324, 3.5665267], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.440323294887099}
episode index:139
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.52786405, 12.05572809]), 'previousTarget': array([ 8.52786405, 12.05572809]), 'currentState': array([13.       , 21.       ,  3.3166215], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.2699336488814205
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.370456 , 5.98694  , 4.9315987], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1706304936760181}
episode index:140
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.72151454, 20.36510798]), 'previousTarget': array([20.24275371, 21.6284586 ]), 'currentState': array([25.382393 , 27.823841 ,  3.2363682], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2680192258397083
{'scaleFactor': 1.0, 'currentTarget': array([10.14806513, 20.83107768]), 'previousTarget': array([10.14806513, 20.83107768]), 'currentState': array([13.240537 , 30.340895 ,  1.9548409], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:141
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.1611777 , 17.50046064]), 'previousTarget': array([17.56705854, 16.31035268]), 'currentState': array([26.658163  , 24.118256  ,  0.69159365], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26613176650280895
{'scaleFactor': 1.0, 'currentTarget': array([21.46642058, 19.6516903 ]), 'previousTarget': array([21.35005839, 18.06821641]), 'currentState': array([28.937162 , 26.299097 ,  2.1061957], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:142
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 11.52786405]), 'previousTarget': array([18.05572809, 11.52786405]), 'currentState': array([27.       , 16.       ,  3.4997904], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26427070519859347
{'scaleFactor': 1.0, 'currentTarget': array([15.71944639, 16.34516428]), 'previousTarget': array([15.71944639, 16.34516428]), 'currentState': array([22.587229, 23.613834,  4.896393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:143
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.14322327, 10.54284764]), 'previousTarget': array([9.6284586 , 9.24275371]), 'currentState': array([18.567778 , 17.241802 ,  1.2869227], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2624354919680477
{'scaleFactor': 1.0, 'currentTarget': array([18.72202705,  8.8868298 ]), 'previousTarget': array([18.72202705,  8.8868298 ]), 'currentState': array([28.343493  , 11.612156  ,  0.20367971], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:144
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.      , 2.      , 6.129545], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2669257109716035
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8289456, 5.0887394, 2.0886943], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.833681923119211}
episode index:145
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.83201586, 12.88973915]), 'previousTarget': array([10.49208627, 11.40743398]), 'currentState': array([18.37817  , 20.44936  ,  1.1383791], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26509745267727747
{'scaleFactor': 1.0, 'currentTarget': array([ 4.60362841, 20.71438985]), 'previousTarget': array([ 4.60362841, 20.71438985]), 'currentState': array([ 4.351474 , 30.71121  ,  2.5926218], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:146
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.16227766, 7.51316702]), 'previousTarget': array([4.16227766, 7.51316702]), 'currentState': array([ 1.       , 17.       ,  2.3248553], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26329406864545923
{'scaleFactor': 1.0, 'currentTarget': array([ 4.19876336, 12.55922247]), 'previousTarget': array([ 4.19876336, 12.55922247]), 'currentState': array([ 3.144722 , 22.503517 ,  5.9050183], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:147
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.30923231, 23.34961605]), 'previousTarget': array([18.75304952, 22.19131191]), 'currentState': array([26.715492  , 31.028147  ,  0.75739825], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2615150546681251
{'scaleFactor': 1.0, 'currentTarget': array([18.98329496, 14.69849868]), 'previousTarget': array([18.98329496, 14.69849868]), 'currentState': array([27.20033  , 20.39765  ,  1.9493182], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:148
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.3655064 , 20.51534875]), 'previousTarget': array([10.35600651, 18.6875722 ]), 'currentState': array([13.633788 , 29.966187 ,  2.4941163], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.259759920073037
{'scaleFactor': 1.0, 'currentTarget': array([ 9.41443326, 21.64807966]), 'previousTarget': array([ 9.41443326, 21.64807966]), 'currentState': array([11.977476 , 31.314041 ,  1.0717363], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:149
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.48305299,  7.60089275]), 'previousTarget': array([15.56141644,  8.69649575]), 'currentState': array([25.188791, 10.00893 ,  4.775073], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2580281872725501
{'scaleFactor': 1.0, 'currentTarget': array([14.02378962, 20.89481147]), 'previousTarget': array([14.02378962, 20.89481147]), 'currentState': array([18.960842 , 29.591105 ,  2.6373856], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:150
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.86197056, 6.89633523]), 'previousTarget': array([5.86197056, 6.89633523]), 'currentState': array([10.       , 16.       ,  0.3913675], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.2594673386283988
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.967493 , 6.263006 , 4.9737105], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5909831875773504}
episode index:151
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.999262  ,  0.94566286,  0.42724293], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.088715361932014}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.25938754486040844
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.137473 , 5.506794 , 4.6348314], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5251085496510227}
episode index:152
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.06010481, 14.21450215]), 'previousTarget': array([ 3.06010481, 14.21450215]), 'currentState': array([ 1.      , 24.      ,  3.197557], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.2597088319569721
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.806169 , 5.9610634, 5.337644 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5326040609544003}
episode index:153
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.598156 , 9.3177872]), 'previousTarget': array([8.598156 , 9.3177872]), 'currentState': array([15.       , 17.       ,  5.8437777], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2580224109702385
{'scaleFactor': 1.0, 'currentTarget': array([19.7685796 , 14.94955313]), 'previousTarget': array([19.7685796 , 14.94955313]), 'currentState': array([28.062073 , 20.536858 ,  4.5309424], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:154
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.54098699, 16.90062686]), 'previousTarget': array([ 5.52434851, 16.01131862]), 'currentState': array([ 7.825145, 26.817831,  0.261634], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.25735273058845526
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.537691 , 6.9173074, 5.501087 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9722568497245039}
episode index:155
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.35236179, 5.36882594]), 'previousTarget': array([6.35236179, 5.36882594]), 'currentState': array([16.       ,  8.       ,  6.2268963], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2557030335975036
{'scaleFactor': 1.0, 'currentTarget': array([17.25030887,  2.02347841]), 'previousTarget': array([17.25030887,  2.02347841]), 'currentState': array([26.967583  , -0.33757842,  3.8932066 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:156
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.52740882,  3.36014191]), 'previousTarget': array([13.3648209 ,  2.67643864]), 'currentState': array([21.226028 ,  0.9235916,  2.2746944], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.2558163209696356
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.660368  , 5.68196   , 0.29183388], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7618526791694236}
episode index:157
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.      , 29.      ,  4.040405], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.25506242023537584
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8476458, 4.371951 , 3.484202 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.951471308951745}
episode index:158
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.98031432, 5.8564755 ]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.75053  , 15.853835 ,  2.0988722], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.25519568594269554
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2082338, 5.009737 , 5.5459385], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.791792623475415}
episode index:159
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.90535746, 17.04106794]), 'previousTarget': array([ 3.90535746, 17.04106794]), 'currentState': array([ 3.        , 27.        ,  0.57534814], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.2581776089658937
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.234215 , 6.1903524, 4.85161  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2131757892289616}
episode index:160
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 18.92893219]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([26.       , 26.       ,  1.2748251], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2565740213325651
{'scaleFactor': 1.0, 'currentTarget': array([ 5.88395055, 21.2755861 ]), 'previousTarget': array([ 5.88395055, 21.2755861 ]), 'currentState': array([ 6.4262657, 31.26087  ,  6.1675377], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:161
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.08714999, 15.88552499]), 'previousTarget': array([19.08714999, 15.88552499]), 'currentState': array([27.      , 22.      ,  3.172554], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25499023107742586
{'scaleFactor': 1.0, 'currentTarget': array([17.96905883, 14.87733631]), 'previousTarget': array([17.96905883, 14.87733631]), 'currentState': array([25.924515 , 20.936274 ,  1.2303934], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:162
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.60206577,  9.58256937]), 'previousTarget': array([17.60206577,  9.58256937]), 'currentState': array([27.      , 13.      ,  2.295965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2534258738315521
{'scaleFactor': 1.0, 'currentTarget': array([20.28624088, 14.3086582 ]), 'previousTarget': array([20.28624088, 14.3086582 ]), 'currentState': array([28.827236, 19.509754,  6.045061], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:163
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.42009499, 20.12559368]), 'previousTarget': array([ 7.42009499, 20.12559368]), 'currentState': array([ 9.       , 30.       ,  1.4439558], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.251880594113067
{'scaleFactor': 1.0, 'currentTarget': array([ 2.64792863, 12.15827437]), 'previousTarget': array([ 2.64792863, 12.15827437]), 'currentState': array([-0.47368443, 21.658566  ,  1.3682611 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:164
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.96562  , 7.6307597, 5.806735 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.802377338418619}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.25449073957305074
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.742151  , 5.6960974 , 0.54550785], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4376145539052434}
episode index:165
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.17261291, 19.30749943]), 'previousTarget': array([ 6.85375892, 21.06591064]), 'currentState': array([ 6.9894533, 29.274082 ,  4.4069138], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.25773847176624715
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.518666 , 6.916083 , 4.6645675], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4219256504716804}
episode index:166
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.00866927,  5.58369455]), 'previousTarget': array([19.00866927,  5.58369455]), 'currentState': array([29.      ,  6.      ,  4.676574], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25619512762393426
{'scaleFactor': 1.0, 'currentTarget': array([19.82424698, 12.10583545]), 'previousTarget': array([20.45794725, 13.55308793]), 'currentState': array([28.841803, 16.428299,  4.059411], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:167
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.18761806, 10.86266529]), 'previousTarget': array([ 9.18761806, 10.86266529]), 'currentState': array([15.        , 19.        ,  0.54371756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25467015662617276
{'scaleFactor': 1.0, 'currentTarget': array([19.64142033, 11.59988648]), 'previousTarget': array([18.67672795, 12.44767795]), 'currentState': array([28.758013 , 15.709356 ,  4.1597543], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:168
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.68533801, 10.33720955]), 'previousTarget': array([17.14168465, 11.35993005]), 'currentState': array([26.902727 , 14.21532  ,  5.6179028], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2531632326224676
{'scaleFactor': 1.0, 'currentTarget': array([19.46788214,  2.99126471]), 'previousTarget': array([19.46788214,  2.99126471]), 'currentState': array([29.37287  ,  1.6160463,  1.9494579], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:169
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.35601013, 7.13606076]), 'previousTarget': array([5.35601013, 7.13606076]), 'currentState': array([ 7.      , 17.      ,  6.168674], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.2527394898019
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.810742 , 3.4853332, 2.3930502], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3607205966751237}
episode index:170
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.44310917, 11.45069462]), 'previousTarget': array([12.44310917, 11.45069462]), 'currentState': array([20.        , 18.        ,  0.15741068], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.25505740289293216
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.297468 , 4.8401403, 4.4136825], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.33770168990994726}
episode index:171
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.09031785, 13.85159131]), 'previousTarget': array([16.61556384, 12.55011649]), 'currentState': array([26.374216  , 19.453112  ,  0.56925756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25357451101564765
{'scaleFactor': 1.0, 'currentTarget': array([17.78881922, 17.33452879]), 'previousTarget': array([19.4893056 , 17.81512182]), 'currentState': array([24.986572, 24.2766  ,  3.041358], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:172
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.79745222, 20.03184721]), 'previousTarget': array([ 3.79745222, 20.03184721]), 'currentState': array([ 3.       , 30.       ,  4.3649573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.253145269238648
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3754652, 4.9643774, 5.3784084], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6249253623357476}
episode index:173
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.31035268, 7.56705854]), 'previousTarget': array([7.31035268, 7.56705854]), 'currentState': array([14.       , 15.       ,  5.8250203], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25169041136946035
{'scaleFactor': 1.0, 'currentTarget': array([8.9549952 , 4.40092694]), 'previousTarget': array([8.9549952 , 4.40092694]), 'currentState': array([18.842213 ,  2.9032853,  1.1647613], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:174
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.40284012, 19.67574305]), 'previousTarget': array([11.01273889, 18.82929944]), 'currentState': array([16.906569  , 28.604149  ,  0.11440717], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25025218044734915
{'scaleFactor': 1.0, 'currentTarget': array([4.61331431, 5.71143512]), 'previousTarget': array([4.61331431, 5.71143512]), 'currentState': array([-0.16216373, 14.497493  ,  2.2860463 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:175
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.57662651, 10.3376506 ]), 'previousTarget': array([ 3.57662651, 10.3376506 ]), 'currentState': array([ 1.       , 20.       ,  1.8216596], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24883029305844373
{'scaleFactor': 1.0, 'currentTarget': array([ 3.03422131, 12.31794813]), 'previousTarget': array([ 3.03422131, 12.31794813]), 'currentState': array([ 0.4399482, 21.975574 ,  2.3123996], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:176
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.65196555, 10.21719897]), 'previousTarget': array([10.65196555, 10.21719897]), 'currentState': array([18.       , 17.       ,  4.2361717], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24742447219370678
{'scaleFactor': 1.0, 'currentTarget': array([ 7.54329577, 21.42304775]), 'previousTarget': array([ 7.54329577, 21.42304775]), 'currentState': array([ 9.073668 , 31.305252 ,  0.3394732], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:177
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.1592281, 8.5640157]), 'previousTarget': array([7.6676221 , 8.26042701]), 'currentState': array([12.340874 , 17.116825 ,  2.6665998], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.24765008532389304
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1317935, 3.449775 , 2.945182 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9194150502970315}
episode index:178
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.49208627, 18.40743398]), 'previousTarget': array([16.49208627, 18.40743398]), 'currentState': array([23.     , 26.     ,  3.39243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2462665652941506
{'scaleFactor': 1.0, 'currentTarget': array([12.45398494,  4.2391928 ]), 'previousTarget': array([10.51175684,  4.56511225]), 'currentState': array([22.4023   ,  3.2237964,  5.8716083], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:179
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.11063647,  3.48340453]), 'previousTarget': array([15.11063647,  3.48340453]), 'currentState': array([25.      ,  2.      ,  4.027681], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.2488064604591807
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2478285, 5.6431885, 3.0323446], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6892825064203476}
episode index:180
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.93989519, 14.21450215]), 'previousTarget': array([ 6.93989519, 14.21450215]), 'currentState': array([ 9.       , 24.       ,  5.6141706], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.24841262657133312
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.297653 , 5.8273873, 3.719641 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5389846073234323}
episode index:181
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.52750637, 22.63030386]), 'previousTarget': array([16.52750637, 22.63030386]), 'currentState': array([22.       , 31.       ,  1.6220632], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24704772202973238
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.44956994, 10.288422  ,  5.065493  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.976662316002732}
episode index:182
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9500047 ,  4.47274884]), 'previousTarget': array([9.10050506, 4.41421356]), 'currentState': array([20.910973  ,  3.590072  ,  0.23927861], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2484845418342123
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5888634, 4.6121807, 1.9850022], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6355092843683785}
episode index:183
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.73772657, 14.36663175]), 'previousTarget': array([18.73772657, 14.36663175]), 'currentState': array([27.       , 20.       ,  2.1961322], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24713408236772202
{'scaleFactor': 1.0, 'currentTarget': array([18.093244  , 14.50352718]), 'previousTarget': array([18.093244  , 14.50352718]), 'currentState': array([26.186138 , 20.377628 ,  2.3449612], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:184
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.57464375, 7.298575  ]), 'previousTarget': array([5.57464375, 7.298575  ]), 'currentState': array([ 8.       , 17.       ,  3.3320367], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.2483419340391703
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4775314, 5.3259077, 2.6492276], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5130482376436047}
episode index:185
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.84513404,  6.78075347]), 'previousTarget': array([17.1613009 ,  7.21114562]), 'currentState': array([28.763432 ,  8.056439 ,  5.3389072], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24700676235078764
{'scaleFactor': 1.0, 'currentTarget': array([16.69672177,  4.13612979]), 'previousTarget': array([16.69672177,  4.13612979]), 'currentState': array([26.66956  ,  3.3995783,  5.902031 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:186
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.025413 , 4.7124705]), 'previousTarget': array([9.025413 , 4.7124705]), 'currentState': array([19.       ,  4.       ,  0.4470563], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2456858705735107
{'scaleFactor': 1.0, 'currentTarget': array([17.07018132, 13.34490065]), 'previousTarget': array([17.07018132, 13.34490065]), 'currentState': array([25.29573  , 19.031757 ,  3.9506588], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:187
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.55202174, 16.82161431]), 'previousTarget': array([ 7.57464375, 15.298575  ]), 'currentState': array([11.429616 , 26.398642 ,  1.1836219], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24437903083641757
{'scaleFactor': 1.0, 'currentTarget': array([ 9.38326128, 20.46521223]), 'previousTarget': array([10.71653792, 20.94105615]), 'currentState': array([12.110123 , 30.086243 ,  2.7796469], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:188
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.     , 13.     ,  3.20332], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.2464184202563478
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.977096  , 5.8339295 , 0.14821787], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.834244008115575}
episode index:189
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.      , 0.      , 5.698682], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.2482423572360591
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4583418, 3.799265 , 1.7348343], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9540917719785718}
episode index:190
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.55756141, 7.60627844]), 'previousTarget': array([8.05572809, 6.52786405]), 'currentState': array([18.238386 , 12.570478 ,  1.1443357], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24694265903063473
{'scaleFactor': 1.0, 'currentTarget': array([19.76775676, 12.52513355]), 'previousTarget': array([19.76775676, 12.52513355]), 'currentState': array([28.67768 , 17.06532 ,  5.429328], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:191
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.95823859, 16.05608939]), 'previousTarget': array([19.95823859, 16.05608939]), 'currentState': array([28.      , 22.      ,  4.767288], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.2466287455810778
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.611938 , 3.279875 , 3.4911134], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8257321800680337}
episode index:192
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.75902574, 9.48341683]), 'previousTarget': array([7.75902574, 9.48341683]), 'currentState': array([13.       , 18.       ,  5.6547556], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.24626148421273636
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4201028, 6.673581 , 5.428801 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3015101193256626}
episode index:193
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.78549991, 14.46718166]), 'previousTarget': array([10.78549991, 14.46718166]), 'currentState': array([16.       , 23.       ,  1.5011967], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24499209511885625
{'scaleFactor': 1.0, 'currentTarget': array([ 1.92922009, 18.47942413]), 'previousTarget': array([ 1.92922009, 18.47942413]), 'currentState': array([-0.2919941, 28.229614 ,  2.048311 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:194
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.73954082, 11.58448351]), 'previousTarget': array([11.69407375, 13.23886   ]), 'currentState': array([17.310387, 19.122652,  4.645695], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24373572540029803
{'scaleFactor': 1.0, 'currentTarget': array([18.94030567,  6.61070063]), 'previousTarget': array([18.94030567,  6.61070063]), 'currentState': array([28.874216 ,  7.7584915,  0.6884903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:195
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.96045746, 19.2101958 ]), 'previousTarget': array([ 7.96045746, 19.2101958 ]), 'currentState': array([10.       , 29.       ,  2.6088274], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24249217578090876
{'scaleFactor': 1.0, 'currentTarget': array([7.02851995, 4.1428335 ]), 'previousTarget': array([8.86339184, 3.555674  ]), 'currentState': array([16.239908 ,  0.2504915,  2.8681734], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:196
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 7.       , 5.8141975], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.24623637793430517
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.366519 , 4.8730044, 5.222603 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6460851979941071}
episode index:197
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([27.       ,  5.       ,  6.1834006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.24707836426359597
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.330497 , 5.2055836, 1.6444622], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3462861167598321}
episode index:198
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 6.       , 3.2066991], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.24758597607379118
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9828029, 3.7979589, 1.7350477], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5746723245800698}
episode index:199
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.95644621, 14.18541579]), 'previousTarget': array([10.56748722, 13.60429843]), 'currentState': array([17.993809, 22.157257,  6.221228], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24634804619342224
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.22333908, 13.223685  ,  4.3404536 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.742292876120834}
episode index:200
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  1.       ,  4.6605678], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.24667298666733128
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6174293, 6.5110617, 3.1886897], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0481233426348937}
episode index:201
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.034742,  8.000301,  4.642224], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.739430506558963}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.24818789008562364
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.335212 , 3.1699376, 2.3621767], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9470673145104083}
episode index:202
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.79745222, 20.03184721]), 'previousTarget': array([ 3.79745222, 20.03184721]), 'currentState': array([ 3.       , 30.       ,  5.1836634], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24696528964185208
{'scaleFactor': 1.0, 'currentTarget': array([17.62228675, 10.19070076]), 'previousTarget': array([17.62228675, 10.19070076]), 'currentState': array([26.870798 , 13.993994 ,  4.9089007], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:203
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.       , 5.       , 6.0069785], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2503236232607988
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8832443 , 4.326657  , 0.37666577], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3040453930761626}
episode index:204
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.10127807, 15.14718931]), 'previousTarget': array([18.56338512, 13.63124508]), 'currentState': array([27.218185 , 20.988064 ,  1.4192169], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2491025324156242
{'scaleFactor': 1.0, 'currentTarget': array([17.82364483, 12.71631558]), 'previousTarget': array([18.86417917, 14.17994545]), 'currentState': array([26.392044 , 17.87214  ,  4.2447453], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:205
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.      , 0.      , 4.762317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.2505228360250962
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3640735, 3.8814511, 2.6531682], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9817686644253705}
episode index:206
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.80583107, 14.87813917]), 'previousTarget': array([16.54758686, 15.33205141]), 'currentState': array([25.723846, 20.985924,  5.383823], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.25056903228283833
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.645825 , 6.5359545, 2.7393637], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.251198743400229}
episode index:207
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.25099062,  8.71797602]), 'previousTarget': array([15.298575  ,  7.57464375]), 'currentState': array([25.745983 , 11.855669 ,  1.7696557], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2493643734737862
{'scaleFactor': 1.0, 'currentTarget': array([17.33366241,  8.38006425]), 'previousTarget': array([17.33366241,  8.38006425]), 'currentState': array([26.97805 , 11.023128,  4.204328], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:208
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.30790021, 22.77807808]), 'previousTarget': array([17.30790021, 22.77807808]), 'currentState': array([23.       , 31.       ,  2.8868604], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24817124250022743
{'scaleFactor': 1.0, 'currentTarget': array([21.71061718, 23.71516701]), 'previousTarget': array([21.71061718, 23.71516701]), 'currentState': array([28.370918, 31.174416,  1.390641], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:209
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.       , 12.       ,  2.2618563], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.2511681462168881
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3346105, 5.7664065, 4.7942543], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8332761088367417}
episode index:210
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.80941823, 8.92040615]), 'previousTarget': array([6.80941823, 8.92040615]), 'currentState': array([11.       , 18.       ,  3.1061568], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24997777585567063
{'scaleFactor': 1.0, 'currentTarget': array([ 2.29810809, 21.07020681]), 'previousTarget': array([ 2.29810809, 21.07020681]), 'currentState': array([ 0.6400743, 30.931795 ,  1.3742986], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:211
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.57728864, 5.52342877]), 'previousTarget': array([5., 5.]), 'currentState': array([12.985492  , 12.240461  ,  0.40513206], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24879863540352123
{'scaleFactor': 1.0, 'currentTarget': array([19.13746363,  5.19399495]), 'previousTarget': array([19.13746363,  5.19399495]), 'currentState': array([29.136522 ,  5.3312025,  0.7229374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:212
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.40797505,  7.18744535]), 'previousTarget': array([18.22823636,  7.87570356]), 'currentState': array([26.22906  ,  9.070609 ,  3.9486482], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24763056669270658
{'scaleFactor': 1.0, 'currentTarget': array([17.18487369,  2.39468309]), 'previousTarget': array([17.18487369,  2.39468309]), 'currentState': array([26.963839  ,  0.30378717,  3.781825  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:213
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  2.       ,  4.4793453], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.2502954095474006
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8134556, 3.094691 , 2.6099653], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0716930813992724}
episode index:214
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.598022 , 8.573656 , 3.6476402], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.07117179978377}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2536898495029941
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.792705 , 5.905709 , 4.1767693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.203615271842679}
episode index:215
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.81884617, 17.32197457]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([25.028255 , 24.25194  ,  3.9266586], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2525153594589988
{'scaleFactor': 1.0, 'currentTarget': array([ 9.46924643, 19.48615669]), 'previousTarget': array([ 9.46924643, 19.48615669]), 'currentState': array([12.4173155, 29.041725 ,  3.4884388], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:216
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.48876558, 11.63670822]), 'previousTarget': array([ 7.48876558, 11.63670822]), 'currentState': array([11.      , 21.      ,  2.966535], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25135169420803566
{'scaleFactor': 1.0, 'currentTarget': array([20.85795067, 21.0165683 ]), 'previousTarget': array([20.85795067, 21.0165683 ]), 'currentState': array([27.893744  , 28.122736  ,  0.16095191], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:217
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.19548901, 11.67206508]), 'previousTarget': array([11.19548901, 11.67206508]), 'currentState': array([18.      , 19.      ,  2.836594], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.25231440188331117
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.5635214, 3.4237144, 6.023259 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6356007691484626}
episode index:218
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.59742305, 19.91662478]), 'previousTarget': array([14.85504245, 21.42507074]), 'currentState': array([18.59102  , 28.580572 ,  3.9362314], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2511622813267664
{'scaleFactor': 1.0, 'currentTarget': array([12.0818392 , 17.00513895]), 'previousTarget': array([12.0818392 , 17.00513895]), 'currentState': array([17.162693 , 25.618206 ,  2.6084795], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:219
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.06663153, 6.95081754]), 'previousTarget': array([5.28609324, 5.71523309]), 'currentState': array([10.863987 , 15.724949 ,  0.5154964], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2500206345934629
{'scaleFactor': 1.0, 'currentTarget': array([ 2.37145607, 18.67234506]), 'previousTarget': array([ 2.37145607, 18.67234506]), 'currentState': array([ 0.48350382, 28.49251   ,  3.7025607 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:220
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.40634498,  7.17833676]), 'previousTarget': array([12.40634498,  7.17833676]), 'currentState': array([22.       , 10.       ,  6.2697325], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24888931950480472
{'scaleFactor': 1.0, 'currentTarget': array([17.93042221, 15.99932532]), 'previousTarget': array([17.93042221, 15.99932532]), 'currentState': array([25.547354, 22.478704,  5.596886], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:221
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([19.       ,  5.       ,  2.5587528], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.24882773080992007
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.569801  , 5.3381104 , 0.50007737], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5471654044027656}
episode index:222
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.52786405, 6.05572809]), 'previousTarget': array([5.52786405, 6.05572809]), 'currentState': array([10.       , 15.       ,  3.6208527], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.2502919775326421
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.68104  , 3.2604976, 6.1205287], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1830080295874636}
episode index:223
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.1266394 , 12.30153096]), 'previousTarget': array([18.82929944, 11.01273889]), 'currentState': array([29.132385 , 16.648546 ,  0.8813283], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2491746026329428
{'scaleFactor': 1.0, 'currentTarget': array([20.56160261, 14.7453971 ]), 'previousTarget': array([20.56160261, 14.7453971 ]), 'currentState': array([29.036835, 20.05298 ,  4.193824], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:224
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([ 5.       , 18.       ,  5.3997335], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.2508383839318684
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.003279 , 6.0799384, 4.7023916], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.469598349220314}
episode index:225
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.76923077, 3.84615385]), 'previousTarget': array([7.76923077, 3.84615385]), 'currentState': array([17.       ,  0.       ,  6.2073865], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.25098824848647605
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.2612343 , 3.4980032 , 0.36918265], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.673848570745433}
episode index:226
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.89633523, 5.86197056]), 'previousTarget': array([6.89633523, 5.86197056]), 'currentState': array([16.      , 10.      ,  4.451297], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.25172011029901675
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0580666, 3.903398 , 5.322768 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2301661754355298}
episode index:227
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.44004672, 16.65512265]), 'previousTarget': array([ 9.44004672, 16.65512265]), 'currentState': array([13.       , 26.       ,  3.0010805], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25061607472752984
{'scaleFactor': 1.0, 'currentTarget': array([ 4.27951935, 13.96356553]), 'previousTarget': array([ 4.27951935, 13.96356553]), 'currentState': array([ 3.4783154, 23.931417 ,  2.5139484], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:228
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.03759412,  3.86629616]), 'previousTarget': array([18.03759412,  3.86629616]), 'currentState': array([28.        ,  3.        ,  0.12330168], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.2515766448884823
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.153958 , 3.8321404, 1.5460746], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1779639027617892}
episode index:229
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([25.      ,  5.      ,  4.383357], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25048283338896715
{'scaleFactor': 1.0, 'currentTarget': array([ 7.21151991, 17.39152545]), 'previousTarget': array([ 7.21151991, 17.39152545]), 'currentState': array([ 8.968462  , 27.235973  ,  0.11168273], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:230
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.01539647,  5.4452998 ]), 'previousTarget': array([13.01539647,  5.4452998 ]), 'currentState': array([23.       ,  6.       ,  2.0371583], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.25116827642374456
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.80503  , 4.8076525, 0.9773532], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8276899546812908}
episode index:231
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.26047557, 21.75207573]), 'previousTarget': array([18.598156 , 21.3177872]), 'currentState': array([26.994766, 29.144594,  0.746091], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2500856545426077
{'scaleFactor': 1.0, 'currentTarget': array([17.38404726, 20.47683895]), 'previousTarget': array([17.38404726, 20.47683895]), 'currentState': array([23.63179  , 28.284893 ,  1.1140965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:232
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.49345201, 12.00891423]), 'previousTarget': array([18.97865778, 11.68544503]), 'currentState': array([26.214766 , 16.901632 ,  2.3246834], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24901232555315447
{'scaleFactor': 1.0, 'currentTarget': array([ 8.20686829, 16.92373761]), 'previousTarget': array([ 8.20686829, 16.92373761]), 'currentState': array([10.804059 , 26.58058  ,  1.6384099], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:233
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 7.       , 2.2374225], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.251206045163385
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1740193 , 6.6057696 , 0.08348548], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8057519692399528}
episode index:234
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.68379473, 16.27193785]), 'previousTarget': array([ 7.68379473, 16.27193785]), 'currentState': array([10.       , 26.       ,  3.0572107], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.25078678425748085
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1748896, 5.348625 , 5.780938 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8581086136215594}
episode index:235
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.58578644, 16.10050506]), 'previousTarget': array([ 6.58578644, 16.10050506]), 'currentState': array([ 8.       , 26.       ,  2.6047568], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.24972412839198305
{'scaleFactor': 1.0, 'currentTarget': array([12.12169991, 11.30588234]), 'previousTarget': array([12.12169991, 11.30588234]), 'currentState': array([19.608578 , 17.93511  ,  5.8506083], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:236
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.      , 6.      , 4.723062], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.25191956728402176
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.470029  , 3.9889445 , 0.05151141], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.141535173953097}
episode index:237
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.6284586 , 9.24275371]), 'previousTarget': array([9.6284586 , 9.24275371]), 'currentState': array([17.       , 16.       ,  6.2655687], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.252478292122717
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.878698 , 5.408539 , 3.7167273], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.193407681105807}
episode index:238
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.0448231 , 15.04730788]), 'previousTarget': array([ 3.90535746, 17.04106794]), 'currentState': array([ 3.0984108, 25.002422 ,  3.86384  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2514218975950069
{'scaleFactor': 1.0, 'currentTarget': array([5.62430363, 5.88570017]), 'previousTarget': array([5.36523875, 5.79101782]), 'currentState': array([11.3856125, 14.059275 ,  0.6523501], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:239
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.      ,  0.      ,  6.223038], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2526541256983225
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6781387, 6.5045066, 3.2036347], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2538166917864193}
episode index:240
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([4.87347886, 5.42173715]), 'currentState': array([ 2.3647504, 13.033542 ,  4.4541903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.45472249374464}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2554732594004332
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2500873, 5.45962  , 5.4793463], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8092664637451374}
episode index:241
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.86266529, 14.18761806]), 'previousTarget': array([17.86266529, 14.18761806]), 'currentState': array([26.       , 20.       ,  0.5024475], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.2552205955139967
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8142278, 3.0315251, 1.8860722], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2980315398535485}
episode index:242
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.007063, 9.167933, 3.342862], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.781714498431625}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.2548376707999926
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.489541  , 3.7474608 , 0.15361327], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.962228470446685}
episode index:243
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.298575  ,  8.57464375]), 'previousTarget': array([19.298575  ,  8.57464375]), 'currentState': array([29.        , 11.        ,  0.12338221], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.254673887098788
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2995205, 3.6154687, 3.3475595], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4165589765675195}
episode index:244
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.43073251, 16.89332954]), 'previousTarget': array([ 4.47565149, 16.01131862]), 'currentState': array([ 5.7926583 , 26.886778  ,  0.15891382], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25363440184532354
{'scaleFactor': 1.0, 'currentTarget': array([16.63406479, 22.91396706]), 'previousTarget': array([16.63406479, 22.91396706]), 'currentState': array([22.080652 , 31.300545 ,  1.3097107], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:245
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.19419324, 5.03883865]), 'previousTarget': array([5.19419324, 5.03883865]), 'currentState': array([15.      ,  7.      ,  1.169946], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.2538450710489129
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.1175   , 6.801303 , 3.7018142], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8051311261782488}
episode index:246
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.455671 , 1.9473994, 2.0354147], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.38191486206763}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.25455779956095737
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.062791 , 4.5680404, 1.0904582], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1472199248220518}
episode index:247
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.52786405, 12.05572809]), 'previousTarget': array([ 8.52786405, 12.05572809]), 'currentState': array([13.       , 21.       ,  1.9974959], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25353135682079225
{'scaleFactor': 1.0, 'currentTarget': array([16.67260658,  4.86971363]), 'previousTarget': array([16.67260658,  4.86971363]), 'currentState': array([26.671984 ,  4.7581034,  6.0236497], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:248
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.      ,  1.      ,  5.807166], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.2546239878677994
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3278394, 3.3872635, 2.4875462], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3231530423509743}
episode index:249
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.93261538, 13.60361488]), 'previousTarget': array([18.91263916, 15.11828302]), 'currentState': array([26.258501, 19.142534,  4.606796], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2536054919163282
{'scaleFactor': 1.0, 'currentTarget': array([15.6009014, 20.3616564]), 'previousTarget': array([15.6009014, 20.3616564]), 'currentState': array([21.280647, 28.592117,  3.21235 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:250
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.53669807, 16.02071425]), 'previousTarget': array([20.29038248, 17.63118553]), 'currentState': array([27.505491 , 22.0621   ,  3.7111568], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2525951114704464
{'scaleFactor': 1.0, 'currentTarget': array([17.82164403,  2.03734413]), 'previousTarget': array([17.82164403,  2.03734413]), 'currentState': array([27.56492   , -0.21400326,  2.9436755 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:251
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.08838384, 18.4332612 ]), 'previousTarget': array([ 9.08838384, 18.4332612 ]), 'currentState': array([12.      , 28.      ,  2.424562], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.25226944607131596
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0973957, 6.0666633, 5.5878844], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1812092521600404}
episode index:252
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 4.       , 3.7051046], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.2543159824339003
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5277886, 4.2618036, 0.5413219], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6469183878823273}
episode index:253
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.10360009, 21.79032749]), 'previousTarget': array([12.10360009, 21.79032749]), 'currentState': array([16.       , 31.       ,  1.0841292], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25331473840857
{'scaleFactor': 1.0, 'currentTarget': array([ 4.74286252, 17.59221415]), 'previousTarget': array([ 4.74286252, 17.59221415]), 'currentState': array([ 4.5387015 , 27.59013   ,  0.06072745], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:254
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.52001696, 14.7000106 ]), 'previousTarget': array([20.52001696, 14.7000106 ]), 'currentState': array([29.       , 20.       ,  2.6712606], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.252321347277556
{'scaleFactor': 1.0, 'currentTarget': array([7.62981375, 5.09337599]), 'previousTarget': array([6.82045857, 4.79041901]), 'currentState': array([17.623516 ,  5.4482193,  1.165413 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:255
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.25462229, 19.91756476]), 'previousTarget': array([20.45394478, 18.43821285]), 'currentState': array([27.404236 , 26.909204 ,  1.4836599], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25133571701475305
{'scaleFactor': 1.0, 'currentTarget': array([21.96820138, 23.24390957]), 'previousTarget': array([21.96820138, 23.24390957]), 'currentState': array([28.778622, 30.566353,  3.142006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:256
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.04111171, 14.57762745]), 'previousTarget': array([11.1808125 , 16.23784091]), 'currentState': array([16.376043 , 23.035671 ,  5.2455597], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.25363769163097555
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9651837, 3.8976307, 3.1991646], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.253256577523179}
episode index:257
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.08859747, 4.49537632]), 'previousTarget': array([5.19419324, 4.96116135]), 'currentState': array([16.80891  ,  2.1468625,  5.948717 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.25477536198275763
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.883528 , 4.4494176, 1.9119251], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.962350527276089}
episode index:258
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.074646  ,  0.45869398,  3.4498794 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.40678751525819}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.2555195621826845
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8473206, 6.1154675, 4.0374355], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.157976156136674}
episode index:259
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.756833 ,  7.0148373,  4.623315 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.099237351908182}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.25580999882167804
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9410882, 6.5905337, 0.3510462], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.910783022212329}
episode index:260
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.      , 1.      , 4.141747], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.25676431257633475
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3887095, 5.350537 , 5.550891 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7046645192420073}
episode index:261
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.46725218, 19.42586681]), 'previousTarget': array([11.97261386, 20.84684968]), 'currentState': array([17.0642   , 28.306637 ,  4.6786017], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.256838691794361
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.501532 , 4.0201406, 5.8822494], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.099361005036123}
episode index:262
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.598156 , 9.3177872]), 'previousTarget': array([8.598156 , 9.3177872]), 'currentState': array([15.      , 17.      ,  5.511942], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.2574481297720753
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2628546, 5.899951 , 3.8028975], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5507138513791587}
episode index:263
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.       , 10.       ,  2.9334939], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25647294746233257
{'scaleFactor': 1.0, 'currentTarget': array([9.34446276, 3.6765442 ]), 'previousTarget': array([9.34446276, 3.6765442 ]), 'currentState': array([18.910448  ,  0.76245344,  4.458705  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:264
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.45879805, 12.46917437]), 'previousTarget': array([10.08636336, 11.93595004]), 'currentState': array([17.999722, 20.033321,  5.218864], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.2566013080164746
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.662742  , 3.7269669 , 0.05515211], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.316949599036112}
episode index:265
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.79297056, 21.15954176]), 'previousTarget': array([17.79297056, 21.15954176]), 'currentState': array([24.       , 29.       ,  2.9026365], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.25685636148206464
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.827285, 3.698676, 3.634947], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.243304182678934}
episode index:266
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.71120845, 12.84358325]), 'previousTarget': array([ 6.70960666, 12.26582832]), 'currentState': array([10.978141 , 22.294888 ,  6.0796742], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25589435263756255
{'scaleFactor': 1.0, 'currentTarget': array([16.6245453 , 23.00400755]), 'previousTarget': array([16.6245453 , 23.00400755]), 'currentState': array([22.048798 , 31.405048 ,  1.8386326], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:267
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.905344 , 6.3920007, 6.2355833], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.2215973821559793}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.25808483338661614
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.238702 , 3.054882 , 2.5436597], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3060498782373484}
episode index:268
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.14788245,  2.71341175]), 'previousTarget': array([18.14788245,  2.71341175]), 'currentState': array([28.       ,  1.       ,  4.3068523], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.25878906528392914
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2412674, 3.4116035, 6.1553755], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3698404599645517}
episode index:269
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.94898643, 15.35558296]), 'previousTarget': array([ 4.45407661, 17.0103146 ]), 'currentState': array([ 2.939249 , 25.304474 ,  4.0638742], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.2585431254602626
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.950826 , 5.4941278, 5.6261125], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.49656852443521343}
episode index:270
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.70511198, 5.23798736]), 'previousTarget': array([7.298575  , 5.57464375]), 'currentState': array([17.666636 ,  6.1143713,  5.4761577], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.26015888225948514
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2784677, 3.8794508, 3.3101995], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1546318673099403}
episode index:271
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.51316702,  8.83772234]), 'previousTarget': array([16.51316702,  8.83772234]), 'currentState': array([26.      , 12.      ,  2.210752], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.2608643409653932
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.048136  , 3.6840618 , 0.32659656], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3168183373649174}
episode index:272
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.49937617, 15.01247661]), 'previousTarget': array([ 4.49937617, 15.01247661]), 'currentState': array([ 4.      , 25.      ,  5.407873], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.25990879392888994
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.5006418, 6.5995736, 2.2194161], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.775233999239266}
episode index:273
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.05943307, 5.26310675]), 'previousTarget': array([5.35601013, 7.13606076]), 'currentState': array([ 7.262813, 15.017343,  5.078469], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.26059351808887143
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2908204, 3.995677 , 2.7093399], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9824126061173892}
episode index:274
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.10647783, 5.86059386]), 'previousTarget': array([6.10647783, 5.86059386]), 'currentState': array([14.       , 12.       ,  2.4983187], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.262078529872438
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.873221, 5.898534, 4.031196], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.077575424600803}
episode index:275
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.42173715, 5.12652114]), 'currentState': array([13.013958 ,  7.7641287,  2.7977085], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.477259571882605}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26112896998159585
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.004158 , 3.2882433, 5.786752 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.354697747334112}
episode index:276
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.03954254, 19.2101958 ]), 'previousTarget': array([ 2.03954254, 19.2101958 ]), 'currentState': array([ 0.      , 29.      ,  3.051129], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.26245999763943567
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3945427, 5.1171947, 6.093215 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6166953347202027}
episode index:277
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.8763954 , 22.18861162]), 'previousTarget': array([20.39372105, 22.49286483]), 'currentState': array([28.88236  , 29.324188 ,  5.4379134], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26151589692850247
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4901834, 7.6306615, 0.4303514], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.0234130199255587}
episode index:278
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.33708362,  3.90135357]), 'previousTarget': array([11.01947422,  4.62378286]), 'currentState': array([22.226826,  2.420475,  4.729927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26057856396460105
{'scaleFactor': 1.0, 'currentTarget': array([6.42695035, 4.27252457]), 'previousTarget': array([6.42695035, 4.27252457]), 'currentState': array([15.335985  , -0.26940215,  0.0643256 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:279
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9381686 ,  7.77114535]), 'previousTarget': array([10.9381686 ,  7.77114535]), 'currentState': array([20.       , 12.       ,  2.8405588], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.26287786221833037
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9637694, 6.2430186, 3.4551442], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.572878518203142}
episode index:280
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.        , 9.        , 0.34472963], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.0}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.26281376056999084
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8481503, 4.8970747, 5.878417 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.18344471395305512}
episode index:281
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.56167832, 10.31967823]), 'previousTarget': array([ 8.52786405, 12.05572809]), 'currentState': array([11.900318 , 19.329462 ,  4.3429074], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.2651539411865083
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7388396, 6.368003 , 3.8845756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5547719783532497}
episode index:282
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.10360009, 21.79032749]), 'previousTarget': array([12.10360009, 21.79032749]), 'currentState': array([16.       , 31.       ,  2.3695824], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.26510873525683637
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.2683764, 3.4210076, 2.0431192], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7402557452953293}
episode index:283
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.298575  ,  2.42535625]), 'previousTarget': array([15.298575  ,  2.42535625]), 'currentState': array([25.        ,  0.        ,  0.09154927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.2663929426369293
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.769274 , 6.0603924, 2.1121912], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0627077633251236}
episode index:284
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.4,  9.2]), 'previousTarget': array([19.4,  9.2]), 'currentState': array([29.       , 12.       ,  5.8895736], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2654582305575015
{'scaleFactor': 1.0, 'currentTarget': array([16.02471418,  6.01263686]), 'previousTarget': array([16.02471418,  6.01263686]), 'currentState': array([25.982796 ,  6.927302 ,  0.6178487], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:285
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.7281166 , 16.48573514]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 6.360778  , 26.465702  ,  0.46946967], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.2651384109698049
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8042052, 3.8390646, 1.8581884], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6666422114060695}
episode index:286
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.20254778, 20.03184721]), 'previousTarget': array([ 6.20254778, 20.03184721]), 'currentState': array([ 7.       , 30.       ,  5.4824395], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.2657275848925297
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.525155 , 6.0512695, 4.975594 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.852367568028614}
episode index:287
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.85362269, 4.18398379]), 'previousTarget': array([8.66654394, 3.58979079]), 'currentState': array([16.006004 ,  0.1548511,  2.099821 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.26655799566994126
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8418665, 3.285997 , 2.268111 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9095930905670633}
episode index:288
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.029518 , 13.657653 ,  3.8822894], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.657703175562343}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.2683270730530873
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9016924 , 4.43001   , 0.61507154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5784057109039421}
episode index:289
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.73385803, 17.10674921]), 'previousTarget': array([ 6.75965265, 19.07722123]), 'currentState': array([ 8.151535, 27.005749,  5.346912], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.26865136596621625
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8898716, 4.5862546, 1.5764186], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.42815128614414133}
episode index:290
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.95262505, 17.81774616]), 'previousTarget': array([ 7.87570356, 18.22823636]), 'currentState': array([11.89941  , 27.37371  ,  5.5482106], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26772816539588556
{'scaleFactor': 1.0, 'currentTarget': array([19.22161275, 20.56781479]), 'previousTarget': array([18.7576136 , 21.96485184]), 'currentState': array([25.966251, 27.950893,  5.521999], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:291
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.81673649,  9.58001021]), 'previousTarget': array([17.89633523, 10.86197056]), 'currentState': array([26.140879, 13.193924,  4.096366], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.26755459255779285
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.674634 , 3.2653437, 0.8446098], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7649066955289348}
episode index:292
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.3648209 ,  2.67643864]), 'previousTarget': array([13.3648209 ,  2.67643864]), 'currentState': array([23.       ,  0.       ,  4.1515656], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.2673602029797687
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.4479356, 4.043946 , 0.8054913], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1039994457530757}
episode index:293
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08772099,  4.3216372 ]), 'previousTarget': array([10.08772099,  4.3216372 ]), 'currentState': array([20.        ,  3.        ,  0.66490597], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.2671743713479634
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.24091  , 3.1802573, 2.805498 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.202571492643958}
episode index:294
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.21367154, 17.74453044]), 'previousTarget': array([10.21367154, 17.74453044]), 'currentState': array([14.       , 27.       ,  3.9788663], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.26898608734183144
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0427804, 6.938521 , 2.8595858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.201193829117162}
episode index:295
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.92893219, 12.92893219]), 'previousTarget': array([12.92893219, 12.92893219]), 'currentState': array([20.       , 20.       ,  2.6690905], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2699258527305103
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0393736, 3.335228 , 5.790447 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.572065641154852}
episode index:296
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.29146718, 18.14850365]), 'previousTarget': array([ 7.42009499, 20.12559368]), 'currentState': array([ 9.008348, 28.000017,  4.164174], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26901701147552537
{'scaleFactor': 1.0, 'currentTarget': array([ 8.34517517, 21.52395348]), 'previousTarget': array([ 8.34517517, 21.52395348]), 'currentState': array([10.329364 , 31.325127 ,  2.2798476], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:297
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.44037427, 14.12237035]), 'previousTarget': array([ 6.44037427, 14.12237035]), 'currentState': array([ 8.        , 24.        ,  0.21636888], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2681142698262787
{'scaleFactor': 1.0, 'currentTarget': array([21.48178115, 19.14034249]), 'previousTarget': array([21.48178115, 19.14034249]), 'currentState': array([29.071365  , 25.651733  ,  0.57825774], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:298
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.82137551, 21.58260993]), 'previousTarget': array([16.21982659, 20.83975519]), 'currentState': array([23.938103  , 29.49372   ,  0.41293827], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2672175665827125
{'scaleFactor': 1.0, 'currentTarget': array([17.60389123,  2.0842551 ]), 'previousTarget': array([17.60389123,  2.0842551 ]), 'currentState': array([27.34659   , -0.16959047,  0.568938  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:299
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.75876926, 5.67562402]), 'previousTarget': array([4.42535625, 7.298575  ]), 'currentState': array([ 1.396191 , 15.093324 ,  3.8081956], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.267285453391993
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9732275 , 6.7964196 , 0.28312272], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7966191081235305}
episode index:300
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.3177872, 13.598156 ]), 'previousTarget': array([15.3177872, 13.598156 ]), 'currentState': array([23.       , 20.       ,  3.3560414], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.26931281408836166
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6437364, 6.9254465, 4.7433214], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.531642449744486}
episode index:301
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.025544 ,  7.2534504,  4.4140744], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.335910139045797}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.2690710217040394
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2871666, 3.6928515, 1.0569669], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1546311422686744}
episode index:302
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.06080701, 16.8085497 ]), 'previousTarget': array([10.06080701, 16.8085497 ]), 'currentState': array([14.       , 26.       ,  3.3625307], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26818299853009864
{'scaleFactor': 1.0, 'currentTarget': array([9.60407083, 3.44032342]), 'previousTarget': array([9.60407083, 3.44032342]), 'currentState': array([19.07537   ,  0.23182267,  0.7168944 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:303
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.98868632, 14.46417335]), 'previousTarget': array([ 7.98868632, 14.46417335]), 'currentState': array([11.       , 24.       ,  2.9614305], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26730081761388125
{'scaleFactor': 1.0, 'currentTarget': array([18.08306809, 10.69491779]), 'previousTarget': array([18.08306809, 10.69491779]), 'currentState': array([27.25207  , 14.686085 ,  2.2420142], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:304
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.01543108, 11.454527  ]), 'previousTarget': array([ 7.32356136, 13.3648209 ]), 'currentState': array([ 9.996014 , 21.000004 ,  5.2838864], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.26907926696013496
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2018018, 3.1135364, 5.026655 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.6061968311415806}
episode index:305
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.753888, 12.038815,  5.827245], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.254035688331092}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2699880165530455
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.809731 , 4.7562537, 1.5867238], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.30921600731769544}
episode index:306
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.31620527, 16.27193785]), 'previousTarget': array([ 2.31620527, 16.27193785]), 'currentState': array([ 0.       , 26.       ,  2.6652656], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2691085767597131
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6278217 , 2.71513   , 0.30834782], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.665239920454207}
episode index:307
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.43680431, 7.30132143]), 'previousTarget': array([5.90470911, 9.22197586]), 'currentState': array([ 7.30157 , 17.125916,  4.290782], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.27123077194694756
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.056197, 5.857026, 4.902631], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8588666123848471}
episode index:308
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.63079215, 18.95279911]), 'previousTarget': array([18.27327206, 19.60059927]), 'currentState': array([23.03376  , 26.634075 ,  3.3333795], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2703530024584461
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.42798126,  9.2236805 ,  0.6745846 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.8776782030870915}
episode index:309
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.41743063, 6.60206577]), 'previousTarget': array([4.41743063, 6.60206577]), 'currentState': array([ 1.       , 16.       ,  1.3658192], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.27063814857207014
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.680217 , 3.7253833, 2.25166  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1089751628250344}
episode index:310
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.67179882, 12.57009666]), 'previousTarget': array([ 7.67179882, 12.57009666]), 'currentState': array([11.       , 22.       ,  3.2331338], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.2719189640383004
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.009252 , 6.5127068, 4.653151 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5127350502527883}
episode index:311
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.7124705, 9.025413 ]), 'previousTarget': array([4.7124705, 9.025413 ]), 'currentState': array([ 4.       , 19.       ,  2.7962487], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.272411499805449
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2800865, 3.0556133, 3.2012382], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9644562038032098}
episode index:312
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.16953938, 10.21561144]), 'previousTarget': array([ 3.73785334, 12.15216441]), 'currentState': array([ 2.5970883, 20.091208 ,  4.6171317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.2729008883791973
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5871496, 6.5117946, 3.585403 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6218099424623762}
episode index:313
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.21719897, 23.65196555]), 'previousTarget': array([22.21719897, 23.65196555]), 'currentState': array([29.      , 31.      ,  6.098374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.27249418779905904
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.788691 , 4.676221 , 5.6099715], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2538350357255934}
episode index:314
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.67409319, 12.7654917 ]), 'previousTarget': array([17.14168465, 11.35993005]), 'currentState': array([26.200855  , 17.98989   ,  0.47850966], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.2720763789898692
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7430205, 4.002659 , 2.848658 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2436916403090543}
episode index:315
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.09529089, 9.22197586]), 'previousTarget': array([4.09529089, 9.22197586]), 'currentState': array([ 2.       , 19.       ,  1.9558271], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.2727348127796842
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.445235 , 6.5274863, 5.841248 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6251088964194333}
episode index:316
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.14357069, 5.25842724]), 'currentState': array([ 8.03325  , 13.636829 ,  3.3958917], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.153984179419265}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.27230136859081144
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4826224, 5.517987 , 5.299275 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6033543354251565}
episode index:317
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.63168174, 4.71350575]), 'previousTarget': array([9.025413 , 5.2875295]), 'currentState': array([19.612606 ,  4.0961323,  5.3919034], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.27325436664548475
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6596518, 3.4934397, 2.7911246], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.544526122064218}
episode index:318
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.41495392, 13.47423305]), 'previousTarget': array([12.41495392, 13.47423305]), 'currentState': array([19.        , 21.        ,  0.08898443], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.27303834500003255
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.277851  , 6.2666593 , 0.77684987], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.137807872070172}
episode index:319
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.       ,  3.       ,  2.0964332], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2739296818548695
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.840458 , 3.6220999, 5.79756  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8008738419364763}
episode index:320
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.20230252, 18.97222016]), 'previousTarget': array([ 6.64886795, 17.09169832]), 'currentState': array([ 8.759281 , 28.850267 ,  1.6228937], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.2735519561552465
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7467127, 6.226515 , 3.1032715], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7535870920569012}
episode index:321
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.94367207,  4.69767155]), 'previousTarget': array([17.0103146 ,  4.45407661]), 'currentState': array([28.941322 ,  4.4809012,  5.8828034], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.27419353845467564
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0662603, 3.354464 , 2.576412 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9607905694520895}
episode index:322
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.      , 9.      , 2.807609], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.403124237432848}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.27377213682815177
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.19177  , 5.7906837, 2.3882854], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4302086220421468}
episode index:323
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.7615218, 11.429595 ,  3.254468 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.70093430018125}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2729271610972007
{'scaleFactor': 1.0, 'currentTarget': array([ 9.3222361 , 12.33760888]), 'previousTarget': array([ 9.3222361 , 12.33760888]), 'currentState': array([14.397667 , 20.953873 ,  2.4612386], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:324
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.3648209 ,  2.67643864]), 'previousTarget': array([13.3648209 ,  2.67643864]), 'currentState': array([23.       ,  0.       ,  1.7050558], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.27457885558065936
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7738156, 4.60077  , 3.3903835], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8707325810495772}
episode index:325
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.       , 1.       , 3.4801786], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.2763484228061631
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3278632 , 4.370576  , 0.07826447], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7866773952305588}
episode index:326
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  0.       ,  0.7400067], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.27790600604164034
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3043427, 6.9524956, 2.0547538], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3480947534465293}
episode index:327
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.27936603,  2.91845673]), 'previousTarget': array([17.04106794,  3.90535746]), 'currentState': array([28.158731 ,  1.3698642,  5.9674473], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.2774969783925915
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.92941  , 5.600213 , 3.0229743], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0206134168394123}
episode index:328
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.00297911, 19.5361327 ]), 'previousTarget': array([ 6.70660816, 18.08399589]), 'currentState': array([ 6.691333, 29.512413,  2.524846], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.27757268494966275
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.966324 , 6.905592 , 3.1216877], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1678945133346263}
episode index:329
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.       ,  3.       ,  4.0170364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.2786593904075846
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9611635, 5.1291695, 4.037674 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.969804137029961}
episode index:330
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.       , 4.       , 4.7424064], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.2804421288437217
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9568496 , 4.467238  , 0.31148648], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0951696269754287}
episode index:331
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.13606076,  2.64398987]), 'previousTarget': array([19.13606076,  2.64398987]), 'currentState': array([29.       ,  1.       ,  4.2253942], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.2803967195693361
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.479827 , 5.993742 , 2.9587927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.782529347694852}
episode index:332
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.83975519, 16.21982659]), 'previousTarget': array([20.83975519, 16.21982659]), 'currentState': array([29.       , 22.       ,  4.9644847], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.28148445194926214
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.779929 , 5.4948664, 3.9117558], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9236786354517635}
episode index:333
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.69850888, 3.93626762]), 'previousTarget': array([6.35236179, 4.63117406]), 'currentState': array([17.00179  ,  0.2689829,  4.1661043], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.2831402283250131
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0581527, 3.650728 , 2.4986293], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6454821016071506}
episode index:334
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.56393467, 17.67955052]), 'previousTarget': array([ 3.24034735, 19.07722123]), 'currentState': array([ 0.6771859, 27.499947 ,  3.6413283], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.28229503361359515
{'scaleFactor': 1.0, 'currentTarget': array([ 8.24277093, 12.48472253]), 'previousTarget': array([ 8.24277093, 12.48472253]), 'currentState': array([12.218217, 21.660551,  2.633698], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:335
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 3.       , 1.8923837], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.28208798203506963
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.578485 , 3.3389025, 0.9662536], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.291475491030183}
episode index:336
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.31756858,  8.03861062]), 'previousTarget': array([10.31756858,  8.03861062]), 'currentState': array([19.       , 13.       ,  1.0069373], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.2827491034200904
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5647244, 4.392969 , 3.9738593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.678347122976243}
episode index:337
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.92549892, 14.05642167]), 'previousTarget': array([11.92549892, 14.05642167]), 'currentState': array([18.        , 22.        ,  0.31852642], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.2823293854008128
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.836863  , 6.887862  , 0.34065026], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.217410943542942}
episode index:338
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.79094798, 15.28935512]), 'previousTarget': array([19.79094798, 15.28935512]), 'currentState': array([28.       , 21.       ,  1.7022377], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2814965553553827
{'scaleFactor': 1.0, 'currentTarget': array([10.55392373, 21.30252033]), 'previousTarget': array([10.55392373, 21.30252033]), 'currentState': array([13.77871  , 30.768288 ,  1.6020243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:339
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.42507074, 5.85504245]), 'previousTarget': array([6.42507074, 5.85504245]), 'currentState': array([15.        , 11.        ,  0.14669058], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.28193300376176067
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1729198, 3.62594  , 1.6468997], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.603777537904693}
episode index:340
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.68080228, 21.24424806]), 'previousTarget': array([ 8.57464375, 19.298575  ]), 'currentState': array([10.890692, 30.997011,  1.466779], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.28234180498871964
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.875217 , 4.3809233, 1.2736747], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9747644534578506}
episode index:341
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.67422815, 18.90432368]), 'previousTarget': array([13.07942743, 19.29437161]), 'currentState': array([16.00162  , 27.919516 ,  2.7010996], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.2820512047352703
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5481114, 3.0665805, 5.784055 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4178691498641927}
episode index:342
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.1227087 , 8.82492245]), 'previousTarget': array([8.598156 , 9.3177872]), 'currentState': array([16.453575 , 15.626275 ,  5.4569435], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.2820424318018877
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.499978  , 3.0110743 , 0.27572834], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.050816265999856}
episode index:343
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.40387291, 18.45196516]), 'previousTarget': array([13.26537656, 16.80768079]), 'currentState': array([20.133383 , 26.647861 ,  0.8219772], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.28312852711808606
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.000752 , 6.7560725, 3.772565 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.020467104610482}
episode index:344
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.13606076, 5.35601013]), 'previousTarget': array([7.13606076, 5.35601013]), 'currentState': array([17.       ,  7.       ,  4.3002152], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.28420832622955283
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.965125 , 3.5887222, 1.8317134], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4193845379295156}
episode index:345
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.83975519, 16.21982659]), 'previousTarget': array([20.83975519, 16.21982659]), 'currentState': array([29.      , 22.      ,  6.160067], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.2838651304764753
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.363262  , 3.329704  , 0.52100194], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7093413883718813}
episode index:346
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.       , 8.       , 2.4904616], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.28420178616084146
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0748057, 4.694311 , 1.1463344], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9743870396619118}
episode index:347
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.      , 29.      ,  2.539568], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2833851143615287
{'scaleFactor': 1.0, 'currentTarget': array([12.96735088,  4.93352805]), 'previousTarget': array([12.96735088,  4.93352805]), 'currentState': array([22.967003 ,  4.8501005,  6.0878553], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:348
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.41421356, 16.10050506]), 'previousTarget': array([ 3.41421356, 16.10050506]), 'currentState': array([ 2.       , 26.       ,  2.1531804], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.28334116479865
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4875946, 4.5720353, 4.468948 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.547931355367749}
episode index:349
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.25608804, 5.75140493]), 'previousTarget': array([8.25608804, 5.75140493]), 'currentState': array([18.      ,  8.      ,  5.154676], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.2838102563671219
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.4854426, 3.7124934, 3.0599396], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3759824670488243}
episode index:350
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.04788782, 18.94413714]), 'previousTarget': array([12.73645385, 17.49734853]), 'currentState': array([19.491087  , 27.332914  ,  0.36344382], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.28485097537567255
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.846248 , 4.0919456, 3.2403917], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2412488215488735}
episode index:351
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  9.       ,  2.0547857], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2840417396501735
{'scaleFactor': 1.0, 'currentTarget': array([4.87575953, 5.29750434]), 'previousTarget': array([4.87575953, 5.29750434]), 'currentState': array([ 1.0221983, 14.525184 ,  2.632    ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:352
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.      , 1.      , 5.077886], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.65685424949238}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.2850759064737379
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0407987, 6.421084 , 2.6982212], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7614600157808813}
episode index:353
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9800348, 4.28189  , 1.3713579], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2474017176605443}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2870954660599703
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9800348, 4.28189  , 1.3713579], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2474017176605443}
episode index:354
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 4.       , 4.6530433], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.28863748942726636
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.7878666 , 3.1648357 , 0.30639693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.847384264841856}
episode index:355
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.67491245, 17.77881979]), 'previousTarget': array([ 7.68379473, 16.27193785]), 'currentState': array([11.438683 , 27.389313 ,  0.9079951], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.28828684470895405
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.077368 , 6.697603 , 3.0916705], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9321249753916045}
episode index:356
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.0710298 , 5.77279966]), 'previousTarget': array([7.13606076, 5.35601013]), 'currentState': array([18.895582  ,  7.637785  ,  0.24100599], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2874793185332987
{'scaleFactor': 1.0, 'currentTarget': array([16.53345677, 11.1297504 ]), 'previousTarget': array([16.53345677, 11.1297504 ]), 'currentState': array([25.363789 , 15.822856 ,  1.6749798], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:357
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.19419324, 4.96116135]), 'previousTarget': array([5.19419324, 4.96116135]), 'currentState': array([15.       ,  3.       ,  2.9878573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.28927983816842073
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3739014, 5.9777713, 3.107294 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6863100684770225}
episode index:358
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.70579449, 5.60772004]), 'previousTarget': array([6.92893219, 6.92893219]), 'currentState': array([13.283734, 12.132659,  4.074506], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.29107032705348634
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.280932 , 5.7743874, 4.1276317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8237709287494054}
episode index:359
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.12208718, 18.69481528]), 'previousTarget': array([ 8.66627048, 20.27612698]), 'currentState': array([12.004316 , 28.270449 ,  4.7477555], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29026179836722665
{'scaleFactor': 1.0, 'currentTarget': array([11.95245041,  2.86266587]), 'previousTarget': array([11.95245041,  2.86266587]), 'currentState': array([21.510967  , -0.07582963,  5.083932  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:360
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.64257444, 13.39388033]), 'previousTarget': array([10.56748722, 13.60429843]), 'currentState': array([17.848122, 21.235512,  6.260939], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.290058981465026
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1690602, 6.9476404, 3.4016953], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.67313367458132}
episode index:361
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.88622928, 10.35512466]), 'previousTarget': array([6.80941823, 8.92040615]), 'currentState': array([12.630669  , 19.157982  ,  0.03700346], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.2906384788363911
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.997642 , 4.8196907, 2.638363 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.013805248841655}
episode index:362
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.27193785,  2.31620527]), 'previousTarget': array([16.27193785,  2.31620527]), 'currentState': array([26.        ,  0.        ,  0.84689224], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.29120101376962476
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6472445, 5.3726425, 2.7097335], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7468519451934108}
episode index:363
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.91263916, 7.11828302]), 'previousTarget': array([7.91263916, 7.11828302]), 'currentState': array([16.       , 13.       ,  5.5585537], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2904010109845434
{'scaleFactor': 1.0, 'currentTarget': array([19.7106516,  9.3153424]), 'previousTarget': array([19.7106516,  9.3153424]), 'currentState': array([29.306303  , 12.130209  ,  0.88640475], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:364
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.3789479 , 5.83066233]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.529447 , 14.928649 ,  1.3597509], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.29105992751049015
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3089466, 3.847865 , 2.410324 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7437763742919417}
episode index:365
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.4823698 , 15.29373594]), 'previousTarget': array([ 3.48340453, 15.11063647]), 'currentState': array([ 3.980145, 25.281116,  6.056479], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29026468180690956
{'scaleFactor': 1.0, 'currentTarget': array([16.19709603, 18.5827933 ]), 'previousTarget': array([16.19709603, 18.5827933 ]), 'currentState': array([22.557981, 26.298954,  3.923954], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:366
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.       , 8.       , 1.2385318], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.29079527223870044
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.178827, 6.008011, 5.280267], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5510378937706701}
episode index:367
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.       , 7.       , 5.1941323], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2925634376657719
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.52431  , 5.6500597, 3.3691597], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6571357612229374}
episode index:368
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.24275371, 9.6284586 ]), 'previousTarget': array([9.24275371, 9.6284586 ]), 'currentState': array([16.       , 17.       ,  1.1140546], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.293583514416189
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.343888 , 6.9942756, 4.7991276], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.099432842450261}
episode index:369
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.774424 , 1.9227235, 1.0092934], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.797738623884239}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.29546572113398306
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8393419, 3.6156352, 1.4270829], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.80654180512134}
episode index:370
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.3216372 , 10.08772099]), 'previousTarget': array([ 4.3216372 , 10.08772099]), 'currentState': array([ 3.       , 20.       ,  1.6330024], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.296744905567059
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.246026, 6.57808 , 4.095177], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3593985889294706}
episode index:371
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.67949706, 15.45299804]), 'previousTarget': array([20.67949706, 15.45299804]), 'currentState': array([29.        , 21.        ,  0.13303822], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.29765738024581384
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5871234, 4.1296587, 4.3277907], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8100979664056662}
episode index:372
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.54493299, 14.32925463]), 'previousTarget': array([ 2.54493299, 14.32925463]), 'currentState': array([ 0.       , 24.       ,  3.5455165], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2986892762639494
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7315655, 6.387925 , 5.493605 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5689245556654277}
episode index:373
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.      ,  5.      ,  5.039695], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.29994958072796335
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6859856, 4.516484 , 3.5085075], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7539484762722921}
episode index:374
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.22755129, 15.70801771]), 'previousTarget': array([18.22755129, 15.70801771]), 'currentState': array([26.       , 22.       ,  2.8588614], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.30091580062197937
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3760333, 6.6228485, 4.899708 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2958451845984174}
episode index:375
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.45520022,  2.98274993]), 'previousTarget': array([11.45520022,  2.98274993]), 'currentState': array([21.       ,  0.       ,  0.5785497], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.300888068424341
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.5808773, 5.895221 , 4.765846 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9884760446821252}
episode index:376
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.47942816, 8.11628302]), 'previousTarget': array([5.47942816, 8.11628302]), 'currentState': array([ 7.       , 18.       ,  2.8747337], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.30223777611611
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3274345, 6.6592145, 4.0823855], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7903455096796548}
episode index:377
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.05177448, 5.0433858 ]), 'previousTarget': array([6.52590681, 6.35636161]), 'currentState': array([12.716456, 11.466208,  3.942399], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.3035589211251654
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.762054, 5.604642, 2.751417], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8629079487583073}
episode index:378
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.03883865, 10.19419324]), 'previousTarget': array([ 6.03883865, 10.19419324]), 'currentState': array([ 8.       , 20.       ,  2.8652103], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.30360547034268426
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.7776165, 6.1024446, 4.861115 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0917228850044953}
episode index:379
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.    , 0.    , 4.8406], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.099019513592784}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.3046391749419129
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.804327  , 5.995597  , 0.20131828], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0146433251935294}
episode index:380
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.76923077, 3.84615385]), 'previousTarget': array([7.76923077, 3.84615385]), 'currentState': array([17.      ,  0.      ,  4.762351], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.30383959705492625
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.0312767, 3.5514326, 4.748722 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4948812653748025}
episode index:381
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.74367328, 13.25204827]), 'previousTarget': array([17.19131191, 14.75304952]), 'currentState': array([24.925676, 19.00138 ,  4.522557], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3030442054395992
{'scaleFactor': 1.0, 'currentTarget': array([ 7.24624572, 20.69773778]), 'previousTarget': array([ 7.24624572, 20.69773778]), 'currentState': array([ 8.662753 , 30.596905 ,  3.8383992], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:382
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 3.9797409 , 14.283948  ,  0.20674685], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.339840370985009}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.3033753929280699
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.190558 , 6.12063  , 5.4605694], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6350043941103887}
episode index:383
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  9.       ,  2.1617558], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.30379858402765775
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.012839 , 6.6200347, 5.279362 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6200855680379336}
episode index:384
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.450267 , 7.73575  , 4.4730153], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.7725564689143827}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3055809253678456
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9760785, 5.792777 , 3.9074297], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7931378864051155}
episode index:385
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9688272, 5.713675 , 2.4279659], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2033113782125113}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.307379938514561
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9688272, 5.713675 , 2.4279659], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2033113782125113}
episode index:386
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.01725627,  4.58722022]), 'previousTarget': array([12.01725627,  4.58722022]), 'currentState': array([22.       ,  4.       ,  0.3773238], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.3071190279166356
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.401555  , 4.4473267 , 0.40173858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6912936003251344}
episode index:387
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.202932, 8.87781 , 2.832063], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0295310315325725}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3063274840302525
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.396101 ,  3.8233385,  5.3428802], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.522901252524322}
episode index:388
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.908762 ,  2.3239331,  3.8167677], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.408935550579806}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.307936064657185
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.652768 , 4.077091 , 2.5509071], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1304277331287682}
episode index:389
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.08094637, 21.78011795]), 'previousTarget': array([21.08094637, 21.78011795]), 'currentState': array([28.       , 29.       ,  2.1693616], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.30817388155024744
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.07611  , 3.7203457, 5.8494964], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3105991012333247}
episode index:390
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.00007466, 19.26641303]), 'previousTarget': array([17.11250538, 17.75000566]), 'currentState': array([23.437117 , 26.919157 ,  2.0495753], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.30738571305523404
{'scaleFactor': 1.0, 'currentTarget': array([8.76647625, 5.18537266]), 'previousTarget': array([8.76647625, 5.18537266]), 'currentState': array([18.754387 ,  5.6769423,  5.805153 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:391
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.24275371, 9.6284586 ]), 'previousTarget': array([9.24275371, 9.6284586 ]), 'currentState': array([16.      , 17.      ,  1.833962], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.3085463176503664
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.222866 , 3.9859552, 3.877405 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5886120279378984}
episode index:392
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 3.       , 10.       ,  3.3640451], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.308538461946239
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.826901, 5.969182, 5.488771], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9845186921298361}
episode index:393
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.584487 , 8.587087 , 3.7885764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.821057595591128}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.31024293285500487
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1588526, 6.574213 , 3.2698572], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.954759819795122}
episode index:394
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.       , 26.       ,  1.9980801], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.3098558704480499
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9193153, 4.143164 , 4.335721 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1018893947023947}
episode index:395
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.43245578,  3.07184843]), 'previousTarget': array([16.10050506,  3.41421356]), 'currentState': array([24.229853 ,  1.0690968,  3.8190815], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.31092265706220734
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3911   , 3.2941575, 1.2307644], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2011491617660175}
episode index:396
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.28809655, 12.11806769]), 'previousTarget': array([11.23316802, 13.83032137]), 'currentState': array([16.25162 , 20.145296,  4.341142], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.31062407433130496
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.021647 , 4.0672545, 5.7441063], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1872116433241544}
episode index:397
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.1613009 ,  2.78885438]), 'previousTarget': array([17.1613009 ,  2.78885438]), 'currentState': array([27.      ,  1.      ,  5.607438], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3098436118329851
{'scaleFactor': 1.0, 'currentTarget': array([22.30790732, 22.73173756]), 'previousTarget': array([21.59694307, 21.00859018]), 'currentState': array([29.292934 , 29.887812 ,  0.7515689], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:398
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.71565838, 17.1704023 ]), 'previousTarget': array([ 8.69649575, 15.56141644]), 'currentState': array([13.328621  , 26.494913  ,  0.25740612], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.30906706142738866
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.477008,  9.819695,  4.92836 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.073480993025042}
episode index:399
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.03454242,  6.1695452 ]), 'previousTarget': array([19.03454242,  6.1695452 ]), 'currentState': array([29.      ,  7.      ,  6.226921], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.30950687219950557
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6615906, 5.7301073, 3.5057755], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8149214647449154}
episode index:400
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.57505135, 16.05307498]), 'previousTarget': array([12.55011649, 16.61556384]), 'currentState': array([19.70475  , 23.954138 ,  6.0434933], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.3093395649023611
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.872516 , 6.15376  , 2.7803106], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.446529114141641}
episode index:401
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.      , 0.      , 2.484327], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.3100750537138914
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3842745, 5.4507995, 1.0547951], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7631107851597388}
episode index:402
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85504245, 21.42507074]), 'previousTarget': array([14.85504245, 21.42507074]), 'currentState': array([20.       , 30.       ,  1.3716071], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.30930563670715716
{'scaleFactor': 1.0, 'currentTarget': array([20.03078589, 14.69718508]), 'previousTarget': array([20.03078589, 14.69718508]), 'currentState': array([28.433777 , 20.118416 ,  5.9521136], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:403
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.84336791, 18.16542125]), 'previousTarget': array([ 7.96045746, 19.2101958 ]), 'currentState': array([ 8.2300005, 28.068817 ,  4.3376255], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.30974050238430323
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9868805 , 4.087813  , 0.00843399], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3632667908110563}
episode index:404
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.010048, 12.799772,  2.85522 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.799778734589494}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3089757110203914
{'scaleFactor': 1.0, 'currentTarget': array([13.12550103,  7.29137554]), 'previousTarget': array([13.12550103,  7.29137554]), 'currentState': array([22.750132 , 10.005503 ,  1.4520451], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:405
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.97414318, 17.1550043 ]), 'previousTarget': array([ 9.08838384, 18.4332612 ]), 'currentState': array([10.350876 , 26.868456 ,  3.2908804], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.3090382754190848
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.465566 , 6.6451855, 2.8724742], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.249694018940507}
episode index:406
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.13041453, 12.08216674]), 'previousTarget': array([ 5.4452998 , 13.01539647]), 'currentState': array([ 7.706605  , 21.957167  ,  0.22968549], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3082789676170723
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.7762575 , -0.30356205,  3.9382558 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.510598373839446}
episode index:407
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42507074, 11.85504245]), 'previousTarget': array([16.42507074, 11.85504245]), 'currentState': array([25.       , 17.       ,  2.4673858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.30837655131039426
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.524805 , 4.0157604, 2.1299334], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7733943813842943}
episode index:408
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.70546718, 10.00746617]), 'previousTarget': array([ 6.03883865, 10.19419324]), 'currentState': array([ 9.929457 , 19.473505 ,  5.7363586], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3076225744123248
{'scaleFactor': 1.0, 'currentTarget': array([20.86519618, 15.96692462]), 'previousTarget': array([20.86519618, 15.96692462]), 'currentState': array([29.091162, 21.65318 ,  5.708036], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:409
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.92893219, 6.92893219]), 'previousTarget': array([6.92893219, 6.92893219]), 'currentState': array([14.       , 14.       ,  2.2056565], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.3075807839730508
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.168193 , 3.6562626, 0.836424 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.580358571750393}
episode index:410
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.80572209,  8.14551052]), 'previousTarget': array([13.16212084,  9.3211228 ]), 'currentState': array([20.883078 , 12.340939 ,  3.9428775], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3068324122358901
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.53734  , 1.7775302, 4.563906 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.785090081917003}
episode index:411
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.29685126, 9.70714312]), 'previousTarget': array([5.47942816, 8.11628302]), 'currentState': array([ 5.926241 , 19.687317 ,  1.8010063], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3060876733712399
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.055766 , 0.4583416, 4.7718334], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.638775558598823}
episode index:412
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.39065688, 18.37602062]), 'previousTarget': array([ 9.44004672, 16.65512265]), 'currentState': array([14.128611, 27.651133,  1.001876], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3053465409901957
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.35522497,  6.104963  ,  3.1361618 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.468032305460152}
episode index:413
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([3.36336397, 9.58258088]), 'previousTarget': array([3.36336397, 9.58258088]), 'currentState': array([ 0.       , 19.       ,  3.5906043], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.3061456688768471
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4008234 , 5.7356586 , 0.94739974], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7602725884633803}
episode index:414
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.47441167, 14.01382171]), 'previousTarget': array([ 5.47441167, 14.01382171]), 'currentState': array([ 6.       , 24.       ,  3.0728476], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3054079684699149
{'scaleFactor': 1.0, 'currentTarget': array([ 3.31227086, 14.81730798]), 'previousTarget': array([ 4.03378444, 15.9875047 ]), 'currentState': array([ 1.6179888, 24.672733 ,  3.8429263], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:415
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.65459378, 4.13470455]), 'previousTarget': array([11.298575  ,  3.42535625]), 'currentState': array([19.48615  ,  2.3070045,  2.7945607], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.3068045475642568
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6683903, 6.467624 , 3.5795114], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2220366473537116}
episode index:416
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.54511097, 11.66007009]), 'previousTarget': array([15.54511097, 11.66007009]), 'currentState': array([24.       , 17.       ,  3.5912597], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.3078786836305383
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4951267, 4.253783 , 3.904664 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6710007167339536}
episode index:417
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.07831829, 10.81518477]), 'previousTarget': array([ 3.29039334, 12.26582832]), 'currentState': array([ 2.5129018, 20.691898 ,  5.9311085], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.30806293577231564
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.151137 , 6.433604 , 4.0771894], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8385689696847731}
episode index:418
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1805797, 5.1695123, 2.7697937], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.192687196799302}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3097143368802576
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1805797, 5.1695123, 2.7697937], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.192687196799302}
episode index:419
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 2.       , 2.2613955], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3102928348333103
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3705177, 6.4200835, 5.989306 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1614462004962225}
episode index:420
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.52558833, 14.01382171]), 'previousTarget': array([ 4.52558833, 14.01382171]), 'currentState': array([ 4.      , 24.      ,  4.733783], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.31088184505591016
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.682513 , 5.9284635, 5.837689 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.921690710651554}
episode index:421
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.       , 10.       ,  1.6093106], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3101451582192848
{'scaleFactor': 1.0, 'currentTarget': array([ 3.55350424, 10.67796109]), 'previousTarget': array([ 2.74483683, 12.35084073]), 'currentState': array([ 1.0847936 , 20.368444  ,  0.07910316], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:422
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.6875722 , 10.35600651]), 'previousTarget': array([18.6875722 , 10.35600651]), 'currentState': array([28.       , 14.       ,  1.0667679], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.3101197095977553
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0350337, 5.350414 , 4.9334683], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.026620565430429}
episode index:423
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.      , 7.      , 0.713186], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.31096605876985894
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1404638, 3.6200383, 5.4697075], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.315635797086495}
episode index:424
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.6166263, 4.8224945, 0.6124837], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3899745965015193}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.31256378569040044
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.253072 , 5.9722977, 0.9667195], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2260768795186472}
episode index:425
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.       , 14.       ,  5.0617514], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.055385138137419}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.31360170940287285
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.312773 , 4.409391 , 2.7641811], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.439511245252559}
episode index:426
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.       ,  6.       ,  5.7417765], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.31492236353307446
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.67488  , 5.482712 , 3.6136372], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8297431687049248}
episode index:427
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.33471177, 12.75958076]), 'previousTarget': array([10.33471177, 12.75958076]), 'currentState': array([16.       , 21.       ,  6.0061655], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.31453972468125224
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.4611673, 5.885982 , 3.0497177], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9988190429215955}
episode index:428
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.19918288, 21.37691221]), 'previousTarget': array([ 6.20254778, 20.03184721]), 'currentState': array([ 8.530092 , 31.28795  ,  1.4044145], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31380653184982743
{'scaleFactor': 1.0, 'currentTarget': array([16.20801311, 22.23583899]), 'previousTarget': array([14.9040075 , 20.75872225]), 'currentState': array([21.65951   , 30.619226  ,  0.48627713], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:429
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.2153563, 12.4116335,  4.2092123], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.510619286710992}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.31533325851994415
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8485074, 6.70974  , 5.306147 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7164385883983546}
episode index:430
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.07468051, 17.83311688]), 'previousTarget': array([18.40743398, 16.49208627]), 'currentState': array([25.211369, 24.83795 ,  2.30291 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.31592320734583096
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.4146595 , 3.3166087 , 0.47756255], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.782254156835898}
episode index:431
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.9119386 , 10.85373412]), 'previousTarget': array([ 6.70960666, 12.26582832]), 'currentState': array([ 7.4512463, 20.73455  ,  3.4094548], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.31557878773395376
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.375863 , 6.3078394, 5.4846473], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3607780608292814}
episode index:432
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.07964612, 11.47435713]), 'previousTarget': array([6.63663603, 9.58258088]), 'currentState': array([10.137876 , 20.995241 ,  1.5211443], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3148499683627437
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([6.04778753, 5.4686469 ]), 'currentState': array([13.519137 ,  8.431835 ,  4.0952787], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.18439951261838}
episode index:433
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.86197056, 17.89633523]), 'previousTarget': array([10.86197056, 17.89633523]), 'currentState': array([15.    , 27.    ,  4.7734], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3141245076061475
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.724814, 11.682779,  2.463007], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.184339349676767}
episode index:434
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.58256937, 6.60206577]), 'previousTarget': array([5.58256937, 6.60206577]), 'currentState': array([ 9.       , 16.       ,  2.0310862], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.31407691729331777
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4081793, 5.2018642, 2.0402179], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4225744500349895}
episode index:435
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 4.       , 4.2355766], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.31462418004531106
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1207895, 6.472305 , 0.8410291], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.387281609793579}
episode index:436
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.42320055, 6.77782212]), 'previousTarget': array([9., 8.]), 'currentState': array([15.485968, 12.693208,  4.721062], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.31429053697102133
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.543611 , 6.1165557, 4.2402287], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9051067380040974}
episode index:437
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.60059927, 8.27327206]), 'previousTarget': array([8.60059927, 8.27327206]), 'currentState': array([16.      , 15.      ,  5.940015], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3135729786674345
{'scaleFactor': 1.0, 'currentTarget': array([ 9.4384521 , 14.14884686]), 'previousTarget': array([ 9.4384521 , 14.14884686]), 'currentState': array([13.803295 , 23.145966 ,  0.7793727], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:438
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.      , 2.      , 3.901168], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.3146483890595525
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.497896 , 3.9186635, 2.5036364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1904575771215211}
episode index:439
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.52750637, 22.63030386]), 'previousTarget': array([16.52750637, 22.63030386]), 'currentState': array([22.       , 31.       ,  1.7197015], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31393327908441715
{'scaleFactor': 1.0, 'currentTarget': array([5.15017631, 5.36349361]), 'previousTarget': array([5.15017631, 5.36349361]), 'currentState': array([ 8.9685955, 14.60577  ,  3.6839695], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:440
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.       , 0.       , 4.5250773], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.31464958373774776
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.694301 , 3.760332 , 1.8212533], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8004517903910995}
episode index:441
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.31726583,  7.27103523]), 'previousTarget': array([16.27193785,  7.68379473]), 'currentState': array([24.032822 ,  9.639152 ,  2.8951187], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.31502399973963396
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2776265 , 4.1796756 , 0.54234123], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5183088177176867}
episode index:442
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.48750743, 10.62608287]), 'previousTarget': array([ 6.26214666, 12.15216441]), 'currentState': array([ 9.043623, 20.293879,  5.299246], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.3148825475124324
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0342026 , 3.3610268 , 0.44696808], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6393300719449393}
episode index:443
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.00496281, 5.0496281 ]), 'currentState': array([ 6.038836 , 13.000377 ,  4.4471245], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.067540368870914}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.3146771559053743
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.467776 , 3.4013567, 2.2596586], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1702594943205087}
episode index:444
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.88552499, 19.08714999]), 'previousTarget': array([15.88552499, 19.08714999]), 'currentState': array([22.       , 27.       ,  1.5053408], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31397001622918247
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.1287694 , -0.41121927,  4.2717795 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.527695209656948}
episode index:445
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.03861062, 10.31756858]), 'previousTarget': array([ 8.03861062, 10.31756858]), 'currentState': array([13.      , 19.      ,  3.695182], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.31525345760919804
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.232377 , 6.329237 , 4.9422193], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.211642341228856}
episode index:446
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.73785334, 12.15216441]), 'previousTarget': array([ 3.73785334, 12.15216441]), 'currentState': array([ 2.        , 22.        ,  0.22579783], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3145481926033609
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.818141  ,  3.4973717 ,  0.15519488], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.945250264451424}
episode index:447
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 2.1174045, 12.341263 ,  5.470963 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.886919347621504}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.316011922084157
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3064094, 6.967881 , 3.921419 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.596306162086349}
episode index:448
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.66841422, 17.62208433]), 'previousTarget': array([15.33205141, 16.54758686]), 'currentState': array([20.749355 , 25.560734 ,  2.0762017], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3153081093400943
{'scaleFactor': 1.0, 'currentTarget': array([7.55529174, 3.80362995]), 'previousTarget': array([7.55529174, 3.80362995]), 'currentState': array([16.611818  , -0.43657362,  5.1706257 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:449
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.84288535, 11.17127813]), 'previousTarget': array([ 3.84288535, 11.17127813]), 'currentState': array([ 2.       , 21.       ,  2.2013404], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.3160211701772583
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.738951 , 3.5023594, 4.1245966], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2949681247276104}
episode index:450
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.32382728,  8.47578103]), 'previousTarget': array([18.32382728,  8.47578103]), 'currentState': array([28.       , 11.       ,  5.9625797], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31532045804826214
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.810778 , -0.3576374,  4.627283 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.903759588340185}
episode index:451
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.654745 , 5.5200377, 4.3158507], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8361401240352796}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.31683523579594297
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.654745 , 5.5200377, 4.3158507], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8361401240352796}
episode index:452
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.82178448, 11.13681661]), 'previousTarget': array([ 9.82178448, 11.13681661]), 'currentState': array([16.      , 19.      ,  1.032625], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3161358202643846
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.892306, 4.802327, 5.24098 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.8990528951464762}
episode index:453
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.502687,  7.064204,  5.323638], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.781468889684708}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.31660888132758
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9526663, 5.4857635, 2.5680294], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0693639569502411}
episode index:454
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.95902011, 7.11194788]), 'previousTarget': array([6.80941823, 8.92040615]), 'currentState': array([10.093633, 16.217165,  3.87962 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.31750640100808614
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.961476 , 5.9885697, 4.5568852], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1965103302034756}
episode index:455
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.64398987, 7.13606076]), 'previousTarget': array([4.64398987, 7.13606076]), 'currentState': array([ 3.       , 17.       ,  3.0953748], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3168101150409631
{'scaleFactor': 1.0, 'currentTarget': array([12.0667646 ,  4.36147445]), 'previousTarget': array([12.0667646 ,  4.36147445]), 'currentState': array([22.026192 ,  3.461579 ,  1.6176476], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:456
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.38955877, 12.7723853 ]), 'previousTarget': array([12.64292747, 14.28069764]), 'currentState': array([19.279892, 20.019682,  4.7202  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.31768740260673406
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7554457, 5.6673265, 5.69486  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4121756088651483}
episode index:457
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.56338512, 13.63124508]), 'previousTarget': array([18.56338512, 13.63124508]), 'currentState': array([27.       , 19.       ,  2.2407014], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.31876172676746445
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.196088, 5.30616 , 4.887894], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.36357168352496677}
episode index:458
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.75000566, 17.11250538]), 'previousTarget': array([17.75000566, 17.11250538]), 'currentState': array([25.       , 24.       ,  2.0031276], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.31938535278134256
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2509203, 5.4415765, 3.1866965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5078885539716465}
episode index:459
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.14073956, 15.48675901]), 'previousTarget': array([13.92893219, 13.92893219]), 'currentState': array([22.092215 , 22.67543  ,  1.1088531], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3186910367970353
{'scaleFactor': 1.0, 'currentTarget': array([19.23601068,  7.19880005]), 'previousTarget': array([19.23601068,  7.19880005]), 'currentState': array([29.118824  ,  8.725234  ,  0.06560724], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:460
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.49972251, 17.26579071]), 'previousTarget': array([ 7.96045746, 19.2101958 ]), 'currentState': array([ 9.496638 , 27.064379 ,  3.9052343], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31799973302957973
{'scaleFactor': 1.0, 'currentTarget': array([10.91808702, 20.39851319]), 'previousTarget': array([10.91808702, 20.39851319]), 'currentState': array([14.505545 , 29.732866 ,  1.1591563], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:461
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1104736, 2.9969466, 1.0688852], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1916843267922683}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.31945427906198326
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.072677 , 4.750276 , 1.6908803], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.260084597559033}
episode index:462
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.92699383,  3.34881202]), 'previousTarget': array([12.06849396,  4.16841248]), 'currentState': array([21.654453 ,  1.0300769,  3.7181768], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.318764313016493
{'scaleFactor': 1.0, 'currentTarget': array([17.54941361,  5.09460222]), 'previousTarget': array([17.54941361,  5.09460222]), 'currentState': array([27.54913  ,  5.169984 ,  1.0110402], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:463
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.39968038, 20.00799041]), 'previousTarget': array([ 4.39968038, 20.00799041]), 'currentState': array([ 4.       , 30.       ,  1.6418525], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.31845229904119077
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4063582, 3.71034  , 5.0269413], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9081578449148295}
episode index:464
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.35241902, 17.14539676]), 'previousTarget': array([ 4.47565149, 16.01131862]), 'currentState': array([ 5.6424637 , 27.14119   ,  0.33261806], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.31872987090080934
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7770567, 6.741062 , 5.962367 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1276484177596386}
episode index:465
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.89817084, 12.14635382]), 'previousTarget': array([15.3177872, 13.598156 ]), 'currentState': array([23.005867, 18.000008,  4.600955], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31804590122076465
{'scaleFactor': 1.0, 'currentTarget': array([14.17980008, 22.48984066]), 'previousTarget': array([12.91584928, 21.03783309]), 'currentState': array([18.8272   , 31.344313 ,  1.3813183], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:466
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.725613 ,  6.4585924,  4.0907154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.88195882152212}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.31934075945033036
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5118227, 5.509845 , 2.3149552], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7224293573827809}
episode index:467
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.81979411,  7.68410771]), 'previousTarget': array([11.17647059,  8.29411765]), 'currentState': array([21.12503 , 11.346426,  4.946294], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.31983935500099714
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.855132 , 3.9244614, 2.3682969], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0852511522752692}
episode index:468
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.       , 12.       ,  1.5733856], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3206724336199125
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.7951837, 5.7167835, 4.7261066], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7454718997728986}
episode index:469
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.47942816, 8.11628302]), 'previousTarget': array([5.47942816, 8.11628302]), 'currentState': array([ 7.       , 18.       ,  1.3336215], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.32038732477543547
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.7183785, 3.0973635, 2.7960207], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9233659044236544}
episode index:470
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.25900177, 9.195289  ]), 'previousTarget': array([7.25900177, 9.195289  ]), 'currentState': array([12.       , 18.       ,  5.3464403], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.32086878829478865
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.400617, 3.530867, 2.1403  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.02979797093765}
episode index:471
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.44585177, 5.65543105]), 'previousTarget': array([11.01947422,  5.37621714]), 'currentState': array([19.33892  ,  7.1139197,  2.0871   ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3201889815399268
{'scaleFactor': 1.0, 'currentTarget': array([4.74463799, 7.05057343]), 'previousTarget': array([4.74463799, 7.05057343]), 'currentState': array([ 3.5088634, 16.973923 ,  5.389352 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:472
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.95181298,  8.74202964]), 'previousTarget': array([12.95181298,  8.74202964]), 'currentState': array([22.        , 13.        ,  0.36660135], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.32004024596893005
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.3495135, 4.8590865, 2.962367 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.37685052585387274}
episode index:473
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.43437224, 18.00943842]), 'previousTarget': array([ 4.43437224, 18.00943842]), 'currentState': array([ 4.      , 28.      ,  4.564166], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.3200634376194619
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.385225, 3.514133, 1.475693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5349915220432162}
episode index:474
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.58578644, 9.10050506]), 'previousTarget': array([5.58578644, 9.10050506]), 'currentState': array([7.0000000e+00, 1.9000000e+01, 6.5206885e-03], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31938961985605246
{'scaleFactor': 1.0, 'currentTarget': array([7.43656896, 9.68541521]), 'previousTarget': array([7.43656896, 9.68541521]), 'currentState': array([12.050324 , 18.557465 ,  2.4015596], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:475
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.92025538,  6.32033256]), 'previousTarget': array([12.40634498,  7.17833676]), 'currentState': array([20.680475,  8.497052,  4.405672], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.3196683047938895
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.192837 , 3.3064423, 2.1300602], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0714725422518345}
episode index:476
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.6676221 , 8.26042701]), 'previousTarget': array([7.6676221 , 8.26042701]), 'currentState': array([14.       , 16.       ,  5.3158307], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31899814063289605
{'scaleFactor': 1.0, 'currentTarget': array([5.27887828, 5.20234989]), 'previousTarget': array([6.6076023 , 6.38172784]), 'currentState': array([13.372732 , 11.075129 ,  4.4966407], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:477
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.01725627,  4.58722022]), 'previousTarget': array([12.01725627,  4.58722022]), 'currentState': array([22.        ,  4.        ,  0.46509862], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.31862849491563494
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2732563, 5.9516363, 3.0940273], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.589589030267445}
episode index:478
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.       , 11.       ,  1.1963775], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.3186827406117349
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6291037, 3.4319353, 0.2976144], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.611332046934005}
episode index:479
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.66845562, 11.4665867 ]), 'previousTarget': array([11.67206508, 11.19548901]), 'currentState': array([17.260212 , 18.986477 ,  2.7235723], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31801881823546047
{'scaleFactor': 1.0, 'currentTarget': array([ 7.08513511, 14.72766842]), 'previousTarget': array([ 6.0746711 , 14.64164067]), 'currentState': array([ 9.181036  , 24.505562  ,  0.25792044], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:480
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.        , 6.        , 0.26906365], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.31873454219127867
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.670826 , 4.968598 , 2.4769573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.671560539968859}
episode index:481
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.2101958 ,  7.96045746]), 'previousTarget': array([19.2101958 ,  7.96045746]), 'currentState': array([29.      , 10.      ,  2.417613], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.3187263982721701
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4267592, 6.5713315, 5.703342 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.122433655871448}
episode index:482
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.025413 , 5.2875295]), 'previousTarget': array([9.025413 , 5.2875295]), 'currentState': array([19.      ,  6.      ,  3.843722], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.3190024174274378
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.577375 , 3.5109043, 3.7713957], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.169220517527361}
episode index:483
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.91807407,  9.77242189]), 'previousTarget': array([9.3177872, 8.598156 ]), 'currentState': array([18.702345  , 16.049772  ,  0.11172748], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3183433215236621
{'scaleFactor': 1.0, 'currentTarget': array([21.18242673, 20.24679779]), 'previousTarget': array([21.18242673, 20.24679779]), 'currentState': array([28.460772 , 27.104326 ,  5.7954683], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:484
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.59995768, 19.44964781]), 'previousTarget': array([20.08777354, 20.77358143]), 'currentState': array([25.453682 , 26.731575 ,  3.8647094], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.3182980499069107
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.489559 , 4.59757  , 3.1603904], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5429635390404688}
episode index:485
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.66654394, 6.41020921]), 'previousTarget': array([8.66654394, 6.41020921]), 'currentState': array([18.     , 10.     ,  5.49147], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.31823485146958547
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.991253 , 3.9936426, 1.9917948], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2311081076922874}
episode index:486
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.86468483, 16.85645759]), 'previousTarget': array([13.80355711, 16.15117234]), 'currentState': array([18.392393 , 25.18979  ,  2.5996866], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3175813918156438
{'scaleFactor': 1.0, 'currentTarget': array([5.90377149, 5.29404258]), 'previousTarget': array([6.79977052, 5.89737239]), 'currentState': array([15.413132 ,  8.387918 ,  4.1894307], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:487
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42173715,  8.12652114]), 'previousTarget': array([15.42173715,  8.12652114]), 'currentState': array([25.       , 11.       ,  3.8070042], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.316930610275038
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4919121, 7.578787 , 4.0532374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.987385253307761}
episode index:488
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.35601013, 19.13606076]), 'previousTarget': array([ 7.35601013, 19.13606076]), 'currentState': array([ 9.      , 29.      ,  3.090534], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31628249041762485
{'scaleFactor': 1.0, 'currentTarget': array([7.61639817, 5.86343121]), 'previousTarget': array([7.61639817, 5.86343121]), 'currentState': array([17.112663 ,  8.997271 ,  4.9738717], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:489
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.      ,  0.      ,  6.081146], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.3164068866553919
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5870967, 4.020076 , 2.7791958], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1423370633639864}
episode index:490
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.95181298,  8.74202964]), 'previousTarget': array([12.95181298,  8.74202964]), 'currentState': array([22.      , 13.      ,  5.899666], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.316888101707341
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6243448, 5.1313   , 5.776521 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.381906958243952}
episode index:491
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.       , 13.       ,  1.4107853], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3176882340763755
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.584207 , 5.1603656, 5.1536202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4248462283644028}
episode index:492
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([27.      ,  5.      ,  5.273311], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.31768239419626304
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4477925, 3.752232 , 1.9483374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9915503571468822}
episode index:493
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.63086668, 7.36770297]), 'previousTarget': array([10.31756858,  8.03861062]), 'currentState': array([17.00724 , 12.82997 ,  3.383755], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.31835328535855484
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3095694, 3.4985213, 1.8813305], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.260971904994562}
episode index:494
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  8.       ,  0.8633786], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.31934596128353
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9655752, 5.246915 , 5.3980637], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0634855800905185}
episode index:495
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.88951266, 13.53927018]), 'previousTarget': array([15.72986847, 15.13376467]), 'currentState': array([22.458376 , 20.074736 ,  4.8872375], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.3192877809065671
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1291165, 5.810572 , 3.6887078], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3899393354111491}
episode index:496
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15117234, 13.80355711]), 'previousTarget': array([16.15117234, 13.80355711]), 'currentState': array([24.       , 20.       ,  2.7428687], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.318645350763898
{'scaleFactor': 1.0, 'currentTarget': array([5.0979593 , 7.64543115]), 'previousTarget': array([5.0979593 , 7.64543115]), 'currentState': array([ 5.468002, 17.638582,  5.124584], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:497
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.2101958 ,  2.03954254]), 'previousTarget': array([19.2101958 ,  2.03954254]), 'currentState': array([29.       ,  0.       ,  3.2358088], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.31871867498867945
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.4007893, 6.7870474, 3.5125182], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.831439431714607}
episode index:498
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.21147869, 18.22104427]), 'previousTarget': array([12.21147869, 18.22104427]), 'currentState': array([17.      , 27.      ,  5.053667], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.3189328462279579
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.270198 , 5.866142 , 2.5221796], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5374017808386706}
episode index:499
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.61390749, 22.22980468]), 'previousTarget': array([16.21982659, 20.83975519]), 'currentState': array([23.52106  , 30.298605 ,  0.6154179], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31829498053550204
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.543563 ,  4.660304 ,  2.6580582], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.552374272641984}
episode index:500
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.       , 14.       ,  5.8127317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.319196673480152
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8429894, 6.5633793, 4.4163857], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5712438104051187}
episode index:501
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.      , 3.      , 2.480227], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3205329350867653
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.421692 , 4.22839  , 2.1744118], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8793211651046693}
episode index:502
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.40026527, 21.34384241]), 'previousTarget': array([ 9.40026527, 21.34384241]), 'currentState': array([12.        , 31.        ,  0.85485154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.32052155583844355
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3039625, 6.8805485, 5.0541425], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.5323913365069126}
episode index:503
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 1.       , 1.9509594], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.32046196643064895
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0239887, 5.681684 , 1.1653856], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.090290329865309}
episode index:504
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.93198331, 16.0487716 ]), 'previousTarget': array([11.1808125 , 16.23784091]), 'currentState': array([14.008144 , 25.180305 ,  2.2818313], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31982738827930113
{'scaleFactor': 1.0, 'currentTarget': array([5.14706223, 5.82871779]), 'previousTarget': array([5.39144984, 7.77053097]), 'currentState': array([ 6.894339, 15.674886,  5.653064], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:505
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92813651,  9.45233239]), 'previousTarget': array([16.8085497 , 10.06080701]), 'currentState': array([24.052616, 13.54426 ,  2.79874 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.3202766555799167
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.572188 , 5.4455585, 4.939864 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4957171560918763}
episode index:506
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([22.      ,  5.      ,  4.538542], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.32043525123548205
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0010347, 3.0515523, 3.6648676], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1905522135748376}
episode index:507
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.37433993, 19.13415154]), 'previousTarget': array([12.37433993, 19.13415154]), 'currentState': array([17.       , 28.       ,  2.6635346], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3198044731818689
{'scaleFactor': 1.0, 'currentTarget': array([17.86531788, 18.27092727]), 'previousTarget': array([17.86531788, 18.27092727]), 'currentState': array([24.825811 , 25.450867 ,  1.8859252], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:508
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.61620254, 7.09796018]), 'previousTarget': array([7.45299804, 8.67949706]), 'currentState': array([12.718971 , 15.019842 ,  4.4125705], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.320989031573315
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.899646 , 6.5890765, 3.1829207], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8260686620104318}
episode index:509
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.       , 13.       ,  2.8894773], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.32116124950047037
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.7304726, 6.6741495, 5.451665 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6957068234450485}
episode index:510
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.89568474, 13.06116265]), 'previousTarget': array([ 5.89568474, 13.06116265]), 'currentState': array([ 7.      , 23.      ,  2.477647], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.3209017496876854
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.820119, 5.107714, 3.707979], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8271622335639308}
episode index:511
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([ 5.        , 16.        ,  0.43500298], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.3206469834964302
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7070093, 6.368323 , 3.7659192], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8825866062607095}
episode index:512
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.14357069, 14.25842724]), 'previousTarget': array([10.14357069, 14.25842724]), 'currentState': array([15.       , 23.       ,  2.4756944], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.32075729473118086
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.17403  , 6.6752567, 3.3909233], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0456859835870214}
episode index:513
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.75304952, 12.19131191]), 'previousTarget': array([10.75304952, 12.19131191]), 'currentState': array([17.        , 20.        ,  0.74338174], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32013325330174275
{'scaleFactor': 1.0, 'currentTarget': array([20.29493478, 16.57090164]), 'previousTarget': array([20.29493478, 16.57090164]), 'currentState': array([28.269913 , 22.60412  ,  1.3109406], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:514
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.       ,  5.       ,  2.0015275], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.3212327710074018
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6192  , 6.022778, 2.520578], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1956102340039725}
episode index:515
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 ,  6.00496281]), 'previousTarget': array([15.0496281 ,  6.00496281]), 'currentState': array([25.       ,  7.       ,  2.6932125], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.3213124982299853
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.199258 , 6.7352877, 4.6187043], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.5007788216235634}
episode index:516
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.61280532, 20.59112588]), 'previousTarget': array([10.61280532, 20.59112588]), 'currentState': array([14.      , 30.      ,  6.177632], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3206910040361169
{'scaleFactor': 1.0, 'currentTarget': array([ 2.97856607, 10.5325527 ]), 'previousTarget': array([ 2.97856607, 10.5325527 ]), 'currentState': array([-0.45325005, 19.925243  ,  2.787955  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:517
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.62389403, 14.53732219]), 'previousTarget': array([19.62389403, 14.53732219]), 'currentState': array([28.       , 20.       ,  1.1282212], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32007190943373054
{'scaleFactor': 1.0, 'currentTarget': array([19.96550971, 10.54639173]), 'previousTarget': array([19.96550971, 10.54639173]), 'currentState': array([29.34226 , 14.021524,  5.036977], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:518
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.91247051,  9.85749469]), 'previousTarget': array([13.38065785,  9.92979873]), 'currentState': array([20.094341 , 15.607016 ,  2.8422782], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.31975235267527224
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.759481 , 6.9317884, 2.9415112], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.612963799808396}
episode index:519
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.71851861, 15.68819774]), 'previousTarget': array([ 9.89635323, 15.88078495]), 'currentState': array([12.004422 , 25.132923 ,  3.6096516], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.31945885571823307
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5743732, 3.1926682, 3.478795 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8964052472516546}
episode index:520
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.27776592, 18.70433166]), 'previousTarget': array([ 6.13370384, 18.03759412]), 'currentState': array([ 8.917351 , 28.569004 ,  0.9665662], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.3198050710237627
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3283887, 4.063034 , 2.123315 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.15280827878325}
episode index:521
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.21588372, 21.1600632 ]), 'previousTarget': array([16.52750637, 22.63030386]), 'currentState': array([20.559376 , 29.612701 ,  3.9600697], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31919241763099687
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 2.2146354, 10.234772 ,  4.1653695], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.929678841191851}
episode index:522
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.56553883, 16.61517663]), 'previousTarget': array([16.12255352, 16.74047316]), 'currentState': array([24.908848 , 23.403093 ,  6.0680413], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31858210708103324
{'scaleFactor': 1.0, 'currentTarget': array([17.16778408, 12.92416865]), 'previousTarget': array([17.16778408, 12.92416865]), 'currentState': array([25.547462 , 18.381365 ,  4.1461163], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:523
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.        ,  9.        ,  0.68127656], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.3190395193548249
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0164359, 5.1386237, 5.057182 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9884021789877204}
episode index:524
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.05407952, 15.45163485]), 'previousTarget': array([ 6.00496281, 15.0496281 ]), 'currentState': array([ 8.982509 , 25.263931 ,  0.5202639], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.31995874044089007
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.66813  , 6.7200527, 3.242412 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7517759862212507}
episode index:525
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.66654394, 3.58979079]), 'previousTarget': array([8.66654394, 3.58979079]), 'currentState': array([18.     ,  0.     ,  4.74764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.32040118290614006
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.116582, 5.53077 , 4.330618], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2363137886702296}
episode index:526
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.51619871, 19.68237948]), 'previousTarget': array([ 7.47942816, 21.11628302]), 'currentState': array([ 7.5434017, 29.629482 ,  3.3797398], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.3204808239591844
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0460806, 6.053792 , 5.5610876], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4214217604212145}
episode index:527
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.29370622, 21.37144798]), 'previousTarget': array([15.75902574, 22.48341683]), 'currentState': array([19.230486, 30.067896,  3.122411], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.320648133335062
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.935713 , 3.9852371, 2.8990045], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1855726478386788}
episode index:528
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.67268932, 20.98476943]), 'previousTarget': array([12.67268932, 20.98476943]), 'currentState': array([17.       , 30.       ,  4.7084284], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3200419931964324
{'scaleFactor': 1.0, 'currentTarget': array([10.03838834,  9.6094737 ]), 'previousTarget': array([9.09222484, 8.22544239]), 'currentState': array([17.416527 , 16.359516 ,  0.9883823], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:529
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.10038794, 14.58814431]), 'previousTarget': array([13.70462796, 13.16058871]), 'currentState': array([22.352972  , 21.472912  ,  0.73314226], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.3196960649167346
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9391685, 6.416465 , 2.4684942], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.401405179361096}
episode index:530
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42173715,  8.12652114]), 'previousTarget': array([15.42173715,  8.12652114]), 'currentState': array([25.       , 11.       ,  5.2427106], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.32015596910795047
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8402357, 5.359768 , 3.6330643], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2142842170753545}
episode index:531
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08636336, 11.93595004]), 'previousTarget': array([10.08636336, 11.93595004]), 'currentState': array([16.       , 20.       ,  0.3292175], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.31955417217353704
{'scaleFactor': 1.0, 'currentTarget': array([5.19348488, 5.95194599]), 'previousTarget': array([5.19348488, 5.95194599]), 'currentState': array([ 7.185279 , 15.751576 ,  2.8004444], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:532
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.80854779, 20.07123162]), 'previousTarget': array([ 6.80854779, 20.07123162]), 'currentState': array([ 8.       , 30.       ,  0.8591135], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.3195947198083222
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2099266, 5.5960355, 2.3377411], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3487700634445963}
episode index:533
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.22943469, 22.64068988]), 'previousTarget': array([21.22943469, 22.64068988]), 'currentState': array([28.       , 30.       ,  5.1386175], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.31991362533865153
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.5214868, 3.6179137, 2.7489114], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.462579023071406}
episode index:534
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.2933511 , 4.94931091]), 'previousTarget': array([7.03454242, 4.8304548 ]), 'currentState': array([15.1473255,  3.2466106,  3.0522542], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.3205411871989048
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.829545 , 3.1140406, 0.1605261], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0603368051646602}
episode index:535
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.       , 2.       , 3.7431982], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.3210058514065135
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.100065 , 4.1826324, 2.968208 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8234699559663219}
episode index:536
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.46279305, 6.36919115]), 'previousTarget': array([11.298575  ,  6.57464375]), 'currentState': array([19.022974 ,  9.302268 ,  2.9421377], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32040807514691105
{'scaleFactor': 1.0, 'currentTarget': array([17.65625512,  2.24765137]), 'previousTarget': array([17.65625512,  2.24765137]), 'currentState': array([27.42786   ,  0.12262595,  5.456816  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:537
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.54057759, 23.36613715]), 'previousTarget': array([20.54057759, 23.36613715]), 'currentState': array([27.       , 31.       ,  0.6660719], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.319812521103887
{'scaleFactor': 1.0, 'currentTarget': array([12.70706897, 21.88281378]), 'previousTarget': array([12.70706897, 21.88281378]), 'currentState': array([16.859858 , 30.979755 ,  3.5229087], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:538
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.44004672, 16.65512265]), 'previousTarget': array([ 9.44004672, 16.65512265]), 'currentState': array([13.       , 26.       ,  1.4260647], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3192191769088891
{'scaleFactor': 1.0, 'currentTarget': array([13.36660892, 20.75715811]), 'previousTarget': array([12.210924  , 21.31101688]), 'currentState': array([18.056246, 29.589333,  5.677711], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:539
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.30605019, 4.17597947]), 'previousTarget': array([11.07722123,  4.24034735]), 'currentState': array([19.12783  ,  2.29645  ,  3.3393803], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3203368167561466
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8630133, 4.630616 , 3.540406 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8992795569573009}
episode index:540
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.        , 7.        , 0.01766607], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.3210713707595516
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4296026, 3.9413824, 3.9623837], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7788858494684345}
episode index:541
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.37689979, 21.73073498]), 'previousTarget': array([16.37689979, 21.73073498]), 'currentState': array([22.       , 30.       ,  5.0463066], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32047898815667425
{'scaleFactor': 1.0, 'currentTarget': array([18.85812096,  7.45963748]), 'previousTarget': array([18.85812096,  7.45963748]), 'currentState': array([28.704239 ,  9.207196 ,  0.7357932], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:542
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.05572809, 5.52786405]), 'previousTarget': array([6.05572809, 5.52786405]), 'currentState': array([15.       , 10.       ,  5.3985987], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3198887874418369
{'scaleFactor': 1.0, 'currentTarget': array([16.84892747, 17.03772948]), 'previousTarget': array([16.84892747, 17.03772948]), 'currentState': array([23.863886 , 24.164465 ,  1.3373754], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:543
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.77114535, 10.9381686 ]), 'previousTarget': array([ 7.77114535, 10.9381686 ]), 'currentState': array([12.       , 20.       ,  6.2104187], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.3203907684325716
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5974646, 5.7409616, 3.6732488], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7609420905231201}
episode index:544
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 2.       , 4.2762947], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.32068388896126676
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0696974, 3.1709023, 2.2549982], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.118926864225598}
episode index:545
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.58722022, 12.01725627]), 'previousTarget': array([ 4.58722022, 12.01725627]), 'currentState': array([ 4.      , 22.      ,  2.318257], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3200965558313011
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.41903353,  3.0356464 ,  4.8625903 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.764079222650972}
episode index:546
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  4.       ,  5.9221325], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.3200477919660249
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.360267 , 6.0781856, 2.8494816], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.136783412271712}
episode index:547
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.15216441,  3.73785334]), 'previousTarget': array([12.15216441,  3.73785334]), 'currentState': array([22.       ,  2.       ,  3.9254973], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.3197313973486272
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.071653 , 6.029275 , 2.9350793], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4858825048312927}
episode index:548
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.57577287, 11.5485573 ]), 'previousTarget': array([13.23886   , 11.69407375]), 'currentState': array([22.830166 , 17.193466 ,  0.5664155], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3191490086467172
{'scaleFactor': 1.0, 'currentTarget': array([20.01649073, 23.51211273]), 'previousTarget': array([20.01649073, 23.51211273]), 'currentState': array([26.316198, 31.278301,  5.727888], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:549
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.    ,  8.    ,  0.9064], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.3201324801848891
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.959375 , 5.322859 , 4.6683965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0895588100590972}
episode index:550
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.       , 5.       , 4.1715903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.32127741225333756
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5301843, 3.643666 , 3.6923356], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0447752291875303}
episode index:551
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.54493299, 14.32925463]), 'previousTarget': array([ 2.54493299, 14.32925463]), 'currentState': array([ 0.       , 24.       ,  3.7652767], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.32211871792100766
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.07503  , 5.8983893, 4.7774696], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4009969962980653}
episode index:552
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.       , 22.       ,  1.5161288], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.32214699221210696
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.495282 , 3.3955793, 3.0205927], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1931790710692702}
episode index:553
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.785382 , 12.7970197]), 'previousTarget': array([14.05642167, 11.92549892]), 'currentState': array([23.889471  , 18.655666  ,  0.45142314], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.32222620764362525
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6634204, 6.298954 , 4.1560345], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8637935625030044}
episode index:554
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.       , 24.       ,  0.6972176], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.3220064793899565
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.070131 , 6.91864  , 5.5631866], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.196897803024703}
episode index:555
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.      ,  4.      ,  3.202342], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.32188580183834925
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.845925, 5.00959 , 2.446018], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8459792137178248}
episode index:556
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.12237035,  3.55962573]), 'previousTarget': array([14.12237035,  3.55962573]), 'currentState': array([24.       ,  2.       ,  5.6867723], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.3224386525194352
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.903317 , 3.7944574, 2.6238184], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.506424386165341}
episode index:557
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.87927736, 14.11270111]), 'previousTarget': array([ 6.83069542, 13.2381294 ]), 'currentState': array([10.8920965, 23.648052 ,  0.436897 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.3225716975981137
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0931873, 6.1147156, 5.04951  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5612973320097259}
episode index:558
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.08399589,  3.29339184]), 'previousTarget': array([18.08399589,  3.29339184]), 'currentState': array([28.       ,  2.       ,  1.2165111], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.32316756078769515
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7232494, 4.1365294, 4.231441 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1263530281731902}
episode index:559
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.       , 11.       ,  3.5506277], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.3237151609134372
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0736523, 3.0846953, 1.435154 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.127560120245836}
episode index:560
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.00943842,  5.56562776]), 'previousTarget': array([18.00943842,  5.56562776]), 'currentState': array([28.       ,  6.       ,  0.9602152], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3241233040796564
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.953891 , 4.13139  , 3.1201892], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2901127186098895}
episode index:561
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.13606076, 5.35601013]), 'previousTarget': array([7.13606076, 5.35601013]), 'currentState': array([17.       ,  7.       ,  2.1620202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.3247982585118982
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.3932595, 6.1792436, 5.9307113], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2430882667383034}
episode index:562
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.94938083, 19.71743995]), 'previousTarget': array([10.72889436, 21.55013927]), 'currentState': array([13.1369   , 29.195822 ,  4.6468673], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3242213521912732
{'scaleFactor': 1.0, 'currentTarget': array([5.2575196 , 5.24040652]), 'previousTarget': array([6.67845893, 6.33485927]), 'currentState': array([12.567295 , 12.064422 ,  2.7326398], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:563
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 1.       , 2.6954007], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.32403521020044906
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.754191 , 4.948152 , 1.3544934], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2468875120482226}
episode index:564
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.47423305, 19.41495392]), 'previousTarget': array([21.47423305, 19.41495392]), 'currentState': array([29.        , 26.        ,  0.69540524], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3234616965540766
{'scaleFactor': 1.0, 'currentTarget': array([11.49143673, 21.9693475 ]), 'previousTarget': array([11.49143673, 21.9693475 ]), 'currentState': array([15.064327 , 31.309286 ,  0.2661188], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:565
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.      , 8.      , 3.848124], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.32447207916486215
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.5784655, 5.115573 , 5.997045 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.4370909161258182}
episode index:566
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.03883865, 15.19419324]), 'previousTarget': array([ 7.03883865, 15.19419324]), 'currentState': array([ 9.       , 25.       ,  0.9910675], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3238998180023139
{'scaleFactor': 1.0, 'currentTarget': array([ 6.11826287, 11.11126784]), 'previousTarget': array([ 6.11826287, 11.11126784]), 'currentState': array([ 7.918215 , 20.947943 ,  2.2822583], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:567
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.       , 3.       , 4.0069385], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.32495412236221816
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.000832 , 3.0576859, 3.6833136], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9423143261795257}
episode index:568
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.67949706, 13.45299804]), 'previousTarget': array([17.67949706, 13.45299804]), 'currentState': array([26.       , 19.       ,  3.9898052], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3243830254863619
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91241312, 19.0253923 ]), 'previousTarget': array([ 9.91241312, 19.0253923 ]), 'currentState': array([13.218031, 28.463236,  3.426637], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:569
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.53997861, 5.97491175]), 'previousTarget': array([3.84615385, 7.76923077]), 'currentState': array([ 0.27259782, 15.018664  ,  4.479681  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.3253077180225172
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0219207, 6.7269926, 4.2924347], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.984727341296964}
episode index:570
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.19419324,  7.03883865]), 'previousTarget': array([15.19419324,  7.03883865]), 'currentState': array([25.       ,  9.       ,  2.1892123], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.3262594484861713
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5353756, 4.468399 , 3.6397595], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7544710731009286}
episode index:571
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.88552499, 19.08714999]), 'previousTarget': array([15.88552499, 19.08714999]), 'currentState': array([22.      , 27.      ,  5.286312], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.32651176875382765
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1289496, 5.848466 , 3.5906034], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4122399623029258}
episode index:572
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.60346687, 16.45067567]), 'previousTarget': array([ 3.86629616, 18.03759412]), 'currentState': array([ 4.2573776, 26.444685 ,  5.031705 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.32649134537586455
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3220334, 6.494599 , 5.966296 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.247086482914427}
episode index:573
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.26621269, 11.77199263]), 'previousTarget': array([13.70462796, 13.16058871]), 'currentState': array([19.581688 , 18.589897 ,  4.0977063], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.3269049583463811
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3661685, 6.7298217, 4.8969164], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.842288094491268}
episode index:574
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.24679025,  5.29999701]), 'previousTarget': array([15.01247661,  4.49937617]), 'currentState': array([26.243235  ,  5.5666423 ,  0.70635545], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32633642798403956
{'scaleFactor': 1.0, 'currentTarget': array([19.73367639,  8.60764583]), 'previousTarget': array([19.73367639,  8.60764583]), 'currentState': array([29.446741, 10.985959,  4.340754], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:575
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.51511034, 14.69360244]), 'previousTarget': array([14.55864254, 16.35088802]), 'currentState': array([20.114721, 22.2066  ,  3.867724], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.32632751243990893
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3080564, 6.1357646, 3.7191277], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3299425673724459}
episode index:576
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.298575  ,  8.57464375]), 'previousTarget': array([19.298575  ,  8.57464375]), 'currentState': array([29.       , 11.       ,  5.2371764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3257619534928727
{'scaleFactor': 1.0, 'currentTarget': array([16.69174738, 22.2407635 ]), 'previousTarget': array([16.69174738, 22.2407635 ]), 'currentState': array([22.304348 , 30.517159 ,  3.1654956], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:577
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.        , 9.        , 0.73878163], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.32674737961876515
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.002681  , 6.4643745 , 0.13902158], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4643769960373043}
episode index:578
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.09555291,  5.86502597]), 'previousTarget': array([12.26582832,  6.70960666]), 'currentState': array([20.996355 ,  7.2700586,  4.1625686], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.3276834736311144
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8730392, 5.086552 , 2.5488985], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8773190969322601}
episode index:579
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.13415154, 12.37433993]), 'previousTarget': array([19.13415154, 12.37433993]), 'currentState': array([28.      , 17.      ,  5.383364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3271185021248539
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.12855827, 4.97275796]), 'currentState': array([13.054659 ,  3.6432345,  2.398781 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.168129680411916}
episode index:580
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.      , 8.      , 5.191758], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.328259434134966
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.922439 , 6.2254276, 5.443406 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5338079281890726}
episode index:581
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.23793222,  3.37478033]), 'previousTarget': array([15.19419324,  2.96116135]), 'currentState': array([23.048828 ,  1.4392387,  2.983708 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.32899218302340016
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.045375 , 3.634303 , 2.0630593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3664504823676682}
episode index:582
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9450474, 6.3008604, 4.535447 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6079030881553742}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.33014313982781973
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9450474, 6.3008604, 4.535447 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6079030881553742}
episode index:583
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.16358629,  8.53116159]), 'previousTarget': array([13.86188451,  8.93861534]), 'currentState': array([24.609707, 11.813052,  5.034164], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.32985273751837296
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.972581 , 4.4776783, 2.909319 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.040562521662805}
episode index:584
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13376467, 15.72986847]), 'previousTarget': array([15.13376467, 15.72986847]), 'currentState': array([22.        , 23.        ,  0.44914407], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.33014330895834015
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.514003 , 6.2793236, 4.615322 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.378719585454988}
episode index:585
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.55962573, 14.12237035]), 'previousTarget': array([ 3.55962573, 14.12237035]), 'currentState': array([ 2.      , 24.      ,  4.232254], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.3310040093892135
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3330173, 4.697831 , 6.079614 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7322375761884766}
episode index:586
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.37426118, 23.51003438]), 'previousTarget': array([21.37426118, 23.51003438]), 'currentState': array([28.       , 31.       ,  5.8766704], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.33071638738005127
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8692384, 5.0786858, 3.212229 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8727925322146518}
episode index:587
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.18761806, 17.86266529]), 'previousTarget': array([14.18761806, 17.86266529]), 'currentState': array([20.       , 26.       ,  4.4541693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.3309074275063202
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.994803 , 4.6903934, 3.4621148], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0418680962835336}
episode index:588
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.6049589 , 17.50595915]), 'previousTarget': array([19.76644456, 19.09524253]), 'currentState': array([27.200756 , 24.010101 ,  3.8442297], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3303456152355115
{'scaleFactor': 1.0, 'currentTarget': array([12.44486019, 13.34530768]), 'previousTarget': array([12.44486019, 13.34530768]), 'currentState': array([19.10188  , 20.807486 ,  0.9597757], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:589
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.79745222, 20.03184721]), 'previousTarget': array([ 3.79745222, 20.03184721]), 'currentState': array([ 3.       , 30.       ,  2.2753596], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.33100219984121104
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.382695 , 5.802679 , 4.8497987], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.805538498611503}
episode index:590
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.      , 0.      , 4.354058], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.3319120875111396
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2120733, 4.246174 , 1.130483 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7830893608276992}
episode index:591
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.      , 23.      ,  4.415666], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.3323530577119679
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0328858, 4.4932766, 5.1749434], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0313313180723793}
episode index:592
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.90535746, 6.04106794]), 'previousTarget': array([4.90535746, 6.04106794]), 'currentState': array([ 4.        , 16.        ,  0.42840093], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.3322974545645823
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3478003, 6.560504 , 4.8092084], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0619743293743884}
episode index:593
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.005687 ,  1.1507167,  3.6138902], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.133390194829888}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.332150262383556
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0719357, 5.554261 , 3.3803554], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2067524733237642}
episode index:594
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.38259632, 8.21415834]), 'previousTarget': array([8.598156 , 9.3177872]), 'currentState': array([15.63185 , 15.102432,  5.030903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3321596811037499
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6285822, 6.6993346, 4.560104 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1836952237802856}
episode index:595
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.52001696, 14.7000106 ]), 'previousTarget': array([20.52001696, 14.7000106 ]), 'currentState': array([29.       , 20.       ,  3.5393505], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3316023662025691
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.102756 , 3.9444575, 4.9873357], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.277386441310283}
episode index:596
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.      , 27.      ,  1.885912], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33104691835298355
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.881943 , 6.93956  , 2.6411815], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.4738292278767213}
episode index:597
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.96128974, 8.36221099]), 'previousTarget': array([6.96128974, 8.36221099]), 'currentState': array([12.        , 17.        ,  0.56987274], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3304933281885137
{'scaleFactor': 1.0, 'currentTarget': array([19.52614332,  6.10807836]), 'previousTarget': array([19.52614332,  6.10807836]), 'currentState': array([29.497175 ,  6.8686852,  5.058482 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:598
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.55864254, 16.35088802]), 'previousTarget': array([14.55864254, 16.35088802]), 'currentState': array([21.       , 24.       ,  6.2086825], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.3306887036235309
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.620504 , 5.065294 , 3.6389375], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6239297811514144}
episode index:599
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.48876558, 11.63670822]), 'previousTarget': array([ 7.48876558, 11.63670822]), 'currentState': array([11.        , 21.        ,  0.95294154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3301375557841583
{'scaleFactor': 1.0, 'currentTarget': array([19.03503615, 20.84203327]), 'previousTarget': array([19.03503615, 20.84203327]), 'currentState': array([25.66632  , 28.32709  ,  0.6822753], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:600
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.04711  ,  5.5684714,  3.8878899], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.079023048017394}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.3312027162570632
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.402768 , 3.923376 , 3.3134544], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.768297910031302}
episode index:601
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.36663175, 18.73772657]), 'previousTarget': array([14.36663175, 18.73772657]), 'currentState': array([20.       , 27.       ,  5.1341777], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3306525456320515
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 3.8966181, 11.554123 ,  1.9468342], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.6463507934278825}
episode index:602
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.08371249, 12.12649666]), 'previousTarget': array([ 5.89568474, 13.06116265]), 'currentState': array([ 5.201171 , 22.125807 ,  4.4402146], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33010419978523214
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.109061  , -0.23692656,  5.454459  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.829731110261491}
episode index:603
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 8.       , 2.1149032], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.33032128218197526
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1946454, 5.695539 , 3.7770565], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9347040372122377}
episode index:604
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.34738085,  4.00526258]), 'previousTarget': array([11.01947422,  4.62378286]), 'currentState': array([20.178722 ,  2.176404 ,  4.1439342], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.330985715384409
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.472448 , 4.723839 , 1.8073416], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5472403465550004}
episode index:605
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  3.       ,  5.4214973], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3304395343359199
{'scaleFactor': 1.0, 'currentTarget': array([14.59428978,  5.86322805]), 'previousTarget': array([14.59428978,  5.86322805]), 'currentState': array([24.554058 ,  6.7593393,  6.180453 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:606
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7006642 , 16.97139481]), 'previousTarget': array([10.7006642 , 16.97139481]), 'currentState': array([15.       , 26.       ,  3.5769231], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3308056693323391
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0695477, 4.71246  , 5.544952 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9517493413006703}
episode index:607
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.68944732, 12.1768175 ]), 'previousTarget': array([12.68944732, 12.1768175 ]), 'currentState': array([20.       , 19.       ,  1.9649881], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3302615810604109
{'scaleFactor': 1.0, 'currentTarget': array([3.28303657, 9.44517498]), 'previousTarget': array([3.28303657, 9.44517498]), 'currentState': array([-0.3200605, 18.773502 ,  2.1304343], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:608
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.      , 15.      ,  5.950196], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.33110342278836413
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5564754, 5.476988 , 4.6747417], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5202896653093099}
episode index:609
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.34363395, 5.27126166]), 'previousTarget': array([8.25608804, 5.75140493]), 'currentState': array([16.145868 ,  7.250201 ,  2.8395116], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.33138003525903764
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8335624, 6.394776 , 5.4386644], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4046712084203385}
episode index:610
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.       , 5.       , 2.6588528], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.3321629286026746
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.743658, 5.97962 , 5.801294], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0126038179709125}
episode index:611
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.       , 21.       ,  2.1163287], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.33209008657777944
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.964733 , 5.5803795, 2.3680801], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0486621477302362}
episode index:612
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.67206508, 11.19548901]), 'previousTarget': array([11.67206508, 11.19548901]), 'currentState': array([19.       , 18.       ,  4.9800262], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33154834092267704
{'scaleFactor': 1.0, 'currentTarget': array([5.33226856, 7.54633728]), 'previousTarget': array([5.33226856, 7.54633728]), 'currentState': array([ 6.6261873, 17.462273 ,  3.039678 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:613
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.44374643, 8.77067238]), 'previousTarget': array([9.92893219, 9.92893219]), 'currentState': array([15.187464 , 16.154593 ,  2.9624097], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.3323812315618647
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.440936 , 6.936295 , 2.8768535], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.413614564135975}
episode index:614
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.44310917, 11.45069462]), 'previousTarget': array([12.44310917, 11.45069462]), 'currentState': array([20.       , 18.       ,  2.1990418], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.3325907286933383
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.797047 , 6.2990885, 4.505548 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3148462792315543}
episode index:615
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.73033432, 12.53212805]), 'previousTarget': array([10.14357069, 14.25842724]), 'currentState': array([15.048708 , 21.000593 ,  4.3853593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.33242505513753495
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.7691636, 6.355963 , 3.6393049], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2290303172667163}
episode index:616
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.49164863, 11.73290659]), 'previousTarget': array([ 9.45299804, 11.67949706]), 'currentState': array([13.095352 , 20.610176 ,  2.7790344], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.33280945732122963
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.512264 , 6.886283 , 4.7120533], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4176445111373317}
episode index:617
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.70122561, 17.63647286]), 'previousTarget': array([ 9.83772234, 19.51316702]), 'currentState': array([13.188095, 27.008865,  4.592245], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.33366261087676374
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.580925 , 5.842009 , 3.8263566], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.791173661831826}
episode index:618
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.37433993, 19.13415154]), 'previousTarget': array([12.37433993, 19.13415154]), 'currentState': array([17.       , 28.       ,  2.3919559], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.3333466467446324
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0117273, 4.7000794, 2.9496393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0552461978832959}
episode index:619
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.35993005, 17.14168465]), 'previousTarget': array([11.35993005, 17.14168465]), 'currentState': array([16.       , 26.       ,  2.9203067], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3328089908627862
{'scaleFactor': 1.0, 'currentTarget': array([19.3819685 ,  5.90505052]), 'previousTarget': array([19.3819685 ,  5.90505052]), 'currentState': array([29.362226  ,  6.5331035 ,  0.21775281], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:620
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  3.       ,  0.9332797], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.33367201312028416
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5835876, 5.8624077, 3.592725 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0413076179083691}
episode index:621
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.01395434, 19.39021856]), 'previousTarget': array([ 9.40026527, 21.34384241]), 'currentState': array([11.700751, 29.022514,  4.942856], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33313556293841873
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.943549  ,  8.000187  ,  0.17957248], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.65784485610545}
episode index:622
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.93595004, 10.08636336]), 'previousTarget': array([11.93595004, 10.08636336]), 'currentState': array([20.       , 16.       ,  5.5108933], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.33284102978057795
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.771612 , 4.8111753, 3.928475 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7943803161158455}
episode index:623
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.2178089 , 17.61176538]), 'previousTarget': array([ 9.83772234, 19.51316702]), 'currentState': array([12.389483 , 27.09546  ,  4.3118896], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3323076306943911
{'scaleFactor': 1.0, 'currentTarget': array([ 3.85674185, 13.07558829]), 'previousTarget': array([ 3.85674185, 13.07558829]), 'currentState': array([ 2.4550223, 22.97686  ,  1.5725292], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:624
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.36882594, 17.35236179]), 'previousTarget': array([ 8.36882594, 17.35236179]), 'currentState': array([11.        , 27.        ,  0.29123834], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.3322745962155987
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.764873 , 4.2247853, 4.524827 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9276240795556465}
episode index:625
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.      , 13.      ,  5.301165], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.332626687079731
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1721754, 5.5270886, 3.8938544], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9813847322786491}
episode index:626
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.65062118, 18.58581637]), 'previousTarget': array([13.07942743, 19.29437161]), 'currentState': array([16.047346, 27.567398,  4.045562], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33209618199666924
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.339002 ,  7.2365093,  6.2238097], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.672217362209495}
episode index:627
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.21147869, 18.22104427]), 'previousTarget': array([12.21147869, 18.22104427]), 'currentState': array([17.        , 27.        ,  0.37574977], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3315673664202414
{'scaleFactor': 1.0, 'currentTarget': array([ 6.51552897, 10.83089796]), 'previousTarget': array([ 6.51552897, 10.83089796]), 'currentState': array([ 9.031083 , 20.509327 ,  5.7001805], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:628
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.       , 9.       , 5.7212176], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.3321585911079669
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5655184, 5.0689754, 5.6027884], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5670371431971546}
episode index:629
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.83975519, 16.21982659]), 'previousTarget': array([20.83975519, 16.21982659]), 'currentState': array([29.       , 22.       ,  3.7552745], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3316313552490654
{'scaleFactor': 1.0, 'currentTarget': array([18.27462288,  2.73016197]), 'previousTarget': array([18.27462288,  2.73016197]), 'currentState': array([28.131563 ,  1.0447161,  4.7103515], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:630
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.2381294 ,  6.83069542]), 'previousTarget': array([13.2381294 ,  6.83069542]), 'currentState': array([23.       ,  9.       ,  2.1820765], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33110579050223643
{'scaleFactor': 1.0, 'currentTarget': array([10.94978702,  9.8458222 ]), 'previousTarget': array([10.94978702,  9.8458222 ]), 'currentState': array([18.70351 , 16.160866,  5.865286], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:631
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.41020921, 21.66654394]), 'previousTarget': array([11.41020921, 21.66654394]), 'currentState': array([15.       , 31.       ,  1.1151289], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.3308627790089715
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.188031 , 4.990042 , 2.3941875], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.188072927757503}
episode index:632
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.58623374,  5.99965972]), 'previousTarget': array([12.26582832,  6.70960666]), 'currentState': array([20.429863,  7.761183,  4.059723], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3303400889947393
{'scaleFactor': 1.0, 'currentTarget': array([17.03151424, 21.36884723]), 'previousTarget': array([16.92821689, 19.68415902]), 'currentState': array([22.954012 , 29.426392 ,  1.3592596], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:633
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.22943469, 22.64068988]), 'previousTarget': array([21.22943469, 22.64068988]), 'currentState': array([28.      , 30.      ,  4.989341], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3303517834930109
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8559113, 6.9902034, 4.3472095], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2956150706933984}
episode index:634
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.      , 2.      , 4.541193], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.331093954841115
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.681173 , 3.8804264, 1.2982615], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1640858144779829}
episode index:635
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.90535746, 17.04106794]), 'previousTarget': array([ 3.90535746, 17.04106794]), 'currentState': array([ 3.       , 27.       ,  3.2335136], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.3312358420538723
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.035425 , 6.1103315, 4.2951365], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1108965128144586}
episode index:636
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.82166324, 12.40634498]), 'previousTarget': array([ 2.82166324, 12.40634498]), 'currentState': array([ 0.       , 22.       ,  5.4399524], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.3318091189392659
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.111899 , 3.3956954, 2.8888242], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.608202244568119}
episode index:637
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.7107353, 3.8694534, 1.2842628], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.553207456681224}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.33284076608826396
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2759929, 5.7879124, 1.9458055], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8955227469006817}
episode index:638
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 17.92893219]), 'previousTarget': array([17.92893219, 17.92893219]), 'currentState': array([25.       , 25.       ,  5.5607886], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33231988852005073
{'scaleFactor': 1.0, 'currentTarget': array([ 4.7627823 , 10.07035932]), 'previousTarget': array([ 4.7627823 , 10.07035932]), 'currentState': array([ 4.2954416, 20.059433 ,  3.1545236], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:639
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.020521 , 4.2857666, 2.9651103], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1430434249857773}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3333475136942382
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.051586 , 4.636902 , 3.3623815], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.36674431631550386}
episode index:640
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.75304952, 12.19131191]), 'previousTarget': array([10.75304952, 12.19131191]), 'currentState': array([17.      , 20.      ,  5.558851], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.33324145868028104
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0450983, 6.857708 , 4.9540186], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1315040219275803}
episode index:641
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5.000000e+00, 2.100000e+01, 2.079266e-02], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.334103052781583
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6785274, 5.5917435, 4.851624 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.779779316522425}
episode index:642
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.13370384, 18.03759412]), 'previousTarget': array([ 6.13370384, 18.03759412]), 'currentState': array([ 7.       , 28.       ,  2.9193506], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3341087313945182
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2004843, 5.7992325, 4.857195 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4421979955001643}
episode index:643
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.       ,  3.       ,  3.6044931], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.33502276239301726
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9994392, 6.327679 , 3.1650393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6618094770674263}
episode index:644
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.29125995, 14.15034377]), 'previousTarget': array([18.22755129, 15.70801771]), 'currentState': array([25.312542 , 20.121862 ,  4.7770147], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.3356972125998578
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.762184 , 5.8618145, 3.3793652], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.961636353810588}
episode index:645
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.04684785, 14.05494547]), 'previousTarget': array([ 4.04684785, 14.05494547]), 'currentState': array([ 3.       , 24.       ,  1.4744794], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33517755747199424
{'scaleFactor': 1.0, 'currentTarget': array([ 6.8175957 , 17.28672467]), 'previousTarget': array([ 6.8175957 , 17.28672467]), 'currentState': array([ 8.280987 , 27.17907  ,  3.2849536], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:646
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.       , 8.       , 3.9036484], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.6055512754639887}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3360856983328226
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5839496, 6.0871477, 5.0387883], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7852419951355416}
episode index:647
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.51316702,  7.83772234]), 'previousTarget': array([13.51316702,  7.83772234]), 'currentState': array([23.       , 11.       ,  3.3984766], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.3369767964024998
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9322085, 6.7977448, 4.161552 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0250676403733294}
episode index:648
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.71523309,  9.28609324]), 'previousTarget': array([15.71523309,  9.28609324]), 'currentState': array([25.       , 13.       ,  6.1255097], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3364575717547302
{'scaleFactor': 1.0, 'currentTarget': array([ 4.17747582, 17.18121093]), 'previousTarget': array([ 4.17747582, 17.18121093]), 'currentState': array([ 3.5037699, 27.158491 ,  1.2182785], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:649
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.685147 ,  9.507038 ,  2.5240333], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06254220525333}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.33684307823193443
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2278593, 5.061796 , 4.9536567], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7732178591593104}
episode index:650
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.      , 2.      , 5.155127], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.33775739815463035
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.660812 , 3.548981 , 1.6362255], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.205391677338546}
episode index:651
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.11152606, 21.17993553]), 'previousTarget': array([ 8.11152606, 21.17993553]), 'currentState': array([10.       , 31.       ,  2.9202764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3372393653353748
{'scaleFactor': 1.0, 'currentTarget': array([21.26546223, 17.92462933]), 'previousTarget': array([21.26546223, 17.92462933]), 'currentState': array([29.09471 , 24.145794,  5.870814], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:652
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.96045746, 19.2101958 ]), 'previousTarget': array([ 7.96045746, 19.2101958 ]), 'currentState': array([10.       , 29.       ,  2.1748114], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.3369763074951441
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.644643 , 6.2396774, 3.5609157], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3972703770570114}
episode index:653
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.09529089, 9.22197586]), 'previousTarget': array([4.09529089, 9.22197586]), 'currentState': array([ 2.       , 19.       ,  2.3442261], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.3376623959252849
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.675047 , 6.4026623, 5.7681212], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9294978948122472}
episode index:654
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.92893219, 9.92893219]), 'previousTarget': array([9.92893219, 9.92893219]), 'currentState': array([17.       , 17.       ,  1.3928481], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.3373707946210205
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.437471 , 5.2900734, 1.5591381], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5892261394420428}
episode index:655
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([21.        , 17.        ,  0.08149573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.33783610072995907
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.064746 , 6.662443 , 3.8049812], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9741836544706037}
episode index:656
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.      , 0.      , 4.372242], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.33824275212479554
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.854545 , 3.5186968, 0.9838948], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4884274715298842}
episode index:657
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.1613009 ,  7.21114562]), 'previousTarget': array([17.1613009 ,  7.21114562]), 'currentState': array([27.       ,  9.       ,  1.1768368], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.3388989837261335
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8121843, 5.9397354, 4.9684944], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.041351195988784}
episode index:658
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.68544503, 18.97865778]), 'previousTarget': array([11.68544503, 18.97865778]), 'currentState': array([16.       , 28.       ,  4.1882844], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33838472123186014
{'scaleFactor': 1.0, 'currentTarget': array([ 8.63044602, 17.76009324]), 'previousTarget': array([ 9.72992251, 19.18435681]), 'currentState': array([11.366997 , 27.378372 ,  4.4041657], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:659
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.22755129, 15.70801771]), 'previousTarget': array([18.22755129, 15.70801771]), 'currentState': array([26.        , 22.        ,  0.40479195], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33787201710878156
{'scaleFactor': 1.0, 'currentTarget': array([19.12271056,  8.14771486]), 'previousTarget': array([19.12271056,  8.14771486]), 'currentState': array([28.883213 , 10.323167 ,  1.1120446], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:660
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.99503719, 15.0496281 ]), 'previousTarget': array([ 3.99503719, 15.0496281 ]), 'currentState': array([ 3.     , 25.     ,  5.52357], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33736086428410866
{'scaleFactor': 1.0, 'currentTarget': array([22.4013186 , 24.06170993]), 'previousTarget': array([22.4013186 , 24.06170993]), 'currentState': array([29.14341  , 31.447115 ,  2.8475108], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:661
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.14788245,  2.71341175]), 'previousTarget': array([18.14788245,  2.71341175]), 'currentState': array([28.       ,  1.       ,  3.4829545], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33685125572778823
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.3030176, 14.295414 ,  5.984677 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.30035163674217}
episode index:662
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.67949706,  9.45299804]), 'previousTarget': array([11.67949706,  9.45299804]), 'currentState': array([20.       , 15.       ,  5.2025604], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.33768011487709193
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5360703, 5.292634 , 3.7620652], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5636965096879993}
episode index:663
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.       , 6.       , 0.1806764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.338020815292115
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0905447, 6.2471848, 3.8180153], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2504671734872004}
episode index:664
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.73780149, 17.18187745]), 'previousTarget': array([14., 17.]), 'currentState': array([18.0995  , 25.622978,  3.523796], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.33807978646750053
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.156186, 6.632866, 4.754683], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8380078774543267}
episode index:665
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.17127813,  3.84288535]), 'previousTarget': array([11.17127813,  3.84288535]), 'currentState': array([21.      ,  2.      ,  0.723214], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.3382176381597774
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.883434 , 6.4648976, 3.4885502], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.469528070024302}
episode index:666
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([19.       ,  5.       ,  5.6241574], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.3384090371657894
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.706068, 4.599481, 3.688867], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8117557885354534}
episode index:667
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([26.      ,  5.      ,  5.528554], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.3388172829777161
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.531539 , 3.1460342, 2.49936  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9286582657634956}
episode index:668
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.05572809,  8.52786405]), 'previousTarget': array([12.05572809,  8.52786405]), 'currentState': array([21.        , 13.        ,  0.78446233], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3383108296399318
{'scaleFactor': 1.0, 'currentTarget': array([19.59589651,  8.55679871]), 'previousTarget': array([19.59589651,  8.55679871]), 'currentState': array([29.311586, 10.924365,  1.316539], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:669
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.22943469, 22.64068988]), 'previousTarget': array([21.22943469, 22.64068988]), 'currentState': array([28.     , 30.     ,  3.45725], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.338453994561054
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6243196, 5.791081 , 4.2349744], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8757538867193215}
episode index:670
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.        , 3.        , 0.86306196], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.3389465694701578
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.438179 , 4.8696175, 1.9679564], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.4571656771194342}
episode index:671
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.25278872, 16.38476052]), 'previousTarget': array([ 8.25278872, 16.38476052]), 'currentState': array([11.      , 26.      ,  2.240788], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.3394081112244707
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7881927, 6.025796 , 4.3866296], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5876819949324201}
episode index:672
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.55448868, 18.67192356]), 'previousTarget': array([10.21367154, 17.74453044]), 'currentState': array([15.877492, 27.68922 ,  5.800805], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.33921987584854874
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.589374, 5.473506, 2.353095], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6584082208560247}
episode index:673
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 2.       , 0.9581521], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.34008563967433414
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.865858 , 4.321266 , 0.5974153], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6918624584150777}
episode index:674
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.01725007, 11.45520022]), 'previousTarget': array([ 7.01725007, 11.45520022]), 'currentState': array([10.      , 21.      ,  1.080081], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33958180909703883
{'scaleFactor': 1.0, 'currentTarget': array([20.74941575, 17.47952158]), 'previousTarget': array([20.74941575, 17.47952158]), 'currentState': array([28.587152, 23.689987,  4.210163], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:675
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.21621104, 17.24867144]), 'previousTarget': array([ 2.21621104, 17.24867144]), 'currentState': array([ 0.       , 27.       ,  2.9776769], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3390794691427533
{'scaleFactor': 1.0, 'currentTarget': array([4.34037662, 7.73694324]), 'previousTarget': array([4.4894239 , 9.09329876]), 'currentState': array([ 1.9973886, 17.45859  ,  4.971269 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:676
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.31952431, 16.49092299]), 'previousTarget': array([16.92893219, 16.92893219]), 'currentState': array([22.001188, 23.931042,  2.584339], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.3391528878265516
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8621492, 4.8475795, 3.9920254], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.868376782644123}
episode index:677
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.86197056, 17.89633523]), 'previousTarget': array([10.86197056, 17.89633523]), 'currentState': array([15.       , 27.       ,  3.3268034], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.33981147964510716
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0800095, 5.3539653, 3.9829104], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9857352656955608}
episode index:678
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.87347886, 5.42173715]), 'previousTarget': array([4.87347886, 5.42173715]), 'currentState': array([ 2.       , 15.       ,  0.5298907], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.33977002103215287
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6423273, 4.699337 , 1.1337662], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.669621880829813}
episode index:679
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.19366026,  4.43301939]), 'previousTarget': array([16.04504527,  3.94809093]), 'currentState': array([24.174698 ,  3.8174806,  2.1292417], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.339539412939913
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.481206 , 6.0442276, 6.224681 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8122864890042831}
episode index:680
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.01947422,  4.62378286]), 'previousTarget': array([11.01947422,  4.62378286]), 'currentState': array([21.       ,  4.       ,  1.6713328], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.3401715770116681
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3002663, 5.7814136, 3.59835  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5170034628576292}
episode index:681
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.14788245,  2.71341175]), 'previousTarget': array([18.14788245,  2.71341175]), 'currentState': array([28.       ,  1.       ,  5.4924107], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.3405510263217187
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9807014, 3.099732 , 2.845919 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.7448491739976815}
episode index:682
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.15384615, 7.76923077]), 'previousTarget': array([6.15384615, 7.76923077]), 'currentState': array([10.        , 17.        ,  0.50930923], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.3406273952851683
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5920696, 5.93214  , 4.681576 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.104278581534896}
episode index:683
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.71523309, 4.71390676]), 'currentState': array([13.02317  ,  1.3035517,  2.7493753], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.833741825376457}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.34153378214878644
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.437106 , 3.4771712, 2.9599648], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.584319847152264}
episode index:684
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.50290592, 20.0680703 ]), 'previousTarget': array([14.85504245, 21.42507074]), 'currentState': array([18.41742  , 28.77712  ,  3.8864145], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.3417648817805388
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7085123, 6.6131525, 4.1025887], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.761888387245833}
episode index:685
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.79631762,  4.94047809]), 'previousTarget': array([13.01539647,  4.5547002 ]), 'currentState': array([24.796133 ,  4.8797197,  1.0380014], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.34153338270842293
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.041157 , 6.1024914, 5.6329317], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5164084731724028}
episode index:686
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.1966997 , 7.64245149]), 'previousTarget': array([8.92040615, 6.80941823]), 'currentState': array([17.65895  , 12.970707 ,  1.5318497], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.3419347753696076
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.696095 , 3.6792798, 3.4061432], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4929333777190275}
episode index:687
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.57990501, 20.12559368]), 'previousTarget': array([ 2.57990501, 20.12559368]), 'currentState': array([ 1.      , 30.      ,  1.578849], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3414377771495936
{'scaleFactor': 1.0, 'currentTarget': array([ 4.72169255, 21.21736117]), 'previousTarget': array([ 4.72169255, 21.21736117]), 'currentState': array([ 4.550107 , 31.215889 ,  0.7343141], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:688
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.06466425, 18.56922195]), 'previousTarget': array([21.6284586 , 20.24275371]), 'currentState': array([28.704142 , 25.022003 ,  3.9850516], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.34122426600551986
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.3854866, 4.2464423, 0.7678733], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8464331639786122}
episode index:689
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.06849396,  4.16841248]), 'previousTarget': array([12.06849396,  4.16841248]), 'currentState': array([22.      ,  3.      ,  3.103542], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.34185701541623525
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0331135, 5.948694 , 2.0329046], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4026204768709212}
episode index:690
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.01926591,  3.33697143]), 'previousTarget': array([10.19419324,  3.96116135]), 'currentState': array([21.749891 ,  1.0315583,  5.9878335], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.34136228746338976
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.03224  , 5.8267384, 4.7465296], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1429246264471447}
episode index:691
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.06787877, 7.08395066]), 'previousTarget': array([7.56338512, 6.63124508]), 'currentState': array([17.967955  , 11.643408  ,  0.02729386], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.34179761884290033
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5786595, 5.2439623, 3.2309754], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6279844382894408}
episode index:692
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.      , 11.      ,  4.395393], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.999999999999999}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.34207050185027726
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5060706, 6.9120846, 5.6454263], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9779218653051458}
episode index:693
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.       , 1.       , 3.7362533], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.3425912182669189
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5802193, 3.196186 , 1.6185364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8948348502058723}
episode index:694
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.16058871, 13.70462796]), 'previousTarget': array([13.16058871, 13.70462796]), 'currentState': array([20.       , 21.       ,  4.8205123], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.3431733490238465
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.233934 , 3.3101592, 2.6740112], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7059563269050073}
episode index:695
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.45299804, 14.67949706]), 'previousTarget': array([11.45299804, 14.67949706]), 'currentState': array([17.       , 23.       ,  1.6451405], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.34268028386720306
{'scaleFactor': 1.0, 'currentTarget': array([ 3.4935479 , 16.06163528]), 'previousTarget': array([ 3.4935479 , 16.06163528]), 'currentState': array([ 2.144133, 25.970171,  4.140436], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:696
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.24034735, 19.07722123]), 'previousTarget': array([ 3.24034735, 19.07722123]), 'currentState': array([ 2.       , 29.       ,  1.2206395], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.3425193879338477
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.16779  , 5.846749 , 4.1971045], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.018409596094243}
episode index:697
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.67949706, 15.45299804]), 'previousTarget': array([20.67949706, 15.45299804]), 'currentState': array([29.       , 21.       ,  3.9655762], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.34304665704464776
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0343847, 3.386412 , 2.9097111], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8804465942753434}
episode index:698
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.41130706, 21.59912935]), 'previousTarget': array([11.97261386, 20.84684968]), 'currentState': array([17.931417 , 30.519253 ,  5.9019156], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.34307954071307206
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.5868707, 5.82647  , 5.6449466], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9239742072770134}
episode index:699
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08772099,  4.3216372 ]), 'previousTarget': array([10.08772099,  4.3216372 ]), 'currentState': array([20.      ,  3.      ,  1.888713], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.34339500592698535
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.2801614, 4.9447446, 2.4118028], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.281353316412496}
episode index:700
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.9488032, 3.7606173, 0.9119389], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3965554351551464}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.3442893054905703
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.86646   , 4.328589  , 0.28899166], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6845624542733697}
episode index:701
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.59521791, 9.16653274]), 'previousTarget': array([11.13681661,  9.82178448]), 'currentState': array([17.00339  , 15.8836   ,  3.5174541], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3451133160161221
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.784938 , 4.1413684, 4.1457644], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9807199201763726}
episode index:702
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.05051351, 17.73642633]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 6.8725324, 27.702583 ,  6.1662426], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.34484623204185705
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.263631 , 4.029133 , 3.5129743], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.59353255522233}
episode index:703
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.42009499, 20.12559368]), 'previousTarget': array([ 7.42009499, 20.12559368]), 'currentState': array([ 9.       , 30.       ,  2.5154417], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.345287727764204
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.934183 , 4.2203593, 4.642353 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2167734729722923}
episode index:704
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.      , 10.      ,  2.084319], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.34568239112183097
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.3404894, 6.5706997, 4.7157116], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7035409504083336}
episode index:705
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.63117406, 6.35236179]), 'previousTarget': array([4.63117406, 6.35236179]), 'currentState': array([ 2.       , 16.       ,  0.9513586], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.34632821009975906
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.02337  , 6.800107 , 4.8736696], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8002586943611865}
episode index:706
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.      , 10.      ,  2.925326], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.34703063581869
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7164998, 5.7715387, 5.6265745], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4975462490834528}
episode index:707
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.52001696, 14.7000106 ]), 'previousTarget': array([20.52001696, 14.7000106 ]), 'currentState': array([29.       , 20.       ,  1.8383276], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.34654047955340933
{'scaleFactor': 1.0, 'currentTarget': array([ 5.5167871 , 10.22418882]), 'previousTarget': array([ 5.5167871 , 10.22418882]), 'currentState': array([ 6.501202 , 20.175617 ,  1.2175769], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:708
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4315276, 8.047109 , 5.3933873], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.4270945055472617}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.34744803882061187
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6906657, 6.4932194, 5.2477965], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5249235388405395}
episode index:709
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.20708033, 13.23716851]), 'previousTarget': array([20.20708033, 13.23716851]), 'currentState': array([2.900000e+01, 1.800000e+01, 1.263636e-02], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.34720620679620423
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3097787, 5.66239  , 2.6356003], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4677469254701458}
episode index:710
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.05608939, 19.95823859]), 'previousTarget': array([16.05608939, 19.95823859]), 'currentState': array([22.       , 28.       ,  1.3308744], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3467178717655485
{'scaleFactor': 1.0, 'currentTarget': array([ 8.39548021, 19.14985727]), 'previousTarget': array([ 8.39548021, 19.14985727]), 'currentState': array([10.728894 , 28.873806 ,  1.6970341], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:711
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.05695873, 15.60646161]), 'previousTarget': array([ 2.21621104, 17.24867144]), 'currentState': array([ 1.255005, 25.44277 ,  5.515453], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.34623090846250704
{'scaleFactor': 1.0, 'currentTarget': array([7.92787115, 5.39606615]), 'previousTarget': array([7.6471009 , 5.78830628]), 'currentState': array([17.837612 ,  6.736601 ,  4.6660576], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:712
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.29077192, 15.35377431]), 'previousTarget': array([16.12255352, 16.74047316]), 'currentState': array([23.661045 , 22.112404 ,  4.9779096], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3457453111154348
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.921263  , 0.36282456, 4.495049  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.019427100895614}
episode index:713
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.      , 20.      ,  4.385653], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.345261073985021
{'scaleFactor': 1.0, 'currentTarget': array([3.98801503, 9.12149436]), 'previousTarget': array([3.98801503, 9.12149436]), 'currentState': array([ 1.6034608, 18.833029 ,  2.4192095], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:714
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.307878, 8.123576, 4.600184], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.321140569328072}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.3459221171509122
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.743757 , 3.4875114, 3.5848048], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6854660810986837}
episode index:715
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.2875295, 9.025413 ]), 'previousTarget': array([5.2875295, 9.025413 ]), 'currentState': array([ 6.       , 19.       ,  1.5819173], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.34581713410951703
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.705737 , 6.2447996, 5.094491 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4309405832901765}
episode index:716
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.09529089, 9.22197586]), 'previousTarget': array([4.09529089, 9.22197586]), 'currentState': array([ 2.       , 19.       ,  3.8333642], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.34556559360959405
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0019884, 4.7653704, 1.9351833], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.011740881361814}
episode index:717
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.61208359, 11.80646171]), 'previousTarget': array([11., 13.]), 'currentState': array([15.221611 , 20.08494  ,  3.8321135], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.3463566126261317
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9136934, 6.090968 , 3.9921136], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.202824096836351}
episode index:718
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.       , 10.       ,  5.7465954], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.3469351746591233
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.056226 , 6.1375265, 4.711791 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1389152311728235}
episode index:719
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.      , 2.      , 6.204527], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.3469878981372266
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.988573 , 3.9989653, 1.3603561], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4068927703482461}
episode index:720
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2348411 , 20.67778634]), 'previousTarget': array([13.80053053, 19.45801444]), 'currentState': array([20.70134   , 29.0514    ,  0.31864464], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.3468168867306266
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.488645 , 6.2545643, 2.6273499], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3463675415463587}
episode index:721
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06116265,  5.89568474]), 'previousTarget': array([13.06116265,  5.89568474]), 'currentState': array([23.      ,  7.      ,  4.754547], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.34653562364256985
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.604472 , 4.0619636, 3.1338255], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8585594635239497}
episode index:722
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.76195515, 21.45948442]), 'previousTarget': array([18.09983411, 22.92608878]), 'currentState': array([22.576033, 29.595608,  3.637412], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.3470897543074233
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8045254, 5.070198 , 4.521427 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8075820992982862}
episode index:723
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.21719897, 10.65196555]), 'previousTarget': array([10.21719897, 10.65196555]), 'currentState': array([17.       , 18.       ,  3.6510267], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.3474802707119755
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6973076, 4.5033326, 4.2149243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7684828342934673}
episode index:724
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.64797467, 14.94420101]), 'previousTarget': array([10.06080701, 16.8085497 ]), 'currentState': array([13.882325 , 24.003466 ,  4.0935483], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.34758800843980536
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.314687 , 3.4007778, 3.8737848], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6298893698093637}
episode index:725
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.60059927, 8.27327206]), 'previousTarget': array([8.60059927, 8.27327206]), 'currentState': array([16.       , 15.       ,  4.7515383], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.3483675252979925
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.136558 , 6.3384104, 3.6215494], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3453588522142785}
episode index:726
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.16642602, 10.50386078]), 'previousTarget': array([6.25278872, 9.38476052]), 'currentState': array([10.829093  , 19.80896   ,  0.55473495], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3486485651217399
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.010985 , 5.4114933, 1.0535322], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0712038133981265}
episode index:727
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.186912, 5.844221, 2.322959], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.296834046281644}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.3495159434663529
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3356495, 6.601076 , 2.5245204], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0850430025906985}
episode index:728
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.40634498,  2.82166324]), 'previousTarget': array([12.40634498,  2.82166324]), 'currentState': array([22.       ,  0.       ,  5.7196155], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.3499634237209399
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.35779  , 5.3715057, 1.9545022], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4076967637663507}
episode index:729
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.      , 23.      ,  3.593437], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.3497873803832932
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.666821 , 6.7833266, 5.200384 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.903918092871008}
episode index:730
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 1.       , 10.       ,  0.7939495], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.3502240211195263
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6090117, 6.0268693, 5.883765 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0987868050476561}
episode index:731
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.76689977, 7.0642843 ]), 'previousTarget': array([5.75140493, 8.25608804]), 'currentState': array([ 9.249426 , 16.438292 ,  5.6872096], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.35040150395484304
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0451603, 3.5133076, 2.7104259], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7669106491787896}
episode index:732
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.28869484, 15.37447706]), 'previousTarget': array([ 5.47441167, 14.01382171]), 'currentState': array([ 7.521399 , 25.298208 ,  1.0556571], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.35038890444264553
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.074764 , 4.648319 , 2.7584386], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1308390075673775}
episode index:733
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.08094637, 21.78011795]), 'previousTarget': array([21.08094637, 21.78011795]), 'currentState': array([28.       , 29.       ,  4.8859696], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3499115353630234
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.869263  , 12.306822  ,  0.66774285], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.2680613785918}
episode index:734
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.42337349, 10.3376506 ]), 'previousTarget': array([ 6.42337349, 10.3376506 ]), 'currentState': array([ 9.       , 20.       ,  5.6676254], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.35037349121405204
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8398075, 5.260974 , 3.450496 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.858224708576122}
episode index:735
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.13606076,  2.64398987]), 'previousTarget': array([19.13606076,  2.64398987]), 'currentState': array([29.       ,  1.       ,  4.1796374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.35022681071789785
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0999646, 5.9858794, 4.3314085], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.334923938221063}
episode index:736
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.25821513, 10.06437066]), 'previousTarget': array([12.75958076, 10.33471177]), 'currentState': array([19.03176  , 16.355    ,  3.5301912], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.35002335103830434
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.977332 , 6.3633065, 3.7042751], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.401759140220097}
episode index:737
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.95033796,  9.47782117]), 'previousTarget': array([12.05572809,  8.52786405]), 'currentState': array([20.356756, 14.893736,  1.538768], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3495490646547836
{'scaleFactor': 1.0, 'currentTarget': array([19.17479748,  8.51057328]), 'previousTarget': array([19.17479748,  8.51057328]), 'currentState': array([28.881536 , 10.914574 ,  6.0428123], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:738
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.28609324, 5.71523309]), 'currentState': array([ 8.253286 , 13.144624 ,  4.8987303], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.770334559447729}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.35033731402318985
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.462414 , 6.245187 , 4.1190844], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3282758344327696}
episode index:739
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.      , 11.      ,  5.497901], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.3505062366285716
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2146444, 6.910101 , 4.710667 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9221232586190855}
episode index:740
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51954206,  8.10761017]), 'previousTarget': array([10.86266529,  9.18761806]), 'currentState': array([19.233362, 13.013661,  4.80928 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.35027765459955323
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.238929 , 5.3069644, 4.28731  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2763901049542625}
episode index:741
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.60429843, 10.56748722]), 'previousTarget': array([13.60429843, 10.56748722]), 'currentState': array([22.       , 16.       ,  1.2001855], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.35040871330462636
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.494069 , 3.7027686, 3.4780293], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9786490016739338}
episode index:742
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.31756858,  8.03861062]), 'previousTarget': array([10.31756858,  8.03861062]), 'currentState': array([19.       , 13.       ,  5.4660234], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.35032806698027286
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.2799845, 4.082211 , 0.6434154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1665157408358229}
episode index:743
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.92752121, 12.38472232]), 'previousTarget': array([ 2.54493299, 14.32925463]), 'currentState': array([ 0.22547182, 22.01275   ,  5.4216433 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.3508716035665945
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2639256, 3.5245786, 4.4696517], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2783377449205275}
episode index:744
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92893219, 14.92893219]), 'previousTarget': array([14.92893219, 14.92893219]), 'currentState': array([22.       , 22.       ,  6.0571218], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.35075683127958934
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1714   , 4.121546 , 2.1544862], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2075842164795203}
episode index:745
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.75862069, 18.10344828]), 'previousTarget': array([18.75862069, 18.10344828]), 'currentState': array([26.      , 25.      ,  5.372097], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.35112247278577113
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1456268, 6.699617 , 4.9672966], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.5154319040677526}
episode index:746
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.29437161, 13.07942743]), 'previousTarget': array([19.29437161, 13.07942743]), 'currentState': array([28.       , 18.       ,  1.0335608], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3506524293148397
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.704402 , 4.37726  , 1.9717364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.775174744629558}
episode index:747
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([29.       ,  5.       ,  1.2365773], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.35068796971271227
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6233902, 3.969905 , 1.9070418], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7193459406163583}
episode index:748
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.52139106,  9.83440086]), 'previousTarget': array([11.93595004, 10.08636336]), 'currentState': array([18.045008 , 16.421904 ,  3.4375963], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.35124785646316947
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2680228, 6.3574657, 5.1919575], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2005586454014114}
episode index:749
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 1.63245044, 20.56948226]), 'previousTarget': array([ 2.64398987, 19.13606076]), 'currentState': array([-0.48158216, 30.343472  ,  2.4197307 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.3511950740964841
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9624438, 6.1970696, 3.848069 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5841396856508552}
episode index:750
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.83040131, 18.88222331]), 'previousTarget': array([11.15384615, 19.76923077]), 'currentState': array([13.116701 , 28.32681  ,  3.4499555], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35072743751313323
{'scaleFactor': 1.0, 'currentTarget': array([12.07091605, 21.99558901]), 'previousTarget': array([12.07091605, 21.99558901]), 'currentState': array([15.912172 , 31.228397 ,  2.6749885], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:751
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80768079, 13.26537656]), 'previousTarget': array([16.80768079, 13.26537656]), 'currentState': array([25.       , 19.       ,  1.1576202], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.3507333329615268
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.22448  , 4.956593 , 3.8590891], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2252492837899345}
episode index:752
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 11.52786405]), 'previousTarget': array([18.05572809, 11.52786405]), 'currentState': array([27.       , 16.       ,  5.8778186], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3502675516428528
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.62150323, 11.954107  ,  3.5427437 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.217715135951012}
episode index:753
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.35236179, 4.63117406]), 'previousTarget': array([6.35236179, 4.63117406]), 'currentState': array([16.      ,  2.      ,  5.817158], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.35074538410389977
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.4211593, 5.3920007, 2.4354587], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.575360459100885}
episode index:754
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.76644456, 19.09524253]), 'previousTarget': array([19.76644456, 19.09524253]), 'currentState': array([27.       , 26.       ,  0.5942211], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3502808206812456
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.882222 , 1.8930093, 2.7060223], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.78700998363723}
episode index:755
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.76923077, 11.15384615]), 'previousTarget': array([19.76923077, 11.15384615]), 'currentState': array([29.       , 15.       ,  0.2355873], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.3503582536888399
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.680384 , 3.9034994, 3.045326 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.142133060829508}
episode index:756
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 7.       , 10.       ,  1.6035985], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.349895429047243
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.5050726 , 10.034017  ,  0.35087442], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.058287888123572}
episode index:757
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 8.       , 5.9695187], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.35063899345151267
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.7642374, 6.1517735, 5.064106 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1756555993206663}
episode index:758
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 2.      , 12.      ,  4.327374], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.3506088007821929
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.624518  , 6.919901  , 0.62519234], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3617727245288056}
episode index:759
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.68944732, 12.1768175 ]), 'previousTarget': array([12.68944732, 12.1768175 ]), 'currentState': array([20.        , 19.        ,  0.53314257], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.3503545172049898
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.724416 , 6.369133 , 3.9199786], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5489684892484465}
episode index:760
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.5838678 , 18.88011819]), 'previousTarget': array([ 6.75965265, 19.07722123]), 'currentState': array([ 6.0041466, 28.871283 ,  2.4289787], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.3502836000830373
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.839255, 6.646446, 5.774903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0144760900453025}
episode index:761
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.86955478, 14.45346293]), 'previousTarget': array([13.80355711, 16.15117234]), 'currentState': array([19.2674   , 22.139006 ,  3.9239924], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.35046038193368184
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.752824, 5.34729 , 2.061804], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7868972407567274}
episode index:762
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.128696 , 12.70585  ,  3.3774755], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.316781842408174}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.3510411865230789
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.136373 , 3.470518 , 5.6528287], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.4108961569909524}
episode index:763
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.96195868, 5.43597873]), 'previousTarget': array([5.09464254, 6.04106794]), 'currentState': array([ 4.0927114, 15.398128 ,  2.9417496], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.35078356972369507
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5696597, 6.782711 , 4.3855033], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3752663467672646}
episode index:764
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.4,  9.2]), 'previousTarget': array([19.4,  9.2]), 'currentState': array([29.      , 12.      ,  5.842406], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.3507324292161466
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8914037, 3.4850569, 3.9148953], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7577409301308295}
episode index:765
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.02945514, 5.23303501]), 'previousTarget': array([8.02945514, 5.23303501]), 'currentState': array([18.       ,  6.       ,  5.0134106], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.3506938765338341
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.062252 , 6.695632 , 2.9406996], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0008866087460597}
episode index:766
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.13606076,  7.35601013]), 'previousTarget': array([19.13606076,  7.35601013]), 'currentState': array([29.        ,  9.        ,  0.14894488], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35023664853313813
{'scaleFactor': 1.0, 'currentTarget': array([ 4.47364223, 21.40367321]), 'previousTarget': array([ 4.47364223, 21.40367321]), 'currentState': array([ 4.1529293, 31.39853  ,  1.659025 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:767
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.03454242,  3.8304548 ]), 'previousTarget': array([19.03454242,  3.8304548 ]), 'currentState': array([29.       ,  3.       ,  6.0771017], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.35018236184316615
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0831084, 6.58481  , 5.4066586], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9195691964023522}
episode index:768
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.51316702,  8.83772234]), 'previousTarget': array([16.51316702,  8.83772234]), 'currentState': array([26.      , 12.      ,  4.381005], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.3499443274779798
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.441796 , 3.471078 , 3.3395534], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6276346732813203}
episode index:769
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.60206577, 4.41743063]), 'previousTarget': array([6.60206577, 4.41743063]), 'currentState': array([16.      ,  1.      ,  4.634208], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.35060681322754256
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4233127, 3.455268 , 2.4494476], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.10047998829878}
episode index:770
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.95102525, 5.11986041]), 'previousTarget': array([4.13802944, 6.89633523]), 'currentState': array([ 1.1686039, 14.376927 ,  5.5853853], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.3510024713434266
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0471253, 6.920934 , 3.4217396], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9215121647950835}
episode index:771
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92893219, 14.92893219]), 'previousTarget': array([14.92893219, 14.92893219]), 'currentState': array([22.       , 22.       ,  1.6469903], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35054780492976934
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.3397827 , -0.24053645,  5.008993  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.409088649746315}
episode index:772
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.42173715, 4.87347886]), 'previousTarget': array([5.42173715, 4.87347886]), 'currentState': array([15.       ,  2.       ,  1.1478477], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.35125257912036306
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0961075, 3.3267052, 3.4057686], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.534703561806803}
episode index:773
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.211581 ,  3.104702 ,  3.0409021], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.54551471399452}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.3520650434884246
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3205843, 3.9268043, 3.105681 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7016732611191663}
episode index:774
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.52558833, 14.01382171]), 'previousTarget': array([ 4.52558833, 14.01382171]), 'currentState': array([ 4.       , 24.       ,  2.2404928], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35161076601295566
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.618392 , 13.137551 ,  6.0343966], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.905756671006666}
episode index:775
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.00943842,  5.56562776]), 'previousTarget': array([18.00943842,  5.56562776]), 'currentState': array([28.       ,  6.       ,  4.5007224], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.35175201756115815
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.6849494, 6.2872057, 4.287021 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8401784124918297}
episode index:776
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.21621104, 17.24867144]), 'previousTarget': array([ 2.21621104, 17.24867144]), 'currentState': array([ 0.      , 27.      ,  4.014216], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.35153955843523094
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.448003 , 5.553901 , 4.6143928], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.712399506508905}
episode index:777
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.13510138, 8.24975017]), 'previousTarget': array([9.6284586 , 9.24275371]), 'currentState': array([15.078079 , 15.446629 ,  3.7483513], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.3522618947964757
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.501296 , 6.63151  , 5.6487556], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7067870072870692}
episode index:778
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.26582832,  6.70960666]), 'previousTarget': array([12.26582832,  6.70960666]), 'currentState': array([22.       ,  9.       ,  1.2579532], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.3523138192298021
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0954676, 3.0088644, 3.3712006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2725910670144427}
episode index:779
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.33205022,  4.65852997]), 'previousTarget': array([12.01725627,  5.41277978]), 'currentState': array([21.317541 ,  4.12004  ,  4.1511717], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3518621348461742
{'scaleFactor': 1.0, 'currentTarget': array([19.87608248, 11.24830868]), 'previousTarget': array([19.87608248, 11.24830868]), 'currentState': array([29.095825, 15.12082 ,  5.151217], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:780
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.     , 7.     , 4.56073], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.35264156362357985
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.632796 , 5.5402904, 4.1879654], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6532630290191309}
episode index:781
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.37553739, 13.67995328]), 'previousTarget': array([ 8.37553739, 13.67995328]), 'currentState': array([12.       , 23.       ,  2.9545655], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.35267301513675114
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.619351 , 4.900053 , 3.4392104], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6224323616952077}
episode index:782
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08772099,  4.3216372 ]), 'previousTarget': array([10.08772099,  4.3216372 ]), 'currentState': array([20.      ,  3.      ,  5.774764], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35222260260145516
{'scaleFactor': 1.0, 'currentTarget': array([6.96313413, 8.71010988]), 'previousTarget': array([6.96313413, 8.71010988]), 'currentState': array([11.640073 , 17.549015 ,  3.8270197], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:783
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.       , 8.       , 2.8943725], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.35284852172235115
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.0145564, 6.192434 , 3.9761283], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5469316474246246}
episode index:784
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.72889436, 21.55013927]), 'previousTarget': array([10.72889436, 21.55013927]), 'currentState': array([14.      , 31.      ,  3.179054], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3523990331596475
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.98841  , 7.8969555, 4.223126 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.0609321181544056}
episode index:785
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.       , 9.       , 1.0910155], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.3524553674767752
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5284185, 5.8953967, 3.9417925], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7225815678469416}
episode index:786
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.3376506 ,  6.42337349]), 'previousTarget': array([10.3376506 ,  6.42337349]), 'currentState': array([20.       ,  9.       ,  2.7177758], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.35285754840573696
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.2806263, 3.4740725, 3.387838 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6869953743302821}
episode index:787
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06116265,  4.10431526]), 'previousTarget': array([13.06116265,  4.10431526]), 'currentState': array([23.       ,  3.       ,  1.0395886], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.35261555899153046
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9586716, 6.3298025, 4.330072 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6890055131347055}
episode index:788
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.97008486,  2.36188549]), 'previousTarget': array([16.10050506,  3.41421356]), 'currentState': array([2.5692892e+01, 2.3719549e-02, 3.9137082e+00], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.3525016135267126
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.660219 , 5.471069 , 2.2536504], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.725755961072035}
episode index:789
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.298575  , 4.42535625]), 'previousTarget': array([7.298575  , 4.42535625]), 'currentState': array([17.       ,  2.       ,  0.8126546], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35205540895262816
{'scaleFactor': 1.0, 'currentTarget': array([6.56882373, 9.9170534 ]), 'previousTarget': array([5.85246696, 9.76114878]), 'currentState': array([ 9.608437 , 19.443897 ,  0.6109893], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:790
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.42337349, 10.3376506 ]), 'previousTarget': array([ 6.42337349, 10.3376506 ]), 'currentState': array([ 9.       , 20.       ,  1.3658314], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.3521761014997978
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3729753, 6.845315 , 5.282051 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.300054061081108}
episode index:791
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  8.       ,  0.0508787], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3524292673781596
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.490216 , 6.95559  , 3.9959385], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0209431724175824}
episode index:792
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 7.       , 1.7716473], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.3529856167051022
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.660459 , 5.6466546, 5.3395977], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9243204665656979}
episode index:793
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.47347885, 7.15936116]), 'previousTarget': array([7.56338512, 6.63124508]), 'currentState': array([15.006686, 13.735894,  1.555383], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.3533503849486534
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7001543, 4.2026296, 3.494085 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0611388499278456}
episode index:794
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.41495392, 21.47423305]), 'previousTarget': array([19.41495392, 21.47423305]), 'currentState': array([26.    , 29.    ,  2.4626], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.3536823846417272
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.0391126, 6.3689876, 3.7303581], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3695461778077929}
episode index:795
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.14357069, 14.25842724]), 'previousTarget': array([10.14357069, 14.25842724]), 'currentState': array([15.       , 23.       ,  1.8994095], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3532380600379059
{'scaleFactor': 1.0, 'currentTarget': array([5.00325891, 5.41938359]), 'previousTarget': array([5.00325891, 5.41938359]), 'currentState': array([ 5.0809636, 15.419082 ,  2.243517 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:796
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.1380629 , 10.55659631]), 'previousTarget': array([14.25842724, 10.14357069]), 'currentState': array([21.396595 , 16.195448 ,  3.0081072], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.35326343798071186
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.917199 , 4.6148634, 3.778608 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.95550063322895}
episode index:797
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.75862069, 18.10344828]), 'previousTarget': array([18.75862069, 18.10344828]), 'currentState': array([26.       , 25.       ,  1.2090871], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.35302194017761684
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4161634, 6.837029 , 2.5444086], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3195246012683786}
episode index:798
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.      , 12.      ,  2.303294], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35258011046525434
{'scaleFactor': 1.0, 'currentTarget': array([17.77431459, 22.87178667]), 'previousTarget': array([17.77431459, 22.87178667]), 'currentState': array([23.589334 , 31.007236 ,  2.2087183], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:799
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.43186077, 14.01320241]), 'previousTarget': array([11.66007009, 15.54511097]), 'currentState': array([15.593535 , 22.57808  ,  4.0405116], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.3527704926881566
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5248227, 6.6214323, 5.19918  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.704253970706079}
episode index:800
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7006642 , 16.97139481]), 'previousTarget': array([10.7006642 , 16.97139481]), 'currentState': array([15.        , 26.        ,  0.50627583], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.3528253083107956
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.405486, 4.291871, 2.114627], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5737972477082844}
episode index:801
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.       , 12.       ,  0.8762614], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3523853765049218
{'scaleFactor': 1.0, 'currentTarget': array([ 2.61152153, 19.22125095]), 'previousTarget': array([ 2.98353498, 21.00394933]), 'currentState': array([ 0.95520586, 29.083128  ,  4.857328  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:802
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.08402493, 22.40836774]), 'previousTarget': array([20.08777354, 20.77358143]), 'currentState': array([26.63253  , 29.965952 ,  2.1453187], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3519465404196106
{'scaleFactor': 1.0, 'currentTarget': array([ 4.06665005, 10.31266461]), 'previousTarget': array([ 4.06665005, 10.31266461]), 'currentState': array([ 2.3363109, 20.161823 ,  3.4716418], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:803
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([17.       , 14.       ,  1.2141211], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35150879596635237
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.614417 ,  7.060338 ,  1.1544323], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.927878897234145}
episode index:804
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.8763446 , 17.15774669]), 'previousTarget': array([ 6.75965265, 19.07722123]), 'currentState': array([ 8.401619 , 27.04074  ,  4.2884474], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.3512436680373103
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4505763, 5.24902  , 4.278166 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5693071067975861}
episode index:805
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.08399589,  3.29339184]), 'previousTarget': array([18.08399589,  3.29339184]), 'currentState': array([28.        ,  2.        ,  0.38575304], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3514218255950806
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.6028852, 6.7594886, 4.611292 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8037461572969062}
episode index:806
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.      ,  8.      ,  4.883514], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.3512607722637841
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.0595446, 5.9661775, 3.177276 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1676868299826273}
episode index:807
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.52950917, 17.19980337]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.144141, 27.192375,  3.587453], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35082604358523983
{'scaleFactor': 1.0, 'currentTarget': array([ 3.60599915, 10.56398945]), 'previousTarget': array([ 3.60599915, 10.56398945]), 'currentState': array([ 1.175715 , 20.264181 ,  2.2030354], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:808
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.       , 12.       ,  1.5560544], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.3508015776331209
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8104577, 6.444707 , 5.1541266], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6565083064755497}
episode index:809
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.27327206, 8.60059927]), 'previousTarget': array([8.27327206, 8.60059927]), 'currentState': array([15.      , 16.      ,  5.884339], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.3512545763429544
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9635177, 6.198428 , 4.5843396], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.300354682312313}
episode index:810
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.03454242,  3.8304548 ]), 'previousTarget': array([19.03454242,  3.8304548 ]), 'currentState': array([29.        ,  3.        ,  0.21024863], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.3513357925002791
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4158897, 5.1638527, 1.19769  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.592561779156239}
episode index:811
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.47441167, 14.01382171]), 'previousTarget': array([ 5.47441167, 14.01382171]), 'currentState': array([ 6.       , 24.       ,  3.8352227], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.3513493839108212
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.451399, 5.011539, 3.691963], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.45154630928963474}
episode index:812
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.      ,  2.      ,  5.873128], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.3515834991532026
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4898875, 5.247018 , 0.9088287], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5301822305212232}
episode index:813
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.4072485, 9.790396 , 3.3246264], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.509357159392148}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.35201576475006546
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.302433 , 6.4024944, 5.390038 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4347321546573186}
episode index:814
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([2.       , 3.       , 2.6338518], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3519982661441131
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8463607, 3.8154366, 1.2577052], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.194485447516928}
episode index:815
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.41020921, 21.66654394]), 'previousTarget': array([11.41020921, 21.66654394]), 'currentState': array([15.       , 31.       ,  2.3782237], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.35217331564589754
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.90059  , 6.1895237, 5.570528 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.193670383703762}
episode index:816
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.01539647,  4.5547002 ]), 'previousTarget': array([13.01539647,  4.5547002 ]), 'currentState': array([23.      ,  4.      ,  5.684375], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3517422589560005
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.5094438, -0.2904818,  5.626367 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.314953497636189}
episode index:817
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.45520022,  2.98274993]), 'previousTarget': array([11.45520022,  2.98274993]), 'currentState': array([21.        ,  0.        ,  0.10447996], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.352022728583338
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.851294 , 4.5527687, 1.9976214], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.961622261020038}
episode index:818
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.4813721 , 6.32192191]), 'previousTarget': array([7.91263916, 7.11828302]), 'currentState': array([16.307081 , 11.023715 ,  4.6076465], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.3526118507235904
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.751146, 3.987228, 3.345704], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2609232193760478}
episode index:819
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  1.       ,  1.4533778], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35218183627148847
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.074317 ,  3.3732572,  5.3324604], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.25894296564827}
episode index:820
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([3.97209722, 8.42452265]), 'previousTarget': array([ 3.57662651, 10.3376506 ]), 'currentState': array([ 1.0972174, 18.002365 ,  4.712188 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.35287679712186176
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1531253, 6.234378 , 5.768446 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.496958745197379}
episode index:821
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.24275371, 21.6284586 ]), 'previousTarget': array([20.24275371, 21.6284586 ]), 'currentState': array([27.       , 29.       ,  5.8373704], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3524475066144142
{'scaleFactor': 1.0, 'currentTarget': array([6.14561433, 8.54618839]), 'previousTarget': array([6.14561433, 8.54618839]), 'currentState': array([ 9.219731 , 18.061954 ,  1.7633076], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:822
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.25842724, 5.14357069]), 'previousTarget': array([5.25842724, 5.14357069]), 'currentState': array([14.      , 10.      ,  5.982698], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.3530538374339531
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.414784 , 3.4444852, 2.7686253], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1026744802900894}
episode index:823
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.64398987, 7.13606076]), 'previousTarget': array([4.64398987, 7.13606076]), 'currentState': array([ 3.      , 17.      ,  2.109429], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35262537403900895
{'scaleFactor': 1.0, 'currentTarget': array([3.65905268, 8.75980566]), 'previousTarget': array([3.65905268, 8.75980566]), 'currentState': array([ 0.29977798, 18.178684  ,  1.4489671 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:824
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.40426895, 6.99755481]), 'previousTarget': array([9., 8.]), 'currentState': array([15.095918 , 13.388059 ,  3.2048879], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.35283503599475025
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.807215, 5.775116, 5.288809], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.966426095963326}
episode index:825
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.98544381,  6.35116767]), 'previousTarget': array([12.06849396,  5.83158752]), 'currentState': array([20.739988 ,  8.553181 ,  2.2760775], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.3527631082532616
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6241055, 5.62764  , 2.7294564], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7411634632001076}
episode index:826
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.12114334,  5.54604298]), 'previousTarget': array([13.06116265,  5.89568474]), 'currentState': array([21.08159 ,  6.434575,  4.520786], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.352770345483526
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.1452036, 4.9055424, 2.5768394], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.1732233409138284}
episode index:827
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  3.       ,  1.0837734], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.3527563306478141
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0973463, 5.962701 , 5.250594 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.459781432143492}
episode index:828
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.41495392, 5.47423305]), 'currentState': array([10.202087 , 12.123924 ,  4.0529366], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.82111161577089}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.35309054934570966
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.018287 , 6.752765 , 3.7393327], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7528605742085634}
episode index:829
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.282129 , 5.975833 , 3.9570184], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0157984992389668}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.35386995832240153
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.282129 , 5.975833 , 3.9570184], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0157984992389668}
episode index:830
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.42717045, 4.48445487]), 'previousTarget': array([6.04106794, 4.90535746]), 'currentState': array([17.208946 ,  2.4067488,  5.853344 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35344412203079817
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.829063 , 10.366673 ,  3.5618892], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.592640652184188}
episode index:831
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.05049257,  3.52345138]), 'previousTarget': array([15.11063647,  3.48340453]), 'currentState': array([26.97626  ,  2.3072457,  0.6772252], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3538733397053673
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3897142, 4.7804403, 2.1064613], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4069513567228096}
episode index:832
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 3.       , 13.       ,  2.2192545], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.3536511894255057
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.103726 , 6.9756117, 4.828741 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.738411393554386}
episode index:833
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0751728 , 19.76126347]), 'previousTarget': array([15.60946304, 21.57728599]), 'currentState': array([20.712618 , 28.020756 ,  3.7217946], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.35344215404681173
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1394289, 6.4342356, 3.163848 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3492034113072884}
episode index:834
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.75140493, 21.25608804]), 'previousTarget': array([ 8.75140493, 21.25608804]), 'currentState': array([11.       , 31.       ,  2.3442674], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35301887002998916
{'scaleFactor': 1.0, 'currentTarget': array([18.41694921,  2.35329313]), 'previousTarget': array([18.41694921,  2.35329313]), 'currentState': array([28.22788   ,  0.41793072,  0.96208715], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:835
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08636336, 11.93595004]), 'previousTarget': array([10.08636336, 11.93595004]), 'currentState': array([16.      , 20.      ,  0.889251], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.3535554869193541
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.7489076, 5.0351024, 4.024274 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.749259800846061}
episode index:836
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.51316702, 4.16227766]), 'previousTarget': array([7.51316702, 4.16227766]), 'currentState': array([17.       ,  1.       ,  1.5075326], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.35331549199146467
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.618383 , 6.3037887, 3.4723167], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.078227173724808}
episode index:837
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.305937 , 8.936915 , 3.4073892], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.834414682623115}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.3540517491609259
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.500081 , 6.1300707, 4.1116657], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8781115381274593}
episode index:838
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.20421148, 9.19078506]), 'previousTarget': array([ 9.18761806, 10.86266529]), 'currentState': array([14.278107, 17.134827,  4.492042], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.3538910644412663
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6288085, 4.5870204, 1.7503096], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7522979994512274}
episode index:839
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.52558833, 14.01382171]), 'previousTarget': array([ 4.52558833, 14.01382171]), 'currentState': array([ 4.       , 24.       ,  1.5932516], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.3537986365880019
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.913448 , 5.5232697, 4.602738 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9837070930893337}
episode index:840
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.22912609,  6.99641283]), 'previousTarget': array([16.17661434,  7.12887892]), 'currentState': array([24.003065,  9.110678,  3.27266 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.35426638148424877
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.713214 , 5.5309544, 4.9920692], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3920240470550207}
episode index:841
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.87921595, 13.32426418]), 'previousTarget': array([13.38378363, 14.50162145]), 'currentState': array([18.249489 , 21.032677 ,  3.5820408], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3538456375632461
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.4194752,  4.815027 ,  2.09902  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.422630942868769}
episode index:842
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58225026, 21.10479042]), 'previousTarget': array([17.15695274, 21.88465659]), 'currentState': array([21.073704, 29.462059,  3.772065], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.3540012078274346
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.8812795, 6.954303 , 4.596154 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9579055010185404}
episode index:843
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.38500232, 6.37311986]), 'previousTarget': array([5.75140493, 8.25608804]), 'currentState': array([ 8.08474 , 16.001797,  4.760029], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.354560647893795
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.825729 , 3.5923138, 4.5753546], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6319953239028033}
episode index:844
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.       , 11.       ,  3.3437617], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.3544989245657183
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9855733, 3.7302408, 3.0936685], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.607371482885307}
episode index:845
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.37865267,  6.5033286 ]), 'previousTarget': array([12.26582832,  6.70960666]), 'currentState': array([20.009544 ,  9.195154 ,  2.9391558], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.3545779260995115
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.466485 , 5.6192007, 5.9571323], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.817341877710352}
episode index:846
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.01725627,  5.41277978]), 'previousTarget': array([12.01725627,  5.41277978]), 'currentState': array([22.       ,  6.       ,  1.8489335], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.35463713984989975
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.255198 , 4.4022293, 5.5260363], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9550182277587903}
episode index:847
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.23021355,  3.71403893]), 'previousTarget': array([13.13606076,  3.64398987]), 'currentState': array([21.023767 ,  1.6925787,  3.258375 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.35503196525794123
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.208848 , 3.426931 , 2.2440722], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.98390016523999}
episode index:848
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.21982659, 20.83975519]), 'previousTarget': array([16.21982659, 20.83975519]), 'currentState': array([22.       , 29.       ,  1.4463288], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35461378862041715
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.7745075 ,  8.950899  ,  0.97957945], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.996752176370225}
episode index:849
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.12614943, 14.62590492]), 'previousTarget': array([11.1808125 , 16.23784091]), 'currentState': array([16.495262, 23.062292,  4.553824], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.35477875905686396
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5979934 , 6.753835  , 0.44116277], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8529797593023434}
episode index:850
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.55962573, 14.12237035]), 'previousTarget': array([ 3.55962573, 14.12237035]), 'currentState': array([ 2.       , 24.       ,  1.3883319], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.3552052593782992
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.099702  , 6.1809697 , 0.24249572], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2373695286091033}
episode index:851
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.39191443, 4.89595916]), 'previousTarget': array([8.11628302, 4.52057184]), 'currentState': array([16.364096 ,  4.150573 ,  2.7844074], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.35582870962752206
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5153604, 5.7378435, 2.794108 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6854465452948535}
episode index:852
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.25730636, 20.61514506]), 'previousTarget': array([19.25730636, 20.61514506]), 'currentState': array([26.       , 28.       ,  2.9570537], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.3556440033519785
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.2909412 , 4.748597  , 0.48767775], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7274505046605582}
episode index:853
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.21450215,  6.93989519]), 'previousTarget': array([14.21450215,  6.93989519]), 'currentState': array([24.      ,  9.      ,  4.757281], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35522755838318226
{'scaleFactor': 1.0, 'currentTarget': array([13.37938722, 17.8212409 ]), 'previousTarget': array([13.37938722, 17.8212409 ]), 'currentState': array([18.850174 , 26.192053 ,  5.3598933], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:854
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.47719786, 19.77576257]), 'previousTarget': array([11.52786405, 18.05572809]), 'currentState': array([15.492045 , 28.93442  ,  1.5016677], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.35516934957329355
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1997766, 5.4884796, 4.8647504], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9375338629994983}
episode index:855
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.       , 4.       , 3.7108324], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.35581095322450323
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9314823 , 3.4809844 , 0.13582677], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.520560066605197}
episode index:856
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.25900177, 22.195289  ]), 'previousTarget': array([14.25900177, 22.195289  ]), 'currentState': array([19.       , 31.       ,  2.2760384], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3553957712487454
{'scaleFactor': 1.0, 'currentTarget': array([ 4.92779891, 14.16346881]), 'previousTarget': array([ 4.92779891, 14.16346881]), 'currentState': array([ 4.849009, 24.163158,  5.795487], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:857
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.51597018, 11.44732987]), 'previousTarget': array([ 8.29411765, 11.17647059]), 'currentState': array([11.151319, 20.763136,  2.651593], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3549815570631408
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.689224  , -0.35216373,  5.397205  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.811077333101809}
episode index:858
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.80617463, 16.12653834]), 'previousTarget': array([10.06080701, 16.8085497 ]), 'currentState': array([12.042844 , 25.58825  ,  3.2554984], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.35501638188482915
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4909391, 4.698964 , 0.5240051], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.538794099252299}
episode index:859
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.       , 9.       , 2.1620789], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.35525271881118153
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7452798, 3.438809 , 5.2402864], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0029079865244457}
episode index:860
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.98007044, 17.19112474]), 'previousTarget': array([ 5.58369455, 19.00866927]), 'currentState': array([ 6.7814064, 27.158966 ,  5.5829706], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3548401140274287
{'scaleFactor': 1.0, 'currentTarget': array([19.51926326,  6.1974283 ]), 'previousTarget': array([19.51926326,  6.1974283 ]), 'currentState': array([29.485428 ,  7.019355 ,  1.4573445], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:861
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.28658825, 18.14788245]), 'previousTarget': array([ 7.28658825, 18.14788245]), 'currentState': array([ 9.        , 28.        ,  0.29830706], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.35522029323970594
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.948664 , 6.30757  , 4.4953485], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.346706494923533}
episode index:862
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.01131862,  4.47565149]), 'previousTarget': array([16.01131862,  4.47565149]), 'currentState': array([26.       ,  4.       ,  0.9399244], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.35540559662810006
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5430355, 5.212591 , 3.409548 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.557611499246929}
episode index:863
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.052962 ,  7.4572144,  3.0022588], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.468746858646408}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.3555612501887206
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.49564 , 5.974787, 4.238693], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7852586838990934}
episode index:864
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.81719011, 11.97748782]), 'previousTarget': array([ 6.83772234, 10.51316702]), 'currentState': array([11.561088 , 21.250202 ,  0.5289183], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3551501967202943
{'scaleFactor': 1.0, 'currentTarget': array([ 4.09170172, 21.19387166]), 'previousTarget': array([ 4.09170172, 21.19387166]), 'currentState': array([ 3.5316918, 31.178179 ,  3.4504616], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:865
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06116265,  4.10431526]), 'previousTarget': array([13.06116265,  4.10431526]), 'currentState': array([23.      ,  3.      ,  4.639423], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3547400925670376
{'scaleFactor': 1.0, 'currentTarget': array([19.13177562,  2.58770018]), 'previousTarget': array([19.13177562,  2.58770018]), 'currentState': array([28.989191 ,  0.9050354,  5.60892  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:866
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.       , 3.       , 4.8282948], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.3553329480690006
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6559477, 4.481055 , 2.4098961], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8364039120895549}
episode index:867
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.67643864, 13.3648209 ]), 'previousTarget': array([ 2.67643864, 13.3648209 ]), 'currentState': array([ 0.       , 23.       ,  3.5049622], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.3552719705201527
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1872344, 5.868782 , 4.3471947], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1896933036747954}
episode index:868
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([20.        , 25.        ,  0.22689503], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.3550269020705117
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.32164  , 4.098561 , 3.4467182], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5997890302871567}
episode index:869
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.      ,  7.      ,  5.722973], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.3552050156845106
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.610534 , 3.9184434, 2.5351064], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9399961263554248}
episode index:870
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.63339837, 20.8827828 ]), 'previousTarget': array([ 9.30370074, 20.3703598 ]), 'currentState': array([13.976217  , 30.307514  ,  0.39013612], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3547972028077201
{'scaleFactor': 1.0, 'currentTarget': array([19.32394217,  5.8112286 ]), 'previousTarget': array([19.32394217,  5.8112286 ]), 'currentState': array([29.307943,  6.376667,  5.161009], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:871
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.195289  , 7.25900177]), 'previousTarget': array([9.195289  , 7.25900177]), 'currentState': array([18.       , 12.       ,  1.4479918], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.3551498230349865
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7402942, 6.2838326, 4.82938  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7986341133230772}
episode index:872
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.83842665,  7.99181166]), 'previousTarget': array([11.83842665,  7.99181166]), 'currentState': array([21.        , 12.        ,  0.71308386], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3547430076592305
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.666024 ,  4.7856674,  2.4222174], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.669469034162476}
episode index:873
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([14.       ,  4.       ,  1.2647313], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.3549445666240999
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7880163, 5.753291 , 5.0221987], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4270080479730811}
episode index:874
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.47752312, 15.64514664]), 'previousTarget': array([10.7006642 , 16.97139481]), 'currentState': array([13.354677 , 24.862938 ,  3.5981302], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.3553428559136719
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5774875, 6.489285 , 5.965591 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0594930661927284}
episode index:875
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.31035268, 17.56705854]), 'previousTarget': array([16.31035268, 17.56705854]), 'currentState': array([23.       , 25.       ,  1.9752034], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35493721338409007
{'scaleFactor': 1.0, 'currentTarget': array([22.32827891, 23.61374447]), 'previousTarget': array([22.32827891, 23.61374447]), 'currentState': array([29.142094  , 30.93303   ,  0.62093514], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:876
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.49573151,  7.70819595]), 'previousTarget': array([14.21450215,  6.93989519]), 'currentState': array([23.023365 , 10.745333 ,  1.7004845], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.35500336213865086
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0583143, 3.7831614, 4.0318785], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6126764651849508}
episode index:877
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.04493723,  4.34434271]), 'previousTarget': array([17.0103146 ,  4.45407661]), 'currentState': array([25.023703 ,  3.6930046,  3.6163096], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.3550886533133493
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1554875, 5.820277 , 2.3927135], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.417041339178193}
episode index:878
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.42173715, 4.87347886]), 'currentState': array([13.045846 ,  2.425773 ,  3.0827594], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.447619946222224}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.3553592765136771
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.495698 , 5.1856914, 3.9876006], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5071807167130442}
episode index:879
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.25842724, 10.14357069]), 'previousTarget': array([14.25842724, 10.14357069]), 'currentState': array([23.      , 15.      ,  5.385417], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.35545892276948676
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0476513, 5.0358257, 3.9926362], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0482636644450825}
episode index:880
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.47191578,  6.15916125]), 'previousTarget': array([13.2381294 ,  6.83069542]), 'currentState': array([21.315279,  7.92217 ,  2.966858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.3559295064505716
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2887125, 5.3418145, 2.8337126], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.4474282883722397}
episode index:881
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 6.       , 3.0914934], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.3564721189845846
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.463222 , 4.444944 , 0.5768525], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6339442486111715}
episode index:882
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.13370384, 18.03759412]), 'previousTarget': array([ 6.13370384, 18.03759412]), 'currentState': array([ 7.       , 28.       ,  3.0969265], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.35644709603770497
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4180455 , 6.184844  , 0.03171819], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9764704194612597}
episode index:883
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.49937617, 15.01247661]), 'previousTarget': array([ 4.49937617, 15.01247661]), 'currentState': array([ 4.       , 25.       ,  3.7306435], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.3564881185855784
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8517184, 5.0569572, 4.5290594], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.852594196257858}
episode index:884
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.89215338, 16.38049943]), 'previousTarget': array([16.54758686, 15.33205141]), 'currentState': array([22.80652  , 23.60487  ,  1.9548419], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.3567620936001327
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.025955 , 4.8478284, 1.8015616], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.037178997204108}
episode index:885
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.18655156, 19.08392763]), 'previousTarget': array([ 3.14624108, 21.06591064]), 'currentState': array([ 1.909493 , 29.002048 ,  4.5277886], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.356834974106402
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7807865, 6.923735 , 4.583552 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2775509696548935}
episode index:886
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.04106794,  3.90535746]), 'previousTarget': array([17.04106794,  3.90535746]), 'currentState': array([27.      ,  3.      ,  2.341047], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.3573830104302775
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.1605525, 5.871792 , 4.113533 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8864525466079305}
episode index:887
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.00589637, 6.58676072]), 'previousTarget': array([6.52786405, 8.05572809]), 'currentState': array([11.360018, 15.03267 ,  4.877453], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.3573886286818881
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8307478, 4.599746 , 5.784987 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2358615172147587}
episode index:888
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7006642 , 16.97139481]), 'previousTarget': array([10.7006642 , 16.97139481]), 'currentState': array([15.     , 26.     ,  0.27552], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3569866167261154
{'scaleFactor': 1.0, 'currentTarget': array([4.8097055 , 5.82617224]), 'previousTarget': array([4.4194837 , 7.77364368]), 'currentState': array([ 2.5651493, 15.571015 ,  4.63779  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:889
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.36882594, 6.35236179]), 'previousTarget': array([5.36882594, 6.35236179]), 'currentState': array([ 8.        , 16.        ,  0.64234304], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.35737589883653503
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.545289 , 6.576617 , 5.4819145], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6682507477783224}
episode index:890
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.42337349, 10.3376506 ]), 'previousTarget': array([ 6.42337349, 10.3376506 ]), 'currentState': array([ 9.       , 20.       ,  2.7893336], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.35760136487437044
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.7271318, 3.940135 , 4.481663 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.65635357256782}
episode index:891
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.2101958 ,  2.03954254]), 'previousTarget': array([19.2101958 ,  2.03954254]), 'currentState': array([29.       ,  0.       ,  0.6020684], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.3574249922981183
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.687152 , 4.300052 , 2.0130734], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7666817452261411}
episode index:892
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45299804, 20.67949706]), 'previousTarget': array([15.45299804, 20.67949706]), 'currentState': array([21.      , 29.      ,  1.092489], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.35735999274494273
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4960575, 4.4967155, 3.7301512], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5784433205973591}
episode index:893
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.78388355, 15.58801258]), 'previousTarget': array([12.03861062, 17.31756858]), 'currentState': array([17.178682 , 24.007998 ,  5.2217054], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.3573305443059898
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.696594 , 5.0768127, 3.2270322], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.698332183947401}
episode index:894
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.      ,  9.      ,  5.042781], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.944271909999161}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.3573745077273485
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6265993, 4.309357 , 1.5326368], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7671482238329375}
episode index:895
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.        ,  3.        ,  0.18213895], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.3573341355921224
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.9566054, 4.276833 , 1.7229483], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1991932221380337}
episode index:896
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.28582466, 17.40755695]), 'previousTarget': array([12.37433993, 19.13415154]), 'currentState': array([17.349447 , 26.030766 ,  3.8295548], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35693576977763847
{'scaleFactor': 1.0, 'currentTarget': array([11.57639701,  3.09996978]), 'previousTarget': array([11.57639701,  3.09996978]), 'currentState': array([21.183468 ,  0.3243276,  4.8239274], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:897
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28935512, 19.79094798]), 'previousTarget': array([15.28935512, 19.79094798]), 'currentState': array([21.        , 28.        ,  0.64473134], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.3571537516344143
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.425371 , 4.0826654, 3.8034072], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6950473919886482}
episode index:898
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.3177872, 8.598156 ]), 'previousTarget': array([9.3177872, 8.598156 ]), 'currentState': array([17.       , 15.       ,  2.4463925], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.35749316018763966
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.914133  , 6.081139  , 0.02886117], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0845436165708096}
episode index:899
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.58256937, 6.60206577]), 'previousTarget': array([5.58256937, 6.60206577]), 'currentState': array([ 9.       , 16.       ,  1.6958776], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.3578934239347626
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.021354 , 6.5775423, 5.384203 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5776868275526974}
episode index:900
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.743829 , 7.4437113, 4.5850067], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.457101650585024}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.358594985062471
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.489753 , 5.4599156, 5.0421596], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6869313141156406}
episode index:901
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.04504527,  6.05190907]), 'previousTarget': array([16.04504527,  6.05190907]), 'currentState': array([26.       ,  7.       ,  4.2879086], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.35864164766545226
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.973824 , 4.4066644, 2.9328337], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1403422285299827}
episode index:902
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.42009499, 20.12559368]), 'previousTarget': array([ 7.42009499, 20.12559368]), 'currentState': array([ 9.       , 30.       ,  1.9491577], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.3585239500081144
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.741935 , 3.7879028, 5.947707 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7469708216720523}
episode index:903
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.82166324, 12.40634498]), 'previousTarget': array([ 2.82166324, 12.40634498]), 'currentState': array([ 0.       , 22.       ,  1.4111118], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.35869719466233524
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5460167, 4.710672 , 0.7147179], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4824905413404115}
episode index:904
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.32382728,  8.47578103]), 'previousTarget': array([18.32382728,  8.47578103]), 'currentState': array([28.      , 11.      ,  4.979023], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.3586557624854319
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3110306, 5.626947 , 4.6331277], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.8015770850769053}
episode index:905
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.57990501, 20.12559368]), 'previousTarget': array([ 2.57990501, 20.12559368]), 'currentState': array([ 1.       , 30.       ,  2.6182168], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.35847224101789166
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.83908  , 5.4985104, 5.710212 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5238395481510735}
episode index:906
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.29331177,  7.2909945 ]), 'previousTarget': array([17.1613009 ,  7.21114562]), 'currentState': array([25.05446  ,  9.463545 ,  3.2059035], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.3584171938620116
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8281527, 4.1160097, 3.0447369], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.211311541862831}
episode index:907
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.616617 , 10.037089 ,  5.0480847], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.2009829131684855}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.35900851661575245
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6560616, 3.5487332, 3.6989803], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2019798843940115}
episode index:908
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06116265,  5.89568474]), 'previousTarget': array([13.06116265,  5.89568474]), 'currentState': array([23.      ,  7.      ,  5.439703], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.35902856956438584
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.964551 , 6.5042505, 2.2573109], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7869326299042532}
episode index:909
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.66654394, 3.58979079]), 'previousTarget': array([8.66654394, 3.58979079]), 'currentState': array([18.      ,  0.      ,  4.890891], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.359579151745789
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1883554, 5.597399 , 2.728926 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3300656041772088}
episode index:910
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.3686438, 7.0342684, 4.927224 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.122294205175616}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.3596659117348825
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.099597 , 5.13442  , 2.8374846], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1077825719486831}
episode index:911
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8026175 , 7.1336455 , 0.01893538], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.446664668231817}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.36008261618844983
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.629221, 5.312457, 5.387097], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.48487784480516305}
episode index:912
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.21147869, 18.22104427]), 'previousTarget': array([12.21147869, 18.22104427]), 'currentState': array([17.       , 27.       ,  4.1704054], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.3598537775452567
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2930517, 3.2985973, 2.989965 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.726456004679319}
episode index:913
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([6.10647783, 5.86059386]), 'currentState': array([12.251741, 11.028614,  3.265176], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.430373309746553}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3604696319400955
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5863795, 5.298091 , 3.9796734], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6141431821200123}
episode index:914
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.20708033, 13.23716851]), 'previousTarget': array([20.20708033, 13.23716851]), 'currentState': array([29.        , 18.        ,  0.09835833], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.3602296480941547
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1263514, 3.0887794, 4.1263647], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2184299337817865}
episode index:915
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.        ,  0.        ,  0.59095836], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.3602565765120579
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0411954, 3.9174294, 3.146316 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5020142633927616}
episode index:916
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.83772234, 16.51316702]), 'previousTarget': array([ 8.83772234, 16.51316702]), 'currentState': array([12.       , 26.       ,  2.3413951], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35986371219743185
{'scaleFactor': 1.0, 'currentTarget': array([6.17134592, 4.6023916 ]), 'previousTarget': array([6.17134592, 4.6023916 ]), 'currentState': array([15.640672 ,  1.3880692,  5.210465 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:917
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.      , 2.      , 4.715036], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.36037166961751704
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.024397 , 3.4495745, 3.0360022], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.831835346937814}
episode index:918
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.23303501, 21.02945514]), 'previousTarget': array([ 6.23303501, 21.02945514]), 'currentState': array([ 7.       , 31.       ,  1.0617822], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.36067179346566325
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5855255, 5.867389 , 3.3268561], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.807278390109973}
episode index:919
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.85952947, 4.78838862]), 'previousTarget': array([7.51316702, 4.16227766]), 'currentState': array([15.569587 ,  2.3978264,  2.7893548], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.36105209937197474
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.6469555, 6.538171 , 3.687037 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.253537628049334}
episode index:920
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.27193785,  7.68379473]), 'previousTarget': array([16.27193785,  7.68379473]), 'currentState': array([26.       , 10.       ,  1.9005913], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36066007754855245
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.21996295, 12.345349  ,  3.310486  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.011224653694669}
episode index:921
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.02358287, 18.45051402]), 'previousTarget': array([20.12494999, 16.83691738]), 'currentState': array([28.68282  , 24.87983  ,  0.6166169], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.36048180361848137
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.5215735, 4.2965264, 6.2474256], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6372598596305923}
episode index:922
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.82178448, 11.13681661]), 'previousTarget': array([ 9.82178448, 11.13681661]), 'currentState': array([16.        , 19.        ,  0.28919995], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.36049182294029203
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.7427993, 4.981936 , 2.9623446], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.743018897702305}
episode index:923
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 0.       , 13.       ,  1.4666322], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36010168027477224
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.072723  , -0.40009046,  2.5033286 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.763730681636684}
episode index:924
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 6.       , 12.       ,  0.1822724], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.36022112347618945
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.258281 , 4.3960586, 2.629405 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3957137671293987}
episode index:925
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.298575  ,  6.57464375]), 'previousTarget': array([11.298575  ,  6.57464375]), 'currentState': array([21.       ,  9.       ,  5.4110107], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.36024777029629446
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.420127 , 3.0794196, 4.085554 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.3885957125511323}
episode index:926
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 2.       , 5.5058384], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.3606651643891049
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6360846, 3.3958743, 2.7974887], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7256369684539192}
episode index:927
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.61514506, 19.25730636]), 'previousTarget': array([20.61514506, 19.25730636]), 'currentState': array([28.       , 26.       ,  1.5903753], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36027651658265114
{'scaleFactor': 1.0, 'currentTarget': array([ 4.92140587, 21.20282875]), 'previousTarget': array([ 4.92140587, 21.20282875]), 'currentState': array([ 4.8729  , 31.202711,  2.835085], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:928
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.94760122, 19.34534279]), 'previousTarget': array([11.52786405, 18.05572809]), 'currentState': array([14.777494 , 28.58287  ,  2.0429842], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.360230997257825
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8153534, 4.4763627, 2.904992 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9690186648638108}
episode index:929
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.00911449, 3.7370717 ]), 'previousTarget': array([7.298575  , 4.42535625]), 'currentState': array([18.547062 ,  0.7324821,  0.1365158], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.36025751885098156
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1123915, 5.41071  , 3.0173988], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1857897680797886}
episode index:930
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.54511097, 11.66007009]), 'previousTarget': array([15.54511097, 11.66007009]), 'currentState': array([24.       , 17.       ,  1.7446041], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3598705612582308
{'scaleFactor': 1.0, 'currentTarget': array([18.8317736 ,  6.92403003]), 'previousTarget': array([18.8317736 ,  6.92403003]), 'currentState': array([28.736408 ,  8.301786 ,  2.2515035], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:931
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([10.504515  , 10.682261  ,  0.12973756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.911243732684631}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.3599186958198404
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.1334233, 5.903591 , 3.967409 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2519712875996525}
episode index:932
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.92893219, 16.92893219]), 'previousTarget': array([16.92893219, 16.92893219]), 'currentState': array([24.       , 24.       ,  1.2831838], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3595329308725522
{'scaleFactor': 1.0, 'currentTarget': array([ 3.66271056, 17.95847678]), 'previousTarget': array([ 3.66271056, 17.95847678]), 'currentState': array([ 2.6361818, 27.90565  ,  2.2618828], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:933
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.71244825, 21.18911176]), 'previousTarget': array([20.08777354, 20.77358143]), 'currentState': array([25.175707, 28.819727,  2.51651 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3591479919744017
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([-0.08014959,  7.540164  ,  5.8213277 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.67981980191286}
episode index:934
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.54294641, 14.61656883]), 'previousTarget': array([ 8.54294641, 14.61656883]), 'currentState': array([12.      , 24.      ,  1.568221], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.359020552923717
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8776407, 4.049853 , 3.1816003], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.104356029634773}
episode index:935
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.00866927,  4.41630545]), 'previousTarget': array([19.00866927,  4.41630545]), 'currentState': array([29.       ,  4.       ,  2.2032485], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.3589039038890318
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3549097, 6.1442766, 5.981575 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0039189635843933}
episode index:936
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.75001064, 10.94090911]), 'previousTarget': array([13.47423305, 12.41495392]), 'currentState': array([20.686453 , 17.02473  ,  4.3497186], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3585208687728215
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.243877 , 10.234938 ,  5.4874372], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.289262186458465}
episode index:937
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.26537656, 6.80768079]), 'previousTarget': array([6.26537656, 6.80768079]), 'currentState': array([12.       , 15.       ,  3.3538785], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.35875823076146257
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2222   , 5.1556783, 2.6894236], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.27130891524871414}
episode index:938
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.2381294 ,  6.83069542]), 'previousTarget': array([13.2381294 ,  6.83069542]), 'currentState': array([23.      ,  9.      ,  5.544361], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35837616661794663
{'scaleFactor': 1.0, 'currentTarget': array([5.50081287, 5.55873532]), 'previousTarget': array([5.50081287, 5.55873532]), 'currentState': array([12.17536  , 13.0052395,  1.5985154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:939
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5.52434851, 16.01131862]), 'previousTarget': array([ 5.52434851, 16.01131862]), 'currentState': array([ 6.       , 26.       ,  5.9894757], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.35819753395108184
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.676433 , 5.8194013, 5.401464 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.865970612018856}
episode index:940
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.72240203, 10.76747944]), 'previousTarget': array([ 6.83772234, 10.51316702]), 'currentState': array([11.991016 , 19.81065  ,  5.5844216], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.35844072124968585
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.156401 , 4.4788833, 1.8249038], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5440808506246366}
episode index:941
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.       , 27.       ,  3.6210523], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3580602109298879
{'scaleFactor': 1.0, 'currentTarget': array([7.98803841, 7.84275324]), 'previousTarget': array([7.98803841, 7.84275324]), 'currentState': array([15.233047  , 14.735493  ,  0.16337243], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:942
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.10029057, 17.33545581]), 'previousTarget': array([15.5235158 , 17.38060682]), 'currentState': array([20.036934, 25.382584,  2.660582], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.35794812233196577
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.9715033, 6.643499 , 3.546364 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9387867791668325}
episode index:943
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.99069356, 17.25943472]), 'previousTarget': array([ 3.90535746, 17.04106794]), 'currentState': array([ 4.9831023, 27.259432 ,  0.3542477], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.3581485550862653
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9654655, 6.300177 , 4.3254256], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3006356572591202}
episode index:944
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28069764, 12.64292747]), 'previousTarget': array([14.28069764, 12.64292747]), 'currentState': array([22.        , 19.        ,  0.47437036], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35776956190627984
{'scaleFactor': 1.0, 'currentTarget': array([19.80705855,  9.26733496]), 'previousTarget': array([19.80705855,  9.26733496]), 'currentState': array([29.415974 , 12.036586 ,  5.7963133], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:945
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.00496281, 15.0496281 ]), 'previousTarget': array([ 6.00496281, 15.0496281 ]), 'currentState': array([ 7.       , 25.       ,  2.7423527], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.3576302158107489
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.3790636, 6.4075723, 2.6300185], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9705522382856395}
episode index:946
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([12.       , 11.       ,  2.3677146], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.35779109810265897
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.9908   , 5.6658177, 3.3175688], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.0991897286626098}
episode index:947
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.26249242, 15.04766094]), 'previousTarget': array([15.72986847, 15.13376467]), 'currentState': array([21.040443 , 22.40017  ,  2.9844422], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.35760667344042946
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.339699 , 4.208543 , 1.2701926], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0307289399805015}
episode index:948
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.19172661, 13.17352362]), 'previousTarget': array([18.39073472, 12.91270688]), 'currentState': array([28.998043, 17.91154 ,  5.772548], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.3574802096694578
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2975783, 5.388677 , 3.960608 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.48951278749494354}
episode index:949
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.89320772, 12.14509446]), 'previousTarget': array([14.89320772, 12.14509446]), 'currentState': array([23.       , 18.       ,  1.8069359], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.3576915632788035
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4141896, 5.3762517, 3.978972 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6298343588450275}
episode index:950
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.38294559,  9.34342732]), 'previousTarget': array([11.17647059,  8.29411765]), 'currentState': array([19.650389 , 14.969206 ,  1.8746102], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.35807777649928624
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.4147298, 6.4159455, 4.5931683], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1255548085663074}
episode index:951
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([7.068859 , 8.479716 , 3.8566191], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.048284020701132}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.3587311611878374
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8426638, 6.141215 , 3.045849 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6253610335102353}
episode index:952
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  0.       ,  0.7848939], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.3593133081829012
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.1264296, 5.3800445, 2.1827178], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.188813417020653}
episode index:953
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.77448407, 5.49460026]), 'previousTarget': array([6.89633523, 5.86197056]), 'currentState': array([14.202478 , 10.876878 ,  2.3121057], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.3593918386007302
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.24434 , 6.077695, 5.666251], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.31622505826115}
episode index:954
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.11722482, 4.81302841]), 'previousTarget': array([9.025413 , 5.2875295]), 'currentState': array([18.099285 ,  4.214303 ,  4.8135386], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.35966841824082496
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.467001 , 4.330361 , 1.8728261], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6126091773472409}
episode index:955
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 5.       , 1.1852014], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.36027700791777073
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.9295235, 5.564732 , 6.0250573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5691126934075936}
episode index:956
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.01200209, 5.67928914]), 'previousTarget': array([10.3376506 ,  6.42337349]), 'currentState': array([18.871675 ,  7.3486724,  4.407051 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.36018920714429264
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2764497, 4.4887733, 2.2402472], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5811859578203965}
episode index:957
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42507074, 11.85504245]), 'previousTarget': array([16.42507074, 11.85504245]), 'currentState': array([25.      , 17.      ,  6.214521], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35981322676105226
{'scaleFactor': 1.0, 'currentTarget': array([20.92825499, 14.90039027]), 'previousTarget': array([20.92825499, 14.90039027]), 'currentState': array([29.421337 , 20.179363 ,  0.6735197], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:958
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45632659, 19.97611553]), 'previousTarget': array([16.37689979, 21.73073498]), 'currentState': array([21.181042 , 28.175362 ,  3.8627706], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3599540249183402
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.8160586, 3.6172261, 4.595121 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2825714799359433}
episode index:959
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 9.       , 2.1789608], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.35990699903111373
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.611148, 6.48793 , 5.572858], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1931102674690677}
episode index:960
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.        , 11.        ,  0.09571397], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.3605121740054841
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.4956074, 5.2447486, 5.2478895], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5155010052490951}
episode index:961
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.       ,  0.       ,  1.5994222], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.3603374059585906
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8204248, 3.0893872, 2.0057344], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2454039765434466}
episode index:962
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.17289671, 6.83548626]), 'previousTarget': array([5.03883865, 5.19419324]), 'currentState': array([ 6.110712 , 16.791414 ,  2.3843703], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.360548795142904
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.3515801, 5.0024896, 5.695301 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6484217369889511}
episode index:963
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5.0496281 , 5.00496281]), 'currentState': array([14.152777,  7.811688,  2.647568], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.574910483111475}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.3611129375494039
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.40147  , 6.96854  , 3.9925737], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.009061670294331}
episode index:964
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.2101958 ,  7.96045746]), 'previousTarget': array([19.2101958 ,  7.96045746]), 'currentState': array([29.       , 10.       ,  0.8931076], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.36117097686793637
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.847642 , 5.7980533, 4.977426 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1642104104912374}
episode index:965
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57464375, 19.298575  ]), 'previousTarget': array([ 8.57464375, 19.298575  ]), 'currentState': array([11.       , 29.       ,  1.2155056], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.3611432396836941
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.1842558, 5.9152174, 3.9405122], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.033359227714345}
episode index:966
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 2.12429644, 18.22823636]), 'previousTarget': array([ 2.12429644, 18.22823636]), 'currentState': array([ 0.       , 28.       ,  4.5036902], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.3611758847598824
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.518174 , 3.6900234, 0.1074754], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.3957774762965216}
episode index:967
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.00866927,  5.58369455]), 'previousTarget': array([19.00866927,  5.58369455]), 'currentState': array([29.       ,  6.       ,  3.7594306], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.36095735740538215
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8145747, 3.8379903, 2.3986695], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.659969782325896}
episode index:968
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.       , 0.       , 5.2003603], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.3615088340791214
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.0503087, 3.8167124, 1.1450164], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.5821877156084112}
episode index:969
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([9.       , 5.       , 0.5685245], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.36166721375267263
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.090052 , 6.9543657, 2.9596195], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.155817835014556}
episode index:970
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.93595004, 10.08636336]), 'previousTarget': array([11.93595004, 10.08636336]), 'currentState': array([20.      , 16.      ,  6.103584], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36129474494345254
{'scaleFactor': 1.0, 'currentTarget': array([11.98018157, 14.92098646]), 'previousTarget': array([11.98018157, 14.92098646]), 'currentState': array([17.734425 , 23.099537 ,  1.3871124], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
episode index:971
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.05914151, 8.44220991]), 'previousTarget': array([6.05914151, 8.44220991]), 'currentState': array([ 9.       , 18.       ,  1.6724911], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.3614372781584276
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.5510015, 5.686494 , 2.440364 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8802707226920791}
episode index:972
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.03883865, 10.19419324]), 'previousTarget': array([ 6.03883865, 10.19419324]), 'currentState': array([ 8.       , 20.       ,  1.0008233], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36106581127440046
{'scaleFactor': 1.0, 'currentTarget': array([18.24222992,  1.90332148]), 'previousTarget': array([18.24222992,  1.90332148]), 'currentState': array([27.97953   , -0.37373376,  6.0215864 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:973
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5.42173715, 5.12652114]), 'previousTarget': array([5.42173715, 5.12652114]), 'currentState': array([15.        ,  8.        ,  0.36802214], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.36088484490137596
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.750836 , 6.4874916, 3.4289484], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2974023626657853}
episode index:974
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.192426 , 2.8559656, 2.2728486], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.2910824694603313}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.36150988506045145
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.499894 , 5.584281 , 0.8323451], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7690839474894445}
episode index:975
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([1.       , 7.       , 3.1253142], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.36173494297956793
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.822193 , 6.9130893, 5.741495 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.9213343938303524}
episode index:976
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.        , 29.        ,  0.55479246], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.3617282140867588
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.942868 , 5.1377583, 4.1099405], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9528787126952843}
episode index:977
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.60206577,  9.58256937]), 'previousTarget': array([17.60206577,  9.58256937]), 'currentState': array([27.       , 13.       ,  1.1801391], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3613583488371814
{'scaleFactor': 1.0, 'currentTarget': array([18.08188293,  5.28960264]), 'previousTarget': array([18.08188293,  5.28960264]), 'currentState': array([28.079433 ,  5.5109253,  5.7237415], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
episode index:978
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.3177872, 18.598156 ]), 'previousTarget': array([21.3177872, 18.598156 ]), 'currentState': array([29.        , 25.        ,  0.10251205], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.3612131791952297
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.304568 , 4.482813 , 4.1312537], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4033458934160832}
episode index:979
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([20.       , 25.       ,  1.9931288], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.36122952967250344
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.734821 , 5.2728686, 2.728933 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.783848813198181}
episode index:980
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8317722, 20.8952531]), 'previousTarget': array([16.37689979, 21.73073498]), 'currentState': array([20.092167 , 29.399855 ,  3.1961362], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3608613038522461
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 2.3746881, 12.039629 ,  5.2454343], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.513230898189324}
episode index:981
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([4.24859507, 8.25608804]), 'previousTarget': array([4.24859507, 8.25608804]), 'currentState': array([ 2.       , 18.       ,  3.0316806], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.36113519624262386
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.478809, 5.27052 , 3.853364], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5872149247790299}
episode index:982
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.      , 26.      ,  4.242278], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.3614483565298335
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.420363 , 6.385722 , 5.2230835], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.101304146803691}
episode index:983
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.50698447, 17.52256118]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([24.57365  , 24.598028 ,  3.0452209], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.36135343616437315
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.551983, 6.70479 , 3.342196], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7919247854838223}
episode index:984
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.07711395, 11.06203262]), 'previousTarget': array([19.13415154, 12.37433993]), 'currentState': array([28.261703, 15.017196,  5.272601], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.36133643996862
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.4170256, 6.146197 , 4.873648 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2859340541545516}
episode index:985
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.37211436, 19.10529558]), 'previousTarget': array([14.7000106 , 20.52001696]), 'currentState': array([18.476192 , 27.70462  ,  4.3148146], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3609699729909642
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([11.225451  , -0.01844764,  5.613712  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.996315569625314}
episode index:986
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.48904088,  7.09202931]), 'previousTarget': array([17.04106794,  6.09464254]), 'currentState': array([26.32727  ,  8.883464 ,  1.4874117], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.36126854365720856
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.416657 , 5.314764 , 4.2110314], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4512041081413314}
episode index:987
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([0.       , 3.       , 3.1629367], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.361145794604503
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.4285965, 3.6170342, 1.8568553], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4963610447985443}
episode index:988
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([6.60206577, 4.41743063]), 'previousTarget': array([6.60206577, 4.41743063]), 'currentState': array([16.       ,  1.       ,  2.0121844], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3612809744477747
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.75227 , 5.956501, 5.512551], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2168831755830651}
episode index:989
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.      , 1.      , 4.958677], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.3613866308121404
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.571674 , 4.3873796, 2.2635198], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.6868498604437705}
episode index:990
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.08399589,  3.29339184]), 'previousTarget': array([18.08399589,  3.29339184]), 'currentState': array([28.       ,  2.       ,  0.9143164], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.36130072267580043
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.391079 , 3.6551325, 5.408991 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.4762971762299415}
episode index:991
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15117234, 13.80355711]), 'previousTarget': array([16.15117234, 13.80355711]), 'currentState': array([24.       , 20.       ,  1.4742401], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3609365082376192
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([8.186211  , 0.63055074, 4.144271  ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 5.407774497005904}
episode index:992
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.08772099,  5.6783628 ]), 'previousTarget': array([10.08772099,  5.6783628 ]), 'currentState': array([20.      ,  7.      ,  5.720718], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.360573027363261
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 7.0289407, 12.411303 ,  1.0378293], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.684009760927101}
episode index:993
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 3.29039334, 12.26582832]), 'previousTarget': array([ 3.29039334, 12.26582832]), 'currentState': array([ 1.        , 22.        ,  0.05160015], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.36069819672232634
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.862953 , 6.9979916, 4.2132573], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.1763865655417156}
episode index:994
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.54016533, 10.08302492]), 'previousTarget': array([14.67949706, 11.45299804]), 'currentState': array([22.133274 , 15.197561 ,  4.1139193], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.36116600619681205
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.2791176, 4.7608705, 2.2553046], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.3675453227923791}
episode index:995
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.       , 9.       , 2.0757618], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.3616082397142239
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.4842315, 5.173343 , 5.1206756], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5441185836471086}
episode index:996
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8793244, 3.684207 , 5.733239 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7283590589442133}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.36224855241260484
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([3.8793244, 3.684207 , 5.733239 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.7283590589442133}
episode index:997
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10050506,  6.58578644]), 'previousTarget': array([16.10050506,  6.58578644]), 'currentState': array([26.       ,  8.       ,  2.0133572], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.36203856361487263
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.6311   , 4.669903 , 1.8069154], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7122159748442893}
episode index:998
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.64886795, 17.09169832]), 'previousTarget': array([ 6.64886795, 17.09169832]), 'currentState': array([ 8.       , 27.       ,  4.6935377], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.36233247818640346
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.659529 , 5.0811944, 3.6927972], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6645083208087353}
episode index:999
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 4.16841248, 12.06849396]), 'previousTarget': array([ 4.16841248, 12.06849396]), 'currentState': array([ 3.       , 22.       ,  0.6180636], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.3623361780494903
{'scaleFactor': 1.0, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([6.5962725, 6.6902833, 3.734262 ], dtype=float32), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 2.32489643276149}
episode index:0
done in step count: 1
reward sum = 1.0
episode index:1
done in step count: 113
reward sum = 1.0
episode index:2
done in step count: 156
reward sum = 1.0
episode index:3
done in step count: 83
reward sum = 1.0
episode index:4
done in step count: 13
reward sum = 1.0
episode index:5
done in step count: 40
reward sum = 1.0
episode index:6
reward sum = 0.0
episode index:7
reward sum = 0.0
episode index:8
done in step count: 39
reward sum = 1.0
episode index:9
done in step count: 96
reward sum = 1.0
episode index:10
done in step count: 167
reward sum = 1.0
episode index:11
done in step count: 92
reward sum = 1.0
episode index:12
done in step count: 122
reward sum = 1.0
episode index:13
done in step count: 47
reward sum = 1.0
episode index:14
done in step count: 56
reward sum = 1.0
episode index:15
done in step count: 157
reward sum = 1.0
episode index:16
done in step count: 44
reward sum = 1.0
episode index:17
done in step count: 43
reward sum = 1.0
episode index:18
done in step count: 19
reward sum = 1.0
episode index:19
done in step count: 1
reward sum = 1.0
episode index:20
done in step count: 53
reward sum = 1.0
episode index:21
done in step count: 40
reward sum = 1.0
episode index:22
done in step count: 82
reward sum = 1.0
episode index:23
done in step count: 116
reward sum = 1.0
episode index:24
done in step count: 55
reward sum = 1.0
episode index:25
done in step count: 91
reward sum = 1.0
episode index:26
done in step count: 18
reward sum = 1.0
episode index:27
done in step count: 62
reward sum = 1.0
episode index:28
done in step count: 83
reward sum = 1.0
episode index:29
done in step count: 75
reward sum = 1.0
episode index:30
done in step count: 30
reward sum = 1.0
episode index:31
done in step count: 123
reward sum = 1.0
episode index:32
reward sum = 0.0
episode index:33
done in step count: 154
reward sum = 1.0
episode index:34
done in step count: 156
reward sum = 1.0
episode index:35
done in step count: 129
reward sum = 1.0
episode index:36
done in step count: 12
reward sum = 1.0
episode index:37
done in step count: 93
reward sum = 1.0
episode index:38
done in step count: 59
reward sum = 1.0
episode index:39
done in step count: 67
reward sum = 1.0
episode index:40
done in step count: 55
reward sum = 1.0
episode index:41
done in step count: 27
reward sum = 1.0
episode index:42
done in step count: 87
reward sum = 1.0
episode index:43
done in step count: 163
reward sum = 1.0
episode index:44
reward sum = 0.0
episode index:45
done in step count: 33
reward sum = 1.0
episode index:46
done in step count: 200
reward sum = 1.0
episode index:47
done in step count: 16
reward sum = 1.0
episode index:48
done in step count: 56
reward sum = 1.0
episode index:49
done in step count: 52
reward sum = 1.0
episode index:50
done in step count: 89
reward sum = 1.0
episode index:51
done in step count: 161
reward sum = 1.0
episode index:52
done in step count: 59
reward sum = 1.0
episode index:53
done in step count: 124
reward sum = 1.0
episode index:54
done in step count: 118
reward sum = 1.0
episode index:55
done in step count: 82
reward sum = 1.0
episode index:56
done in step count: 6
reward sum = 1.0
episode index:57
done in step count: 25
reward sum = 1.0
episode index:58
done in step count: 133
reward sum = 1.0
episode index:59
done in step count: 12
reward sum = 1.0
episode index:60
done in step count: 108
reward sum = 1.0
episode index:61
done in step count: 83
reward sum = 1.0
episode index:62
reward sum = 0.0
episode index:63
done in step count: 148
reward sum = 1.0
episode index:64
done in step count: 48
reward sum = 1.0
episode index:65
done in step count: 97
reward sum = 1.0
episode index:66
done in step count: 37
reward sum = 1.0
episode index:67
done in step count: 157
reward sum = 1.0
episode index:68
done in step count: 129
reward sum = 1.0
episode index:69
done in step count: 117
reward sum = 1.0
episode index:70
done in step count: 21
reward sum = 1.0
episode index:71
reward sum = 0.0
episode index:72
done in step count: 5
reward sum = 1.0
episode index:73
done in step count: 196
reward sum = 1.0
episode index:74
done in step count: 30
reward sum = 1.0
episode index:75
done in step count: 30
reward sum = 1.0
episode index:76
done in step count: 112
reward sum = 1.0
episode index:77
done in step count: 44
reward sum = 1.0
episode index:78
done in step count: 114
reward sum = 1.0
episode index:79
done in step count: 6
reward sum = 1.0
episode index:80
done in step count: 67
reward sum = 1.0
episode index:81
done in step count: 147
reward sum = 1.0
episode index:82
done in step count: 13
reward sum = 1.0
episode index:83
done in step count: 123
reward sum = 1.0
episode index:84
done in step count: 17
reward sum = 1.0
episode index:85
done in step count: 32
reward sum = 1.0
episode index:86
done in step count: 51
reward sum = 1.0
episode index:87
done in step count: 125
reward sum = 1.0
episode index:88
done in step count: 177
reward sum = 1.0
episode index:89
done in step count: 62
reward sum = 1.0
episode index:90
reward sum = 0.0
episode index:91
done in step count: 16
reward sum = 1.0
episode index:92
done in step count: 15
reward sum = 1.0
episode index:93
done in step count: 11
reward sum = 1.0
episode index:94
done in step count: 115
reward sum = 1.0
episode index:95
reward sum = 0.0
episode index:96
done in step count: 76
reward sum = 1.0
episode index:97
done in step count: 22
reward sum = 1.0
episode index:98
done in step count: 65
reward sum = 1.0
episode index:99
done in step count: 77
reward sum = 1.0

Process finished with exit code 0
