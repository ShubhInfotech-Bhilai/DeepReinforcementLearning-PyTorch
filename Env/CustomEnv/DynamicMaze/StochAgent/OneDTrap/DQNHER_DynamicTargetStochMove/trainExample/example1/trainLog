17], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.007415783305383}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.29236639521531077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.3412485, 119.713425 ,   0.670254 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3715221413430996}
episode index:3437
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.81122478, 79.86645136]), 'previousTarget': array([12.00124766, 79.97504678]), 'currentState': array([12.712906, 59.886787,  4.444703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2922813555424733
{'currentTarget': array([ 4.31593225, 88.53278818]), 'previousTarget': array([ 4.26897248, 88.53607197]), 'currentState': array([ 0.76077473, 68.8513    ,  3.2910445 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3438
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.98863969, 111.27545922]), 'previousTarget': array([  7.9223227 , 109.61161351]), 'currentState': array([ 3.4956813, 91.78666  ,  2.1160133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2921963653256828
{'currentTarget': array([  8.35568529, 115.80397996]), 'previousTarget': array([  8.35839268, 115.72437999]), 'currentState': array([ 1.0584865, 97.18273  ,  5.419949 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3439
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.03139821, 72.72005132]), 'previousTarget': array([10., 74.]), 'currentState': array([ 8.621754 , 52.724247 ,  4.0780096], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2921114245218091
{'currentTarget': array([12.00430053, 92.95793262]), 'previousTarget': array([12.00430053, 92.95793262]), 'currentState': array([13.482603 , 73.01264  ,  5.0230045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3440
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.52151013, 88.48333635]), 'previousTarget': array([ 7.50515339, 86.94328241]), 'currentState': array([ 4.327436 , 68.60405  ,  2.3625045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29202653308777193
{'currentTarget': array([  8.73524417, 115.58416795]), 'previousTarget': array([  8.73524417, 115.58416795]), 'currentState': array([ 3.228386 , 96.35725  ,  2.3941233], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3441
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.011229 , 118.10803  ,   1.5702939], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.231605257708795}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.2921088480839628
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.849312, 119.84667 ,   3.161037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8556572143622354}
episode index:3442
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.76153954, 82.43509142]), 'previousTarget': array([13.97653796, 80.89737675]), 'currentState': array([17.276516, 62.59385 ,  0.756631], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2920240067107174
{'currentTarget': array([13.86254164, 92.15682264]), 'previousTarget': array([13.84381727, 91.98217031]), 'currentState': array([16.610722 , 72.346535 ,  2.8806472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3443
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.17829318, 78.31142367]), 'previousTarget': array([10., 77.]), 'currentState': array([ 8.784158, 58.315308,  2.528728], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29193921460656214
{'currentTarget': array([  8.54860987, 115.85538889]), 'previousTarget': array([  8.54860987, 115.85538889]), 'currentState': array([ 1.938452, 96.979324,  1.605618], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3444
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.349423 , 107.40792  ,   0.5628321], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.699797664589246}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.2919555147226393
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.404503 , 118.65933  ,   1.1550765], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.466971976982712}
episode index:3445
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.43902782, 115.83195965]), 'previousTarget': array([ 11.4295875 , 115.88993593]), 'currentState': array([17.966026, 96.92698 ,  4.894542], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2918707917061789
{'currentTarget': array([  8.13941539, 115.14176356]), 'previousTarget': array([  8.17358933, 115.28639716]), 'currentState': array([ 0.98652565, 96.46461   ,  0.6143    ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3446
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.65754935, 108.63543505]), 'previousTarget': array([  7.99610759, 106.77431008]), 'currentState': array([ 6.311342  , 88.77353   ,  0.78585917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.29189337912884894
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.264442 , 120.44487  ,   3.0793004], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3404193397437234}
episode index:3447
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.34123998, 71.53084088]), 'previousTarget': array([ 7.85635677, 69.98165792]), 'currentState': array([ 6.2457933, 51.560863 ,  2.1354399], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29180872327643337
{'currentTarget': array([11.62354115, 92.84258315]), 'previousTarget': array([11.77251742, 92.77376702]), 'currentState': array([12.817062 , 72.87823  ,  1.3798589], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3448
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.39388466, 87.03234573]), 'previousTarget': array([14.42891943, 85.83405013]), 'currentState': array([15.441978 , 67.13749  ,  2.7567146], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2917241165141033
{'currentTarget': array([ 5.38086207, 90.07563428]), 'previousTarget': array([ 5.42096883, 89.94524856]), 'currentState': array([ 2.3297887, 70.30973  ,  0.7857872], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3449
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.60017345, 110.48908946]), 'previousTarget': array([  8.55942675, 108.83555733]), 'currentState': array([ 5.687925 , 90.702255 ,  1.7280406], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 218
reward sum = 0.11180788242357734
running average episode reward sum: 0.2916719668810336
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.37423  , 119.01107  ,   1.2619011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.902921146332311}
episode index:3450
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.04084504, 79.07075866]), 'previousTarget': array([ 7.98241918, 78.97585674]), 'currentState': array([ 7.0846024, 59.09363  ,  5.384791 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.2916484741193169
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.810747 , 119.02564  ,   1.8634379], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0562533820827285}
episode index:3451
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.47305131, 88.84542326]), 'previousTarget': array([ 5.71960041, 88.81423159]), 'currentState': array([ 2.5971324, 69.053276 ,  0.4965412], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.291563987307579
{'currentTarget': array([ 10.32299301, 106.84501035]), 'previousTarget': array([ 10.25102821, 105.0881625 ]), 'currentState': array([10.813903 , 86.851036 ,  1.3738345], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3452
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.17364549, 91.033427  ]), 'previousTarget': array([14.17157288, 90.79898987]), 'currentState': array([17.025888 , 71.237854 ,  3.9753215], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2914795494311505
{'currentTarget': array([ 10.71803288, 116.41844546]), 'previousTarget': array([ 10.7288038 , 115.11917462]), 'currentState': array([14.649421, 96.80865 ,  1.302296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3453
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.33226349, 101.37061077]), 'previousTarget': array([ 8.49579896, 99.9439862 ]), 'currentState': array([ 8.61586   , 81.383446  ,  0.60299563], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2913951604475283
{'currentTarget': array([  8.9209445, 116.5731505]), 'previousTarget': array([  8.91289649, 116.50121996]), 'currentState': array([ 2.9140494, 97.49654  ,  4.0929937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3454
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.28693143, 78.84290674]), 'previousTarget': array([15.39931926, 78.83019061]), 'currentState': array([17.83514  , 59.005905 ,  5.0891333], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.29138762675412744
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.548456, 119.85001 ,   1.553186], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5557032039917191}
episode index:3455
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.50168299, 74.77552446]), 'previousTarget': array([ 6.53392998, 74.94108971]), 'currentState': array([ 4.959201, 54.835094,  5.587302], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2913033132047194
{'currentTarget': array([14.57592932, 92.03662842]), 'previousTarget': array([14.57592932, 92.03662842]), 'currentState': array([17.805773, 72.29915 ,  4.687919], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3456
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.013332, 123.879906,   5.5073  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.371175524498965}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.29146288505319473
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.040928 , 121.83941  ,   2.8580787], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1135174173612805}
episode index:3457
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.76690301, 98.70320383]), 'previousTarget': array([ 5.83020719, 98.62981184]), 'currentState': array([ 1.8678426, 79.08695  ,  6.2168097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.29142410263476054
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.178762 , 120.16334  ,   3.1421392], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1900252463781966}
episode index:3458
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.35888796, 66.99683192]), 'previousTarget': array([ 9.2739469 , 66.99812374]), 'currentState': array([ 9.116991, 46.998295,  4.000403], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29133985166551085
{'currentTarget': array([15.42318385, 92.43302424]), 'previousTarget': array([15.31081303, 92.17352524]), 'currentState': array([19.28374 , 72.80916 ,  4.515544], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3459
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.62758325, 108.3303079 ]), 'previousTarget': array([  6.5762039 , 108.20692454]), 'currentState': array([ 1.0750086, 89.11654  ,  5.3883576], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2912556493962434
{'currentTarget': array([13.80585564, 90.80732346]), 'previousTarget': array([13.9326044 , 90.77274029]), 'currentState': array([16.39138  , 70.97515  ,  2.3746533], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3460
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.09628 , 113.19664 ,   4.012133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.941361201408589}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.29135165336778196
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.93867  , 118.31928  ,   1.6411012], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.92507477111018}
episode index:3461
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.13396419, 69.7078654 ]), 'previousTarget': array([12.14364323, 69.98165792]), 'currentState': array([12.981829 , 49.725845 ,  2.6018848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.2913187737818175
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.13519  , 119.362564 ,   1.5665858], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9707462652080412}
episode index:3462
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.50728447, 109.31859939]), 'previousTarget': array([  9.64482588, 108.98960229]), 'currentState': array([ 8.585697, 89.33984 ,  6.071297], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.29140416737354596
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.357422 , 118.353615 ,   2.9823618], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7673400493492204}
episode index:3463
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.30663149, 76.1485615 ]), 'previousTarget': array([ 5.17453183, 75.88143384]), 'currentState': array([ 3.1782107, 56.26214  ,  1.4625905], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29132004376864595
{'currentTarget': array([ 10.25600097, 114.15348488]), 'previousTarget': array([ 10.25383128, 114.27867037]), 'currentState': array([11.130901, 94.17263 ,  4.992414], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3464
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.50507239, 74.63783359]), 'previousTarget': array([ 6.53392998, 74.94108971]), 'currentState': array([ 4.9687257, 54.69693  ,  3.9905643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2912359687199393
{'currentTarget': array([15.31382806, 92.23098955]), 'previousTarget': array([15.2609447 , 92.27645055]), 'currentState': array([19.072788 , 72.58741  ,  3.3232207], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3465
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 14.00395656, 100.27890766]), 'previousTarget': array([ 13.98111713, 100.59205401]), 'currentState': array([17.98335 , 80.678795,  4.347709], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.2912177905582011
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.435445 , 119.0161   ,   1.8688072], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0759532066909492}
episode index:3466
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.120466, 116.81643 ,   2.693086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.79975402160403}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.2912765217001226
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.895988, 121.16428 ,   1.856359], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2249334626383885}
episode index:3467
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.84505788, 102.47923733]), 'previousTarget': array([  7.67835792, 102.81984861]), 'currentState': array([ 3.3006868, 82.79581  ,  3.7033515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29119253193031286
{'currentTarget': array([  9.79278215, 118.41982062]), 'previousTarget': array([  9.81051424, 118.5305376 ]), 'currentState': array([ 7.192333 , 98.5896   ,  5.4637575], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3468
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.34826726, 78.97663305]), 'previousTarget': array([ 9.3278248 , 78.99731309]), 'currentState': array([ 9.03057  , 58.979156 ,  5.3554597], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2911085905835471
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.004196 , 106.879364 ,   2.5153592], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.159008269148949}
episode index:3469
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.3013435 , 82.73079687]), 'previousTarget': array([ 9.35082321, 82.99692284]), 'currentState': array([ 8.926485  , 62.73431   ,  0.06386951], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29102469761796107
{'currentTarget': array([15.49950139, 91.92593643]), 'previousTarget': array([15.41757417, 91.79142714]), 'currentState': array([19.344278 , 72.29897  ,  3.9289975], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3470
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.9980515 , 76.01748051]), 'previousTarget': array([12.06352827, 75.97806349]), 'currentState': array([12.905682, 56.038086,  4.471855], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29094085299173866
{'currentTarget': array([ 10.73543045, 117.62468154]), 'previousTarget': array([ 10.73543045, 117.62468154]), 'currentState': array([16.650667, 98.51945 ,  3.028647], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3471
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.301699 , 123.061844 ,   0.9849285], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.83049666910822}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2910146478619573
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.847053, 121.97386 ,   6.021273], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9797784985797071}
episode index:3472
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.43569447, 87.04015255]), 'previousTarget': array([ 9.37729134, 86.99644096]), 'currentState': array([ 9.093325, 67.04308 ,  4.677395], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29093085441310557
{'currentTarget': array([ 11.34654476, 102.78766504]), 'previousTarget': array([ 11.15653412, 100.81064488]), 'currentState': array([12.906406 , 82.84859  ,  1.2937925], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3473
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.23656276, 80.7010593 ]), 'previousTarget': array([11.32242309, 80.98851894]), 'currentState': array([11.865562, 60.710953,  4.66366 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29084710920458134
{'currentTarget': array([ 12.18646638, 113.38907808]), 'previousTarget': array([ 12.19547843, 113.41067143]), 'currentState': array([18.466608 , 94.400665 ,  6.0065103], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3474
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.89115319, 93.06808159]), 'previousTarget': array([10., 93.]), 'currentState': array([ 9.810323, 73.068245,  4.764355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2907634121947383
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.3482214e+01, 1.0735423e+02, 9.1850974e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.116449311404166}
episode index:3475
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.91660376, 105.58555604]), 'previousTarget': array([ 10.44465866, 103.99228841]), 'currentState': array([12.185825 , 85.62587  ,  1.0820003], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.2906982700152338
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.370984 , 119.69123  ,   2.6108263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4826671945498063}
episode index:3476
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.47004408, 108.69223109]), 'previousTarget': array([ 10.39421747, 106.99082358]), 'currentState': array([11.300692 , 88.70949  ,  1.4654473], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906146639554077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.908632 , 121.324905 ,   2.6130736], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.196172091462323}
episode index:3477
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.19829959, 98.24565674]), 'previousTarget': array([ 8.90815322, 95.9793708 ]), 'currentState': array([ 8.461751, 78.259224,  2.06727 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905311059726718
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.8972397, 101.39252  ,   4.04655  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.86439879306341}
episode index:3478
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.669277 , 108.93829  ,   3.0922797], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.186949449411655}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.2905045877635934
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.346806 , 119.59925  ,   1.1475638], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7010737025859823}
episode index:3479
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  5.8507125, 103.40285  ]), 'previousTarget': array([  5.8507125, 103.40285  ]), 'currentState': array([ 1.       , 84.       ,  2.8928747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29042110943377625
{'currentTarget': array([  8.54561377, 111.93358488]), 'previousTarget': array([  8.27416858, 113.01557262]), 'currentState': array([ 4.996807, 92.25095 ,  5.762143], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3480
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.00153272, 85.492273  ]), 'previousTarget': array([ 4.87879691, 84.79172879]), 'currentState': array([ 3.699502, 65.6252  ,  0.475879], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.29041545966361776
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.788503 , 119.505005 ,   1.9894736], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3087191793813595}
episode index:3481
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.95944937, 103.39083953]), 'previousTarget': array([ 11.79136948, 103.87767469]), 'currentState': array([12.112852, 83.424126,  3.50302 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.2903541932600182
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.702477 , 121.52943  ,   3.6271505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6830389842183906}
episode index:3482
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.2940371 , 109.30441269]), 'previousTarget': array([ 13.25304229, 109.1565257 ]), 'currentState': array([19.180794 , 90.190384 ,  3.2980976], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.290420224362341
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.754462 , 118.16254  ,   2.1651433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2198279156525182}
episode index:3483
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.35727575, 115.87984822]), 'previousTarget': array([ 10.3390904 , 115.93091516]), 'currentState': array([12.085076  , 95.95462   ,  0.14658624], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.2905623764623539
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.144814, 118.08892 ,   2.668649], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6634453831719664}
episode index:3484
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.081587 , 118.801025 ,   2.1078224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.170041978491904}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.2906768346286112
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.608477 , 120.05015  ,   3.3323643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6105396325304387}
episode index:3485
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.27978544, 87.07682839]), 'previousTarget': array([ 9.38454428, 87.9963028 ]), 'currentState': array([ 7.236222 , 67.10407  ,  3.7185159], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905934505681899
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.5974226, 100.899315 ,   2.2125437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.401384100203163}
episode index:3486
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.       , 108.       ,   3.2972732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2906702135988695
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.977895, 121.86285 ,   2.666843], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1039189199418367}
episode index:3487
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.68763145, 71.87955013]), 'previousTarget': array([10.70591415, 71.99783772]), 'currentState': array([10.973398, 51.88159 ,  4.863962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29058687924864046
{'currentTarget': array([ 8.23325475, 90.52689222]), 'previousTarget': array([ 8.36746459, 90.64861682]), 'currentState': array([7.0365167e+00, 7.0562729e+01, 2.0968109e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3488
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.76990702, 114.46175013]), 'previousTarget': array([  9.29248216, 113.86817872]), 'currentState': array([8.9397001e+00, 9.4478989e+01, 1.2589097e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29050359266817366
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.808452 , 111.45815  ,   4.3141932], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.731188433607926}
episode index:3489
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.856989 , 110.12937  ,   3.8640904], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.704851473914982}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.290563573595747
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.111265, 119.57158 ,   2.041929], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1909890329633022}
episode index:3490
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.782714 , 102.82747  ,   3.9588335], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.31508541800985}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.29053713724025954
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.451746  , 119.99409   ,   0.64302653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5482653034727116}
episode index:3491
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.33565883, 81.96794608]), 'previousTarget': array([ 5.3563548 , 80.86070472]), 'currentState': array([ 1.3894365, 62.186142 ,  2.4764829], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2904539364563992
{'currentTarget': array([ 4.99253883, 89.79177627]), 'previousTarget': array([ 4.99086749, 89.86331295]), 'currentState': array([ 1.7218733, 70.06102  ,  4.673714 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3492
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.08487603, 100.77397632]), 'previousTarget': array([  7.04114369, 100.76743395]), 'currentState': array([ 4.086667 , 80.999985 ,  4.7015924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.29049509803393586
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.615934 , 118.1191   ,   2.4786856], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3352544081785145}
episode index:3493
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.36101515, 112.56169174]), 'previousTarget': array([  8.05211208, 110.58520839]), 'currentState': array([ 4.0573754, 93.03021  ,  1.7109231], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.29044118942802993
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.456789, 119.76507 ,   5.648304], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5609910819932806}
episode index:3494
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.35474557, 81.54559523]), 'previousTarget': array([ 5.3964032 , 81.85591226]), 'currentState': array([ 2.956202, 61.68994 ,  2.807966], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29035808751403047
{'currentTarget': array([ 5.75516361, 89.32133698]), 'previousTarget': array([ 5.59372657, 89.4767325 ]), 'currentState': array([3.0139897e+00, 6.9510078e+01, 5.7229217e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3495
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.2932692 , 105.18977797]), 'previousTarget': array([  6.33860916, 103.5237412 ]), 'currentState': array([ 1.4374089, 85.788216 ,  1.7759639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.29041657738018783
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.442682 , 119.02621  ,   1.0763206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0696917563940656}
episode index:3496
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.799566 , 106.83283  ,   2.8742547], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.55053812215128}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.29051183297570143
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.2179    , 118.0369    ,   0.62975764], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.651344519256039}
episode index:3497
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.       , 103.       ,   2.6975892], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.290558011311119
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.321124, 118.279884,   1.60551 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1689091073092794}
episode index:3498
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.03140447, 113.34984653]), 'previousTarget': array([ 12.11828302, 113.11558017]), 'currentState': array([17.874231, 94.22234 ,  5.850099], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.29051066058261027
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.381051, 119.35014 ,   2.558876], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7533348997582059}
episode index:3499
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.2904889, 77.1734987]), 'previousTarget': array([ 9.31742033, 76.99748095]), 'currentState': array([ 8.959192  , 57.176243  ,  0.08097046], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2904276575367295
{'currentTarget': array([ 9.25609901, 99.82360869]), 'previousTarget': array([ 9.33228986, 99.8333611 ]), 'currentState': array([ 8.519202 , 79.83719  ,  3.3416262], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3500
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.38906143, 114.10098952]), 'previousTarget': array([  8.39259851, 114.25928039]), 'currentState': array([ 3.1202674, 94.80747  ,  4.1085534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903447019076131
{'currentTarget': array([ 7.31333299, 81.84025458]), 'previousTarget': array([ 7.34960456, 82.15728159]), 'currentState': array([ 5.9086943, 61.88964  ,  3.9489107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3501
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.949026, 108.69506 ,   5.912933], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.323583779429757}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.29046881259694785
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.095992 , 119.34866  ,   1.6505324], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2749267097524806}
episode index:3502
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.898898 , 110.100555 ,   3.9777186], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.136437641197482}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2905452605917954
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.233128 , 118.96552  ,   1.8347669], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0604204932532255}
episode index:3503
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.025574, 118.14246 ,   1.590282], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.309929672978197}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.2906630980445374
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.302141, 118.47113 ,   1.308341], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6806097885825426}
episode index:3504
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.94808928, 111.91550221]), 'previousTarget': array([ 10.86933753, 111.88618308]), 'currentState': array([13.2775755 , 92.05163   ,  0.58186185], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.29060548584324514
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.375288, 118.0638  ,   1.293042], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.374931404909049}
episode index:3505
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.977908 , 121.11455  ,   1.5340889], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.080921172434699}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.2905742597928409
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.322073 , 119.736244 ,   3.0476723], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4162909167876511}
episode index:3506
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.93491734, 118.76301678]), 'previousTarget': array([  9.95130299, 118.97736275]), 'currentState': array([ 8.88409  , 98.79064  ,  4.5469356], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.29058485246396865
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.914129 , 118.93743  ,   2.6252122], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5192654267604424}
episode index:3507
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.38455478, 110.87476192]), 'previousTarget': array([  7.54459231, 109.47682419]), 'currentState': array([ 1.8740946, 91.64887  ,  1.6640075], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.29054642455542323
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.699451 , 118.49676  ,   1.1666954], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.268892507570575}
episode index:3508
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.54441999, 113.66561268]), 'previousTarget': array([  9.13066247, 111.88618308]), 'currentState': array([ 8.109692, 93.71714 ,  1.215256], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.29065426876574363
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.059931 , 118.69211  ,   1.9882183], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.339754169874639}
episode index:3509
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.28499787, 77.88691347]), 'previousTarget': array([16.21490337, 75.80513158]), 'currentState': array([17.775372 , 58.04257  ,  2.5639014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905714612817648
{'currentTarget': array([14.58084971, 89.40948718]), 'previousTarget': array([14.52758402, 89.37434864]), 'currentState': array([17.542772, 69.63003 ,  3.182477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3510
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.888569 , 124.222664 ,   0.3448153], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.947648248787832}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.2905908793496657
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.451536  , 121.7       ,   0.79176027], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.299506431878789}
episode index:3511
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.94444196, 77.65345955]), 'previousTarget': array([ 3.87311278, 77.79255473]), 'currentState': array([ 1.1132418, 57.854866 ,  4.283824 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29050813707194656
{'currentTarget': array([ 8.30477873, 91.64329642]), 'previousTarget': array([ 8.3244384 , 91.62847408]), 'currentState': array([ 7.111269 , 71.67894  ,  4.5690947], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3512
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.8065464 , 106.00668033]), 'previousTarget': array([  8.75787621, 105.922597  ]), 'currentState': array([ 7.106969 , 86.079025 ,  5.7748833], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.2904651445959156
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.020904 , 120.53759  ,   1.9613181], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0508109731452495}
episode index:3513
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.39408785, 85.45593165]), 'previousTarget': array([11.27320764, 84.98678996]), 'currentState': array([12.200567 , 65.4722   ,  5.8784585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 274
reward sum = 0.06368590427489448
running average episode reward sum: 0.2904006086709523
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.6938305, 119.60985  ,   0.79132  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3631938065212532}
episode index:3514
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.1550665, 110.38232  ,   1.1872711], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.654726657105059}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.29059127592595346
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.6318655, 118.06506  ,   1.9789217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9696452367848045}
episode index:3515
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.57998435, 65.89578688]), 'previousTarget': array([ 8.54034323, 65.9926994 ]), 'currentState': array([ 8.055246, 45.90267 ,  4.944281], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.29053099712084757
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.423836, 118.147736,   2.623312], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9398063463499073}
episode index:3516
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.9382814, 112.83526  ,   2.9611163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.55198098475512}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.29068566950194774
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.710218 , 119.08733  ,   5.9863605], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9575662753499417}
episode index:3517
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.588118 , 114.89555  ,   4.6863775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.8633953893719255}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.2906769713188539
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.865284, 120.26204 ,   3.122844], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8835999597333324}
episode index:3518
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.56478491, 110.69064325]), 'previousTarget': array([ 10., 112.]), 'currentState': array([ 8.630799, 90.71246 ,  4.347443], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905943691672998
{'currentTarget': array([  8.45211699, 116.40880831]), 'previousTarget': array([  8.45211699, 116.40880831]), 'currentState': array([ 0.5357217, 98.042244 ,  4.65117  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3519
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.96680906, 79.77872706]), 'previousTarget': array([ 3.96680906, 79.77872706]), 'currentState': array([ 1.      , 60.      ,  2.088732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905118139487864
{'currentTarget': array([ 5.33053649, 89.33692116]), 'previousTarget': array([ 5.28256066, 89.18293917]), 'currentState': array([ 2.3195899 , 69.564865  ,  0.65781677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3520
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.885389, 113.686905,   2.033123], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.374879129249553}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.29070488046001935
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.226082, 119.14697 ,   2.046033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.151783380222412}
episode index:3521
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.62078673, 91.2206132 ]), 'previousTarget': array([13.56917619, 90.85172777]), 'currentState': array([16.117342 , 71.377045 ,  4.8584967], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906223407438183
{'currentTarget': array([15.47903811, 89.90265965]), 'previousTarget': array([15.55339521, 89.93031849]), 'currentState': array([19.061047 , 70.22604  ,  4.1948175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3522
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.994015, 120.63958 ,   5.449528], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.016726868943485}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.2906582472266992
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.7972  , 120.78727 ,   4.163015], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8129705828771696}
episode index:3523
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.02887  , 121.05273  ,   6.0938435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.120090081302828}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.29077538384638507
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.575446 , 120.812645 ,   3.5804482], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.772687883825715}
episode index:3524
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.35527348, 68.88715728]), 'previousTarget': array([ 9.28166221, 68.99801656]), 'currentState': array([ 9.103018  , 48.88875   ,  0.39565438], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906928943757903
{'currentTarget': array([15.58351285, 90.66520102]), 'previousTarget': array([15.61889697, 90.77541977]), 'currentState': array([19.323126 , 71.01793  ,  3.4158127], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3525
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.61562521, 84.69675044]), 'previousTarget': array([11.28616939, 83.98725709]), 'currentState': array([14.093379  , 64.75142   ,  0.71079254], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.29072182748242165
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.044441 , 120.03319  ,   1.6108677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0449683741947793}
episode index:3526
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.70308936, 83.02662015]), 'previousTarget': array([10.64917679, 82.99692284]), 'currentState': array([11.083343, 63.030235,  4.588998], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29063939997250315
{'currentTarget': array([ 4.93280427, 90.01205942]), 'previousTarget': array([ 4.94942646, 90.04849163]), 'currentState': array([ 1.6005523, 70.29161  ,  3.653935 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3527
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.30191721, 81.00613353]), 'previousTarget': array([13.31113847, 80.9285661 ]), 'currentState': array([14.989435, 61.077454,  4.596934], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.29059206143343397
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.933703 , 119.72614  ,   2.1201477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.95299939752536}
episode index:3528
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.313596 , 117.03423  ,   5.5845313], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.044162300142543}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.29070119631233077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.466081 , 119.81571  ,   1.3608527], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5449500317499585}
episode index:3529
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.73408799, 75.0027323 ]), 'previousTarget': array([15.62878378, 72.85893586]), 'currentState': array([18.262281 , 55.16317  ,  1.5009092], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906188446986446
{'currentTarget': array([  7.95440177, 111.35107945]), 'previousTarget': array([  7.13778964, 111.03031635]), 'currentState': array([ 3.3511052, 91.88805  ,  6.134248 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3530
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.964936 , 121.31823  ,   6.0446725], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.157503385105439}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2908031724541536
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.825584 , 119.009315 ,   5.9371552], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0059207694404422}
episode index:3531
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.60881005, 114.46079573]), 'previousTarget': array([ 11.60740149, 114.25928039]), 'currentState': array([17.187105, 95.25448 ,  6.026005], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2907208386001179
{'currentTarget': array([  8.09481081, 114.43073784]), 'previousTarget': array([  8.01361476, 114.39333281]), 'currentState': array([ 1.6213149, 95.50737  ,  3.2688944], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3532
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.60476177, 83.46099352]), 'previousTarget': array([14.47491441, 84.83995823]), 'currentState': array([15.568333, 63.557617,  4.27103 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906385513545475
{'currentTarget': array([ 5.55424065, 86.60913257]), 'previousTarget': array([ 5.20158971, 84.97480379]), 'currentState': array([ 2.9146743, 66.78408  ,  1.0491235], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3533
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.90564486, 104.50521925]), 'previousTarget': array([  6.9223227 , 104.61161351]), 'currentState': array([ 2.988922, 84.89249 ,  5.410291], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29055631067787674
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.8169985, 116.233955 ,   4.3832445], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.703038645509276}
episode index:3534
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.35835255, 107.54684577]), 'previousTarget': array([  7.47570668, 105.69567118]), 'currentState': array([ 3.2081642, 87.982185 ,  1.7241126], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.2905447917940806
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.211519  , 121.046005  ,   0.34812322], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0719050183603116}
episode index:3535
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.15180039, 90.58270603]), 'previousTarget': array([14.17157288, 90.79898987]), 'currentState': array([16.946795 , 70.77897  ,  5.8031974], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2904626241493425
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.70262  , 120.921    ,   2.0302987], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.815445998405567}
episode index:3536
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.70279995, 84.57375351]), 'previousTarget': array([11.2986772 , 82.98769988]), 'currentState': array([12.6630125, 64.59682  ,  1.7053187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29038050296637685
{'currentTarget': array([  8.33345256, 115.71248605]), 'previousTarget': array([  8.42707867, 115.87928717]), 'currentState': array([ 1.0876209, 97.07119  ,  5.222002 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3537
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.97260384, 113.50892758]), 'previousTarget': array([  9.04114369, 113.76743395]), 'currentState': array([ 5.845959 , 93.75484  ,  4.3364615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.2903166105676405
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.434267, 121.25151 ,   5.518648], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9035232559556228}
episode index:3538
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.1573906, 113.87093  ,   4.8920684], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.234023970361966}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.2904255149017724
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.770957 , 119.43385  ,   1.2711741], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6107233051364443}
episode index:3539
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.58678368, 70.83200822]), 'previousTarget': array([ 3.58678368, 70.83200822]), 'currentState': array([ 1.       , 51.       ,  2.3889577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.2903989233761004
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.069668 , 118.40402  ,   0.4735074], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.50466113964311}
episode index:3540
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.83704394, 91.28062288]), 'previousTarget': array([10., 89.]), 'currentState': array([ 9.723564 , 71.280945 ,  1.9822415], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.29037945228427964
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.418577 , 118.91351  ,   1.6015809], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9186848862919292}
episode index:3541
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.59076049, 92.15808935]), 'previousTarget': array([10.60007998, 89.9960012 ]), 'currentState': array([11.015033, 72.16259 ,  1.121945], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902974705078019
{'currentTarget': array([ 10.82791168, 116.98241594]), 'previousTarget': array([ 11.03886296, 117.21988092]), 'currentState': array([16.119608, 97.69517 ,  3.620584], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3542
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8884006, 102.21783  ,   4.709056 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.90710848779882}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.29038289782247695
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.677442 , 120.639595 ,   2.9589171], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7952415202193333}
episode index:3543
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.134018 , 123.35462  ,   3.5316226], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.99139929824951}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2905747477384413
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.850588 , 121.583534 ,   3.5631778], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.797520618336636}
episode index:3544
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.163262 , 115.67575  ,   1.1856415], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.00265651746773}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.2906796016998645
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.960473, 121.61571 ,   2.729602], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8796326487810118}
episode index:3545
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.5150853 , 68.84122844]), 'previousTarget': array([ 3.5150853 , 68.84122844]), 'currentState': array([ 1.       , 49.       ,  3.8872504], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905976277569148
{'currentTarget': array([ 8.15839178, 86.63352389]), 'previousTarget': array([ 8.22782048, 86.84716591]), 'currentState': array([ 7.056202 , 66.66392  ,  5.4866433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3546
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.03505077, 79.51247772]), 'previousTarget': array([ 5.99007438, 79.9007438 ]), 'currentState': array([ 4.0857725, 59.607697 ,  1.4809295], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29051570003552857
{'currentTarget': array([ 4.75085563, 90.2509628 ]), 'previousTarget': array([ 4.57138044, 90.30131366]), 'currentState': array([ 1.2755892, 70.555214 ,  4.301024 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3547
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.13667274, 79.68538566]), 'previousTarget': array([16.03319094, 79.77872706]), 'currentState': array([19.146395 , 59.913143 ,  6.0767365], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29043381849662336
{'currentTarget': array([ 4.32852006, 87.26919216]), 'previousTarget': array([ 4.13536553, 87.08500378]), 'currentState': array([ 0.91387284, 67.56284   ,  3.7655218 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3548
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.424941, 97.949174]), 'previousTarget': array([ 8.424941, 97.949174]), 'currentState': array([ 7.       , 78.       ,  2.4094963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.29036680848743657
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.032958 , 118.35537  ,   3.1723266], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5639936363550997}
episode index:3549
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.92164754, 116.6095487 ]), 'previousTarget': array([ 11.67544468, 114.97366596]), 'currentState': array([16.167988 , 97.30991  ,  2.7070875], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.29040733370386035
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.089198  , 121.653984  ,   0.50119555], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.888179912383374}
episode index:3550
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.56468542, 81.85001957]), 'previousTarget': array([14.00992562, 79.9007438 ]), 'currentState': array([15.425355 , 61.93676  ,  2.1314626], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903255518582665
{'currentTarget': array([ 4.25175586, 83.09097734]), 'previousTarget': array([ 4.26759683, 82.89282155]), 'currentState': array([ 1.1740397, 63.329205 ,  6.105239 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3551
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.86244165, 113.6554506 ]), 'previousTarget': array([  9.20863052, 112.87767469]), 'currentState': array([ 5.3327885, 93.969376 ,  2.7654338], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902438160610091
{'currentTarget': array([  8.85284622, 115.6148212 ]), 'previousTarget': array([  8.85284622, 115.6148212 ]), 'currentState': array([ 3.791214  , 96.26592   ,  0.22781658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3552
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.75653117, 105.16787863]), 'previousTarget': array([ 12.6207237, 104.7124451]), 'currentState': array([14.1086445, 85.30667  ,  3.1495454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2901621262732069
{'currentTarget': array([  7.91983213, 114.01859274]), 'previousTarget': array([  8.11422156, 114.38840196]), 'currentState': array([ 1.3503257, 95.12834  ,  1.6290388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3553
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.81011005, 77.16516143]), 'previousTarget': array([12.04869701, 76.97736275]), 'currentState': array([12.654514 , 57.182995 ,  4.5879374], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2900804824560226
{'currentTarget': array([13.48769946, 91.01101776]), 'previousTarget': array([13.48769946, 91.01101776]), 'currentState': array([15.876696 , 71.15421  ,  1.6757259], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3554
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.63206562, 101.85476037]), 'previousTarget': array([ 10.92049484, 102.97084547]), 'currentState': array([13.423724, 81.93517 ,  5.682759], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.2900238444391616
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.829064 , 118.935486 ,   1.9264724], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1162861016073027}
episode index:3555
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.948199 , 118.2268   ,   3.5657406], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6343352388413823}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.29022350027593347
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.948199 , 118.2268   ,   3.5657406], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6343352388413823}
episode index:3556
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.984868 , 111.234245 ,   2.4678187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.084015317698302}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.29040132462064866
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.615532, 118.30888 ,   2.446918], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1855494303412453}
episode index:3557
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.06712629, 70.10877095]), 'previousTarget': array([12.14364323, 69.98165792]), 'currentState': array([12.895069 , 50.125916 ,  6.1294737], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903197053613399
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.552388, 103.18539 ,   3.032997], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.16441810940795}
episode index:3558
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.41198162, 100.90598914]), 'previousTarget': array([  9.51265202, 100.99342862]), 'currentState': array([ 8.796354, 80.91547 ,  5.674462], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902381319684314
{'currentTarget': array([  8.27310444, 115.51366897]), 'previousTarget': array([  8.22259408, 115.37637423]), 'currentState': array([ 1.0885085, 96.84869  ,  3.3588917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3559
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.258082, 117.70862 ,   4.729327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.8783168134656925}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.29042916030214816
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.465768 , 118.00953  ,   4.6338506], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.513134020913737}
episode index:3560
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.171617, 124.086395,   4.419711], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.3254966876403085}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.2906173565531164
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.416889 , 119.64369  ,   5.8429704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6833546626653573}
episode index:3561
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.6343855 , 91.69329015]), 'previousTarget': array([11.20063923, 89.98401917]), 'currentState': array([11.082496 , 71.69831  ,  2.0897264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.29067330066217056
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.4814205, 119.130035 ,   1.9095817], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0127996271698918}
episode index:3562
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.62408062, 98.09834526]), 'previousTarget': array([11.575059, 97.949174]), 'currentState': array([13.103086 , 78.15311  ,  1.7173965], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.2906057630035133
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.815669  , 118.309105  ,   0.41176265], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4810845023959858}
episode index:3563
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.77141  , 115.92493  ,   6.1121383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.147444429935253}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.2906523410014764
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.912319, 119.272446,   1.504093], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.308581235283331}
episode index:3564
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.120042 , 112.96595  ,   4.0090466], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.12266473802568}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.29060549014401077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.930103, 119.832   ,   2.409519], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9374009678705206}
episode index:3565
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.8289633, 106.90089  ,   5.7453895], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.479933278680946}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.29055069322875987
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.035748 , 118.57548  ,   1.1406231], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7612600630626456}
episode index:3566
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.038952 , 121.003395 ,   4.8652616], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.137882606090641}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.2906829590042346
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.477458, 120.666245,   4.236433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8196632752709565}
episode index:3567
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.07350117, 82.69062013]), 'previousTarget': array([15.22022743, 82.80587954]), 'currentState': array([17.76839 , 62.873013,  5.410664], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906014895650518
{'currentTarget': array([  8.77134915, 116.02567367]), 'previousTarget': array([  8.78452126, 116.06091043]), 'currentState': array([ 2.8642464, 96.91792  ,  4.738668 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3568
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.015239 , 102.72766  ,   4.7934055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.533549404904626}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2907424289862002
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.2466955, 118.87499  ,   1.7573344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.083199167402422}
episode index:3569
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.85534162, 76.86228876]), 'previousTarget': array([14.16166152, 74.91533358]), 'currentState': array([15.635703 , 56.94169  ,  1.6557431], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906609885299015
{'currentTarget': array([ 5.35501446, 89.11608466]), 'previousTarget': array([ 4.92888523, 87.46646009]), 'currentState': array([ 2.3804407, 69.338524 ,  1.5850687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3570
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.84172956, 67.97590216]), 'previousTarget': array([12.17877537, 66.98313264]), 'currentState': array([11.165279 , 47.97852  ,  2.7684388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29057959368573183
{'currentTarget': array([ 4.74863648, 91.00251483]), 'previousTarget': array([ 4.74863648, 91.00251483]), 'currentState': array([ 1.1846633, 71.322624 ,  1.4249178], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3571
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.0371733 , 86.09966247]), 'previousTarget': array([ 8.10940039, 85.96920706]), 'currentState': array([ 6.881111 , 66.1331   ,  4.5811477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.29053500630769225
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.956863 , 120.87338  ,   3.0074408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2955248678924498}
episode index:3572
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.23996072, 88.68082403]), 'previousTarget': array([ 6.33682495, 88.86301209]), 'currentState': array([ 3.85597  , 68.82342  ,  2.1362543], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2904536922840965
{'currentTarget': array([ 4.5616999 , 90.71362853]), 'previousTarget': array([ 4.48928465, 90.42441997]), 'currentState': array([ 0.9102438, 71.04978  ,  3.4227896], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3573
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.585464 , 112.7287   ,   1.4585677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.250867630870603}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.2906254685523462
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.608153 , 118.680984 ,   3.1851523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4524642459678743}
episode index:3574
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.3410873 , 102.27943607]), 'previousTarget': array([  6.01888287, 100.59205401]), 'currentState': array([ 2.2968307, 82.692604 ,  1.4192013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29054417471498895
{'currentTarget': array([  8.58154324, 115.69746974]), 'previousTarget': array([  8.46988867, 115.51077004]), 'currentState': array([ 2.3194814, 96.70309  ,  1.0102754], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3575
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.58315521, 74.88919001]), 'previousTarget': array([13.46607002, 74.94108971]), 'currentState': array([15.166769 , 54.951984 ,  5.7957196], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29046292634398363
{'currentTarget': array([13.20861637, 87.66224262]), 'previousTarget': array([13.03627435, 87.59264917]), 'currentState': array([15.183359, 67.75997 ,  5.766917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3576
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.7226679e+00, 1.0996394e+02, 3.2567199e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.009098992594227}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.29057838196843305
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.3787   , 118.67318  ,   1.1355371], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4650821393664042}
episode index:3577
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.       , 109.       ,   5.5651655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2906500919350128
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.577592 , 118.67334  ,   2.7837596], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4469414530561158}
episode index:3578
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.86295639, 106.84215509]), 'previousTarget': array([  7.99610759, 106.77431008]), 'currentState': array([ 4.656652, 87.10084 ,  2.434629], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29056888207417597
{'currentTarget': array([  8.72617233, 116.77252078]), 'previousTarget': array([  8.72617233, 116.77252078]), 'currentState': array([1.3837258e+00, 9.8169067e+01, 5.7633281e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3579
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.53780965, 87.99427343]), 'previousTarget': array([13.70751784, 87.86817872]), 'currentState': array([15.735162 , 68.11535  ,  4.5148654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29048771758197645
{'currentTarget': array([ 5.94921961, 89.65692087]), 'previousTarget': array([ 6.01466056, 89.94710375]), 'currentState': array([ 3.3027122 , 69.832794  ,  0.76035213], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3580
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.36005988, 87.77306939]), 'previousTarget': array([11.89059961, 85.96920706]), 'currentState': array([13.820799 , 67.826485 ,  1.3170791], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2904065984204065
{'currentTarget': array([14.26360097, 91.44495045]), 'previousTarget': array([14.26360097, 91.44495045]), 'currentState': array([17.217093, 71.66423 ,  2.730813], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3581
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.54557453, 82.22241192]), 'previousTarget': array([15.31272654, 80.81864176]), 'currentState': array([16.934834, 62.36564 ,  2.026987], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.2904108006614752
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.021784 , 118.050354 ,   2.3628678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.7774914455921107}
episode index:3582
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.20478515, 95.26444287]), 'previousTarget': array([10.56532009, 93.99527578]), 'currentState': array([12.177764  , 75.288124  ,  0.42486712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.2903478836046615
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.254798 , 120.52605  ,   5.2809477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8227603880522403}
episode index:3583
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  5.79491494, 102.39513058]), 'previousTarget': array([  6.62324859, 101.66906377]), 'currentState': array([ 1.1484417, 82.94236  ,  2.5164976], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902668713603522
{'currentTarget': array([  8.44531751, 116.35753783]), 'previousTarget': array([  8.44531751, 116.35753783]), 'currentState': array([ 0.5941288, 97.963005 ,  2.8271334], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3584
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.43665359, 78.42366327]), 'previousTarget': array([10., 77.]), 'currentState': array([ 9.165685 , 58.4255   ,  1.6872892], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2901859043111582
{'currentTarget': array([ 4.9945008 , 89.66472385]), 'previousTarget': array([ 4.98428043, 89.32660211]), 'currentState': array([ 1.738412, 69.93156 ,  4.963639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3585
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.92890983, 104.28595844]), 'previousTarget': array([ 14.0191792 , 104.36985865]), 'currentState': array([18.780087  , 84.883224  ,  0.13523814], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.29017123801179323
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.163057, 120.038704,   2.349366], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.163701138532375}
episode index:3586
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.54627144, 111.61257992]), 'previousTarget': array([  7.69281066, 110.44164417]), 'currentState': array([ 1.9306703, 92.41714  ,  2.0605476], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.29019136647007276
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.856857 , 118.59431  ,   1.3820537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.328925072745339}
episode index:3587
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.4054667 , 69.71575967]), 'previousTarget': array([ 3.55043485, 69.83671551]), 'currentState': array([ 0.80483294, 49.885563  ,  3.8655517 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2901104881628068
{'currentTarget': array([ 8.07827808, 62.89988048]), 'previousTarget': array([ 8.1314082, 62.5799029]), 'currentState': array([ 7.405553, 42.911198,  5.348882], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3588
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.003368, 109.0573  ,   3.466163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.48131348182706}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.2901805841750119
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.916379  , 120.03846   ,   0.37272924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9167648603846623}
episode index:3589
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.35038066, 81.98711467]), 'previousTarget': array([ 6.71776684, 81.92609538]), 'currentState': array([ 2.9221396, 62.13507  ,  3.1546502], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.29015667411305407
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.901643 , 119.69615  ,   1.2397077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1396105954562714}
episode index:3590
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.36694973, 97.01664863]), 'previousTarget': array([10.54557189, 95.99483671]), 'currentState': array([ 8.816281 , 77.02423  ,  2.7494526], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29007587303421445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.2807403, 113.53548  ,   6.056831 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.013161571178639}
episode index:3591
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.35236248, 86.56369883]), 'previousTarget': array([ 4.34829399, 86.71773129]), 'currentState': array([ 1.0213968, 66.84303  ,  5.6440935], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28999511694483965
{'currentTarget': array([  8.82374662, 114.64505598]), 'previousTarget': array([  8.88269122, 114.92961476]), 'currentState': array([ 4.5328937, 95.11076  ,  3.836643 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3592
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.140812 , 107.12066  ,   3.6674275], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.907967771234267}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28991440580736544
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.826226 , 116.1446   ,   4.5938325], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.681944848816454}
episode index:3593
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.9089732, 116.4645517]), 'previousTarget': array([ 10.95156206, 116.35234545]), 'currentState': array([15.889064 , 97.094505 ,  3.7357676], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28983373958426933
{'currentTarget': array([  8.55231544, 115.29575221]), 'previousTarget': array([  8.63478968, 115.6763749 ]), 'currentState': array([ 2.669768 , 96.18043  ,  2.0148296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3594
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.20770079, 103.46592053]), 'previousTarget': array([  6.22665585, 102.54828331]), 'currentState': array([3.877232e+00, 8.374517e+01, 7.554370e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28975311823807065
{'currentTarget': array([  8.74092244, 116.3010064 ]), 'previousTarget': array([  8.74200803, 116.27056171]), 'currentState': array([ 2.2963543, 97.36777  ,  0.1237743], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3595
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.54824694, 107.49220833]), 'previousTarget': array([  9.20990121, 106.96336993]), 'currentState': array([ 6.242369, 87.62558 ,  2.757635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896725417313304
{'currentTarget': array([  8.27568715, 114.07220416]), 'previousTarget': array([  8.31001313, 114.26570584]), 'currentState': array([ 2.689505, 94.86818 ,  6.033523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3596
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.51166035, 98.09557435]), 'previousTarget': array([10.54557189, 95.99483671]), 'currentState': array([10.978708 , 78.10103  ,  1.7557173], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895920100266511
{'currentTarget': array([ 10.79903643, 117.09201755]), 'previousTarget': array([ 10.73931464, 117.23680838]), 'currentState': array([16.098104 , 97.80679  ,  5.8182893], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3597
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.14771  , 103.46577  ,   2.6248832], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.57401888119041}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.2895623725914878
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.017586 , 119.02895  ,   1.8995447], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9712056746400859}
episode index:3598
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.06745187, 114.63775013]), 'previousTarget': array([ 11.97753117, 114.72658355]), 'currentState': array([19.262335 , 95.97673  ,  2.9518766], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28948191625011754
{'currentTarget': array([ 10.74196494, 117.701989  ]), 'previousTarget': array([ 10.67838429, 117.84947731]), 'currentState': array([16.887054, 98.66944 ,  4.394514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3599
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.50878117, 112.94322743]), 'previousTarget': array([ 12.45778872, 112.89972147]), 'currentState': array([19.208279 , 94.09869  ,  3.4580448], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28940150460671477
{'currentTarget': array([ 5.46509055, 98.55598537]), 'previousTarget': array([ 5.36535182, 98.38181601]), 'currentState': array([ 1.3270755 , 78.98875   ,  0.49237877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3600
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0351696, 120.08948  ,   0.5012775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.9654051361493785}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.28949959127638375
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.835591, 121.16765 ,   5.923899], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.435832663047235}
episode index:3601
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.82960538, 115.63441822]), 'previousTarget': array([  9.38290441, 114.85753677]), 'currentState': array([ 9.049572  , 95.649635  ,  0.53370607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.2894800848016725
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.044872 , 121.30357  ,   2.4684117], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.670647252489169}
episode index:3602
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0992734 , 112.25327   ,   0.34155387], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.374574982382704}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.28962000547856454
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.99048 , 120.450676,   2.715137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0881912943695875}
episode index:3603
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.37462405, 83.75723181]), 'previousTarget': array([14.51930531, 83.84555753]), 'currentState': array([16.771296, 63.901352,  4.470479], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895396447667225
{'currentTarget': array([10.321397  , 84.79086145]), 'previousTarget': array([10.94713551, 86.31569923]), 'currentState': array([10.503954 , 64.791695 ,  3.8614237], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3604
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.456789 , 121.17793  ,   4.2409506], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.582479011268299}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.28960222381600326
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.631644 , 119.62308  ,   2.2542536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.674614465230068}
episode index:3605
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.1902056, 114.84602  ,   2.0401554], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.870129288583399}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.2897856369679955
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.255679 , 120.06865  ,   0.9604488], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7456712235914247}
episode index:3606
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.65032537, 112.59182842]), 'previousTarget': array([ 11.66139084, 112.5237412 ]), 'currentState': array([15.999142 , 93.07036  ,  5.2960787], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897052971739927
{'currentTarget': array([  8.5358304 , 114.53445765]), 'previousTarget': array([  8.89696567, 116.33158187]), 'currentState': array([ 3.3604984, 95.21566  ,  4.6788383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3607
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.78663221, 82.10305683]), 'previousTarget': array([ 6.68886153, 80.9285661 ]), 'currentState': array([ 6.620521 , 62.13708  ,  0.8868963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28962500191424384
{'currentTarget': array([14.60759055, 90.56547406]), 'previousTarget': array([14.4410558 , 90.40008982]), 'currentState': array([17.700663 , 70.8061   ,  2.8912468], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3608
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.8764749, 110.05019  ,   3.3085   ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.236962619132271}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.28965356551259336
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.206065, 118.99839 ,   5.870731], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0546105085479582}
episode index:3609
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.85089286, 104.44462149]), 'previousTarget': array([ 12.96694159, 105.58914087]), 'currentState': array([18.657013, 85.03068 ,  5.408981], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28957332906785305
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.72847  , 105.85964  ,   1.0404041], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.40118992543098}
episode index:3610
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.32589  , 108.681335 ,   1.7752166], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.665755480353937}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.2895895350455392
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.641703 , 118.90301  ,   1.8601353], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7459565328586717}
episode index:3611
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.84031252, 82.60267784]), 'previousTarget': array([ 5.3563548 , 80.86070472]), 'currentState': array([ 3.6293564, 62.72526  ,  1.3550111], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895093607556595
{'currentTarget': array([ 4.57662113, 90.60884362]), 'previousTarget': array([ 4.57662113, 90.60884362]), 'currentState': array([ 0.94740635, 70.94088   ,  1.6372187 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3612
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.17081779, 112.81994318]), 'previousTarget': array([ 11.80941823, 111.55604828]), 'currentState': array([14.389609 , 93.08066  ,  2.5550432], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.28951292983257987
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.337118 , 120.10007  ,   2.8033977], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.340857328764787}
episode index:3613
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.55942675, 77.83555733]), 'previousTarget': array([ 4.55942675, 77.83555733]), 'currentState': array([ 2.       , 58.       ,  2.5692124], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2894328211082211
{'currentTarget': array([ 4.5755898 , 88.70466947]), 'previousTarget': array([ 4.56384795, 88.6589544 ]), 'currentState': array([ 1.1599249, 68.9985   ,  3.804509 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3614
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.97297837, 112.99876111]), 'previousTarget': array([  8.93097322, 112.78406925]), 'currentState': array([ 6.070215 , 93.21053  ,  2.8252165], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2893527567040418
{'currentTarget': array([  8.24291779, 114.79001096]), 'previousTarget': array([  8.34364982, 115.05766087]), 'currentState': array([ 1.8515546 , 95.838745  ,  0.22232741], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3615
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.232454  , 73.76899946]), 'previousTarget': array([ 4.40662735, 73.85467564]), 'currentState': array([ 1.7565479, 53.922844 ,  4.308075 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28927273658327185
{'currentTarget': array([ 4.67061412, 89.86796833]), 'previousTarget': array([ 4.86808127, 89.70082425]), 'currentState': array([ 1.1873213, 70.17364  ,  4.0229073], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3616
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.177072  , 103.15876   ,   0.11145514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.88232388733656}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.28937586329170445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.094102 , 119.789696 ,   1.7949278], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1141305409178606}
episode index:3617
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.62930933, 71.64020439]), 'previousTarget': array([ 5.0186243 , 70.89786813]), 'currentState': array([ 1.0171722, 51.81152  ,  2.8211472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28929588101882114
{'currentTarget': array([ 6.85230445, 85.64866156]), 'previousTarget': array([ 6.78802636, 85.69034254]), 'currentState': array([ 5.0273023, 65.7321   ,  3.4128778], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3618
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.846329  , 112.88506   ,   0.34372443], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.873932510791134}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.2894287208267201
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.368721 , 120.31027  ,   3.1087358], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4034479139177038}
episode index:3619
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.60617489, 102.88945734]), 'previousTarget': array([ 13.37675141, 101.66906377]), 'currentState': array([15.617722 , 83.11749  ,  2.9120505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.2895077059176456
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.876044  , 118.831635  ,   0.90613806], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6212200241783605}
episode index:3620
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.005774  , 109.96577   ,   0.13086851], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.084513582337305}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.28962796900243987
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.610586 , 119.65805  ,   1.7634355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6464863298891144}
episode index:3621
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.1314087, 112.39747  ,   6.1411066], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.12571844744182}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.28963066155415434
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.300319 , 118.79086  ,   5.5659075], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3969846425320287}
episode index:3622
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.993796 , 124.23248  ,   0.5704463], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.555607626957707}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.2898054101141526
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.199617 , 121.90198  ,   5.0805674], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9124240272067081}
episode index:3623
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.32085185, 91.11343266]), 'previousTarget': array([14.17157288, 90.79898987]), 'currentState': array([17.279535 , 71.33349  ,  2.0889885], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.289821493917789
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.882616 , 119.06124  ,   1.8197773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9460693272569035}
episode index:3624
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.040223 , 107.468346 ,   5.4777107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.8766458982683}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.28980325038125404
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.659136, 119.5743  ,   2.253026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7846516686599274}
episode index:3625
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.56223074, 89.76053121]), 'previousTarget': array([ 9.38454428, 87.9963028 ]), 'currentState': array([ 9.272726 , 69.76263  ,  1.9594299], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.28975149505268744
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.597763  , 118.40356   ,   0.66610897], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6463369551013995}
episode index:3626
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.43336937, 108.34525623]), 'previousTarget': array([ 13.4237961 , 108.20692454]), 'currentState': array([19.085033, 89.1604  ,  4.096651], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896716076815673
{'currentTarget': array([  7.88178553, 114.31095108]), 'previousTarget': array([  7.85222228, 114.38620024]), 'currentState': array([ 0.90317845, 95.56798   ,  5.4404674 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3627
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.31468281, 93.93299292]), 'previousTarget': array([ 8.30158275, 93.95760212]), 'currentState': array([ 7.0243115, 73.97466  ,  4.087824 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.28972953640874965
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.522785 , 118.24751  ,   0.8896824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8288017051449668}
episode index:3628
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.262114 , 112.04708  ,   5.9985375], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.467843798395041}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.28983778249819625
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.134849 , 120.7504   ,   1.0506176], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.01044398454374}
episode index:3629
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.69533667, 115.1689229 ]), 'previousTarget': array([ 10.92091492, 115.57960839]), 'currentState': array([13.544575 , 95.37292  ,  2.8069658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.2899422271197035
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.600531 , 120.95063  ,   2.7587168], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.861557421602609}
episode index:3630
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.59801299, 112.91971843]), 'previousTarget': array([ 11.06902678, 112.78406925]), 'currentState': array([12.281255 , 92.99068  ,  3.5465217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.289944826448867
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.838455, 119.44242 ,   5.60665 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9211485912236408}
episode index:3631
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.30355533, 107.68533226]), 'previousTarget': array([  9.17444044, 105.96548746]), 'currentState': array([ 8.174278 , 87.71724  ,  1.3646405], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.29002825751162925
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.006764 , 118.2156   ,   2.8636568], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.675270981419902}
episode index:3632
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.48708577, 101.91046299]), 'previousTarget': array([ 10.47386636, 101.99307839]), 'currentState': array([11.025418, 81.91771 ,  5.858609], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28994842589656966
{'currentTarget': array([  9.08816093, 112.3131753 ]), 'previousTarget': array([  9.11159825, 112.44460627]), 'currentState': array([ 6.7322063, 92.45242  ,  3.7285814], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3633
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.39368236, 99.02569528]), 'previousTarget': array([ 8.45951277, 98.94667447]), 'currentState': array([ 6.866454, 79.08409 ,  0.722388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.2900132712630058
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.493304  , 120.05114   ,   0.85352415], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.50756338424944}
episode index:3634
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.68029282, 115.09843662]), 'previousTarget': array([ 11.67544468, 114.97366596]), 'currentState': array([18.16594  , 96.17923  ,  1.9919096], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899334876945703
{'currentTarget': array([ 11.28076308, 117.1131523 ]), 'previousTarget': array([ 11.26352443, 117.02570474]), 'currentState': array([19.391472  , 98.831566  ,  0.21146268], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3635
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.15814987, 78.88201349]), 'previousTarget': array([ 7.30866485, 78.95713898]), 'currentState': array([ 5.779149 , 58.92961  ,  4.2262735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289853748011486
{'currentTarget': array([ 4.62227376, 90.29270058]), 'previousTarget': array([ 4.56406669, 90.2063629 ]), 'currentState': array([ 1.0597006, 70.61256  ,  1.9508305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3636
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.998092 , 105.00945  ,   0.7382146], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.288163897397682}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.2898368045724219
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.908927  , 118.08841   ,   0.58743656], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.116678236398483}
episode index:3637
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.11237289, 81.85231486]), 'previousTarget': array([ 8.03310171, 81.9732997 ]), 'currentState': array([ 7.1239405, 61.876755 ,  4.357226 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289757135302336
{'currentTarget': array([ 5.43422817, 90.05313611]), 'previousTarget': array([ 5.43422817, 90.05313611]), 'currentState': array([ 2.419813  , 70.28161   ,  0.44291586], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3638
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.5974273 , 103.89359004]), 'previousTarget': array([  7.60909025, 101.82908591]), 'currentState': array([ 4.646701 , 84.11246  ,  1.5348806], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.2898162528493228
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.339221 , 119.389305 ,   1.8898387], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8997650409017931}
episode index:3639
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.56742989, 111.50260196]), 'previousTarget': array([  7.9223227 , 109.61161351]), 'currentState': array([ 5.2425637, 91.78091  ,  1.0884825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 271
reward sum = 0.06563533949318147
running average episode reward sum: 0.28975466468631284
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.140565 , 119.457016 ,   1.9259287], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9370933008108737}
episode index:3640
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.97050421, 81.00642586]), 'previousTarget': array([10., 81.]), 'currentState': array([ 9.955376 , 61.00643  ,  6.0324144], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28967508361938443
{'currentTarget': array([ 5.24408347, 89.19990715]), 'previousTarget': array([ 5.27112805, 89.04878313]), 'currentState': array([ 2.1920066, 69.43416  ,  3.6158407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3641
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.28719443, 72.0803492 ]), 'previousTarget': array([ 9.29408585, 71.99783772]), 'currentState': array([ 8.989727 , 52.08256  ,  2.8789015], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28959554625430495
{'currentTarget': array([  8.68161869, 115.75308875]), 'previousTarget': array([  8.70080029, 115.84422816]), 'currentState': array([ 2.7520986, 96.65228  ,  2.1636114], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3642
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.148437, 122.80438 ,   1.803165], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.4032746434234245}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.28967882127493283
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.826603 , 120.91764  ,   5.3847337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9338795267824255}
episode index:3643
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.651883 , 114.99474  ,   1.2702547], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.047528679440758}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2898682892164051
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.603381 , 118.76292  ,   1.3862349], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.025143589888612}
episode index:3644
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8501267, 113.99188  ,   2.7534683], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.381178028063263}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2900393863242974
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.04655  , 120.31066  ,   1.4321553], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9779985208744755}
episode index:3645
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.4441864 , 95.12603092]), 'previousTarget': array([11.66961979, 94.95570316]), 'currentState': array([10.801279 , 75.12922  ,  2.6139119], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28995983630062094
{'currentTarget': array([  9.70796588, 104.42227534]), 'previousTarget': array([  9.65178736, 104.451293  ]), 'currentState': array([ 9.333094, 84.42579 ,  4.82708 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3646
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.66799053, 81.84442722]), 'previousTarget': array([ 6.6609096 , 79.93091516]), 'currentState': array([ 4.9280734, 61.920254 ,  1.6943774], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28988032990185464
{'currentTarget': array([ 4.77522382, 88.79878633]), 'previousTarget': array([ 4.73541162, 88.9478308 ]), 'currentState': array([ 1.4721293, 69.07343  ,  2.8900518], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3647
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.03151599, 110.03598745]), 'previousTarget': array([ 13.0720157 , 110.10128274]), 'currentState': array([18.852974 , 90.90197  ,  5.5713124], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.28998424750839735
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.050985 , 118.71549  ,   2.5761852], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.285519221206463}
episode index:3648
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.80164635, 114.56670712]), 'previousTarget': array([ 11.06902678, 112.78406925]), 'currentState': array([13.72091  , 94.78091  ,  1.3675776], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.29005472500768
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.283017, 118.933784,   3.826655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.103138357926317}
episode index:3649
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.24756572, 91.96105157]), 'previousTarget': array([ 8.24756572, 91.96105157]), 'currentState': array([ 7.       , 72.       ,  2.4071014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28997525795973267
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.400799 , 114.49799  ,   0.7595296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.4681920446406584}
episode index:3650
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.71387328, 104.73818004]), 'previousTarget': array([  7.28797975, 103.72787848]), 'currentState': array([ 2.5040183, 85.18627  ,  2.4311883], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898958344434468
{'currentTarget': array([  8.65726245, 116.28760574]), 'previousTarget': array([  8.62337985, 116.24528481]), 'currentState': array([ 1.854735 , 97.48001  ,  2.8099818], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3651
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.2545094 , 108.46202543]), 'previousTarget': array([ 12.19956187, 108.63559701]), 'currentState': array([16.089956, 88.83324 ,  4.670145], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.289853885968779
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.189619 , 119.38959  ,   3.1945505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6391861457629034}
episode index:3652
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.8471972, 110.21546  ,   4.393115 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.398171095012165}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.28993686230615445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.7182   , 120.21344  ,   5.7711773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2994493222000296}
episode index:3653
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.444247 , 108.828674 ,   5.0471087], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.279134903048098}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.2899649887883799
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.216638 , 119.78225  ,   1.4624281], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7966069995520326}
episode index:3654
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.62119101, 90.53187892]), 'previousTarget': array([ 8.78371133, 88.98463901]), 'currentState': array([ 9.364115 , 70.53353  ,  0.8652092], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.28999750730702123
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.311155 , 119.79515  ,   1.3830497], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.701222962531515}
episode index:3655
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.0509491 , 113.17249309]), 'previousTarget': array([ 10., 113.]), 'currentState': array([10.2001915, 93.17305  ,  5.4624176], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.2900820145551501
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.116004 , 118.22076  ,   1.4045414], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9867451442539206}
episode index:3656
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.59345538, 87.74964296]), 'previousTarget': array([ 5.66824024, 87.82121323]), 'currentState': array([ 2.885902 , 67.93376  ,  2.1518583], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29000269215576396
{'currentTarget': array([ 4.72817071, 90.52743753]), 'previousTarget': array([ 4.80244165, 90.59649745]), 'currentState': array([ 1.206615 , 70.83991  ,  4.0397177], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3657
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.93294185, 83.46371781]), 'previousTarget': array([ 4.82842712, 83.79898987]), 'currentState': array([ 2.1855245, 63.653324 ,  3.2939239], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899234131256503
{'currentTarget': array([  6.75090844, 108.11092319]), 'previousTarget': array([  6.70391296, 108.0854068 ]), 'currentState': array([ 1.4785683, 88.818375 ,  5.6914864], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3658
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.169571, 122.00582 ,   2.919652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0129761788659724}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2901147431575919
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.005094 , 121.97316  ,   2.8151922], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9731663643819868}
episode index:3659
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.9149311 , 84.91919658]), 'previousTarget': array([ 8.72679236, 84.98678996]), 'currentState': array([ 8.296616 , 64.92876  ,  5.2804193], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2900354768343248
{'currentTarget': array([ 6.23580036, 88.37572427]), 'previousTarget': array([ 6.2317639 , 88.46008545]), 'currentState': array([ 3.871911 , 68.515915 ,  0.2121799], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3660
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.15059199, 71.31803992]), 'previousTarget': array([12.10537398, 72.97998109]), 'currentState': array([11.6231575, 51.323624 ,  3.9990318], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28995625381415696
{'currentTarget': array([ 4.38010347, 90.93802324]), 'previousTarget': array([ 4.41206303, 91.08088269]), 'currentState': array([ 0.5829232, 71.301796 ,  2.9166532], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3661
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.84939819, 118.00379438]), 'previousTarget': array([  9.81071492, 117.91786413]), 'currentState': array([ 8.344793  , 98.06047   ,  0.15258142], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.28990413709152035
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.005388, 119.024574,   1.737358], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3930929076073402}
episode index:3662
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.342145 , 117.80478  ,   1.0406246], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.291673679411583}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.29009526345322073
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.074772  , 119.071655  ,   0.34098077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.420196651082938}
episode index:3663
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.6806614 , 113.99953034]), 'previousTarget': array([ 10.70751784, 113.86817872]), 'currentState': array([12.934898 , 94.126976 ,  6.1972837], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.29007900814614246
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.107651 , 119.69089  ,   0.8230886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9174295882630528}
episode index:3664
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.66223215, 107.81491   ]), 'previousTarget': array([ 12.09012019, 105.78718271]), 'currentState': array([14.3655   , 87.99844  ,  1.8814831], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.290099732111525
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.970972 , 118.76038  ,   2.0695667], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.574622350891796}
episode index:3665
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.87548463, 72.18008417]), 'previousTarget': array([ 7.86874449, 70.98112317]), 'currentState': array([ 8.405302 , 52.18561  ,  0.4302373], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29002059961504073
{'currentTarget': array([ 4.93071691, 89.68376058]), 'previousTarget': array([ 4.91326501, 90.00266356]), 'currentState': array([ 1.6322432, 69.957634 ,  0.9088886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3666
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.65078052, 67.16928646]), 'previousTarget': array([11.45226033, 66.99249812]), 'currentState': array([12.275408 , 47.179043 ,  4.2073298], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28994151027781273
{'currentTarget': array([14.36250555, 91.43278435]), 'previousTarget': array([14.35374492, 91.42254235]), 'currentState': array([17.381708 , 71.66199  ,  2.9005508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3667
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.92068373, 102.98171638]), 'previousTarget': array([ 10., 103.]), 'currentState': array([ 9.827472 , 82.98193  ,  4.1530566], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28986246406454175
{'currentTarget': array([  7.72531671, 113.10242687]), 'previousTarget': array([  7.74230197, 113.20197471]), 'currentState': array([ 1.461532 , 94.10861  ,  3.6957815], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3668
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.52917256, 85.24137248]), 'previousTarget': array([ 7.424941, 83.949174]), 'currentState': array([ 7.68362  , 65.259254 ,  0.5666589], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.2898214755403419
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.408589 , 119.58144  ,   1.5797045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.469460206195229}
episode index:3669
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.93990343, 76.07417545]), 'previousTarget': array([12.06352827, 75.97806349]), 'currentState': array([12.822307, 56.09365 ,  3.253529], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28974250511103933
{'currentTarget': array([ 4.27531804, 79.03647428]), 'previousTarget': array([ 4.28538333, 79.18698145]), 'currentState': array([ 1.507204 , 59.228962 ,  4.6786914], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3670
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.8570065, 112.143875 ,   5.4284344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.881615440095013}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.2897716348580595
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.07731 , 119.373024,   1.529404], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0223346070395625}
episode index:3671
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.03679762, 88.59158478]), 'previousTarget': array([10.60007998, 89.9960012 ]), 'currentState': array([10.060229 , 68.5916   ,  3.9441903], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.28972605237138654
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.092818, 118.27147 ,   4.667305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5739388502577802}
episode index:3672
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.3128686 , 66.86784435]), 'previousTarget': array([11.45226033, 66.99249812]), 'currentState': array([11.806908  , 46.873947  ,  0.38901567], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896471724224698
{'currentTarget': array([ 4.41567724, 90.39784435]), 'previousTarget': array([ 4.41567724, 90.39784435]), 'currentState': array([ 0.7081545, 70.74449  ,  1.1102357], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3673
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.849592 , 119.937195 ,   2.1643043], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1521209033843736}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.289840518320014
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.849592 , 119.937195 ,   2.1643043], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1521209033843736}
episode index:3674
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.088567  , 102.33735   ,   0.53537756], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.930657748489352}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.28991820926740364
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.468185 , 120.59055  ,   0.5978357], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6417091927803469}
episode index:3675
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.23179819, 99.70420111]), 'previousTarget': array([ 10.97570496, 100.97375327]), 'currentState': array([10.460203 , 79.705505 ,  4.2760468], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898393414193984
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.122331 , 106.53002  ,   1.4207758], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.60021905735108}
episode index:3676
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.007841 , 117.88626  ,   1.8306619], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.676275392609364}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2900243997981258
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.358665, 119.49657 ,   2.499543], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.448936459590256}
episode index:3677
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.079069 , 121.06272  ,   3.5610175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.984007237280283}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.29009431068518193
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.051058 , 120.30374  ,   0.1973173], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9724691404062227}
episode index:3678
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.37272098, 103.54970075]), 'previousTarget': array([  6.33860916, 103.5237412 ]), 'currentState': array([ 2.0661848, 84.01886  ,  5.4739366], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29001545928244066
{'currentTarget': array([ 13.55217534, 104.87216613]), 'previousTarget': array([ 13.59036397, 104.83210893]), 'currentState': array([18.12404 , 85.401726,  4.927286], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3679
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.87568   , 110.76754   ,   0.33849794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.111791817919046}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.29012027275792374
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.48992  , 119.52494  ,   1.4192172], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.563822924114554}
episode index:3680
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.71289616, 110.4341729 ]), 'previousTarget': array([  7.69281066, 110.44164417]), 'currentState': array([ 3.0621562 , 90.98242   ,  0.32183188], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29004145714456925
{'currentTarget': array([  9.12315334, 113.54777793]), 'previousTarget': array([  9.09402541, 113.38607844]), 'currentState': array([ 6.429941 , 93.72994  ,  5.5369587], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3681
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.24180599, 103.38574738]), 'previousTarget': array([ 12.88074853, 101.75525931]), 'currentState': array([17.072014  , 83.755936  ,  0.92379326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899626843425202
{'currentTarget': array([  8.02421533, 113.93929766]), 'previousTarget': array([  8.11194024, 114.18919829]), 'currentState': array([ 1.8253101, 94.92421  ,  3.6368396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3682
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.880514, 112.28591 ,   2.804198], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.94454624064505}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.2900930347257574
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.413487 , 121.89887  ,   0.5646579], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.474416783656261}
episode index:3683
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.46594445, 74.95254939]), 'previousTarget': array([15.55689597, 74.85022022]), 'currentState': array([17.875025, 55.09817 ,  4.718974], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29001429068810114
{'currentTarget': array([ 4.52384085, 90.29389897]), 'previousTarget': array([ 4.50647062, 90.31730309]), 'currentState': array([ 0.8980419, 70.625305 ,  2.5709949], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3684
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.40243002, 71.26272566]), 'previousTarget': array([ 9.28982464, 70.99789993]), 'currentState': array([ 9.1572275, 51.26423  ,  0.7123896], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899355893880501
{'currentTarget': array([ 5.99334789, 90.07258878]), 'previousTarget': array([ 5.99334789, 90.07258878]), 'currentState': array([ 3.3394458, 70.24945  ,  2.2990472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3685
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.87845189, 86.91159005]), 'previousTarget': array([ 6.87845189, 86.91159005]), 'currentState': array([ 5.       , 67.       ,  1.6221505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898569307908206
{'currentTarget': array([ 4.09105386, 81.77263391]), 'previousTarget': array([ 4.00977101, 81.78225421]), 'currentState': array([ 1.0358629, 62.007366 ,  3.3099282], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3686
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.16440251, 87.9666738 ]), 'previousTarget': array([ 4.54305997, 89.68366648]), 'currentState': array([ 0.57994163, 68.290504  ,  5.0377493 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28977831486166655
{'currentTarget': array([ 4.00375672, 85.23149674]), 'previousTarget': array([ 4.00375672, 85.23149674]), 'currentState': array([ 0.604697 , 65.52245  ,  3.4687715], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3687
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.15675859, 82.61393833]), 'previousTarget': array([ 4.11925147, 82.75525931]), 'currentState': array([ 1.0683597, 62.853832 ,  4.984654 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896997415658798
{'currentTarget': array([ 4.59342722, 89.49827503]), 'previousTarget': array([ 4.50284204, 89.42714777]), 'currentState': array([ 1.1027472, 69.80525  ,  4.9919825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3688
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.41992052, 77.74549217]), 'previousTarget': array([11.35517412, 77.98960229]), 'currentState': array([12.091621, 57.756775,  5.392962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.2896755011986506
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.844788 , 119.75504  ,   1.5634615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.2899910465827419}
episode index:3689
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.08462 , 121.13467 ,   5.977903], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.20968648952963}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.28976597542458354
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.7161665, 119.54123  ,   3.421997 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8505087026227909}
episode index:3690
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.2929005 , 74.60057907]), 'previousTarget': array([10., 76.]), 'currentState': array([ 8.981437, 54.603004,  4.164883], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896874693353328
{'currentTarget': array([ 5.59473964, 89.49076163]), 'previousTarget': array([ 5.71927647, 89.45292824]), 'currentState': array([ 2.73656  , 69.696045 ,  5.9101567], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3691
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.94550866, 90.96908098]), 'previousTarget': array([10., 91.]), 'currentState': array([ 9.9079685, 70.96912  ,  5.6702623], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.28978481905338077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.385321, 118.21446 ,   1.370022], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.259924398990779}
episode index:3692
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.5478888, 120.08568  ,   5.761345 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.452935578656722}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28997174436639095
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.200089 , 119.22364  ,   5.8909893], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.114715678557742}
episode index:3693
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.98148105, 80.17493235]), 'previousTarget': array([ 5.99007438, 79.9007438 ]), 'currentState': array([ 3.9735918, 60.275978 ,  3.1941152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28989324633055813
{'currentTarget': array([ 4.63683781, 88.42222097]), 'previousTarget': array([ 4.65396546, 88.40088668]), 'currentState': array([ 1.2879989, 68.70458  ,  4.5689397], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3694
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.199326  , 113.089905  ,   0.47845173], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.28337510262105}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.28997366406685227
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.453877  , 118.08276   ,   0.23381454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4061493666638043}
episode index:3695
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.9566  , 114.73064 ,   2.188066], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.37167328133517}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2901603865603407
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.448486 , 118.155846 ,   1.3363785], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9248565209033286}
episode index:3696
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.17643108, 77.01503933]), 'previousTarget': array([12.04869701, 76.97736275]), 'currentState': array([13.187783 , 57.040627 ,  5.0288434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.2901167132994614
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.202301 , 118.219635 ,   1.1601822], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7918217554562577}
episode index:3697
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.73472134, 106.34482129]), 'previousTarget': array([ 13.73765188, 106.29527642]), 'currentState': array([19.010983 , 87.053345 ,  3.8598824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.29020186455793573
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.253194 , 119.28419  ,   1.7698879], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8877816358113244}
episode index:3698
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.863797 , 124.13562  ,   6.2521954], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.659706923699948}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.29034901565198606
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.967504 , 120.25848  ,   3.6419978], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0014355148242877}
episode index:3699
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.26802639, 85.87113412]), 'previousTarget': array([ 9.35708593, 83.99681199]), 'currentState': array([ 8.839178 , 65.87573  ,  1.3229795], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902705429450531
{'currentTarget': array([  8.46389576, 115.98982716]), 'previousTarget': array([  8.50430422, 116.18981075]), 'currentState': array([1.3097589e+00, 9.7313148e+01, 7.7785186e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3700
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.23853231, 82.96902615]), 'previousTarget': array([15.22022743, 82.80587954]), 'currentState': array([18.039911 , 63.16619  ,  4.2424664], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.2902690399000188
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.226011 , 118.166595 ,   1.9516082], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8472826822137054}
episode index:3701
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.00124766, 119.97504678]), 'currentState': array([ 11.147249 , 100.2109   ,   4.6873918], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.822327945852837}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.2902756687852919
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.455714 , 119.01358  ,   0.5861773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1266191835095747}
episode index:3702
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.51606146, 95.78164517]), 'previousTarget': array([10.55566525, 94.99506356]), 'currentState': array([ 9.116495, 75.78564 ,  2.676984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2903377981004053
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.795343 , 118.53923  ,   1.6160446], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4750363739158545}
episode index:3703
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.69236089, 85.75225018]), 'previousTarget': array([ 6.77863876, 83.92075411]), 'currentState': array([ 4.769712 , 65.84488  ,  1.8291779], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29025941316571297
{'currentTarget': array([ 4.460608  , 86.25144756]), 'previousTarget': array([ 4.54732954, 86.38822242]), 'currentState': array([ 1.2212108, 66.51553  ,  0.700185 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3704
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.872353, 110.06524 ,   5.286877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.065229185511033}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.29030185953564497
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.612672  , 121.41799   ,   0.37425336], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9837791398947238}
episode index:3705
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.64447508, 79.67859017]), 'previousTarget': array([14.75619127, 77.87373448]), 'currentState': array([16.93307  , 59.809963 ,  1.1365218], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29022352659999046
{'currentTarget': array([ 4.80338007, 90.5483996 ]), 'previousTarget': array([ 4.85558551, 90.56138283]), 'currentState': array([ 1.3281412, 70.852646 ,  2.4757257], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3706
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.322733 , 104.80336  ,   1.1511523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.373124470244205}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.29028560294098055
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.914706 , 118.44561  ,   1.5070952], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4662173618156826}
episode index:3707
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.08362057, 101.10541033]), 'previousTarget': array([  9.02429504, 100.97375327]), 'currentState': array([ 8.114768, 81.12889 ,  5.588419], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.2902468699147376
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.1611395, 119.78097  ,   1.7079588], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.851859369604851}
episode index:3708
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.01503175, 110.56707341]), 'previousTarget': array([  6.9279843 , 110.10128274]), 'currentState': array([ 0.98110217, 91.49899   ,  2.0025973 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29016861516415393
{'currentTarget': array([  8.06629615, 113.93959036]), 'previousTarget': array([  8.79271073, 115.76299218]), 'currentState': array([ 1.9868321, 94.88598  ,  4.1488643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3709
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.804733 , 111.85012  ,   4.7321625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.99766073900371}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.2901984038535845
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([9.2854891e+00, 1.2019094e+02, 5.9447825e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7395838440007854}
episode index:3710
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.48264996, 66.89886547]), 'previousTarget': array([10., 65.]), 'currentState': array([ 9.287805, 46.899815,  2.061325], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2901202043375905
{'currentTarget': array([ 4.88532465, 90.78769984]), 'previousTarget': array([ 4.70682001, 90.84630219]), 'currentState': array([ 1.4360672, 71.08738  ,  0.8094109], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3711
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.83261089, 91.98266146]), 'previousTarget': array([ 8.83261089, 91.98266146]), 'currentState': array([ 8.       , 72.       ,  3.5569594], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29004204695495645
{'currentTarget': array([  7.30901876, 112.09503054]), 'previousTarget': array([  7.33905327, 112.28713296]), 'currentState': array([ 0.86389774, 93.16198   ,  2.820574  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3712
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.069922 , 122.9068   ,   1.6305919], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.844107792172285}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.2901755336486953
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.339692 , 121.807526 ,   3.7602744], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.839168196185568}
episode index:3713
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.8968115 , 112.760506  ,   0.42254025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.4838607599323845}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.29027217530047766
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.348579  , 121.18447   ,   0.40558004], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0322799596835726}
episode index:3714
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.03052134, 73.90618972]), 'previousTarget': array([11.41201897, 71.99135509]), 'currentState': array([11.4775505, 53.911186 ,  1.5168874], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29019404012543043
{'currentTarget': array([ 5.21101347, 89.91630699]), 'previousTarget': array([ 5.21101347, 89.91630699]), 'currentState': array([ 2.066827 , 70.165    ,  2.3794336], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3715
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.993198, 112.69921 ,   3.668683], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.109715230914269}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.2902316329600371
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.301296 , 118.880295 ,   2.7283225], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7167153591401172}
episode index:3716
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.0032876 , 119.21292215]), 'previousTarget': array([  9.89618185, 118.90990945]), 'currentState': array([10.086826 , 99.2131   ,  6.0860925], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.29029075432492535
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.594401, 120.06591 ,   6.039938], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4109190065721788}
episode index:3717
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.020332, 117.93089 ,   4.714052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2893127219433733}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.2904710408380171
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.816624, 118.545815,   6.243685], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6677918559017195}
episode index:3718
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.58510876, 111.622904  ]), 'previousTarget': array([  9.05798302, 110.89383588]), 'currentState': array([ 5.254285 , 91.902214 ,  3.0695744], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.29051448601398605
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.195545 , 120.929054 ,   2.3882816], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5140905307971342}
episode index:3719
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.279568  , 100.71206   ,   0.71118456], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.301390879461433}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.29047661690634885
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.040987 , 121.34762  ,   0.5501722], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3777734181172065}
episode index:3720
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.64828144, 79.29189798]), 'previousTarget': array([10.6721752 , 78.99731309]), 'currentState': array([10.966743, 59.294434,  3.099459], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29039855277925763
{'currentTarget': array([ 5.88714596, 90.07607179]), 'previousTarget': array([ 5.88714596, 90.07607179]), 'currentState': array([ 3.1638749 , 70.262344  ,  0.43185437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3721
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.82310511, 88.83123154]), 'previousTarget': array([12.49484661, 86.94328241]), 'currentState': array([12.990937 , 68.86536  ,  2.0391052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.29035993509759533
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.104643 , 119.49988  ,   1.8537195], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0255664186008289}
episode index:3722
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.98405887, 84.88128495]), 'previousTarget': array([ 8.72679236, 84.98678996]), 'currentState': array([ 8.4057255, 64.88965  ,  5.3448215], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.29037830425219757
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.711284 , 118.79385  ,   1.3775318], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.24022008695432}
episode index:3723
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.07326435, 75.81462486]), 'previousTarget': array([11.39421747, 73.99082358]), 'currentState': array([11.558922 , 55.820522 ,  1.8266015], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.29035303927093303
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.300457, 120.56185 ,   2.406455], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4166388108661523}
episode index:3724
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.43269359, 111.80373491]), 'previousTarget': array([  9.33038021, 109.95570316]), 'currentState': array([ 8.051693 , 91.85147  ,  1.0833353], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.2903870709060101
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.540804 , 119.799934 ,   1.6471416], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5537383744832352}
episode index:3725
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.38084  , 116.82691  ,   1.0992984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.902961813828915}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.2905264538360464
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.155927 , 118.138145 ,   1.7913605], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.620516875899892}
episode index:3726
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.186045, 109.18743 ,   3.885923], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.963670689210057}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.290538219439227
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.578436  , 120.577034  ,   0.38381928], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7146219458232681}
episode index:3727
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.35806658, 104.9641651 ]), 'previousTarget': array([ 11.29197717, 104.92693298]), 'currentState': array([13.157183  , 85.04525   ,  0.40550107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2905998616879424
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.703122  , 120.3311    ,   0.17418061], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4447066241227224}
episode index:3728
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.48490106, 76.47756766]), 'previousTarget': array([ 5.83833848, 74.91533358]), 'currentState': array([ 4.8748393, 56.54248  ,  0.83419  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.2906210817941268
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.2675295, 118.03653  ,   3.0437074], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.095645317038497}
episode index:3729
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.22698466, 75.78989541]), 'previousTarget': array([16.21490337, 75.80513158]), 'currentState': array([19.016447, 55.98538 ,  4.522699], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.2905605879346909
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.129343 , 118.8724   ,   2.3775194], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1842259292927815}
episode index:3730
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.95762153, 90.64982106]), 'previousTarget': array([11.21628867, 88.98463901]), 'currentState': array([13.28864   , 70.69416   ,  0.68824565], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29048271053240343
{'currentTarget': array([15.27488178, 92.60755983]), 'previousTarget': array([15.32170156, 92.60376624]), 'currentState': array([19.056742 , 72.968376 ,  4.2615395], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3731
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.04589695, 80.69483064]), 'previousTarget': array([ 4.01595979, 80.77129198]), 'currentState': array([1.0503922e+00, 6.0920429e+01, 2.6162799e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29040487486505817
{'currentTarget': array([  9.98945349, 119.40540111]), 'previousTarget': array([  9.98714967, 119.22369329]), 'currentState': array([ 9.634766, 99.40855 ,  5.786038], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3732
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.6587378, 118.905464 ,   3.341678 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.405168183809707}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.2904891505661759
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.068836 , 120.5867   ,   5.8378706], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1005832115030525}
episode index:3733
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.1276407, 110.75509  ,   2.8465033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.680848660706697}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.29064873699390753
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.032775 , 119.39517  ,   0.9635522], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1407633894567857}
episode index:3734
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.288767 , 100.77711  ,   2.3309455], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.29890994242703}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.29067712496430326
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.465459 , 121.722244 ,   3.6815484], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.803291302069377}
episode index:3735
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.85539434, 113.82461234]), 'previousTarget': array([ 10.20063923, 114.98401917]), 'currentState': array([ 9.387194, 93.83009 ,  3.72188 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.29073859803648905
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.185381 , 118.93267  ,   2.7562351], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5950924816874021}
episode index:3736
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.00565967, 76.81231425]), 'previousTarget': array([16.17157288, 76.79898987]), 'currentState': array([18.760344 , 57.00293  ,  0.4506771], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29066079803701445
{'currentTarget': array([ 4.67472025, 74.19084251]), 'previousTarget': array([ 4.40626564, 74.33605442]), 'currentState': array([ 2.3652878, 54.324627 ,  4.990442 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3737
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.44601522, 80.92297067]), 'previousTarget': array([ 9.33893437, 80.99712788]), 'currentState': array([ 9.162509 , 60.92498  ,  5.2307634], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29058303966407784
{'currentTarget': array([ 4.77881022, 91.06036344]), 'previousTarget': array([ 4.77881022, 91.06036344]), 'currentState': array([ 1.227809, 71.37813 ,  1.571471], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3738
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.939738, 111.91949 ,   4.934929], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.989784647904042}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.29055888721347434
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.637556, 121.94156 ,   3.930646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9750990656166105}
episode index:3739
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.99427962, 78.65643863]), 'previousTarget': array([10., 78.]), 'currentState': array([ 8.507905 , 58.662354 ,  3.0967188], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29048119767143865
{'currentTarget': array([ 4.19848463, 88.8275815 ]), 'previousTarget': array([ 4.34148644, 88.91492696]), 'currentState': array([ 0.53910965, 69.16521   ,  5.064492  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3740
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.95343727, 106.18586814]), 'previousTarget': array([ 10., 106.]), 'currentState': array([ 9.886024, 86.18598 ,  4.61141 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29040354966350723
{'currentTarget': array([  8.62682013, 116.63586461]), 'previousTarget': array([  8.61650644, 116.53061894]), 'currentState': array([ 1.0685811, 98.11903  ,  2.1708398], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3741
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.75443155, 112.3388774 ]), 'previousTarget': array([  9.71383061, 111.98725709]), 'currentState': array([ 9.113684 , 92.349144 ,  3.0550156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.2904092304576776
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.808056 , 119.64102  ,   6.2202034], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4070724081646598}
episode index:3742
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.67927888, 116.63574666]), 'previousTarget': array([  8.02246883, 114.72658355]), 'currentState': array([ 1.370785, 98.01893 ,  1.071464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29033164316661225
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.036473 , 109.94229  ,   1.2685374], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.261809221577055}
episode index:3743
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.47020437, 79.69959118]), 'previousTarget': array([ 7.28764556, 77.95850618]), 'currentState': array([ 6.2172017, 59.73888  ,  1.471776 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29025409732174934
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.1469016, 104.89771  ,   3.577136 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.369425880683592}
episode index:3744
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.857508, 111.05914 ,   4.720783], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.585482329387386}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.29035885045864884
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.711346 , 119.59993  ,   3.2492068], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.816130395958729}
episode index:3745
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.71745417, 98.43581126]), 'previousTarget': array([ 5.83020719, 98.62981184]), 'currentState': array([ 1.8216317, 78.81892  ,  3.7281528], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29028133875270684
{'currentTarget': array([ 10.52957072, 118.43349967]), 'previousTarget': array([ 10.52957072, 118.43349967]), 'currentState': array([16.934664 , 99.48687  ,  0.3120104], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3746
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.64607006, 69.71598048]), 'previousTarget': array([ 3.55043485, 69.83671551]), 'currentState': array([ 1.1387913, 49.873764 ,  6.276769 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2902038684194395
{'currentTarget': array([ 4.42857326, 90.3648793 ]), 'previousTarget': array([ 4.42857326, 90.3648793 ]), 'currentState': array([ 0.73329353, 70.70922   ,  1.1596903 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3747
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.61119516, 91.0762723 ]), 'previousTarget': array([13.56917619, 90.85172777]), 'currentState': array([16.089005, 71.230354,  5.410227], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.290126439425731
{'currentTarget': array([ 10.65650264, 118.19093213]), 'previousTarget': array([ 10.64885069, 118.13498577]), 'currentState': array([1.7479061e+01, 9.9390594e+01, 1.0487398e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3748
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.4695008 , 86.57944121]), 'previousTarget': array([14.38123261, 86.8278102 ]), 'currentState': array([17.1206   , 66.75593  ,  4.4398775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.2900645700559127
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.5150385, 118.81512  ,   2.4946253], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8997522701281908}
episode index:3749
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.89898806, 87.14990902]), 'previousTarget': array([12.43617509, 88.93876756]), 'currentState': array([13.053215, 67.18324 ,  4.079867], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899872195038978
{'currentTarget': array([ 4.36094248, 90.67471186]), 'previousTarget': array([ 4.36094248, 90.67471186]), 'currentState': array([ 0.5842665, 71.03453  ,  1.3411777], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3750
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.      , 106.      ,   5.246594], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.29002684367939924
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.6665945 , 119.45865   ,   0.58990246], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.752312328045178}
episode index:3751
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.8736213 , 79.40470364]), 'previousTarget': array([13.3923162, 77.9352791]), 'currentState': array([14.285829, 59.454624,  1.999036], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 201
reward sum = 0.1326398781093821
running average episode reward sum: 0.28998489619390616
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.51965   , 118.22914   ,   0.34655204], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3335120061296304}
episode index:3752
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.58730448, 67.7182359 ]), 'previousTarget': array([10.72224901, 67.99807127]), 'currentState': array([10.811959 , 47.719498 ,  4.8053217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899076287022478
{'currentTarget': array([ 5.15739677, 90.0975031 ]), 'previousTarget': array([ 5.215613  , 90.12180306]), 'currentState': array([ 1.9601233 , 70.35472   ,  0.38038325], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3753
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.8565243, 74.865099 ]), 'previousTarget': array([16.29773591, 73.81660336]), 'currentState': array([16.996178 , 54.97988  ,  1.9343628], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898304023760085
{'currentTarget': array([ 4.8584994 , 89.48172278]), 'previousTarget': array([ 4.74518836, 89.54849907]), 'currentState': array([ 1.5358661, 69.75965  ,  3.1211824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3754
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.820049 , 118.76642  ,   1.2568421], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.324811523778265}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.2898370545120418
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.873668, 118.65    ,   3.126661], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3558966501405378}
episode index:3755
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.17157288, 76.79898987]), 'previousTarget': array([16.17157288, 76.79898987]), 'currentState': array([19.        , 57.        ,  0.88717294], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28975988809710246
{'currentTarget': array([ 4.63066888, 90.44043549]), 'previousTarget': array([ 4.64941108, 90.27042874]), 'currentState': array([ 1.0562693, 70.762436 ,  3.2717257], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3756
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.718123 , 109.08228  ,   3.0443947], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.727361811081973}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.28974793851257696
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.471388  , 118.930855  ,   0.48093632], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1684511037501262}
episode index:3757
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.54928732, 82.06265654]), 'previousTarget': array([ 9.34477635, 81.99702801]), 'currentState': array([ 9.311695 , 62.064068 ,  5.2695956], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289670836879125
{'currentTarget': array([ 4.94834277, 89.40569512]), 'previousTarget': array([ 5.08538569, 89.51933299]), 'currentState': array([ 1.6901087, 69.67288  ,  3.607337 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3758
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.179279 , 107.07889  ,   5.4550343], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.947151011077882}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.289784712829037
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.759087 , 118.33737  ,   3.1477084], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.420478907565943}
episode index:3759
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.6169837 , 89.06618296]), 'previousTarget': array([10.6079185 , 88.99615643]), 'currentState': array([11.01581 , 69.07016 ,  5.346631], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28970764242668884
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.439132, 115.789474,   4.169128], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.247716833991311}
episode index:3760
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.16209314, 111.3475511 ]), 'previousTarget': array([  7.12018361, 111.04057123]), 'currentState': array([ 0.9290205, 92.343636 ,  4.0995207], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.2897746398830941
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.937162 , 118.55367  ,   2.9125407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4476915973105942}
episode index:3761
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.60646933, 102.28729439]), 'previousTarget': array([  9.52613364, 101.99307839]), 'currentState': array([ 9.1622305 , 82.29223   ,  0.25001973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.2898982296351729
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.18186   , 118.17661   ,   0.72778445], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.172909034702436}
episode index:3762
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.26410912, 111.2150951 ]), 'previousTarget': array([ 12.1492875, 111.40285  ]), 'currentState': array([17.255545  , 91.84797   ,  0.12889928], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.28991846192633375
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.032279 , 121.67086  ,   2.5778608], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9640198762789187}
episode index:3763
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.82134686, 83.68672076]), 'previousTarget': array([11.93010557, 83.97136265]), 'currentState': array([12.823217, 63.71183 ,  5.851848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28984143789287825
{'currentTarget': array([ 11.42633464, 116.39840322]), 'previousTarget': array([ 12.05288527, 114.47768621]), 'currentState': array([18.79044  , 97.80351  ,  1.5799346], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3764
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.15981002, 114.74881264]), 'previousTarget': array([  9.15981002, 114.74881264]), 'currentState': array([ 6.       , 95.       ,  2.0517488], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.2898232733110313
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.121095 , 121.11702  ,   4.5787272], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.185867795285098}
episode index:3765
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.102612  , 67.57733138]), 'previousTarget': array([11.45226033, 66.99249812]), 'currentState': array([10.14176  , 47.57737  ,  2.8006654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897463154583199
{'currentTarget': array([ 9.3171259 , 73.52766827]), 'previousTarget': array([ 9.2237852 , 73.66687935]), 'currentState': array([ 9.023273, 53.529827,  4.763579], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3766
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.02958865, 76.12394068]), 'previousTarget': array([ 7.24756572, 75.96105157]), 'currentState': array([ 5.67868 , 56.169617,  5.200547], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28966939846456935
{'currentTarget': array([ 4.74637739, 90.77225427]), 'previousTarget': array([ 4.80542337, 90.75864888]), 'currentState': array([ 1.2081264, 71.08772  ,  3.4375324], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3767
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.1469965, 124.12886  ,   1.5407542], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.371744800296404}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.289748317621542
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.637716  , 121.98987   ,   0.98296577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.022578748643529}
episode index:3768
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.930574, 109.1317  ,   2.07402 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.256475538681416}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.2899237598428947
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.55263  , 118.66173  ,   1.1694592], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4478855568056237}
episode index:3769
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.852022, 117.880554,   2.511194], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.0175916317879605}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.2900638084311585
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.905202 , 121.764206 ,   2.1598024], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0762960834937716}
episode index:3770
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.23228289, 103.55983825]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([17.090597 , 83.93553  ,  4.6677513], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2899868888319988
{'currentTarget': array([  8.24429916, 115.67698694]), 'previousTarget': array([  8.24429916, 115.67698694]), 'currentState': array([ 0.71868324, 97.14687   ,  4.9001985 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3771
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.72166171, 113.71484556]), 'previousTarget': array([  9.71383061, 111.98725709]), 'currentState': array([ 8.836828 , 93.73443  ,  1.5421993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.29013574113376517
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.042558 , 118.631424 ,   1.1863723], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6702383454412555}
episode index:3772
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.48315377, 109.37236723]), 'previousTarget': array([ 12.45540769, 109.47682419]), 'currentState': array([17.033607, 89.89691 ,  4.62512 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.2901750949001781
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.490751 , 119.70399  ,   1.9330771], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5890313207135359}
episode index:3773
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.10427062, 77.07807925]), 'previousTarget': array([ 7.26728946, 76.95980905]), 'currentState': array([ 5.75803 , 57.12344 ,  5.234728], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.2901374610625118
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.045735 , 120.435295 ,   3.1377141], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4376911599522761}
episode index:3774
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.519263 , 117.42267  ,   3.9252884], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.991788555096831}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.2901722151714104
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.185948, 118.36526 ,   3.082003], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8262157235132663}
episode index:3775
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.482323 , 117.22919  ,   1.7550167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.978881970319012}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.2901622015188463
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.283053  , 121.035866  ,   0.20405106], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2597739288270016}
episode index:3776
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.58699731, 112.02222897]), 'previousTarget': array([ 10.575059, 111.949174]), 'currentState': array([12.054612, 92.07615 ,  4.921993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2900853780606734
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.3604193, 120.92482  ,   2.4473355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.730856291614731}
episode index:3777
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.52778613, 74.8035608 ]), 'previousTarget': array([ 3.74306117, 74.81099734]), 'currentState': array([ 0.69267106, 55.005527  ,  3.7456717 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29000859527135087
{'currentTarget': array([12.14318329, 88.81084305]), 'previousTarget': array([12.39748264, 88.7805191 ]), 'currentState': array([13.514263  , 68.857895  ,  0.89024746], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3778
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.30185582, 95.76894499]), 'previousTarget': array([ 8.36047776, 95.95367385]), 'currentState': array([ 6.903659, 75.81788 ,  4.162495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28993185311859315
{'currentTarget': array([  8.75696952, 116.0423254 ]), 'previousTarget': array([  8.71835067, 115.92507787]), 'currentState': array([ 2.7639923, 96.961334 ,  5.9011474], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3779
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.18628389, 79.72270584]), 'previousTarget': array([ 3.96680906, 79.77872706]), 'currentState': array([ 1.3290498, 59.927853 ,  5.697833 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28985515157014907
{'currentTarget': array([ 4.76925905, 89.08382124]), 'previousTarget': array([ 4.70602641, 89.2805146 ]), 'currentState': array([ 1.4328543, 69.364075 ,  3.2323604], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3780
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.6337636 , 112.81893475]), 'previousTarget': array([  9.68924552, 110.98811999]), 'currentState': array([ 8.615082 , 92.844894 ,  1.3791356], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.2899554204426694
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.653847  , 120.5256    ,   0.56929505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4451229715846996}
episode index:3781
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.8513737, 119.877716 ,   2.6371052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.1510000176469712}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2901016361414905
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.092841 , 120.386246 ,   0.5533765], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9458778603417486}
episode index:3782
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.72763411, 102.56653868]), 'previousTarget': array([ 13.77334415, 102.54828331]), 'currentState': array([17.90952  , 83.00863  ,  2.9946074], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.2901032975084632
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.692055 , 119.57718  ,   1.6685127], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7440834005486083}
episode index:3783
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.9805543, 120.18058  ,   3.9712396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.021768048610341}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.2902539198808556
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.15614 , 118.55248 ,   5.473143], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3441683175333203}
episode index:3784
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.83753584, 109.98662028]), 'previousTarget': array([ 12.68142004, 110.27985236]), 'currentState': array([18.29032 , 90.74429 ,  5.218305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29017723456516714
{'currentTarget': array([  8.5924918 , 115.34779818]), 'previousTarget': array([  8.50405095, 115.26187976]), 'currentState': array([ 2.800824 , 96.20474  ,  1.0289215], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3785
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.57265691, 116.51093912]), 'previousTarget': array([  8.57265691, 116.51093912]), 'currentState': array([ 1.       , 98.       ,  2.6622632], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29010058975941827
{'currentTarget': array([  8.58038684, 116.04186938]), 'previousTarget': array([  8.67820261, 116.24371718]), 'currentState': array([ 1.8283765, 97.21608  ,  2.7387717], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3786
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.535784 , 113.46074  ,   2.2225425], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.245453050497156}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.29027510506444615
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.803652, 120.63621 ,   2.403748], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9125689363201441}
episode index:3787
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.579211 , 121.359566 ,   1.0460678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.423194355007608}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.29046246644114504
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.579211 , 121.359566 ,   1.0460678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.423194355007608}
episode index:3788
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.44465355, 73.0163358 ]), 'previousTarget': array([ 4.3034104 , 70.86691472]), 'currentState': array([ 2.0962138, 53.154694 ,  1.5880538], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.290385807041187
{'currentTarget': array([14.02758758, 91.19060736]), 'previousTarget': array([14.07620215, 91.07094163]), 'currentState': array([16.796682 , 71.38323  ,  1.3535088], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3789
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.04175175, 76.09356536]), 'previousTarget': array([ 5.86681417, 75.91268452]), 'currentState': array([ 4.2459965, 56.174347 ,  3.3034728], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903091880947381
{'currentTarget': array([  9.71024743, 117.15314236]), 'previousTarget': array([  9.67832686, 117.24020818]), 'currentState': array([ 7.685114, 97.255936,  4.51631 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3790
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.108587, 118.077286,   4.867908], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.309306740445281}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2904784722308004
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.79879  , 119.38511  ,   3.0280294], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0080458497157918}
episode index:3791
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.90530438, 98.27889391]), 'previousTarget': array([ 8.95130299, 97.97736275]), 'currentState': array([ 7.8986263, 78.304245 ,  2.2931712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.2905377176541108
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.662749  , 119.778336  ,   0.98847187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6774594844293433}
episode index:3792
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.933384 , 117.48284  ,   0.9631627], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.376171082495749}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.29064845203576606
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.466918 , 119.81724  ,   2.5973163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4782592452614323}
episode index:3793
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.16063287, 84.48680627]), 'previousTarget': array([15.12120309, 84.79172879]), 'currentState': array([18.036743 , 64.69469  ,  3.5209959], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29057184464197694
{'currentTarget': array([ 10.74407688, 118.39349659]), 'previousTarget': array([ 10.6948912 , 118.47830035]), 'currentState': array([ 19.149574 , 100.24556  ,   5.2265596], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3794
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.028099 , 116.07703  ,   4.7620163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.041573154206303}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.2906563098316715
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.352095 , 121.31063  ,   0.5627665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.105551027933176}
episode index:3795
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.924737 , 112.79671  ,   2.3330677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.203686025758932}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.2907613658843684
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.75289   , 118.66038   ,   0.54118603], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5366949065162865}
episode index:3796
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.13804673, 88.00405461]), 'previousTarget': array([14.28039959, 88.81423159]), 'currentState': array([15.090208 , 68.099556 ,  3.4610062], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29068478928023767
{'currentTarget': array([ 5.93442285, 89.67400082]), 'previousTarget': array([ 6.01078238, 89.64770372]), 'currentState': array([ 3.276949 , 69.85134  ,  1.9426432], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3797
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.01109735, 94.2715993 ]), 'previousTarget': array([11.66961979, 94.95570316]), 'currentState': array([11.796469 , 74.287025 ,  3.7401106], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.29072608428405117
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.331014 , 118.17871  ,   1.8814234], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.94026713268879}
episode index:3798
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.04782777, 108.82015869]), 'previousTarget': array([  9.05798302, 110.89383588]), 'currentState': array([ 7.350599, 88.8923  ,  4.890554], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2906495572810809
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.547304 , 117.36214  ,   2.8330607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.0114194774496617}
episode index:3799
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.08628312, 88.11605661]), 'previousTarget': array([ 8.1519307 , 87.96679883]), 'currentState': array([ 6.888013 , 68.151985 ,  4.4431553], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2905730705554806
{'currentTarget': array([ 4.85821668, 90.96102924]), 'previousTarget': array([ 4.87210191, 91.10054007]), 'currentState': array([ 1.3711593, 71.267365 ,  5.0867863], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3800
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.72765043, 81.99854762]), 'previousTarget': array([13.3390904 , 79.93091516]), 'currentState': array([14.159517, 62.04987 ,  1.992653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29049662407546073
{'currentTarget': array([  9.4502007 , 115.44453441]), 'previousTarget': array([  8.97890314, 114.93622642]), 'currentState': array([ 7.05379  , 95.58862  ,  6.1469736], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3801
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.79526209, 96.93477796]), 'previousTarget': array([10., 96.]), 'currentState': array([11.484429 , 76.946655 ,  0.8861302], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.2904505256311257
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.000305 , 118.616646 ,   0.7006309], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7067684518682114}
episode index:3802
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.475691 , 113.530205 ,   0.8702316], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.487259209930057}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.29046032637575014
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.290295, 121.17263 ,   3.827519], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3706726567044534}
episode index:3803
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.57656966, 96.64623605]), 'previousTarget': array([11.60803474, 96.95150202]), 'currentState': array([11.070189, 76.65233 ,  3.614822], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.2904626712929259
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.198962, 121.08284 ,   1.081191], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1014945888082814}
episode index:3804
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.42633488, 83.07767608]), 'previousTarget': array([11.2986772 , 82.98769988]), 'currentState': array([12.198373 , 63.092583 ,  1.0947232], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903863341914035
{'currentTarget': array([  8.76160039, 116.89845346]), 'previousTarget': array([  8.76505942, 116.97446101]), 'currentState': array([ 1.3452442, 98.32434  ,  1.4114408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3805
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.68262322, 85.71583246]), 'previousTarget': array([ 6.13066247, 83.88618308]), 'currentState': array([ 4.756396 , 65.80881  ,  1.6586668], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2903100372039648
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.5135546, 108.14218  ,   3.315144 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.115703565828797}
episode index:3806
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.0689413 , 81.91076259]), 'previousTarget': array([15.9332534 , 81.76347807]), 'currentState': array([19.21594 , 62.159904,  4.76374 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29023378029899927
{'currentTarget': array([  8.60813871, 116.70854189]), 'previousTarget': array([  8.60813871, 116.70854189]), 'currentState': array([ 0.8185606, 98.287834 ,  1.1594304], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3807
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.44132237, 100.38587174]), 'previousTarget': array([  6.53328126, 100.68542414]), 'currentState': array([ 2.8709245, 80.707146 ,  2.338552 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29015756344492916
{'currentTarget': array([  8.61912049, 116.43165598]), 'previousTarget': array([  8.61912049, 116.43165598]), 'currentState': array([ 1.4011248, 97.779564 ,  2.0537355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3808
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.09473  , 108.209435 ,   2.4205856], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.841278190709753}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.29023243800164533
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.755396 , 118.67755  ,   0.9049367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8160148780045797}
episode index:3809
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.75025266, 82.64944925]), 'previousTarget': array([15.82643809, 83.74660743]), 'currentState': array([17.273533 , 62.80926  ,  4.1068773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.29015626150873153
{'currentTarget': array([10.41724693, 87.78769008]), 'previousTarget': array([10.36634551, 87.55537342]), 'currentState': array([10.676286 , 67.78937  ,  5.8013973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3810
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.86987552, 87.1729778 ]), 'previousTarget': array([11.86973376, 86.96803691]), 'currentState': array([13.007261, 67.205345,  5.649652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2900801249929853
{'currentTarget': array([  8.99715904, 114.55871844]), 'previousTarget': array([  9.03767466, 114.67218686]), 'currentState': array([ 5.372164 , 94.88998  ,  3.2221854], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3811
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.76765837, 88.81354999]), 'previousTarget': array([14.38123261, 86.8278102 ]), 'currentState': array([16.166431 , 68.957924 ,  2.1479886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2900040284229452
{'currentTarget': array([ 4.54717926, 91.09810281]), 'previousTarget': array([ 4.35876873, 90.90548132]), 'currentState': array([ 0.8392633, 71.444824 ,  6.088784 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3812
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.93682  , 101.28361  ,   3.7499304], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.71649915348378}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.28998782764448006
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.0391   , 121.35288  ,   3.1341035], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7058777567407937}
episode index:3813
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.60238257, 115.98269331]), 'previousTarget': array([  8.5704125 , 115.88993593]), 'currentState': array([ 2.0307395, 97.093185 ,  5.1945877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28991179517787163
{'currentTarget': array([  8.21648991, 115.68503433]), 'previousTarget': array([  8.24076865, 115.67432144]), 'currentState': array([ 0.5767435, 97.20168  ,  2.065027 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3814
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.634926, 114.86027 ,   2.911018], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.317920288714866}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.290070491497421
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([8.38620663e+00, 1.19894035e+02, 1.45909190e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.617268549249789}
episode index:3815
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.2868107 , 83.56366507]), 'previousTarget': array([11.28616939, 83.98725709]), 'currentState': array([11.992704 , 63.576126 ,  4.2934055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.29003654959480396
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.652532, 119.325775,   1.595943], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7847800763372865}
episode index:3816
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.65208182, 112.48704812]), 'previousTarget': array([  8.6417852, 112.6656401]), 'currentState': array([ 5.120223 , 92.80137  ,  4.2637053], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28996056412202564
{'currentTarget': array([  8.6859044 , 115.64392136]), 'previousTarget': array([  8.65313853, 115.61024765]), 'currentState': array([ 2.9096277, 96.496216 ,  6.044246 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3817
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.77282662, 85.60783713]), 'previousTarget': array([15.71202025, 85.72787848]), 'currentState': array([19.08357  , 65.883766 ,  3.3115704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898846184530571
{'currentTarget': array([ 10.87639802, 116.97146807]), 'previousTarget': array([ 10.87639802, 116.97146807]), 'currentState': array([16.435907 , 97.759705 ,  2.4353848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3818
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.87948285, 70.16241773]), 'previousTarget': array([10.71431486, 69.9979595 ]), 'currentState': array([11.2323675, 50.16553  ,  4.8133   ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2898087125566305
{'currentTarget': array([ 4.27312311, 90.14634681]), 'previousTarget': array([ 4.38874112, 90.09061996]), 'currentState': array([ 0.5051917, 70.504486 ,  2.7284572], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3819
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.10020981, 74.96907071]), 'previousTarget': array([12.07790467, 74.9787322 ]), 'currentState': array([13.031982, 54.990788,  5.840559], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897328464015109
{'currentTarget': array([  8.72207616, 116.17266664]), 'previousTarget': array([  8.75981147, 116.21675003]), 'currentState': array([ 2.3879483, 97.202194 ,  4.8437448], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3820
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.79435709, 87.75509567]), 'previousTarget': array([10.62969312, 85.99657153]), 'currentState': array([11.28691  , 67.76116  ,  0.6589464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896570199564962
{'currentTarget': array([ 11.11722011, 116.13066528]), 'previousTarget': array([ 11.12961967, 116.17057461]), 'currentState': array([16.66532  , 96.9156   ,  2.8867993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3821
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.898503, 107.87515 ,   3.072652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.58239278323441}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.2897767256274472
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.187451, 118.43405 ,   0.908434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7642080234365543}
episode index:3822
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.53909189, 88.62052429]), 'previousTarget': array([ 4.47570668, 88.69567118]), 'currentState': array([ 1.1100694, 68.91667  ,  4.42239  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897009273732941
{'currentTarget': array([ 4.59849082, 91.01908135]), 'previousTarget': array([ 4.59849082, 91.01908135]), 'currentState': array([ 0.9339653, 71.357666 ,  1.4001952], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3823
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.056034 , 122.100395 ,   1.9922175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.362015828574343}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.28977412305193
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.884485 , 118.11246  ,   2.5776088], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.192530662591534}
episode index:3824
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.04758673, 113.13696531]), 'previousTarget': array([ 11.66139084, 112.5237412 ]), 'currentState': array([14.06547  , 93.36597  ,  2.9813871], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28969836511126285
{'currentTarget': array([  9.11464455, 114.26444737]), 'previousTarget': array([  9.11464455, 114.26444737]), 'currentState': array([ 6.063527 , 94.49855  ,  1.5732478], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3825
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.851648, 118.04089 ,   5.125325], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.321260571564773}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.28982594561159947
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.091456, 120.666824,   2.306205], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.279035497387218}
episode index:3826
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.228726, 123.11774 ,   5.40586 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.488313373488927}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.28985703895594517
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.336836 , 121.9231   ,   4.0821695], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9523792728212803}
episode index:3827
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.72394355, 103.61067368]), 'previousTarget': array([  8.09369569, 101.89010906]), 'currentState': array([ 4.9728594, 83.80079  ,  1.6557794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28978131872633284
{'currentTarget': array([  5.74454636, 103.23960183]), 'previousTarget': array([  5.6989537 , 103.34984401]), 'currentState': array([ 0.8227248, 83.85467  ,  4.252193 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3828
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.9626281, 106.93987  ,   5.9486794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.835482024320777}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.2898134859638381
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([8.9222889e+00, 1.2077945e+02, 7.0263028e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3300386053750641}
episode index:3829
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.2205873, 105.86226  ,   3.4336476], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.408360708660025}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.28977108383634853
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.124363 , 119.61764  ,   0.8445829], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9554795810666331}
episode index:3830
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.43787333, 88.8336904 ]), 'previousTarget': array([14.28039959, 88.81423159]), 'currentState': array([17.2573   , 69.03342  ,  4.2125034], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289695445338871
{'currentTarget': array([ 4.47480558, 89.8227924 ]), 'previousTarget': array([ 4.54851891, 89.75354458]), 'currentState': array([ 0.8728485, 70.14982  ,  6.1306067], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3831
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.67661798, 64.55031625]), 'previousTarget': array([16.61709559, 64.85753677]), 'currentState': array([19.06752  , 44.69374  ,  5.3029866], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28961984631868865
{'currentTarget': array([ 8.46837649, 65.62294571]), 'previousTarget': array([ 8.46837649, 65.62294571]), 'currentState': array([ 7.9052653, 45.630875 ,  3.394889 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3832
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.9130898, 122.10049  ,   3.6387088], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.59509303312614}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2897513345621859
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.14384   , 121.849106  ,   0.28892648], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.037695417647244}
episode index:3833
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.36027876, 98.58872118]), 'previousTarget': array([ 6.36592926, 98.71472851]), 'currentState': array([ 3.0085444, 78.871574 ,  5.8871455], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28967576040084986
{'currentTarget': array([  8.73755067, 115.16872423]), 'previousTarget': array([  8.72249256, 115.13549343]), 'currentState': array([ 3.6811762, 95.81845  ,  4.0642476], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3834
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.92258803, 94.76101432]), 'previousTarget': array([10., 95.]), 'currentState': array([ 9.861245 , 74.76111  ,  5.7953167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896002256523751
{'currentTarget': array([ 10.69034052, 114.84679798]), 'previousTarget': array([ 10.71847229, 114.88254525]), 'currentState': array([13.345886 , 95.02388  ,  4.8787346], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3835
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.117306 , 108.016136 ,   2.6612337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.38267287315423}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28976527895497556
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.156295, 119.54728 ,   2.285945], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.47894055098189003}
episode index:3836
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.3958822 , 107.33827338]), 'previousTarget': array([  6.41491154, 107.2530188 ]), 'currentState': array([ 0.9204497, 88.10238  ,  5.554812 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896897602479245
{'currentTarget': array([  8.62336709, 115.515717  ]), 'previousTarget': array([  8.56982929, 115.52650475]), 'currentState': array([ 2.7539065, 96.39637  ,  3.7272723], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3837
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.77694787, 113.64012894]), 'previousTarget': array([  8.77694787, 113.64012894]), 'currentState': array([ 5.       , 94.       ,  1.0230036], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.2896892114853187
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.781964 , 120.52133  ,   3.9496963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.32491425899926}
episode index:3838
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.7733493, 120.104904 ,   4.0818114], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.2275343444204045}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.28985170902009294
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.640682  , 118.59326   ,   0.18683939], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4519028406296648}
episode index:3839
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.6521521 , 115.00046931]), 'previousTarget': array([ 11.67544468, 114.97366596]), 'currentState': array([17.927603 , 96.010506 ,  4.1454554], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.2898380998653451
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.177568 , 119.86346  ,   3.8003743], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1854583444129017}
episode index:3840
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.08264014, 110.03899637]), 'previousTarget': array([  9.424941, 111.949174]), 'currentState': array([ 7.2484994, 90.123276 ,  4.0127077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.289874561962106
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.36794  , 119.271614 ,   3.2497263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5497760355395018}
episode index:3841
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.81595867, 88.11501879]), 'previousTarget': array([11.8480693 , 87.96679883]), 'currentState': array([12.953184 , 68.14738  ,  0.9424871], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.2898174418904098
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.066854 , 120.885376 ,   1.8605223], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3863863314222031}
episode index:3842
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.220294 , 110.30582  ,   1.4097451], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.21506163730912}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.2898223149137624
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.491748, 118.08319 ,   3.306886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.428882196249167}
episode index:3843
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.637799, 101.07977 ,   5.198738], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.99098149330359}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.2898296423718544
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.229323, 118.685295,   2.505562], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7999124843092849}
episode index:3844
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.34486059, 97.8183179 ]), 'previousTarget': array([ 8.424941, 97.949174]), 'currentState': array([ 6.85665  , 77.873764 ,  4.4134245], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897542640513415
{'currentTarget': array([  8.15681228, 115.28507946]), 'previousTarget': array([  8.14007675, 115.32834457]), 'currentState': array([ 0.87492824, 96.65784   ,  1.6038833 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3845
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.1904826 , 107.26349259]), 'previousTarget': array([ 13.1492875, 107.40285  ]), 'currentState': array([18.050306, 87.86292 ,  4.793137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.2897479229139771
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.444457 , 118.91002  ,   2.0329912], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8095620835022395}
episode index:3846
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.46439205, 100.25210928]), 'previousTarget': array([ 9.48765985, 98.99405381]), 'currentState': array([10.934583  , 80.25764   ,  0.62870896], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28967260502395525
{'currentTarget': array([  8.9660665 , 116.38126759]), 'previousTarget': array([  8.9660665 , 116.38126759]), 'currentState': array([ 3.471595, 97.1508  ,  5.872248], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3847
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.57434993, 115.18086309]), 'previousTarget': array([  8.6, 115.2]), 'currentState': array([ 2.9007874, 96.00247  ,  5.234369 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28959732628044593
{'currentTarget': array([  8.97016558, 111.15884446]), 'previousTarget': array([  8.96762143, 111.10906541]), 'currentState': array([ 6.656173 , 91.29316  ,  4.7657704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3848
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.7416134 , 113.15316   ,   0.28842133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.2096828481384225}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.28972621191685194
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.229583  , 121.73526   ,   0.15726733], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1267348519494846}
episode index:3849
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.117564, 117.22582 ,   5.323832], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.990821096307502}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.28986554760825944
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.797558 , 119.116776 ,   2.0816329], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0028229203009804}
episode index:3850
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.115157  , 106.145065  ,   0.20347327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.899740672003203}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2899253956931834
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.606117 , 121.23641  ,   2.5583537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.026900927357346}
episode index:3851
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.13358391, 68.02422594]), 'previousTarget': array([10., 66.]), 'currentState': array([10.184986 , 48.024292 ,  1.4799583], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.28993022930557727
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.941966 , 118.96955  ,   2.4533703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4769092011357412}
episode index:3852
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.055814, 102.737076,   1.765225], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.288725787047966}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.2899746860245268
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.250217 , 119.5361   ,   0.9390289], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3335084260762193}
episode index:3853
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.06225663, 80.00137116]), 'previousTarget': array([ 7.96661103, 77.97662792]), 'currentState': array([ 7.0944867, 60.0248   ,  1.7300084], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.289937120435578
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.627586, 121.69829 ,   2.91426 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8105376697079172}
episode index:3854
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.314942 , 104.19046  ,   3.6651955], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.03593108049601}
done in step count: 247
reward sum = 0.08353972967320515
running average episode reward sum: 0.28988358025639194
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.762076 , 118.64678  ,   0.9373493], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8340267344061851}
episode index:3855
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.7799951 , 90.89988128]), 'previousTarget': array([ 8.81564739, 90.98336106]), 'currentState': array([ 7.942243, 70.917435,  5.719317], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28980840297935445
{'currentTarget': array([  9.9608235 , 119.77249338]), 'previousTarget': array([  9.9709414, 119.8244896]), 'currentState': array([  6.566789 , 100.062584 ,   5.1062703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3856
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.92301141, 86.32491187]), 'previousTarget': array([11.8480693 , 87.96679883]), 'currentState': array([11.470992 , 66.33242  ,  4.1108713], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2897332646845711
{'currentTarget': array([ 4.47637324, 90.58384632]), 'previousTarget': array([ 4.47274088, 91.03882754]), 'currentState': array([ 0.78537524, 70.92738   ,  4.105268  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3857
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.62162619, 71.78764585]), 'previousTarget': array([ 8.58798103, 71.99135509]), 'currentState': array([ 8.050067, 51.795815,  5.927452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2896581653417291
{'currentTarget': array([  8.52645201, 116.51445714]), 'previousTarget': array([  8.62283175, 116.69380779]), 'currentState': array([ 0.7386048 , 98.09302   ,  0.27350098], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3858
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.19845941, 84.73757196]), 'previousTarget': array([ 9.36357627, 84.99669503]), 'currentState': array([ 8.743962 , 64.74274  ,  1.9935546], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289583104920547
{'currentTarget': array([  8.85851467, 115.5309589 ]), 'previousTarget': array([  8.92841258, 115.683697  ]), 'currentState': array([ 3.9090035, 96.153076 ,  1.0428964], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3859
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.28699549, 106.72891577]), 'previousTarget': array([ 11.18928508, 106.91786413]), 'currentState': array([13.217487 , 86.8223   ,  6.2632546], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.2895233076991374
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.674804 , 121.235886 ,   3.4202764], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4081098484129457}
episode index:3860
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.656412 , 113.68192  ,   1.3707095], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.480158972923952}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.2896802139272025
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.962074 , 118.797195 ,   3.4326086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2034023362418382}
episode index:3861
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.857363, 101.81478 ,   5.916296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.279824385914786}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.289688375724364
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.997141 , 118.692894 ,   6.2524304], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6474988765424412}
episode index:3862
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.28211409, 96.03596777]), 'previousTarget': array([11.11198773, 94.98027613]), 'currentState': array([10.517546 , 76.03735  ,  1.4769694], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28961338520514984
{'currentTarget': array([  7.81520436, 113.62116201]), 'previousTarget': array([  7.70913833, 113.39652319]), 'currentState': array([ 1.3346491, 94.70021  ,  3.0442164], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3863
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.91130628, 113.34714584]), 'previousTarget': array([  7.88171698, 113.11558017]), 'currentState': array([ 1.920526, 94.265465,  4.945035], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895384335009042
{'currentTarget': array([  9.25985974, 115.24275755]), 'previousTarget': array([  9.24953047, 115.07139324]), 'currentState': array([ 6.1852136, 95.48051  ,  3.943428 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3864
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.09430847, 87.04808148]), 'previousTarget': array([ 8.72679236, 84.98678996]), 'currentState': array([ 6.939588 , 67.08144  ,  2.1580791], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.2895132968590913
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.417749, 121.41496 ,   3.631454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1226484829258427}
episode index:3865
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.9712664, 114.89749  ,   4.352004 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.512946801549873}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.2895917896551447
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.031571  , 119.63419   ,   0.93673474], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.36716645574849804}
episode index:3866
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.91797729, 74.97415857]), 'previousTarget': array([ 7.89462602, 72.97998109]), 'currentState': array([ 6.994152 , 54.995506 ,  1.0537014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895169016826453
{'currentTarget': array([ 4.80384249, 90.73709687]), 'previousTarget': array([ 4.7256331, 90.5719271]), 'currentState': array([ 1.3071787, 71.045135 ,  2.2810202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3867
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.70730184, 93.72848456]), 'previousTarget': array([11.69841725, 93.95760212]), 'currentState': array([13.004302 , 73.770584 ,  0.7830641], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28944205243195176
{'currentTarget': array([ 10.26738357, 116.33391882]), 'previousTarget': array([ 10.24721212, 116.59987284]), 'currentState': array([11.722208 , 96.3869   ,  5.6251793], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3868
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.44310403, 74.85022022]), 'previousTarget': array([ 4.44310403, 74.85022022]), 'currentState': array([ 2.       , 55.       ,  2.8837318], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.28944009291866885
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.91695  , 118.37756  ,   0.8466784], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5113738154432426}
episode index:3869
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.25409451, 97.99899435]), 'previousTarget': array([11.07077201, 96.97840172]), 'currentState': array([10.485064 , 78.00033  ,  2.2305021], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2893653021969844
{'currentTarget': array([ 10.26190589, 118.23148989]), 'previousTarget': array([ 10.68335022, 116.9669979 ]), 'currentState': array([13.191833, 98.447266,  2.864901], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3870
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.49094518, 77.95861101]), 'previousTarget': array([ 6.6076838, 77.9352791]), 'currentState': array([ 4.8273964, 58.027916 ,  3.6821566], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2892905501168509
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.415483 , 100.366104 ,   5.1415577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.78192173699753}
episode index:3871
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.87408075, 73.18637793]), 'previousTarget': array([12.77155462, 74.9622374 ]), 'currentState': array([12.674096 , 53.202385 ,  3.8833065], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.289215836648329
{'currentTarget': array([15.62150357, 91.44491025]), 'previousTarget': array([15.62150357, 91.44491025]), 'currentState': array([19.48466 , 71.821556,  3.869546], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3872
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.997813, 120.06259 ,   5.887987], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.9981398101385786}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.28934607120732597
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.25065  , 121.53366  ,   3.7942286], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.326443578485487}
episode index:3873
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0633005, 110.83675  ,   1.1124425], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.79960226095083}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.2893758559005296
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.190282 , 119.10066  ,   1.7074602], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.020863216377452}
episode index:3874
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.26219748, 80.8658051 ]), 'previousTarget': array([14.0422346 , 78.90394822]), 'currentState': array([14.92362  , 60.934933 ,  2.1320097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.28937035160659835
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.964656, 121.207924,   4.195192], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2084408670240503}
episode index:3875
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.6447283 , 65.48202675]), 'previousTarget': array([12.20063923, 64.98401917]), 'currentState': array([10.881231 , 45.483425 ,  3.2445931], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2892956946531395
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.627119 , 110.054184 ,   5.2127223], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.965567471205327}
episode index:3876
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.19069303, 114.67831589]), 'previousTarget': array([  9.15981002, 114.74881264]), 'currentState': array([ 6.1837215 , 94.905655  ,  0.76016515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.28926332742978506
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.984543, 121.1609  ,   4.259327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2991499587040414}
episode index:3877
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.821672 , 109.27124  ,   2.6421993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.760178051800597}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.2892585545912297
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.811538 , 118.43569  ,   3.5599933], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3934763488926607}
episode index:3878
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.957329 , 110.912224 ,   3.1829677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.912019328808844}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.2893530120972835
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.508186 , 120.81955  ,   1.7877527], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7164753185981738}
episode index:3879
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.8860960e+01, 1.1108496e+02, 1.0623789e-01], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.569587651410188}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2894956899790584
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.157566 , 121.27849  ,   2.0370188], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7246713254607695}
episode index:3880
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.8211144, 116.72149  ,   4.4315605], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.811515420394121}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.2895708434766464
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.447142  , 120.47144   ,   0.44977942], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6228455662448449}
episode index:3881
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.3497299 , 92.99469695]), 'previousTarget': array([11.14970567, 92.98191681]), 'currentState': array([12.348087 , 73.01963  ,  4.0170426], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2894962502660651
{'currentTarget': array([ 5.19698873, 89.95819689]), 'previousTarget': array([ 5.11675402, 90.06600511]), 'currentState': array([ 2.0395358, 70.20901  ,  5.538372 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3882
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.821508 , 109.89752  ,   3.5976486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.194061176242117}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.2895259272484015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.194247 , 118.47598  ,   1.4509912], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.936196095016834}
episode index:3883
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.71117156, 78.0066159 ]), 'previousTarget': array([10.67746131, 77.99739905]), 'currentState': array([11.0498295, 58.009483 ,  3.9461272], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28945138401275566
{'currentTarget': array([  8.73174256, 116.85956766]), 'previousTarget': array([  8.77453258, 116.93506833]), 'currentState': array([ 1.2424515, 98.31474  ,  1.9784665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3884
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.0834875, 116.39227  ,   4.2267804], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.745902411709013}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.28954735071982673
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.609378, 120.73814 ,   2.03711 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9571763486108735}
episode index:3885
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.15804378, 87.59221982]), 'previousTarget': array([14.33175976, 87.82121323]), 'currentState': array([15.097797, 67.68651 ,  2.988273], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.2894999285765248
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.6299305, 118.77904  ,   2.5147102], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.275813827975943}
episode index:3886
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.37782228, 85.07312496]), 'previousTarget': array([11.28616939, 83.98725709]), 'currentState': array([10.59416 , 65.074295,  2.177678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2894254495622267
{'currentTarget': array([ 4.61452957, 90.90201617]), 'previousTarget': array([ 4.5000579 , 90.90374598]), 'currentState': array([ 0.9747342, 71.23601  ,  2.0174146], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3887
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.798333 , 102.14508  ,   2.6924942], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.87275814829094}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.28953935849229157
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.024748  , 121.508286  ,   0.40269017], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7961185863549458}
episode index:3888
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.4520476, 111.1904326]), 'previousTarget': array([  7.49442256, 111.23047895]), 'currentState': array([ 1.895284 , 91.977875 ,  3.8369062], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28946490764156074
{'currentTarget': array([  8.79171273, 117.01822455]), 'previousTarget': array([  8.79171273, 117.01822455]), 'currentState': array([ 1.280497, 98.48227 ,  2.061803], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3889
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.76458691, 83.577773  ]), 'previousTarget': array([12.62395817, 81.95260657]), 'currentState': array([12.732413, 63.601204,  2.083195], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28939049506890224
{'currentTarget': array([  9.74760287, 111.88784091]), 'previousTarget': array([  9.82531086, 111.78466462]), 'currentState': array([ 9.125635 , 91.897514 ,  3.4338937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3890
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.05707   , 123.95586   ,   0.20536967], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.975810418275598}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2893161207448033
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.470384 , 115.64052  ,   5.2266364], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.609212950023199}
episode index:3891
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.08134771, 76.34052883]), 'previousTarget': array([15.55689597, 74.85022022]), 'currentState': array([18.840519  , 56.53177   ,  0.08108437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2892417846397815
{'currentTarget': array([ 6.58844649, 87.32100541]), 'previousTarget': array([ 6.45904111, 87.55560765]), 'currentState': array([ 4.5118136, 67.42911  ,  5.567551 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3892
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.365203 , 116.78339  ,   1.2759683], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.1579754545641965}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.28940932596132307
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.132366 , 118.62348  ,   3.3368537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.320098828773912}
episode index:3893
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.9011793, 116.17511  ,   2.8667936], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.1989859231770374}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.2894727206452588
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.990709 , 121.313614 ,   6.0833144], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.313646745980716}
episode index:3894
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.       , 107.       ,   3.3488152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.811388300841895}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.28946126816217527
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.756137 , 120.86515  ,   4.6141458], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8988629029865886}
episode index:3895
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.71649 , 101.73659 ,   4.664603], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.556230073489637}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.2895552614764494
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.066297 , 118.29414  ,   1.2415106], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.011699328089226}
episode index:3896
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.49040151, 112.95396953]), 'previousTarget': array([ 10.62395817, 110.95260657]), 'currentState': array([11.879036 , 93.002235 ,  1.7773738], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.2896315975094135
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.237958 , 121.98491  ,   0.7874805], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1261636922548353}
episode index:3897
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.8564594 , 106.70995767]), 'previousTarget': array([ 12.00389241, 106.77431008]), 'currentState': array([14.623355 , 86.902275 ,  1.6027123], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2895572948933259
{'currentTarget': array([  8.84120944, 114.41884967]), 'previousTarget': array([  8.82678823, 114.27266094]), 'currentState': array([ 4.775405 , 94.83648  ,  4.8690395], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3898
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.28931318, 109.06010969]), 'previousTarget': array([ 13.25304229, 109.1565257 ]), 'currentState': array([19.048069, 89.90713 ,  5.129536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2894830303909167
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.6753464, 119.69698  ,   4.146151 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.330166689938864}
episode index:3899
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.39934723, 80.93243786]), 'previousTarget': array([11.32242309, 80.98851894]), 'currentState': array([12.115261, 60.945255,  1.039364], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2894088039728678
{'currentTarget': array([ 5.65355178, 89.32961019]), 'previousTarget': array([ 5.63703604, 89.18297986]), 'currentState': array([ 2.8472948, 69.527466 ,  5.6181006], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3900
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.95784671, 119.72986286]), 'previousTarget': array([  9.96680906, 119.77872706]), 'currentState': array([ 6.8742824, 99.969    ,  5.9620113], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.28949127216962756
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.238192 , 118.58822  ,   1.3444161], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.877829843176792}
episode index:3901
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.75695308, 91.7894341 ]), 'previousTarget': array([14.71202025, 91.72787848]), 'currentState': array([18.082468, 72.06785 ,  5.018836], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28941708168470454
{'currentTarget': array([ 3.93333916, 84.77644877]), 'previousTarget': array([ 3.93333916, 84.77644877]), 'currentState': array([ 0.5386598, 65.06665  ,  5.0264454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3902
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.55327631, 101.11567401]), 'previousTarget': array([ 10.48734798, 100.99342862]), 'currentState': array([11.1389885, 81.12425  ,  3.1800628], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 257
reward sum = 0.07555183406752786
running average episode reward sum: 0.28936228659179725
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.902566 , 121.76399  ,   2.0367756], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.981487868588222}
episode index:3903
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0357797, 110.073074 ,   3.038969 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.126179003572183}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.2893993944402091
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.640227  , 118.47978   ,   0.49062425], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.039618640102171}
episode index:3904
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.09203154, 86.85837473]), 'previousTarget': array([10., 87.]), 'currentState': array([10.14757 , 66.85845 ,  5.931164], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 294
reward sum = 0.05208914293358933
running average episode reward sum: 0.2893386235691447
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.116029 , 118.601654 ,   1.6247191], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3462137422929397}
episode index:3905
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.92167009, 87.38836406]), 'previousTarget': array([13.12154811, 86.91159005]), 'currentState': array([13.098147, 67.423   ,  2.661354], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2892645481406836
{'currentTarget': array([ 11.56410389, 115.50207824]), 'previousTarget': array([ 11.86323985, 113.80691211]), 'currentState': array([18.133053 , 96.61163  ,  1.4836504], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3906
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.83177582, 110.38020523]), 'previousTarget': array([  9.28764556, 108.95850618]), 'currentState': array([ 6.420697 , 90.52607  ,  2.1131523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2891905106315613
{'currentTarget': array([  8.61512437, 116.25114947]), 'previousTarget': array([  8.61512437, 116.25114947]), 'currentState': array([ 1.6846275, 97.49033  ,  4.621471 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3907
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.83389315, 113.90395638]), 'previousTarget': array([  9.76866244, 113.98522349]), 'currentState': array([ 9.289129 , 93.91138  ,  5.6393585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2891165110126689
{'currentTarget': array([ 11.19477481, 112.56682396]), 'previousTarget': array([ 11.10462149, 112.7694635 ]), 'currentState': array([1.4368744e+01, 9.2820282e+01, 3.2343149e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3908
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.41293805, 92.7196363 ]), 'previousTarget': array([ 8.27400308, 92.95938166]), 'currentState': array([ 7.2513824, 72.753395 ,  5.051403 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2890425492549271
{'currentTarget': array([  8.71983017, 116.82470153]), 'previousTarget': array([  8.6373676 , 116.65684804]), 'currentState': array([ 1.2414305, 98.27548  ,  2.5102217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3909
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.5775 , 114.62904,   4.18842], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.287931409916451}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.2891655673103108
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.090112 , 120.04246  ,   2.8126636], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9103601337305445}
episode index:3910
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.84797577, 88.85001154]), 'previousTarget': array([13.66317505, 88.86301209]), 'currentState': array([16.29995   , 69.000885  ,  0.67176825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28909163083183714
{'currentTarget': array([ 8.5019054 , 93.39913712]), 'previousTarget': array([ 8.55040167, 93.48554978]), 'currentState': array([ 7.377337 , 73.43078  ,  4.4449534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3911
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.38699498, 88.08617061]), 'previousTarget': array([ 6.29248216, 87.86817872]), 'currentState': array([ 4.1371417, 68.21312  ,  2.9655013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2890177321531992
{'currentTarget': array([ 5.0205887 , 89.83355934]), 'previousTarget': array([ 5.0205887 , 89.83355934]), 'currentState': array([ 1.7633722, 70.10058  ,  1.337488 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3912
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.96774091, 79.94378   ]), 'previousTarget': array([13.3923162, 77.9352791]), 'currentState': array([14.445478, 59.998447,  1.695459], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2889438712454166
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.970673 , 117.0171   ,   1.6417007], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.013300597274748}
episode index:3913
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.70864199, 84.15161871]), 'previousTarget': array([15.82643809, 83.74660743]), 'currentState': array([18.853893  , 64.40048   ,  0.85599744], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28887004807953887
{'currentTarget': array([  8.66339824, 115.30095951]), 'previousTarget': array([  8.76235146, 115.44557435]), 'currentState': array([ 3.1916175, 96.064026 ,  1.8237673], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3914
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.5991273 , 72.91107498]), 'previousTarget': array([ 5.78390595, 72.92028312]), 'currentState': array([ 3.7380621, 52.997852 ,  4.1544495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288796262626645
{'currentTarget': array([  8.72017136, 116.23704377]), 'previousTarget': array([  8.72017136, 116.23704377]), 'currentState': array([ 2.2802064 , 97.30224   ,  0.65310216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3915
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.70681852, 104.02921511]), 'previousTarget': array([ 12.71202025, 103.72787848]), 'currentState': array([16.04887  , 84.310425 ,  3.1236582], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2887225148578435
{'currentTarget': array([ 11.84131376, 114.57612438]), 'previousTarget': array([ 11.95324415, 114.11739953]), 'currentState': array([18.270592  , 95.63769   ,  0.58554006], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3916
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.916395 , 122.93571  ,   3.1974552], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.138083737752841}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2888990217470807
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.079676 , 120.19907  ,   4.0541506], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.21442618207897757}
episode index:3917
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.95719677, 88.94442375]), 'previousTarget': array([10., 89.]), 'currentState': array([ 9.929631, 68.94444 ,  2.635467], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28882528539645613
{'currentTarget': array([  8.54096382, 114.98016022]), 'previousTarget': array([  8.56909812, 114.97520126]), 'currentState': array([ 2.9588912, 95.77494  ,  0.3145811], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3918
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.16680518, 86.97759116]), 'previousTarget': array([15.01494615, 86.77598173]), 'currentState': array([18.258463 , 67.217995 ,  6.1533775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28875158667601813
{'currentTarget': array([ 6.4038184 , 89.07027873]), 'previousTarget': array([ 6.34022378, 89.07178062]), 'currentState': array([ 4.09399  , 69.20411  ,  2.5625818], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3919
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.76155632, 73.84094017]), 'previousTarget': array([ 5.81071492, 73.91786413]), 'currentState': array([ 3.9327981, 53.924725 ,  4.6207495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28867792555696814
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.011016 , 101.322044 ,   2.8608534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.705298168021937}
episode index:3920
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.834818 , 112.92532  ,   1.2227966], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.398585091648222}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2888517641375453
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.88562   , 118.146935  ,   0.28424937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8565921659048872}
episode index:3921
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.823569 , 111.01106  ,   3.2777638], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.026586374294803}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.2890133890560283
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.430197 , 118.36238  ,   1.5349834], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2684969155148873}
episode index:3922
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.9500504, 115.04842  ,   2.8622923], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.072488855941508}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.28913999235751986
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.721111 , 118.09608  ,   0.8341738], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.293573464061411}
episode index:3923
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.37136371, 101.70471133]), 'previousTarget': array([ 13.37675141, 101.66906377]), 'currentState': array([16.995838 , 82.03587  ,  2.7539332], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28906630734417693
{'currentTarget': array([  8.21308171, 115.19596788]), 'previousTarget': array([  8.14455574, 115.15136093]), 'currentState': array([ 1.2405618, 96.45073  ,  4.2699037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3924
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.79934966, 113.74389992]), 'previousTarget': array([  8.77694787, 113.64012894]), 'currentState': array([ 5.029807 , 94.10235  ,  4.0202646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28899265987733763
{'currentTarget': array([  8.98169854, 115.90688094]), 'previousTarget': array([  8.98169854, 115.90688094]), 'currentState': array([ 4.1532063, 96.49849  ,  0.5596017], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3925
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.826084 , 71.1530707]), 'previousTarget': array([ 5.0186243 , 70.89786813]), 'currentState': array([ 2.7194483, 51.264328 ,  3.8265345], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.2889616209764557
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.27084  , 118.10694  ,   1.8178109], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.563916322014015}
episode index:3926
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.93139959, 112.93315988]), 'previousTarget': array([  8.93097322, 112.78406925]), 'currentState': array([ 5.941127 , 93.15797  ,  4.8698106], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.2889179765558716
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.69933 , 119.78918 ,   3.106285], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7304171907559285}
episode index:3927
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0598265, 116.88155  ,   3.115651 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.468442848166491}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.28896914058246226
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.608696 , 118.67176  ,   5.6916523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3846800516496443}
episode index:3928
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.616763 , 104.173805 ,   2.2480967], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.004632414791093}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.28904203078592233
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.880178  , 118.68956   ,   0.49022692], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3159066697301398}
episode index:3929
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.70558985, 116.17257075]), 'previousTarget': array([  8.5704125 , 115.88993593]), 'currentState': array([ 2.2982297, 97.22671  ,  2.896032 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28896848319539153
{'currentTarget': array([ 4.39845898, 89.64844529]), 'previousTarget': array([ 4.36999433, 89.56856031]), 'currentState': array([ 0.7686517, 69.98059  ,  3.2895882], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3930
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.279237 , 123.41396  ,   1.7777221], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.645763611324765}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.289127361029095
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.005299, 121.67674 ,   5.280121], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9550168530589762}
episode index:3931
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.73175859, 87.25053072]), 'previousTarget': array([ 8.7541802 , 86.98577525]), 'currentState': array([ 7.9578276, 67.26551  ,  4.013459 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28905382914683936
{'currentTarget': array([ 5.30516888, 89.64824908]), 'previousTarget': array([ 5.1155245 , 89.55708025]), 'currentState': array([ 2.2479122, 69.8833   ,  2.664052 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3932
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.54737812, 104.28573295]), 'previousTarget': array([ 11.85036314, 102.88414095]), 'currentState': array([13.507292, 84.381996,  2.253158], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28898033465684525
{'currentTarget': array([  8.74404691, 116.24792289]), 'previousTarget': array([  8.74404691, 116.24792289]), 'currentState': array([ 2.3955643, 97.28225  ,  2.9997663], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3933
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.76338794, 85.60602704]), 'previousTarget': array([ 4.93097322, 85.78406925]), 'currentState': array([ 1.7530048, 65.833885 ,  5.932172 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28890687753059796
{'currentTarget': array([ 4.39025695, 90.59981684]), 'previousTarget': array([ 4.33496999, 90.29965877]), 'currentState': array([ 0.641755, 70.95424 ,  5.784734], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3934
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.700687 , 108.90504  ,   0.6408866], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.50550969678897}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.2890311251752914
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.256535, 120.84371 ,   2.919967], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5135153889762107}
episode index:3935
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.037223, 108.05734 ,   5.274052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.322815838008644}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.2890401227374581
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.722557, 121.45853 ,   5.89789 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9388554151350226}
episode index:3936
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.35073564, 75.93642563]), 'previousTarget': array([ 7.24756572, 75.96105157]), 'currentState': array([ 7.6026754, 55.95042  ,  6.123141 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2889667063994501
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.718286, 119.26714 ,   3.965727], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.3439789909568045}
episode index:3937
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.92773982, 74.20465301]), 'previousTarget': array([ 7.90815322, 73.9793708 ]), 'currentState': array([ 7.023656, 54.225098,  2.329751], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28889332734754575
{'currentTarget': array([ 6.03336419, 87.72967417]), 'previousTarget': array([ 7.29628427, 87.1619075 ]), 'currentState': array([ 3.5933485, 67.879074 ,  2.7477458], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3938
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.95368299, 106.08046101]), 'previousTarget': array([  9.10940039, 103.96920706]), 'currentState': array([ 7.4545336, 86.13673  ,  1.757892 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.2888422869011996
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.154624 , 120.084435 ,   1.9384308], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8473066399550242}
episode index:3939
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.921761 , 110.824165 ,   6.0561156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.412476584886676}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.2889304450735759
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.943684, 120.97564 ,   3.287316], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.437941728221501}
episode index:3940
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.18274554, 78.98196871]), 'previousTarget': array([ 7.26728946, 76.95980905]), 'currentState': array([ 5.812308 , 59.028976 ,  1.9256275], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.28889626377611854
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.163438 , 119.78352  ,   2.3945966], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1834059593126143}
episode index:3941
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.70135669, 116.20123179]), 'previousTarget': array([ 10.95156206, 116.35234545]), 'currentState': array([14.332535, 96.53363 ,  3.008162], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.2888698579669284
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.249542 , 121.29913  ,   1.5736803], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1798737843274156}
episode index:3942
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.45230268, 71.96850449]), 'previousTarget': array([ 4.33682495, 71.86301209]), 'currentState': array([ 2.157534 , 52.10059  ,  3.2454224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28879659652691647
{'currentTarget': array([ 10.74009667, 107.73255404]), 'previousTarget': array([ 10.67602146, 107.60980836]), 'currentState': array([11.9445095, 87.76885  ,  6.0258417], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3943
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.026925 , 112.923004 ,   1.1825668], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.142480943268932}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.28892258069128773
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.591198 , 118.059235 ,   3.5092216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.509677516218289}
episode index:3944
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.71215766, 77.58813673]), 'previousTarget': array([ 4.55942675, 77.83555733]), 'currentState': array([ 2.237748 , 57.741795 ,  2.9604588], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2888493430282481
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.910337 , 102.690506 ,   1.9142658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.583077114356335}
episode index:3945
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.59812536, 64.6714832 ]), 'previousTarget': array([11.4668562 , 64.99289268]), 'currentState': array([12.1755705, 44.67982  ,  6.153795 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.2887894763158468
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.692699 , 120.50067  ,   2.9410417], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7651921158431767}
episode index:3946
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.93625103, 81.06482098]), 'previousTarget': array([ 6.02346204, 80.89737675]), 'currentState': array([ 3.8600855, 61.172874 ,  2.2642984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288716309486276
{'currentTarget': array([ 8.33414368, 68.87749374]), 'previousTarget': array([ 8.22244421, 68.96395055]), 'currentState': array([ 7.682778 , 48.888103 ,  3.5291977], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3947
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.308397 , 101.927345 ,   3.2791734], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.15164919491667}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.2887467309312953
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.920491 , 118.01844  ,   1.8735101], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2565279226185235}
episode index:3948
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.02758275, 100.9264238 ]), 'previousTarget': array([  7.04114369, 100.76743395]), 'currentState': array([ 3.9479635, 81.16495  ,  4.386603 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.2887781826760921
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.783178 , 118.492645 ,   2.4894598], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6986720098002333}
episode index:3949
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.37294406, 92.55507598]), 'previousTarget': array([15.23855458, 92.64310384]), 'currentState': array([19.215439, 72.927666,  6.212892], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28870507427541464
{'currentTarget': array([ 8.99658207, 86.58530312]), 'previousTarget': array([ 9.05475095, 86.64792486]), 'currentState': array([ 8.396268, 66.594315,  5.202645], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3950
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.38567273, 115.4483333 ]), 'previousTarget': array([  9.04114369, 113.76743395]), 'currentState': array([ 6.710577 , 95.628044 ,  1.0789806], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.2887408079983325
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.575755  , 119.22423   ,   0.66955066], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8841979765507613}
episode index:3951
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.80281366, 66.87263682]), 'previousTarget': array([10.73335703, 64.99822246]), 'currentState': array([11.105001 , 46.87492  ,  1.8447236], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28866774605298884
{'currentTarget': array([14.88651253, 91.59157075]), 'previousTarget': array([14.95007694, 91.45008215]), 'currentState': array([18.276907 , 71.881035 ,  5.0435796], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3952
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.49147089, 91.5001473 ]), 'previousTarget': array([10.58342373, 91.99566113]), 'currentState': array([10.836313 , 71.50312  ,  5.8211265], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 258
reward sum = 0.07479631572685258
running average episode reward sum: 0.28861364247840593
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.15897 , 119.029884,   2.448823], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2839221398117726}
episode index:3953
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.74124102, 77.80774418]), 'previousTarget': array([ 4.55942675, 77.83555733]), 'currentState': array([ 2.26762  , 57.961304 ,  4.3242607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2885406496502627
{'currentTarget': array([13.18807177, 89.48972328]), 'previousTarget': array([13.18807177, 89.48972328]), 'currentState': array([15.26659  , 69.59802  ,  1.9833244], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3954
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.51032929, 99.62449572]), 'previousTarget': array([ 8.49579896, 99.9439862 ]), 'currentState': array([ 7.0520043, 79.677734 ,  3.3351202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28846769373378983
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.505388 , 101.99577  ,   5.1570525], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.06705212043186}
episode index:3955
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.966924 , 106.981316 ,   2.4140532], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.166432017413104}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.2886210988299791
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.346074 , 120.82882  ,   1.7347536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8499762860335947}
episode index:3956
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.01288742, 73.01330445]), 'previousTarget': array([10.70152578, 72.9977727 ]), 'currentState': array([12.868893 , 53.03163  ,  0.7438617], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28854815945701223
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.728526 , 111.08734  ,   1.4082928], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.354898447852344}
episode index:3957
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.62960386, 65.8896538 ]), 'previousTarget': array([ 7.79936077, 64.98401917]), 'currentState': array([ 8.123247  , 45.896065  ,  0.81150466], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28847525694072695
{'currentTarget': array([ 12.67532042, 112.18845493]), 'previousTarget': array([ 12.67532042, 112.18845493]), 'currentState': array([19.15547 , 93.267365,  5.862802], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3958
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.01504656, 83.1801876 ]), 'previousTarget': array([14.51930531, 83.84555753]), 'currentState': array([19.23959 , 63.44184 ,  6.234201], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28840239125319456
{'currentTarget': array([ 12.12679697, 114.12900946]), 'previousTarget': array([ 12.01169982, 114.33192641]), 'currentState': array([18.938715 , 95.324814 ,  6.2323093], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3959
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.71506008, 114.43672637]), 'previousTarget': array([  8.66745905, 114.44774604]), 'currentState': array([ 4.2141867, 94.94975  ,  2.852804 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28832956236651447
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.6295776, 102.37554  ,   2.5578895], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.943836761773333}
episode index:3960
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.307155 , 104.00894  ,   5.7408795], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.99400797558199}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.28836862281065984
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.426723 , 119.83579  ,   1.1581848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.581823662071737}
episode index:3961
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 14.42716862, 101.42421795]), 'previousTarget': array([ 14.39067232, 101.46160575]), 'currentState': array([19.063904 , 81.969124 ,  4.9231772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.28839799266171173
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.937861 , 119.89342  ,   1.3920854], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9438982704415724}
episode index:3962
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.504338, 102.79939 ,   1.349031], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.3819619613253}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2885557315602285
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.689706 , 118.26483  ,   1.9579381], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.421965199871048}
episode index:3963
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.88284734, 82.8298588 ]), 'previousTarget': array([15.88074853, 82.75525931]), 'currentState': array([1.9009295e+01, 6.3075737e+01, 1.8206295e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288482937480622
{'currentTarget': array([ 4.4445018 , 90.79834374]), 'previousTarget': array([ 4.33511829, 90.42495404]), 'currentState': array([ 0.70662415, 71.15074   ,  3.1042147 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3964
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.32511  , 122.81851  ,   5.5059414], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.45880999703079}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28865736801341374
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.713158, 121.25777 ,   5.714464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7994277854444234}
episode index:3965
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.95870206, 91.84049636]), 'previousTarget': array([ 8.83261089, 91.98266146]), 'currentState': array([ 8.219636 , 71.85416  ,  5.0037127], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2885845850159318
{'currentTarget': array([ 11.11361655, 117.39507372]), 'previousTarget': array([ 11.12483996, 117.34042515]), 'currentState': array([18.975416 , 99.00507  ,  5.6187615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3966
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.00124766, 119.97504678]), 'currentState': array([ 11.053634, 100.16895 ,   4.31445 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.859017381865716}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288511838712676
{'currentTarget': array([  8.7133713 , 116.89603757]), 'previousTarget': array([  8.7133713 , 116.89603757]), 'currentState': array([ 1.0550015, 98.420395 ,  1.4936779], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3967
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.1371743 , 108.44048667]), 'previousTarget': array([ 12.84991583, 106.5646825 ]), 'currentState': array([15.773241 , 88.77379  ,  1.9013493], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.2885020920437611
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.127289 , 120.250114 ,   2.2444565], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1547022618639766}
episode index:3968
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.18920971, 77.81255776]), 'previousTarget': array([12.03338897, 77.97662792]), 'currentState': array([13.225664 , 57.83943  ,  5.3122597], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2884294031820721
{'currentTarget': array([  9.14478429, 110.82942041]), 'previousTarget': array([  9.74842319, 109.81031339]), 'currentState': array([ 7.287713 , 90.915825 ,  2.5794737], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3969
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.92549549, 78.44094942]), 'previousTarget': array([ 3.91921747, 78.78580727]), 'currentState': array([ 1.0329188, 58.65123  ,  5.3280106], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28835675093945695
{'currentTarget': array([ 4.50813062, 91.08419491]), 'previousTarget': array([ 4.50813062, 91.08419491]), 'currentState': array([ 0.776317, 71.43544 ,  3.081881], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3970
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.5150853 , 68.84122844]), 'previousTarget': array([ 3.5150853 , 68.84122844]), 'currentState': array([ 1.      , 49.      ,  1.910422], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28828413528825086
{'currentTarget': array([ 4.51374351, 90.46444943]), 'previousTarget': array([ 4.61284635, 90.27010832]), 'currentState': array([ 0.8612028, 70.800804 ,  2.3728728], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3971
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.096472 , 119.06648  ,   1.2022314], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.157609004223089}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.288366966608909
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.304688 , 121.29744  ,   1.9665766], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8399895142185108}
episode index:3972
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.62853213, 74.10213797]), 'previousTarget': array([12.06352827, 75.97806349]), 'currentState': array([12.337719, 54.114716,  3.974764], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28829438494099835
{'currentTarget': array([14.8471842 , 90.90945704]), 'previousTarget': array([14.86985385, 91.05517822]), 'currentState': array([18.134346 , 71.18144  ,  5.7644057], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3973
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.061494, 107.338715,   3.294885], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.808823286791409}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.28833110787553556
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.174021 , 119.91253  ,   2.6123662], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.177274793384283}
episode index:3974
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.00137593, 110.84442632]), 'previousTarget': array([  7.12018361, 111.04057123]), 'currentState': array([ 0.7763674, 91.83787  ,  2.3624625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882585717477681
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.327543 , 109.40587  ,   1.2892046], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.599193657262278}
episode index:3975
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.95779893, 117.03958921]), 'previousTarget': array([ 10.93592685, 117.0585156 ]), 'currentState': array([17.114315, 98.010735,  5.410692], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.28825083181693717
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.857942 , 119.67842  ,   1.8844042], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1864697072588954}
episode index:3976
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.77429266, 76.86485292]), 'previousTarget': array([14.79136948, 76.87767469]), 'currentState': array([16.9745    , 56.986244  ,  0.44624293], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28817835235205985
{'currentTarget': array([  8.58413738, 116.48235116]), 'previousTarget': array([  8.57588763, 116.54410208]), 'currentState': array([ 1.1163144, 97.92887  ,  2.5700107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3977
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.027071, 118.08696 ,   4.339388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.2828187870589565}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2883178105825857
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.788114 , 119.42453  ,   2.1732473], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9758528189822592}
episode index:3978
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.0685835, 117.97621  ,   2.0791576], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.544127122083421}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28849166888603317
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.647562 , 121.067314 ,   2.4394972], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9630639630819224}
episode index:3979
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.65880482, 68.12569274]), 'previousTarget': array([ 8.54773967, 66.99249812]), 'currentState': array([ 9.527261  , 48.126125  ,  0.64811635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.2884680098232183
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.108094, 118.00741 ,   2.108698], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.279976996023737}
episode index:3980
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.8549656 , 111.79968374]), 'previousTarget': array([  8.82842712, 111.79898987]), 'currentState': array([ 6.0891395, 91.99185  ,  3.120593 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28839554863009514
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.536431 , 108.96802  ,   4.8651924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.319810950713755}
episode index:3981
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.48069469, 115.84555753]), 'previousTarget': array([  9.48069469, 115.84555753]), 'currentState': array([ 7.       , 96.       ,  2.8868692], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.28848777958738897
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.803598 , 118.13599  ,   2.2295485], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5937452017494236}
episode index:3982
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.02607361, 105.12376651]), 'previousTarget': array([ 13.0776773 , 104.61161351]), 'currentState': array([14.725063 , 85.30672  ,  3.5395842], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28841534981596356
{'currentTarget': array([  7.73312478, 112.91377341]), 'previousTarget': array([  7.75233088, 112.91200863]), 'currentState': array([ 1.6393594, 93.86473  ,  2.773231 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3983
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.21246051, 105.12123523]), 'previousTarget': array([  8.6609096 , 103.93091516]), 'currentState': array([ 8.155332  , 85.14919   ,  0.89775026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.2883947655256276
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.6453495, 120.17705  ,   2.3277006], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6548476921167758}
episode index:3984
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.3861619 , 110.26071033]), 'previousTarget': array([ 10.3337034 , 109.98889814]), 'currentState': array([11.178537  , 90.27641   ,  0.68717396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.28845969949723743
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.536838 , 118.004715 ,   1.6601366], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0662422298900225}
episode index:3985
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.093636 , 123.88001  ,   0.7072373], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.085429026338439}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.28852052384331317
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.95779  , 119.44038  ,   1.7556112], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.036202803634196}
episode index:3986
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.79697101, 89.0161599 ]), 'previousTarget': array([14.90064462, 88.75839053]), 'currentState': array([17.856949 , 69.25163  ,  4.5774846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28844815852506805
{'currentTarget': array([  8.71589685, 115.93436012]), 'previousTarget': array([  8.77030641, 116.12492404]), 'currentState': array([ 2.6923444, 96.863    ,  6.1083984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3987
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.5636581 , 77.83346527]), 'previousTarget': array([ 8.64482588, 77.98960229]), 'currentState': array([ 7.882782 , 57.84506  ,  3.2977877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28837582949835666
{'currentTarget': array([  9.78260786, 118.45943222]), 'previousTarget': array([  9.78260786, 118.45943222]), 'currentState': array([ 6.98806  , 98.65563  ,  1.9340326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3988
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.738274  , 89.39686304]), 'previousTarget': array([15.58917133, 87.70701012]), 'currentState': array([19.42416  , 69.73944  ,  1.1728501], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28830353673588527
{'currentTarget': array([ 5.65492743, 90.19158957]), 'previousTarget': array([ 5.65975031, 90.04015707]), 'currentState': array([ 2.7700815, 70.40074  ,  4.684893 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:3989
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.054632  , 108.026695  ,   0.22645277], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.148314287369152}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2883590961868912
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.931982  , 119.84077   ,   0.59131044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9385328943400002}
episode index:3990
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.012558, 104.24438 ,   2.364069], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.755627868424801}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.28834881795830364
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.518536, 121.90512 ,   6.246083], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9650174061797874}
episode index:3991
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.17451232, 83.98371259]), 'previousTarget': array([13.22136124, 83.92075411]), 'currentState': array([14.930525 , 64.06095  ,  0.3790175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882765862904784
{'currentTarget': array([ 10.70064021, 115.72394925]), 'previousTarget': array([ 10.65232019, 116.91010484]), 'currentState': array([13.93456 , 95.98714 ,  4.284193], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3992
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.38097902, 109.15409266]), 'previousTarget': array([ 10.35517412, 108.98960229]), 'currentState': array([11.0830765, 89.16642  ,  5.7398973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.28839340139213465
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.710576 , 118.9796   ,   2.3216314], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.243437387322954}
episode index:3993
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.21355585, 113.27877828]), 'previousTarget': array([  8.19946947, 113.31231517]), 'currentState': array([ 3.076096 , 93.949875 ,  2.7662835], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.28840081142278734
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.135473 , 118.51562  ,   2.4653852], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8688743389974487}
episode index:3994
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.54522905, 89.26347435]), 'previousTarget': array([10., 91.]), 'currentState': array([ 9.249346 , 69.26566  ,  3.6403637], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28832862098188056
{'currentTarget': array([ 5.87800522, 88.34321011]), 'previousTarget': array([ 6.0030578 , 88.49710255]), 'currentState': array([ 3.295627 , 68.51063  ,  1.7447879], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3995
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.756829 , 122.88171  ,   2.6267505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.979433062248763}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.28845111666216516
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.235739 , 118.35378  ,   6.0096745], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.814972948179252}
episode index:3996
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.078884 , 123.88497  ,   1.8612306], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.632965487282838}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.288502752274609
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.527984, 121.05033 ,   4.326788], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8541654550193347}
episode index:3997
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.16904181, 76.49110328]), 'previousTarget': array([ 9.31246186, 75.99755904]), 'currentState': array([ 7.3281384, 56.50879  ,  2.9479294], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28843059050565584
{'currentTarget': array([ 4.35622072, 90.62620287]), 'previousTarget': array([ 4.37069377, 91.00082683]), 'currentState': array([ 0.5825149 , 70.98545   ,  0.32105875], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:3998
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.208616 , 121.89301  ,   3.5929644], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.7254149437164923}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2886060267170823
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.495928 , 120.54838  ,   4.0410256], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5932728691099343}
episode index:3999
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.014786  , 104.14068999]), 'previousTarget': array([  6.22665585, 102.54828331]), 'currentState': array([ 1.1406097, 84.74372  ,  2.199524 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288533875210403
{'currentTarget': array([  8.5035017 , 114.79630004]), 'previousTarget': array([  8.47441752, 114.8624998 ]), 'currentState': array([ 2.9758706 , 95.57534   ,  0.46024847], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4000
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.050257, 124.16495 ,   0.928728], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.962626893655754}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.288668250303786
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.0426445, 121.94604  ,   3.1434622], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.168776312970655}
episode index:4001
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.37455449, 105.64407058]), 'previousTarget': array([  6.45649603, 104.49717013]), 'currentState': array([ 3.7765822 , 85.97037   ,  0.47823325], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.28863788190916106
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.860076 , 119.75015  ,   1.5104834], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8956307108273196}
episode index:4002
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.89830045, 118.95572888]), 'previousTarget': array([  9.89618185, 118.90990945]), 'currentState': array([ 7.9597106, 99.049904 ,  5.0457115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.28859342822984313
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.924341 , 120.85034  ,   5.6116285], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1038465325182982}
episode index:4003
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.0762112 , 77.81647073]), 'previousTarget': array([16.12688722, 77.79255473]), 'currentState': array([18.927628  , 58.02078   ,  0.27427787], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2885213519490664
{'currentTarget': array([  8.31155623, 115.71175668]), 'previousTarget': array([  8.31398533, 115.77912329]), 'currentState': array([ 0.9843125 , 97.10231   ,  0.98339635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4004
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.2616665 , 75.99940122]), 'previousTarget': array([ 9.31246186, 75.99755904]), 'currentState': array([ 8.926112 , 56.002216 ,  5.5267043], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2884493116614387
{'currentTarget': array([ 3.84557989, 82.24691877]), 'previousTarget': array([ 3.91699179, 82.51738353]), 'currentState': array([ 0.6277029, 62.507484 ,  4.84233  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4005
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.6732812, 116.91924  ,   5.6743712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.878361935460807}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.2886030642234325
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.089489 , 120.15174  ,   0.6010594], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9165275069813958}
episode index:4006
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.73528475, 106.2043392 ]), 'previousTarget': array([ 13.73765188, 106.29527642]), 'currentState': array([18.962238 , 86.899445 ,  1.3694777], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.28856764133284324
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.573835 , 121.84268  ,   3.2904058], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9299646534750696}
episode index:4007
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.38267585, 67.85361459]), 'previousTarget': array([11.44465866, 67.99228841]), 'currentState': array([11.912795, 47.86064 ,  5.953505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288495643418339
{'currentTarget': array([  8.13967846, 115.08087082]), 'previousTarget': array([  8.13347893, 114.96503806]), 'currentState': array([ 1.0650654, 96.373924 ,  3.4013903], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4008
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.4870971 , 87.81833236]), 'previousTarget': array([ 4.41082867, 87.70701012]), 'currentState': array([ 1.110174, 68.105484,  2.030366], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2884236814219762
{'currentTarget': array([ 4.76629624, 90.35013225]), 'previousTarget': array([ 4.6221813 , 90.30750745]), 'currentState': array([ 1.2897043, 70.65462  ,  0.1564955], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4009
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.86242742, 99.18696716]), 'previousTarget': array([ 8.97445107, 98.97624702]), 'currentState': array([ 7.7709217, 79.216774 ,  3.6225772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28835175531688345
{'currentTarget': array([  8.20770509, 115.29320419]), 'previousTarget': array([  8.26599687, 115.44573   ]), 'currentState': array([ 1.0904694, 96.60243  ,  1.4833673], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4010
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.        , 105.        ,   0.23337013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.132745950421555}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.28843531892684965
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.335157, 118.48068 ,   2.010846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5558459212266573}
episode index:4011
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.97402059, 112.38149641]), 'previousTarget': array([  8.39813833, 110.70920231]), 'currentState': array([ 6.3047285, 92.560425 ,  1.0557151], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.2884501887662229
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.453962 , 121.29303  ,   3.6041522], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9457472782644143}
episode index:4012
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.02668552, 66.03942627]), 'previousTarget': array([10., 66.]), 'currentState': array([10.036576 , 46.03943  ,  6.0371985], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28837830982558843
{'currentTarget': array([  8.78951612, 115.88690229]), 'previousTarget': array([  8.80625861, 115.92691331]), 'currentState': array([ 3.142973 , 96.70054  ,  1.1663845], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4013
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.0661745, 105.86192  ,   5.3628845], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.288257143336049}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.28839857672340213
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.167051 , 119.15649  ,   1.9451247], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1854613098144184}
episode index:4014
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.0586643, 121.89831  ,   4.99095  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.293427130727588}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.288480492430555
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.181793 , 119.041275 ,   0.4249068], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0554876549137564}
episode index:4015
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.9763036, 113.057335 ,   3.384087 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.19160036574044}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.28860429662586795
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.461014 , 118.61628  ,   1.0091633], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4849876329511873}
episode index:4016
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.297668 , 120.0743   ,   1.1203625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.298106769653227}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.28875310931570863
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.030219 , 118.99902  ,   2.4143083], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4364210481196946}
episode index:4017
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.84077045, 91.50521118]), 'previousTarget': array([14.71202025, 91.72787848]), 'currentState': array([18.190432 , 71.78771  ,  4.9274077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2886812444303638
{'currentTarget': array([ 4.30298348, 89.01560257]), 'previousTarget': array([ 4.38464121, 89.12088482]), 'currentState': array([ 0.68626547, 69.34534   ,  4.410029  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4018
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.14255732, 87.78422472]), 'previousTarget': array([11.23133756, 87.98522349]), 'currentState': array([11.851427, 67.79679 ,  4.310102], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2886094153075894
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.7469518, 111.06356  ,   1.3652855], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.906904361496268}
episode index:4019
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.6848545, 122.61164  ,   4.40263  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6999433409867915}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2887814278908462
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.933313 , 121.03038  ,   3.3618703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4830724273377947}
episode index:4020
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.01195155, 101.18144586]), 'previousTarget': array([ 10., 101.]), 'currentState': array([10.024653 , 81.18145  ,  4.9179487], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.28877694463584025
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.595902 , 118.93764  ,   0.9241781], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9171643257361113}
episode index:4021
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.92945792, 100.94901399]), 'previousTarget': array([ 13.28216664, 102.65140491]), 'currentState': array([17.969614, 81.361336,  5.168086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2887051452960501
{'currentTarget': array([ 11.72510036, 110.49496665]), 'previousTarget': array([ 11.68441479, 110.59022612]), 'currentState': array([15.296621 , 90.816444 ,  5.8929887], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4022
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.5411415, 103.97254  ,   1.7541742], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.034025295460264}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.2888346562885744
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.277566 , 118.50404  ,   1.0041517], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5214889001309535}
episode index:4023
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.24527876, 75.96413251]), 'previousTarget': array([ 7.24756572, 75.96105157]), 'currentState': array([ 5.9965935, 56.00315  ,  4.3438487], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2887628782924788
{'currentTarget': array([ 4.45282961, 90.27396407]), 'previousTarget': array([ 4.492074  , 90.46573944]), 'currentState': array([ 0.78396726, 70.61336   ,  4.6957164 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4024
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.41709911, 106.92977295]), 'previousTarget': array([ 10.39421747, 106.99082358]), 'currentState': array([11.055017 , 86.93995  ,  5.7166758], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28869113596246826
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.363985 , 106.63088  ,   3.7830877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.785851607928802}
episode index:4025
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0643694, 116.0785   ,   5.8797126], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.967505419451622}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.28885564140557246
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.926139 , 121.49829  ,   0.5694057], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.843381045016952}
episode index:4026
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.89715844, 77.58314633]), 'previousTarget': array([ 3.87311278, 77.79255473]), 'currentState': array([ 1.0489327, 57.786995 ,  1.9444525], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2887839116709299
{'currentTarget': array([15.28102884, 92.0454479 ]), 'previousTarget': array([15.36755417, 91.92846487]), 'currentState': array([18.993656, 72.39306 ,  4.957776], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4027
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.0051  , 112.7578  ,   5.287881], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.556008450860487}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.28884010660780995
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.963757 , 119.48276  ,   0.9164374], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.093785373141551}
episode index:4028
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.994348, 116.93423 ,   5.519167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.2229941428729307}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.28891121969874295
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.636766 , 118.11419  ,   1.2881964], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9204742105500967}
episode index:4029
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.59982226, 73.81010436]), 'previousTarget': array([15.59337265, 73.85467564]), 'currentState': array([18.006893 , 53.955482 ,  4.3479505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2888395295697855
{'currentTarget': array([ 3.67500936, 80.64468029]), 'previousTarget': array([ 3.75568959, 80.5093192 ]), 'currentState': array([ 0.5014335, 60.898075 ,  2.592311 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4030
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.70957816, 102.76158182]), 'previousTarget': array([ 10.44465866, 103.99228841]), 'currentState': array([ 9.372679 , 82.76442  ,  3.8400383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2887678750102296
{'currentTarget': array([ 3.43075722, 74.99911025]), 'previousTarget': array([ 3.61246208, 74.82840254]), 'currentState': array([ 0.5417717, 55.208866 ,  3.7091675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4031
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.21813091, 83.74552325]), 'previousTarget': array([ 6.05798302, 81.89383588]), 'currentState': array([ 4.1430993, 63.85346  ,  1.5083693], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 285
reward sum = 0.057020426354371746
running average episode reward sum: 0.28871039796443204
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.315078 , 119.124214 ,   0.6788558], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1118090038498047}
episode index:4032
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.82756007, 111.23869931]), 'previousTarget': array([  7.8507125, 111.40285  ]), 'currentState': array([ 3.0141532, 91.82656  ,  2.4705653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2886388109577461
{'currentTarget': array([ 11.91862336, 113.98837684]), 'previousTarget': array([ 11.9177885 , 114.09620054]), 'currentState': array([17.999485  , 94.93521   ,  0.98234344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4033
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.0322204, 121.113304 ,   3.882051 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.169725867362454}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.28873989534225075
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.01881227e+01, 1.18489624e+02, 5.00227809e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.52204656939007}
episode index:4034
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 14.27296842, 102.43335206]), 'previousTarget': array([ 14.27296842, 102.43335206]), 'currentState': array([19.      , 83.      ,  5.431173], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28866833650821305
{'currentTarget': array([ 10.66404354, 118.22217844]), 'previousTarget': array([ 10.66796697, 118.21937307]), 'currentState': array([17.662119 , 99.486465 ,  2.9588394], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4035
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.85239908, 83.7465535 ]), 'previousTarget': array([13.94201698, 81.89383588]), 'currentState': array([15.96576  , 63.858524 ,  1.7385278], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28859681313444985
{'currentTarget': array([  5.49240489, 101.95586656]), 'previousTarget': array([  5.49240489, 101.95586656]), 'currentState': array([ 0.6451723, 82.55215  ,  5.350119 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4036
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.30633547, 106.41919407]), 'previousTarget': array([  8.70802283, 104.92693298]), 'currentState': array([ 5.8313046, 86.57293  ,  2.3027613], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28852532519460977
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.102604 , 106.03408  ,   4.5669074], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.306399210402564}
episode index:4037
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.7107184, 122.91363  ,   4.977677 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.931398781080782}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.28854182729701455
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.3197155, 119.22342  ,   2.3291154], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8398188829574124}
episode index:4038
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.90308066, 104.67511042]), 'previousTarget': array([  7.82842712, 104.79898987]), 'currentState': array([ 5.1917257, 84.85975  ,  4.408341 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2884703883697313
{'currentTarget': array([  8.01351029, 114.55363214]), 'previousTarget': array([  8.05238297, 114.70285556]), 'currentState': array([ 1.160394, 95.76441 ,  4.258236], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4039
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.34656578, 104.4051353 ]), 'previousTarget': array([ 12.24863257, 103.80984546]), 'currentState': array([13.067099 , 84.47928  ,  3.1532156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28839898480825366
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.7607374, 103.939316 ,   2.2563696], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.38408981822647}
episode index:4040
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.99007438, 69.9007438 ]), 'previousTarget': array([ 4.99007438, 69.9007438 ]), 'currentState': array([ 3.     , 50.     ,  5.20343], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28832761658632633
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([  9.92957044, 119.58173625]), 'currentState': array([  7.0074897, 102.0043   ,   1.5355678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.24281308616967}
episode index:4041
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.95308777, 79.95773754]), 'previousTarget': array([14.00992562, 79.9007438 ]), 'currentState': array([15.917994 , 60.054493 ,  5.5747466], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28825628367772016
{'currentTarget': array([ 5.21394759, 89.28593838]), 'previousTarget': array([ 5.21394759, 89.28593838]), 'currentState': array([ 2.1345875 , 69.52442   ,  0.96554315], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4042
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1044126, 115.96412  ,   4.198491 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.768306494389778}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.28831762722002274
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.241509 , 120.40766  ,   1.2991064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8051251774349555}
episode index:4043
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8981667, 113.755684 ,   1.2088888], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.588564864521541}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2884862675199189
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.405749 , 119.538055 ,   1.2147982], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7526796560122154}
episode index:4044
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.53440521, 84.83428461]), 'previousTarget': array([10.64917679, 82.99692284]), 'currentState': array([10.838305 , 64.83659  ,  1.7470472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.2884597262802665
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.849454 , 119.74269  ,   1.9122944], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8672674488655003}
episode index:4045
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.24646816, 117.24027775]), 'previousTarget': array([ 10.26740767, 116.92481176]), 'currentState': array([12.025568 , 97.319565 ,  4.7364893], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.2885095113882062
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.056807 , 120.22747  ,   1.5419012], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9564619879696248}
episode index:4046
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.919553, 116.97458 ,   4.812584], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.5829954023994683}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2886639486840044
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.690361, 118.043945,   5.647177], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.585240864677149}
episode index:4047
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.87552631, 102.13477611]), 'previousTarget': array([ 10., 102.]), 'currentState': array([ 9.736182 , 82.13526  ,  3.5025408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2885926384200015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.725251, 106.85454 ,   5.868051], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.258192329234438}
episode index:4048
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.751301 , 105.64274  ,   2.2603245], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.463679229068466}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.2885931066481788
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.091946  , 118.82449   ,   0.52604103], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2410906177140997}
episode index:4049
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.18928508, 117.91786413]), 'previousTarget': array([ 10.18928508, 117.91786413]), 'currentState': array([12.       , 98.       ,  6.1487274], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.28866108494047615
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.860974 , 120.91787  ,   0.5453658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.462827420421893}
episode index:4050
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.54034323, 65.9926994 ]), 'previousTarget': array([ 8.54034323, 65.9926994 ]), 'currentState': array([ 8.       , 46.       ,  2.7439704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28858982819277423
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.935737, 106.02628 ,   6.193983], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.97386450452588}
episode index:4051
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.01956648, 91.66824167]), 'previousTarget': array([ 8.19784581, 89.96409691]), 'currentState': array([ 8.327871 , 71.68021  ,  1.1269228], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.2885590330648165
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.628783, 118.14358 ,   2.378703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8931735553810123}
episode index:4052
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.98336715, 119.1861172 ]), 'previousTarget': array([ 10.09184678, 117.9793708 ]), 'currentState': array([ 9.574724 , 99.19029  ,  2.4285698], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.2885917927956554
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.69616  , 118.31379  ,   1.2843095], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.391707782259706}
episode index:4053
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.52143406, 69.79050588]), 'previousTarget': array([ 3.55043485, 69.83671551]), 'currentState': array([ 0.9620378, 49.954945 ,  3.699873 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28852060587094014
{'currentTarget': array([ 10.56165909, 117.10026782]), 'previousTarget': array([ 10.59831766, 116.99805445]), 'currentState': array([14.364842  , 97.4652    ,  0.42697677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4054
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.43403945, 115.90300603]), 'previousTarget': array([ 11.4295875 , 115.88993593]), 'currentState': array([18.041424 , 97.02597  ,  1.5778625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.28851045003370784
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.155506, 118.61258 ,   1.746996], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3961082442602797}
episode index:4055
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.78386043, 72.60362321]), 'previousTarget': array([ 6.46662886, 71.94615251]), 'currentState': array([ 6.8497295, 52.62545  ,  0.3587711], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28843931826594804
{'currentTarget': array([ 5.90490827, 89.15832656]), 'previousTarget': array([ 5.9797223 , 89.36375715]), 'currentState': array([ 3.2724545, 69.33233  ,  5.2758965], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4056
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.93243985, 105.86245779]), 'previousTarget': array([ 10., 106.]), 'currentState': array([ 9.836865 , 85.862686 ,  5.0706334], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2883682215643789
{'currentTarget': array([ 10.55101208, 114.26452385]), 'previousTarget': array([ 10.56281318, 114.41509734]), 'currentState': array([12.463623 , 94.356186 ,  3.6722379], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4057
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.987365 , 123.955635 ,   1.4115075], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.08319469587823}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.2884279892630952
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.142368, 119.697655,   6.002911], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.882075486485756}
episode index:4058
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.18665927, 100.51288414]), 'previousTarget': array([ 9.48765985, 98.99405381]), 'currentState': array([10.378222  , 80.5138    ,  0.96915305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28835693038424254
{'currentTarget': array([  9.57156358, 114.80507034]), 'previousTarget': array([  9.49596248, 114.66848865]), 'currentState': array([ 7.927704 , 94.87274  ,  1.7654839], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4059
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.095161 , 112.10489  ,   4.9610853], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.946792551196764}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.288504227906738
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.879844  , 118.147064  ,   0.92630243], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1652069544276604}
episode index:4060
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.6905562, 113.846971 ]), 'previousTarget': array([  9.71383061, 111.98725709]), 'currentState': array([ 8.686    , 93.872215 ,  1.9965352], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.28850189790444264
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.780281 , 119.79606  ,   1.4333447], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.791924314649246}
episode index:4061
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.74929231, 107.57108999]), 'previousTarget': array([ 13.73765188, 106.29527642]), 'currentState': array([17.068903 , 88.04314  ,  2.6475182], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.28850994792331525
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.514736, 118.79937 ,   2.419955], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2949879642842734}
episode index:4062
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.728263 , 111.89394  ,   3.4256976], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.74141927706413}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.28861558921907576
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.110453 , 119.23304  ,   0.9802208], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.039268752020961}
episode index:4063
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.35002536, 87.18209272]), 'previousTarget': array([13.79136948, 85.87767469]), 'currentState': array([16.978046 , 67.35551  ,  1.0140662], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2885445716036183
{'currentTarget': array([ 4.64749781, 90.72148962]), 'previousTarget': array([ 4.64749781, 90.72148962]), 'currentState': array([ 1.0508386 , 71.04755   ,  0.15195042], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4064
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.69115212, 71.93684386]), 'previousTarget': array([ 3.62417438, 71.82709532]), 'currentState': array([ 1.0882474, 52.106945 ,  2.575517 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.2885356702731105
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.479255, 119.77217 ,   1.219423], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.537716567721571}
episode index:4065
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.54761102, 73.74912346]), 'previousTarget': array([ 9.30299553, 73.99770471]), 'currentState': array([ 9.351996, 53.75008 ,  5.010722], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28846470724549783
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.648274, 118.43067 ,   1.993968], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9714855652000676}
episode index:4066
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.91424813, 87.90818058]), 'previousTarget': array([ 6.91424813, 87.90818058]), 'currentState': array([ 5.       , 68.       ,  2.0951064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2883937791148744
{'currentTarget': array([  8.32112023, 115.59631192]), 'previousTarget': array([  8.28012858, 115.49134561]), 'currentState': array([ 1.1964544, 96.90837  ,  0.1283452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4067
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.90941826, 107.64869575]), 'previousTarget': array([ 12.72606242, 107.53800035]), 'currentState': array([14.964971, 87.883484,  3.119115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288322885855505
{'currentTarget': array([  8.69029938, 116.28276203]), 'previousTarget': array([  8.69029938, 116.28276203]), 'currentState': array([ 2.044122 , 97.41935  ,  1.4962672], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4068
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.2183003, 116.664505 ,   5.13338  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.042497313983918}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.28844903667970834
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.123576 , 118.00257  ,   0.7054394], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.740563554845053}
episode index:4069
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.49529826, 106.7157715 ]), 'previousTarget': array([  7.57770876, 106.6773982 ]), 'currentState': array([ 3.7896504, 87.062065 ,  5.5657463], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28837816468052413
{'currentTarget': array([  8.02122827, 114.09721333]), 'previousTarget': array([  8.03299492, 114.08903915]), 'currentState': array([ 1.6643687, 95.134346 ,  5.080751 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4070
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.73729558, 115.28362252]), 'previousTarget': array([ 11.22305213, 113.64012894]), 'currentState': array([13.826312 , 95.52361  ,  2.4184303], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.28840978902718406
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.026029 , 119.975204 ,   2.4306037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.974127092081376}
episode index:4071
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.89712116, 80.99077007]), 'previousTarget': array([ 6.02346204, 80.89737675]), 'currentState': array([ 3.8051178, 61.100483 ,  4.873878 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28833896147585125
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.178726 , 114.9787   ,   3.9767518], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.759605222139945}
episode index:4072
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.34983467, 84.99049429]), 'previousTarget': array([ 9.36357627, 84.99669503]), 'currentState': array([ 8.978477, 64.99394 ,  4.216295], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882681687035763
{'currentTarget': array([11.51149422, 98.34979826]), 'previousTarget': array([11.62216747, 98.54207   ]), 'currentState': array([12.90439  , 78.39836  ,  5.5061655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4073
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0957932, 108.22644  ,   2.4825115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.648618092715099}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.28834442983213854
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.093584 , 119.42677  ,   2.3860116], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9907312338773615}
episode index:4074
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.780601, 122.96949 ,   3.231103], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.379771789399196}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2884106682882651
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.27603  , 120.4863   ,   4.6476893], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8721344633626248}
episode index:4075
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.058504 , 113.06841  ,   2.2765267], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.512654290056751}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.2884402093839801
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.225133, 118.11925 ,   3.801537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5859975401937896}
episode index:4076
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.76926883, 111.79902452]), 'previousTarget': array([ 10.66961979, 109.95570316]), 'currentState': array([12.637112 , 91.88644  ,  1.4075212], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.288411710419253
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.279979  , 119.40046   ,   0.75406706], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9369523258209319}
episode index:4077
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.41155324, 72.89164494]), 'previousTarget': array([ 4.37121622, 72.85893586]), 'currentState': array([ 2.0554812, 53.030907 ,  2.9423556], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 266
reward sum = 0.06901790349970881
running average episode reward sum: 0.28835791105512365
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.279324 , 119.16071  ,   1.8143992], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9144528433313956}
episode index:4078
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.881464 , 115.52771  ,   2.3034167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.851936210812061}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.2884722433366016
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.587519 , 118.78069  ,   1.9846797], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2871869551520965}
episode index:4079
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.15403764, 102.23271804]), 'previousTarget': array([  6.22665585, 102.54828331]), 'currentState': array([ 1.92277  , 82.68543  ,  2.3049011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28840153935539165
{'currentTarget': array([  8.54425443, 116.6484448 ]), 'previousTarget': array([  8.54425443, 116.6484448 ]), 'currentState': array([5.7641816e-01, 9.8304138e+01, 3.6626160e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4080
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.66069973, 88.47068974]), 'previousTarget': array([15.52429332, 88.69567118]), 'currentState': array([19.194944, 68.78544 ,  5.600178], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 240
reward sum = 0.08962861870232462
running average episode reward sum: 0.28835283244025983
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.489406 , 118.17248  ,   0.7635223], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8919184836097815}
episode index:4081
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.93519094, 73.02053239]), 'previousTarget': array([ 7.89462602, 72.97998109]), 'currentState': array([ 7.0570126, 53.03982  ,  3.6590753], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882821923539197
{'currentTarget': array([ 4.24419902, 88.52732498]), 'previousTarget': array([ 4.38176395, 88.57973872]), 'currentState': array([ 0.6462234, 68.85362  ,  3.52912  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4082
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.91927483, 89.49224929]), 'previousTarget': array([14.84018998, 89.74881264]), 'currentState': array([18.103085 , 69.74729  ,  1.4787682], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882115868696302
{'currentTarget': array([ 5.78974815, 89.40526058]), 'previousTarget': array([ 5.7271477 , 89.51100978]), 'currentState': array([ 3.0631726, 69.59199  ,  6.0976367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4083
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.93434486, 71.78743496]), 'previousTarget': array([ 5.75787621, 71.922597  ]), 'currentState': array([ 4.2537556, 51.85817  ,  4.594817 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2881410159619736
{'currentTarget': array([ 5.52112732, 86.10401449]), 'previousTarget': array([ 5.429127  , 86.23338562]), 'currentState': array([ 2.9011846, 66.27636  ,  2.8978105], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4084
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.53333744, 100.80254321]), 'previousTarget': array([ 10.48734798, 100.99342862]), 'currentState': array([11.088757, 80.81026 ,  2.480971], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288070479605557
{'currentTarget': array([  8.75494332, 115.2662672 ]), 'previousTarget': array([  8.72601785, 115.05649288]), 'currentState': array([ 3.667609, 95.92411 ,  4.273259], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4085
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.10381815, 118.90990945]), 'previousTarget': array([ 10.10381815, 118.90990945]), 'currentState': array([12.      , 99.      ,  0.226515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.28807702358342663
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.334912, 120.81929 ,   4.321014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.855735276502135}
episode index:4086
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.28049073, 86.01378254]), 'previousTarget': array([ 7.47743371, 85.94535509]), 'currentState': array([ 4.1046486, 66.13249  ,  2.538597 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2880065374019773
{'currentTarget': array([  8.44438994, 116.32194003]), 'previousTarget': array([  8.51991289, 116.38208891]), 'currentState': array([ 0.6536811, 97.90171  ,  0.3861816], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4087
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.88253377, 106.52455201]), 'previousTarget': array([ 11.59337265, 106.85467564]), 'currentState': array([12.189573, 86.56731 ,  3.29223 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.288027441937949
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.626433, 118.41887 ,   1.015598], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0944355947783757}
episode index:4088
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.10564293, 105.8290259 ]), 'previousTarget': array([  7.03305841, 105.58914087]), 'currentState': array([ 3.103361, 86.233574,  4.304341], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.2880963032146766
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.073962 , 118.45803  ,   1.1825159], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5437421186132112}
episode index:4089
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.919215 , 124.06473  ,   3.1195316], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.164528686875776}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.28820671985775087
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.935425, 120.36096 ,   5.23358 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0026530150696757}
episode index:4090
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.39090809, 92.09483917]), 'previousTarget': array([ 9.41657627, 91.99566113]), 'currentState': array([ 8.954468, 72.0996  ,  4.009171], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28813627089176264
{'currentTarget': array([  8.95138618, 113.69779987]), 'previousTarget': array([  8.95138618, 113.69779987]), 'currentState': array([ 5.6687455, 93.96903  ,  2.1221797], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4091
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.812921 , 104.770805 ,   1.2199523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.486797330376092}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.2881719734958438
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.315295 , 120.55094  ,   4.2695875], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.772502947269413}
episode index:4092
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.9305947 , 115.05862   ,   0.16126508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.625181799173724}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.2883095952396989
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.76432  , 118.254974 ,   1.9004917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9050722031960083}
episode index:4093
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.7767515, 121.198456 ,   4.3439794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.899758845362378}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.28843302090858114
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.3392315, 118.14299  ,   3.5331159], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.289547271781535}
episode index:4094
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.32950323, 70.9097401 ]), 'previousTarget': array([12.13125551, 70.98112317]), 'currentState': array([13.277506 , 50.93222  ,  5.6685286], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2883625854944399
{'currentTarget': array([11.28848737, 97.37911038]), 'previousTarget': array([11.31064299, 97.50138295]), 'currentState': array([12.425845, 77.411476,  4.854739], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4095
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.026383 , 110.966995 ,   4.8549185], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.246098501072284}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.2883279910013094
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.210138 , 119.918655 ,   1.5904886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.22533321681515528}
episode index:4096
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.63916133, 111.04799655]), 'previousTarget': array([  9.68924552, 110.98811999]), 'currentState': array([ 8.8336525, 91.064224 ,  5.430229 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28825761560687413
{'currentTarget': array([  7.86269593, 112.12001981]), 'previousTarget': array([  7.83384379, 112.10342564]), 'currentState': array([ 2.6272144, 92.81744  ,  5.2312603], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4097
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1780984, 111.80562  ,   5.7515697], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.040508245248073}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.28835893090199194
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.49972  , 118.04862  ,   4.8495502], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0143473594765684}
episode index:4098
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.20580067, 73.01845915]), 'previousTarget': array([ 7.1919076 , 72.96445232]), 'currentState': array([ 6.0184107, 53.053738 ,  3.3360484], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.28831613468927453
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.180672 , 119.43033  ,   1.0670457], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3109200627982909}
episode index:4099
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.1544914, 124.494415 ,   2.866177 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.0416687367534}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.288350664659722
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.31946  , 121.34804  ,   5.4950733], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.385373715356383}
episode index:4100
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.04869701, 118.97736275]), 'currentState': array([ 10.967536 , 100.984024 ,   1.4580374], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.040574236901403}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2884047088151938
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.237058, 118.33446 ,   1.942913], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6823282994986808}
episode index:4101
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.236427 , 119.99991  ,   4.9814367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.236427307637738}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.2884651338557574
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.444879, 121.99117 ,   3.494665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0402661667207243}
episode index:4102
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.30158275, 93.95760212]), 'previousTarget': array([ 8.30158275, 93.95760212]), 'currentState': array([ 7.       , 74.       ,  4.0046206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2883948279493826
{'currentTarget': array([  8.66743879, 114.5330089 ]), 'previousTarget': array([  8.69484944, 114.69959111]), 'currentState': array([ 3.931171, 95.101906,  4.391022], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4103
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.63334174, 86.83163922]), 'previousTarget': array([ 5.61876739, 86.8278102 ]), 'currentState': array([ 3.022841 , 67.00274  ,  3.7567687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2883245563051454
{'currentTarget': array([ 5.33685715, 90.18716786]), 'previousTarget': array([ 5.4803679 , 90.24286364]), 'currentState': array([ 2.2461572, 70.42742  ,  2.3255105], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4104
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.80142919, 118.2568178 ]), 'previousTarget': array([ 10.79270645, 118.2384301 ]), 'currentState': array([ 19.155802 , 100.08529  ,   3.2950966], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.28833178273815735
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.346458 , 120.90462  ,   5.4947457], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.968692894573652}
episode index:4105
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.17703386, 67.94776095]), 'previousTarget': array([ 9.27775099, 67.99807127]), 'currentState': array([ 8.860866, 47.95026 ,  4.157208], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.28830888863590326
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.488586 , 118.84528  ,   1.3277907], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2538288845212686}
episode index:4106
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.67539174, 85.83651574]), 'previousTarget': array([ 8.74023321, 85.98629668]), 'currentState': array([ 7.9005213, 65.85153  ,  3.910856 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2882386892473871
{'currentTarget': array([  8.84719591, 110.77707706]), 'previousTarget': array([  8.89156414, 112.35661468]), 'currentState': array([ 6.366631, 90.9315  ,  4.832206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4107
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.56391802, 68.51716808]), 'previousTarget': array([ 9.28166221, 68.99801656]), 'currentState': array([10.782975 , 48.518368 ,  6.1454782], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2881685240357884
{'currentTarget': array([ 10.83530055, 116.65206779]), 'previousTarget': array([ 10.83530055, 116.65206779]), 'currentState': array([15.676834 , 97.246925 ,  2.8995428], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4108
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.97996224, 82.83038467]), 'previousTarget': array([11.96689829, 81.9732997 ]), 'currentState': array([14.578274 , 62.894352 ,  0.1370995], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28809839297615447
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.6221602, 115.58871  ,   3.8513923], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.75476284332642}
episode index:4109
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.90294068, 116.69803793]), 'previousTarget': array([  9.79936077, 114.98401917]), 'currentState': array([ 9.315306 , 96.70667  ,  1.8042727], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.28819773964892176
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.87553   , 118.241776  ,   0.44742706], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7626247667003334}
episode index:4110
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.76365693, 110.71360751]), 'previousTarget': array([  8.9264839 , 108.90700027]), 'currentState': array([ 6.1242476, 90.888535 ,  2.2631993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2881276356013302
{'currentTarget': array([  8.24439233, 115.60432963]), 'previousTarget': array([  8.27126199, 115.88156386]), 'currentState': array([ 0.8262708, 97.03092  ,  5.904754 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4111
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.33038021, 94.95570316]), 'previousTarget': array([ 8.33038021, 94.95570316]), 'currentState': array([ 7.      , 75.      ,  1.998039], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.288057565651038
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.488571 , 112.04527  ,   5.3971167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.097042763364312}
episode index:4112
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.42303594, 76.35185944]), 'previousTarget': array([10.6923441 , 74.99763356]), 'currentState': array([12.0747385, 56.36248  ,  0.8331826], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28798752977317493
{'currentTarget': array([ 4.84005718, 90.99605557]), 'previousTarget': array([ 4.78918944, 90.96044022]), 'currentState': array([ 1.3369647, 71.30524  ,  2.1064935], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4113
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.53620297, 108.84483856]), 'previousTarget': array([ 11.44057325, 108.83555733]), 'currentState': array([14.264698 , 89.03183  ,  3.5749285], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.2880029944491071
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.1874877e+01, 1.1969170e+02, 8.8883579e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9000553213911173}
episode index:4114
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.169436, 122.8699  ,   4.233059], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.339908834807918}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.2880126468823972
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.756454 , 118.78152  ,   0.4621582], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2425840725483959}
episode index:4115
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.42466509, 105.83260163]), 'previousTarget': array([  9.14099581, 104.96742669]), 'currentState': array([ 6.2144   , 85.95511  ,  2.1631956], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28794267296430137
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.8208117, 109.47904  ,   2.5668254], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.320608223911568}
episode index:4116
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.42132745, 115.7719947 ]), 'previousTarget': array([ 11.4295875 , 115.88993593]), 'currentState': array([17.794256 , 96.81452  ,  4.9381485], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.28793590657819823
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.259829 , 119.37917  ,   1.8488481], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8475990994307667}
episode index:4117
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.66937178, 86.92099527]), 'previousTarget': array([ 5.61876739, 86.8278102 ]), 'currentState': array([ 3.0731714, 67.09022  ,  3.8835318], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.28788011305352573
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([9.9641132e+00, 1.1915348e+02, 9.1217689e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8472798081631537}
episode index:4118
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.44940437, 113.97467552]), 'previousTarget': array([ 10.46607002, 113.94108971]), 'currentState': array([11.936991 , 94.030075 ,  3.5850475], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2878102222758968
{'currentTarget': array([  8.42990022, 112.86787485]), 'previousTarget': array([  8.35635959, 112.5848148 ]), 'currentState': array([ 4.129968, 93.33558 ,  5.98265 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4119
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.77304906, 105.20687742]), 'previousTarget': array([ 13.88214879, 105.33410456]), 'currentState': array([18.715895 , 85.82729  ,  5.2869406], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877403654258298
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.83561  , 103.53992  ,   4.0502505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.601774515796027}
episode index:4120
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.090603 , 103.0532   ,   0.5260489], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.075264521552864}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.2876896278941015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.004316 , 120.14921  ,   0.2701499], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0068013799163305}
episode index:4121
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.0020385, 119.811806 ,   1.2130458], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.000913221692697}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.28785287543949356
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.290707 , 121.01972  ,   0.5753532], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9903559319082555}
episode index:4122
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.65050506, 78.05672812]), 'previousTarget': array([ 6.6076838, 77.9352791]), 'currentState': array([ 5.0584188, 58.120197 ,  5.7281404], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877830590738764
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.2991266, 112.834    ,   3.6422293], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.570284227243887}
episode index:4123
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.62460402, 80.85673444]), 'previousTarget': array([ 7.98241918, 78.97585674]), 'currentState': array([ 6.4131393, 60.89346  ,  2.029791 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.2877782733459043
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.206127 , 121.35524  ,   1.0432614], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5706397345148344}
episode index:4124
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.14512956, 89.25288237]), 'previousTarget': array([10., 89.]), 'currentState': array([10.239531, 69.253105,  4.959037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28770850891600225
{'currentTarget': array([ 4.85886807, 90.43062344]), 'previousTarget': array([ 4.85886807, 90.43062344]), 'currentState': array([ 1.4329296, 70.726234 ,  3.3021045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4125
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.78795037, 105.86945265]), 'previousTarget': array([ 10.82555956, 105.96548746]), 'currentState': array([11.901464  , 85.900475  ,  0.12618344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876387783030803
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1090413, 113.77543  ,   4.155447 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.853314377993375}
episode index:4126
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.167557, 115.53797 ,   3.082022], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.442959689100363}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.2877672658628802
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.842408 , 118.14225  ,   5.8266587], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8644221671553232}
episode index:4127
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.582079 , 116.9894   ,   2.0042844], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.157917420051715}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.28782234576878163
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.778131, 119.09833 ,   2.152651], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9285683129643818}
episode index:4128
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.29668385, 93.87725028]), 'previousTarget': array([ 8.86874449, 93.98112317]), 'currentState': array([10.523815 , 73.87854  ,  6.0331197], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.28781314613465464
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.360477 , 119.632935 ,   2.3356857], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7373778711185288}
episode index:4129
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.46420392, 82.90500776]), 'previousTarget': array([ 8.7013228 , 82.98769988]), 'currentState': array([ 7.6368785, 62.922127 ,  3.923575 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28774345772154697
{'currentTarget': array([ 4.1912211, 88.4897568]), 'previousTarget': array([ 4.36857399, 90.23007473]), 'currentState': array([ 0.5654005, 68.82117  ,  4.7150187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4130
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.80632162, 74.31243092]), 'previousTarget': array([11.38490648, 74.99053926]), 'currentState': array([ 9.721539 , 54.31261  ,  3.0803719], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876738030476856
{'currentTarget': array([ 6.85900276, 88.09761755]), 'previousTarget': array([ 6.85900276, 88.09761755]), 'currentState': array([ 4.899348 , 68.193855 ,  1.7571396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4131
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.999205  , 114.062935  ,   0.19930333], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.17810480567636}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.2877180704335854
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.711437 , 118.14377  ,   3.4833586], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8785272315961894}
episode index:4132
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.0388273 , 109.94895508]), 'previousTarget': array([ 10.71235444, 108.95850618]), 'currentState': array([10.116087, 89.949104,  2.684589], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.2877600505683506
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.723601 , 119.92818  ,   1.9012636], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7271571093227609}
episode index:4133
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.80538692, 89.92538304]), 'previousTarget': array([ 5.77295689, 89.80683493]), 'currentState': array([ 3.0426583 , 70.11712   ,  0.47015902], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876904424283969
{'currentTarget': array([ 5.69495933, 90.02265226]), 'previousTarget': array([ 5.82745517, 90.04622346]), 'currentState': array([ 2.8519309, 70.22575  ,  5.8472953], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4134
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.74910696, 67.95003548]), 'previousTarget': array([10.72224901, 67.99807127]), 'currentState': array([11.036919 , 47.952106 ,  1.6248451], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876208679562256
{'currentTarget': array([ 4.86462665, 89.3743628 ]), 'previousTarget': array([ 4.81114513, 89.20759026]), 'currentState': array([ 1.5571594, 69.64974  ,  0.6651823], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4135
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.49481412, 66.17813481]), 'previousTarget': array([ 8.54034323, 65.9926994 ]), 'currentState': array([ 7.9357114, 46.18595  ,  5.7139025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2875513271274161
{'currentTarget': array([13.416013  , 90.96288263]), 'previousTarget': array([14.39587387, 92.14289702]), 'currentState': array([15.752758 , 71.09986  ,  3.5861228], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4136
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.1957653 , 106.71327182]), 'previousTarget': array([  9.20990121, 106.96336993]), 'currentState': array([ 7.9873934, 86.74981  ,  2.4050183], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.28750326839050233
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.33393  , 118.65613  ,   0.4788829], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3847387467174286}
episode index:4137
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.41667154, 96.85257575]), 'previousTarget': array([ 8.39196526, 96.95150202]), 'currentState': array([ 7.051822 , 76.8992   ,  6.0644274], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28743378959195465
{'currentTarget': array([  8.38269982, 116.14502701]), 'previousTarget': array([  8.39955931, 116.14271932]), 'currentState': array([ 0.64532405, 97.70233   ,  0.86698234], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4138
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.9594555, 114.10831  ,   1.4582509], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.098544607331886}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2875850540176352
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.980265 , 119.445    ,   2.1141775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0565683740224605}
episode index:4139
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.28058882, 86.8602287 ]), 'previousTarget': array([ 6.24978031, 86.87305937]), 'currentState': array([ 4.049913 , 66.985016 ,  4.8843246], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.28754996281806133
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.040628, 118.53459 ,   2.175742], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7973116300723517}
episode index:4140
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.492332 , 117.79576  ,   1.2977273], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.3387225074060116}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.28771959576594397
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.250433 , 119.85297  ,   1.7104723], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7557345201883994}
episode index:4141
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.7949545, 118.25348  ,   5.8856616], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.388868017653877}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.28784562383751694
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.306288 , 118.09284  ,   0.9851738], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5506690065539943}
episode index:4142
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.04179735, 119.72566587]), 'previousTarget': array([ 10.03319094, 119.77872706]), 'currentState': array([13.054219, 99.953835,  5.745641], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.28781689502572433
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.825377 , 118.92686  ,   2.5540817], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.591029230204234}
episode index:4143
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.8096364, 75.5199192]), 'previousTarget': array([ 7.93647173, 75.97806349]), 'currentState': array([ 6.8259544, 55.544125 ,  1.2481165], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877474411417896
{'currentTarget': array([ 8.62588339, 91.49291875]), 'previousTarget': array([ 8.63297534, 91.59223216]), 'currentState': array([ 7.6629486, 71.51611  ,  3.8422368], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4144
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.71579224, 86.70214249]), 'previousTarget': array([14.47491441, 84.83995823]), 'currentState': array([17.520296 , 66.89975  ,  1.3391196], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28767802076998217
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.9986978, 106.93451  ,   1.7174026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.217876474258931}
episode index:4145
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.84827908, 86.96500628]), 'previousTarget': array([ 8.7541802 , 86.98577525]), 'currentState': array([ 8.151429 , 66.97715  ,  4.5092635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.2877165748445103
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.217923 , 118.00235  ,   4.6039934], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1452855951088923}
episode index:4146
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.8951516, 119.26387  ,   3.439333 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.14288160313875}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.2877477791621107
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.966486 , 120.6742   ,   5.5495567], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6750344285913366}
episode index:4147
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.12542966, 107.23008264]), 'previousTarget': array([ 12.00389241, 106.77431008]), 'currentState': array([12.88125  , 87.307304 ,  2.9562545], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28767840891641105
{'currentTarget': array([  7.16690718, 111.97604619]), 'previousTarget': array([  7.25665843, 112.05860316]), 'currentState': array([ 0.5081868, 93.11706  ,  3.6718266], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4148
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.2086865 , 85.72359134]), 'previousTarget': array([ 6.20863052, 85.87767469]), 'currentState': array([ 4.0098963, 65.844826 ,  5.8374486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876090721102128
{'currentTarget': array([ 4.01462599, 85.80221183]), 'previousTarget': array([ 3.99944014, 85.55249617]), 'currentState': array([ 0.56659395, 66.10168   ,  5.621941  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4149
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.39904074, 85.67076675]), 'previousTarget': array([14.42891943, 85.83405013]), 'currentState': array([16.941109 , 65.83298  ,  4.3937044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.28757304120442423
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.321964 , 119.16815  ,   1.1218339], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5619093605683705}
episode index:4150
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.57040563, 104.53522263]), 'previousTarget': array([ 11.38368262, 102.93458096]), 'currentState': array([13.59096  , 84.63755  ,  1.2320412], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.287573743554004
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([9.83670998e+00, 1.21303604e+02, 6.42520562e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.313791212155468}
episode index:4151
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.81578112, 107.02764183]), 'previousTarget': array([ 13.28799949, 106.43700211]), 'currentState': array([1.9459627e+01, 8.7840485e+01, 8.7474942e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 296
reward sum = 0.0510525689892109
running average episode reward sum: 0.2875167779531936
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.902815 , 119.28939  ,   2.5193772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1489301333196686}
episode index:4152
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.20508467, 78.06065489]), 'previousTarget': array([10., 78.]), 'currentState': array([10.302884, 58.060894,  5.59081 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 211
reward sum = 0.11995712819347787
running average episode reward sum: 0.28747643130022954
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.561591 , 118.068146 ,   5.8048034], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0118263974884996}
episode index:4153
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.194834 , 115.97524  ,   5.7233152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.138641331655131}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.28757487539911
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.831213 , 119.60061  ,   1.3720922], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.43359239194916815}
episode index:4154
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.73882989, 70.90388242]), 'previousTarget': array([ 7.86874449, 70.98112317]), 'currentState': array([ 6.8186855, 50.92506  ,  4.4188423], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2875056636360777
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.64415  , 117.39415  ,   4.7221413], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.937479393286282}
episode index:4155
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.32745489, 103.60642672]), 'previousTarget': array([  7.28797975, 103.72787848]), 'currentState': array([ 4.1094575, 83.86701  ,  5.8849883], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.2875735403297353
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.03757  , 118.95434  ,   1.1737137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2236322470560363}
episode index:4156
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.26978697, 86.82503378]), 'previousTarget': array([11.25976679, 85.98629668]), 'currentState': array([13.634969 , 66.87168  ,  0.4711388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 206
reward sum = 0.12613920430197118
running average episode reward sum: 0.28753470599342845
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.260067 , 120.08755  ,   1.2953194], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.27440730198261165}
episode index:4157
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.021271, 123.90997 ,   1.87864 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.401525354199013}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.28752099774723433
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.541987, 120.45758 ,   4.471635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.608448064285255}
episode index:4158
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.1531696 , 109.511665  ,   0.47607088], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.09877515938335}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2876759735467438
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.273585  , 120.477066  ,   0.35836548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8690628829818952}
episode index:4159
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.40300302, 75.76733658]), 'previousTarget': array([15.48054926, 76.84067458]), 'currentState': array([16.38405  , 55.865692 ,  3.8984096], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.28766336312743934
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.190701 , 119.80825  ,   1.5480077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8317051936626966}
episode index:4160
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.41616976, 99.93433199]), 'previousTarget': array([ 8.49579896, 99.9439862 ]), 'currentState': array([ 6.8424177, 79.996346 ,  4.7965674], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2875942298990982
{'currentTarget': array([  8.0700892 , 114.11742165]), 'previousTarget': array([  8.10824191, 114.30530221]), 'currentState': array([ 1.8355856, 95.113976 ,  3.305686 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4161
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.12103234, 83.58749433]), 'previousTarget': array([15.17157288, 83.79898987]), 'currentState': array([17.906408, 63.782402,  5.78923 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28752512989191437
{'currentTarget': array([13.99917338, 92.20732537]), 'previousTarget': array([13.95348598, 92.28553902]), 'currentState': array([16.847696, 72.41122 ,  3.960103], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4162
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.51293259e+01, 1.24149254e+02, 2.90953182e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.597445817800996}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.2875657459423232
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.820534, 118.5807  ,   2.934053], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6394137750246527}
episode index:4163
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.202711 , 113.033005 ,   1.3274506], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.9346345607909665}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28771828651592685
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.457545 , 120.57908  ,   1.0571187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6475735713132604}
episode index:4164
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.29013253, 82.14701047]), 'previousTarget': array([11.96689829, 81.9732997 ]), 'currentState': array([13.497938 , 62.183514 ,  3.5368807], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876492064951547
{'currentTarget': array([  8.5717894 , 116.35732035]), 'previousTarget': array([  8.55558106, 116.39604712]), 'currentState': array([ 1.271329 , 97.73735  ,  0.9938554], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4165
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.8786335, 119.81709  ,   4.897544 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.8844385643095998}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.28778047499130327
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.565422 , 119.10955  ,   3.1846187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0547997243324794}
episode index:4166
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.9099898 , 75.12633631]), 'previousTarget': array([10.70152578, 72.9977727 ]), 'currentState': array([11.315485 , 55.130447 ,  1.1932263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877114132022485
{'currentTarget': array([ 4.3789962 , 90.31879051]), 'previousTarget': array([ 4.27494816, 90.1370507 ]), 'currentState': array([ 0.657558, 70.66807 ,  5.765299], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4167
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0926468, 117.26884  ,   3.722434 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.427703493258217}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.28781458477599997
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([8.8574066e+00, 1.2181976e+02, 6.9426596e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1487339726084755}
episode index:4168
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.55797712, 106.84457349]), 'previousTarget': array([  7.57770876, 106.6773982 ]), 'currentState': array([ 3.9077628, 87.1805   ,  6.158702 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2878703597670948
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.4413185 , 118.87076   ,   0.62973654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.212414696233594}
episode index:4169
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.35303 , 107.20737 ,   4.755708], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.983552066269183}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.2879054583203381
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.2044525, 119.40309  ,   1.9931929], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3442491213274497}
episode index:4170
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.14496098, 107.89065863]), 'previousTarget': array([  9.24756572, 107.96105157]), 'currentState': array([ 7.736271 , 87.94033  ,  2.6208212], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.28795154702766274
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.767156 , 118.07869  ,   2.2775822], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2828357248172964}
episode index:4171
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.88171698, 113.11558017]), 'previousTarget': array([  7.88171698, 113.11558017]), 'currentState': array([ 2.      , 94.      ,  3.040773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.28790580903213603
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.136861, 120.31207 ,   6.195019], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9178227499661409}
episode index:4172
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.65061043, 69.261818  ]), 'previousTarget': array([ 9.28166221, 68.99801656]), 'currentState': array([10.907047  , 49.263462  ,  0.40506437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2878368165066071
{'currentTarget': array([ 4.34057207, 90.46123868]), 'previousTarget': array([ 4.34057207, 90.46123868]), 'currentState': array([ 0.5771578, 70.81851  ,  1.2845166], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4173
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.2565613, 112.503365 ,   0.9858376], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.87128816872305}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.28794154662626475
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.296907 , 118.06613  ,   5.6800756], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3284793082774526}
episode index:4174
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.102802 , 108.56603  ,   4.0639052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.434429726262687}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.2879040309215228
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.819659, 118.24324 ,   2.716132], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.938567951766826}
episode index:4175
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.40938103, 81.035906  ]), 'previousTarget': array([ 6.63386479, 78.93315042]), 'currentState': array([ 4.574117 , 61.12029  ,  2.3664184], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.28787759856899336
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.612645 , 118.68222  ,   2.2197165], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.453229850228646}
episode index:4176
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.01649034, 73.908144  ]), 'previousTarget': array([12.1185045, 71.9805647]), 'currentState': array([11.457455 , 53.913006 ,  2.2600203], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28780867886619976
{'currentTarget': array([ 11.3077029 , 117.03832219]), 'previousTarget': array([ 11.20258969, 117.28156265]), 'currentState': array([19.386093, 98.74243 ,  3.70546 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4177
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.58732645, 112.84205449]), 'previousTarget': array([ 10.94201698, 110.89383588]), 'currentState': array([12.2228775, 92.90904  ,  1.9132458], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.28790816163693533
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.686391 , 119.62095  ,   2.0880494], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7840996456927304}
episode index:4178
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.65930965, 103.71052027]), 'previousTarget': array([ 12.71202025, 103.72787848]), 'currentState': array([15.881715, 83.971825,  4.251419], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2878392676044786
{'currentTarget': array([  8.12939507, 114.68414377]), 'previousTarget': array([  8.17306934, 114.69086807]), 'currentState': array([ 1.4906043, 95.81813  ,  4.5193133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4179
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.10904026, 76.62908222]), 'previousTarget': array([16.17157288, 76.79898987]), 'currentState': array([18.898617 , 56.82458  ,  5.3421307], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 270
reward sum = 0.06629832272038531
running average episode reward sum: 0.2877862673784297
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.915331 , 118.398315 ,   5.3470206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4967750937686755}
episode index:4180
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.24559  , 110.1613   ,   5.3317375], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.65364328396137}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.28783578959613404
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.686456 , 121.99338  ,   0.4369213], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1082637552854866}
episode index:4181
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.40757723, 89.32792787]), 'previousTarget': array([ 9.3920815 , 88.99615643]), 'currentState': array([ 9.021355, 69.33166 ,  4.775759], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877669622911134
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.967192 , 115.06159  ,   3.4006817], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.375845946696501}
episode index:4182
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.23092776, 114.19182208]), 'previousTarget': array([ 11.06902678, 112.78406925]), 'currentState': array([15.377434 , 94.62638  ,  0.7354019], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.287718958369719
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.395965 , 119.81851  ,   1.0833001], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4077127316603217}
episode index:4183
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.033665 , 113.167274 ,   3.6433072], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.486690665070594}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.2877626647949618
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.586894 , 118.24309  ,   2.2253454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.804826065592014}
episode index:4184
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.23779801, 106.98065955]), 'previousTarget': array([ 12.96694159, 105.58914087]), 'currentState': array([15.625768 , 87.26971  ,  2.2387412], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.2877997700080637
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.161242, 119.30662 ,   1.84661 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0882530641319932}
episode index:4185
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.96957705, 107.00161988]), 'previousTarget': array([  7.6857707 , 107.65744374]), 'currentState': array([ 2.4285836, 87.52396  ,  3.5281947], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.28782673725195845
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.678083  , 121.29551   ,   0.34420598], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3349062627895414}
episode index:4186
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.0348215, 100.74787  ,   4.495237 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.489858831499276}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877579943006205
{'currentTarget': array([  8.60505519, 115.8009087 ]), 'previousTarget': array([  8.6138434 , 115.88679532]), 'currentState': array([ 2.2998357, 96.82081  ,  3.2588077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4187
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.93528064, 72.38039801]), 'previousTarget': array([10.70591415, 71.99783772]), 'currentState': array([11.328018, 52.384254,  4.328524], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876892841778171
{'currentTarget': array([  7.9589159 , 100.44217744]), 'previousTarget': array([  7.9589159 , 100.44217744]), 'currentState': array([ 5.88296 , 80.55021 ,  4.904529], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4188
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.47787225, 71.99756325]), 'previousTarget': array([10., 70.]), 'currentState': array([10.676966 , 51.998554 ,  1.1112511], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876206068600377
{'currentTarget': array([13.82645102, 60.84992193]), 'previousTarget': array([13.98064805, 60.64701961]), 'currentState': array([15.117563 , 40.89164  ,  5.6961107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4189
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.87573094, 69.61155461]), 'previousTarget': array([12.13125551, 70.98112317]), 'currentState': array([11.22327 , 49.614574,  3.858223], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28755196232379426
{'currentTarget': array([  8.57319548, 106.1717999 ]), 'previousTarget': array([  8.69159762, 106.35101166]), 'currentState': array([ 6.520478 , 86.27742  ,  4.3348846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4190
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.2086616, 108.68294  ,   2.1157627], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.198415765673968}
done in step count: 250
reward sum = 0.08105851616218128
running average episode reward sum: 0.2875026916375233
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.464689  , 121.856316  ,   0.16286778], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9319589147462592}
episode index:4191
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.15026023, 83.01690283]), 'previousTarget': array([11.2986772 , 82.98769988]), 'currentState': array([11.772006 , 63.02657  ,  3.2341642], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.287434107980167
{'currentTarget': array([ 3.38610727, 71.23742066]), 'previousTarget': array([ 3.38610727, 71.23742066]), 'currentState': array([ 0.6980286, 51.418888 ,  2.903388 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4192
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.03496208, 83.90249675]), 'previousTarget': array([ 5.48069469, 83.84555753]), 'currentState': array([ 0.7742238, 64.1701   ,  3.2410831], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2873655570362175
{'currentTarget': array([ 11.04258659, 117.20523225]), 'previousTarget': array([ 11.03064872, 117.27641843]), 'currentState': array([18.033    , 98.46666  ,  1.5395224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4193
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.811815  , 113.13698   ,   0.59188753], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.56740337809088}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.2874716461665509
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.106838 , 120.35658  ,   1.1255997], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9264507991695412}
episode index:4194
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.12251713, 76.49611744]), 'previousTarget': array([11.36539906, 76.98992951]), 'currentState': array([10.178842 , 56.496197 ,  3.1754918], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.2874513430392475
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([9.3341570e+00, 1.1990981e+02, 1.0594618e-01], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6719230774348383}
episode index:4195
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.67544468, 117.97366596]), 'previousTarget': array([ 10.67544468, 117.97366596]), 'currentState': array([17.       , 99.       ,  3.5532014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 287
reward sum = 0.05588571986991975
running average episode reward sum: 0.28739615580779626
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.83381  , 118.05148  ,   5.5354033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1194236877702988}
episode index:4196
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.75777236, 108.73301438]), 'previousTarget': array([ 11.81535122, 108.74482241]), 'currentState': array([14.840697  , 88.97205   ,  0.18184489], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.28736025012496297
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.150802 , 118.12407  ,   1.1535549], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6341318911711804}
episode index:4197
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.83058554, 114.78921   ,   5.844856  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.546588095774652}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.28739523608891415
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.02818  , 118.018456 ,   3.7564118], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9817448633694563}
episode index:4198
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.16864212, 118.12218495]), 'previousTarget': array([ 10.18928508, 117.91786413]), 'currentState': array([11.957595 , 98.202354 ,  5.7615747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.28746800846574494
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.816399 , 120.08006  ,   0.5753263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1863061526648793}
episode index:4199
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.9467623, 110.01808  ,   3.3662834], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.8254953402186}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.2875510364366016
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.903173, 121.85748 ,   5.222264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.860004877132087}
episode index:4200
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.4499896 , 76.83016661]), 'previousTarget': array([ 7.22844538, 74.9622374 ]), 'currentState': array([ 6.27066  , 56.864967 ,  1.3140215], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2874825882013156
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.6949444, 106.26518  ,   5.4217453], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.126875257196833}
episode index:4201
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.7331245, 109.96918  ,   5.7531214], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.998408969671996}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.28758670475242376
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.213884 , 118.57536  ,   1.5052493], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.284687994103946}
episode index:4202
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.9390693, 107.12636  ,   5.330206 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.229038576625321}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.28761653177273816
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.051532, 119.85821 ,   4.742189], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9536205912095583}
episode index:4203
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.172752 , 118.00904  ,   5.3090324], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.9469936058605177}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.28778360681275417
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.940621, 118.13875 ,   5.471975], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6889160834318515}
episode index:4204
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.079415 , 116.05197  ,   5.4127216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9488272105782167}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.28785760738342087
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.053872 , 118.462395 ,   1.2036033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.805377523287837}
episode index:4205
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.84979692, 86.04186674]), 'previousTarget': array([ 9.35708593, 83.99681199]), 'currentState': array([ 8.172761 , 66.05333  ,  1.9860454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28778916762893125
{'currentTarget': array([ 5.25087582, 89.61356427]), 'previousTarget': array([ 5.26377737, 89.67569477]), 'currentState': array([ 2.1625488 , 69.85345   ,  0.21464843], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4206
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.6709104 , 96.06192103]), 'previousTarget': array([11.63952224, 95.95367385]), 'currentState': array([13.063549 , 76.110466 ,  1.0524971], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.287720760410574
{'currentTarget': array([ 8.59359079, 98.0205826 ]), 'previousTarget': array([ 8.57676874, 97.75396414]), 'currentState': array([ 7.3164515, 78.0614   ,  4.1979523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4207
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.67869123, 77.74271744]), 'previousTarget': array([10.67746131, 77.99739905]), 'currentState': array([10.999868 , 57.745296 ,  0.6194048], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876523857051532
{'currentTarget': array([13.97299824, 91.95489329]), 'previousTarget': array([13.97299824, 91.95489329]), 'currentState': array([16.77828  , 72.15261  ,  0.3472029], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4208
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.62765424, 113.79868602]), 'previousTarget': array([  9.53392998, 113.94108971]), 'currentState': array([ 8.428952 , 93.83464  ,  5.5642624], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 218
reward sum = 0.11180788242357734
running average episode reward sum: 0.28761060749102124
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.738678 , 118.15038  ,   2.7969646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.991664685354774}
episode index:4209
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.84600631, 115.02399127]), 'previousTarget': array([  9.53392998, 113.94108971]), 'currentState': array([ 9.227358 , 95.03356  ,  1.2887045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.28761412146446025
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.572378 , 120.72874  ,   2.8616803], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9266467465816058}
episode index:4210
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.001549, 113.92263 ,   4.455757], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.27125166771757}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.28776275673542184
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.533018 , 120.03661  ,   2.2358522], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5342741372056393}
episode index:4211
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.25491205, 92.93982443]), 'previousTarget': array([11.14970567, 92.98191681]), 'currentState': array([12.181414 , 72.961296 ,  0.2239753], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28769443699260716
{'currentTarget': array([  6.57574261, 101.08743799]), 'previousTarget': array([  7.21997399, 100.03376751]), 'currentState': array([ 3.0125299, 81.40741  ,  2.7837014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4212
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.78699374, 123.153046  ,   1.274575  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.737616813585744}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.2877212564124882
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.918283 , 121.673775 ,   0.3489944], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9928955010760996}
episode index:4213
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.04302 , 124.16275 ,   6.057239], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.2914310939000915}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.2878250198390533
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.796376, 119.60551 ,   4.407878], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8391825254863399}
episode index:4214
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.904716 , 108.02262  ,   5.9987164], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.027354324806605}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.2879154461115872
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.5620365, 118.067726 ,   1.5790362], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.012353679591965}
episode index:4215
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.80555326, 93.92898515]), 'previousTarget': array([11.69841725, 93.95760212]), 'currentState': array([13.187347 , 73.976776 ,  3.6742768], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28784715497161767
{'currentTarget': array([  8.86447057, 107.82124415]), 'previousTarget': array([  8.94594417, 107.99894733]), 'currentState': array([ 7.007753 , 87.907616 ,  1.6072996], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4216
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.06050295, 78.71346506]), 'previousTarget': array([ 8.63460094, 76.98992951]), 'currentState': array([ 7.122008 , 58.735497 ,  2.2906013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877788962201423
{'currentTarget': array([  8.98758548, 112.50259838]), 'previousTarget': array([  8.998308  , 112.34111772]), 'currentState': array([ 6.311169 , 92.68249  ,  4.1971817], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4217
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.60721711, 110.3882141 ]), 'previousTarget': array([  7.69281066, 110.44164417]), 'currentState': array([ 2.7758217, 90.980545 ,  2.120732 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2877106698341252
{'currentTarget': array([  9.10212211, 114.65767171]), 'previousTarget': array([  8.49817692, 113.43754513]), 'currentState': array([ 5.787241 , 94.934296 ,  0.6543516], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4218
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.061657 , 109.73663  ,   3.7769923], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.975129531217858}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.28776958368939254
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.418392 , 119.220726 ,   0.3915105], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8844885324199774}
episode index:4219
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.92406422, 84.03157806]), 'previousTarget': array([ 4.17356191, 83.74660743]), 'currentState': array([ 0.5927763 , 64.31097   ,  0.07351422], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 250
reward sum = 0.08105851616218128
running average episode reward sum: 0.2877206000241017
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.312786 , 119.5492   ,   1.4137207], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.821877003729578}
episode index:4220
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0777111 , 111.02481   ,   0.17343205], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.655483414580557}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.2877324535661237
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.586403 , 121.10441  ,   6.0709825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.179313436203068}
episode index:4221
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.86511044, 111.68417822]), 'previousTarget': array([  8.82842712, 111.79898987]), 'currentState': array([ 6.1607084, 91.86787  ,  2.2089546], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.2877492739934368
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.832385 , 118.09133  ,   6.2498536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2374852295742866}
episode index:4222
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.069323 , 115.02355  ,   3.6703045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.775283767436049}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.28784604499605493
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.0440645, 119.84685  ,   2.2463524], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9619223411514488}
episode index:4223
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.87891509, 85.84709108]), 'previousTarget': array([11.89059961, 85.96920706]), 'currentState': array([12.97755  , 65.87729  ,  3.0091238], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28777789962555395
{'currentTarget': array([13.85882362, 91.64382895]), 'previousTarget': array([13.85882362, 91.64382895]), 'currentState': array([16.555649, 71.826485,  6.211328], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4224
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.44786629, 109.26777154]), 'previousTarget': array([ 12.0776773 , 109.61161351]), 'currentState': array([14.121808, 89.44733 ,  3.356454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28770978651321655
{'currentTarget': array([  8.39421555, 114.58264563]), 'previousTarget': array([  8.48065512, 114.8067326 ]), 'currentState': array([ 2.7103586, 95.4073   ,  4.3246913], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4225
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.8180854, 85.0065657]), 'previousTarget': array([ 8.74023321, 85.98629668]), 'currentState': array([ 9.714116, 65.006836,  5.857063], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28764170563614294
{'currentTarget': array([  8.80249125, 106.31046638]), 'previousTarget': array([  8.69185558, 106.38590645]), 'currentState': array([ 7.0596223, 86.38655  ,  4.6946044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4226
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.       , 102.       ,   2.9588375], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.11077027627483}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.28761051047258734
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.487609 , 119.65148  ,   2.0714338], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6196851497691278}
episode index:4227
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.061883 , 103.19569  ,   6.2724066], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.915710306810848}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.2876421385973939
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.860948, 121.4894  ,   2.194059], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.38357853185063}
episode index:4228
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.81326801, 108.73379101]), 'previousTarget': array([  7.80043813, 108.63559701]), 'currentState': array([ 4.002457 , 89.100204 ,  2.4954927], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2875741220122444
{'currentTarget': array([  8.11762582, 115.22394728]), 'previousTarget': array([  8.01086723, 115.06671476]), 'currentState': array([ 0.7841041, 96.616974 ,  5.3579288], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4229
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.82807989, 109.61578759]), 'previousTarget': array([ 11.13318583, 107.91268452]), 'currentState': array([12.417915 , 89.67908  ,  1.7471913], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2875061375862368
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.8043627, 119.30844  ,   2.9495845], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.221604996777579}
episode index:4230
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.7381554, 115.789566 ,   5.198701 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.779507849571417}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2875675061763584
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.542099 , 118.03025  ,   1.2551564], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.042984146588155}
episode index:4231
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.44843241, 74.59576061]), 'previousTarget': array([15.62878378, 72.85893586]), 'currentState': array([19.260666 , 54.794464 ,  1.2589037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28749955544238476
{'currentTarget': array([15.30195407, 92.40081394]), 'previousTarget': array([15.50317018, 92.2127745 ]), 'currentState': array([19.075071 , 72.75995  ,  1.0322791], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4232
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0140337, 120.903625 ,   4.5703464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.031286166079713}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.2876063829448525
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.926591  , 120.650085  ,   0.47749674], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2549175851016394}
episode index:4233
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.5933483 , 80.76334932]), 'previousTarget': array([ 9.3278248 , 78.99731309]), 'currentState': array([ 9.386078 , 60.764423 ,  1.4087597], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28753845512649046
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.28067  , 114.31277  ,   4.8382616], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.732543438881787}
episode index:4234
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.389836 , 120.222336 ,   3.0143516], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.3971198731431613}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2877043256211477
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.4538765, 120.70131  ,   2.3128433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.614184457712843}
episode index:4235
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.7110792, 116.8040091]), 'previousTarget': array([ 10.75140711, 116.54352728]), 'currentState': array([15.054688 , 97.28138  ,  0.7012915], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2876364067529652
{'currentTarget': array([  8.68195907, 115.36430995]), 'previousTarget': array([  8.68195907, 115.36430995]), 'currentState': array([ 3.2122567, 96.126785 ,  2.0460615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4236
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.21048668, 94.51502335]), 'previousTarget': array([ 8.85029433, 92.98191681]), 'currentState': array([ 6.809569 , 74.56415  ,  2.3876653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28756851994466853
{'currentTarget': array([  8.81703591, 115.85464934]), 'previousTarget': array([  8.83248426, 115.91442966]), 'currentState': array([ 3.3287122, 96.62243  ,  5.6064434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4237
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.80145779, 100.63169511]), 'previousTarget': array([ 10., 100.]), 'currentState': array([11.628347  , 80.648796  ,  0.31870687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.2876448646165865
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.091347 , 119.18351  ,   3.1241736], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0759608539928096}
episode index:4238
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.96232243, 98.92545996]), 'previousTarget': array([ 5.83020719, 98.62981184]), 'currentState': array([ 2.1989644, 79.28272  ,  3.0563748], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28757700784267365
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.960818 , 119.189735 ,   0.2820375], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.042846798188714}
episode index:4239
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.34150807, 88.8822214 ]), 'previousTarget': array([13.12154811, 86.91159005]), 'currentState': array([15.476884 , 68.996544 ,  1.3501419], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.287509183076673
{'currentTarget': array([ 4.77653348, 90.5384206 ]), 'previousTarget': array([ 4.7662601 , 90.66840655]), 'currentState': array([ 1.2850341, 70.84554  ,  2.5776653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4240
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.78509663, 75.80513158]), 'previousTarget': array([ 3.78509663, 75.80513158]), 'currentState': array([ 1.       , 56.       ,  2.9341753], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28744139029594284
{'currentTarget': array([ 6.86288913, 82.85503859]), 'previousTarget': array([ 7.06939102, 82.82445827]), 'currentState': array([ 5.1797633, 62.925987 ,  3.0869834], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4241
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.95239073, 88.61702827]), 'previousTarget': array([13.75021969, 86.87305937]), 'currentState': array([14.825644, 68.70495 ,  2.209939], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.28746714145485985
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.325648 , 118.22471  ,   2.7050962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.440310081998464}
episode index:4242
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.8961687, 101.2648   ,   5.944985 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.990562111281186}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.28750070776927633
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.686996  , 120.792946  ,   0.71237904], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5338648688612937}
episode index:4243
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.525204 , 101.29104  ,   4.9460583], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.71633187477586}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.28748567194604574
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.032953 , 120.05525  ,   1.5539856], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.06433280120995083}
episode index:4244
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.44599548, 92.67509748]), 'previousTarget': array([ 9.42543563, 92.9954746 ]), 'currentState': array([ 9.040585 , 72.67921  ,  4.1321616], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 279
reward sum = 0.06056466128430853
running average episode reward sum: 0.2874322158775742
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.737717 , 121.327126 ,   5.3805103], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3527951679111967}
episode index:4245
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.20387648, 109.49083222]), 'previousTarget': array([  8.48069469, 107.84555753]), 'currentState': array([ 4.834529 , 89.77669  ,  1.9928397], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28736452105518195
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.401576 , 115.9067   ,   4.7736025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.136812181381178}
episode index:4246
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.83001409, 104.29039569]), 'previousTarget': array([ 10.89059961, 103.96920706]), 'currentState': array([11.885239 , 84.31825  ,  2.4331994], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28729685811167943
{'currentTarget': array([  8.35001017, 114.71099846]), 'previousTarget': array([  8.43246757, 114.95318346]), 'currentState': array([ 2.3937955, 95.6185   ,  1.5980002], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4247
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.23350427, 89.59976982]), 'previousTarget': array([ 5.15981002, 89.74881264]), 'currentState': array([ 2.135524, 69.84116 ,  2.29356 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28722922702455334
{'currentTarget': array([ 7.29213672, 83.56963911]), 'previousTarget': array([ 7.30520362, 83.26749753]), 'currentState': array([ 5.8096294, 63.62466  ,  3.5588515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4248
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.00402179, 105.65709558]), 'previousTarget': array([  7.03305841, 105.58914087]), 'currentState': array([ 2.9146378, 86.079636 ,  6.0193257], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.28719043284163276
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.358479 , 120.08491  ,   5.9793224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.643715903804569}
episode index:4249
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.07235  , 121.956245 ,   2.0245962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.301757296113278}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.28732320162710406
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.415919, 121.380455,   5.57656 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4989350598915685}
episode index:4250
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.30533169, 95.06228008]), 'previousTarget': array([11.11198773, 94.98027613]), 'currentState': array([12.350774 , 75.08962  ,  3.9336774], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.2872995243923566
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.682306 , 118.46588  ,   1.3571149], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2767684317342947}
episode index:4251
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.93279  , 100.11251  ,   4.6896014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.88760288767655}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.2873606384840778
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.115462  , 119.22721   ,   0.55899394], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7813669971507732}
episode index:4252
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.34417226, 116.67064543]), 'previousTarget': array([ 11.42734309, 116.51093912]), 'currentState': array([18.831638 , 98.125084 ,  2.1269662], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.2873524090047938
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.062225 , 118.935616 ,   1.0707742], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2108561476541193}
episode index:4253
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.08532813, 89.08974677]), 'previousTarget': array([ 8.17444044, 88.96548746]), 'currentState': array([ 6.8488393, 69.128006 ,  5.368517 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 294
reward sum = 0.05208914293358933
running average episode reward sum: 0.28729710499302336
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.330536 , 118.07582  ,   4.7717514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.037313750332081}
episode index:4254
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.43726815, 77.03494748]), 'previousTarget': array([ 4.51945074, 76.84067458]), 'currentState': array([ 1.8692803, 57.200497 ,  3.8353815], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.2872561273549459
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.022256 , 119.66038  ,   1.6955898], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.3403509450882215}
episode index:4255
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.024173  , 123.868126  ,   0.43164918], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.547035258462279}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2873866929249715
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.118537 , 118.49375  ,   5.8028593], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7452110404739918}
episode index:4256
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.39676541, 80.89203646]), 'previousTarget': array([ 7.35282671, 80.95419404]), 'currentState': array([ 6.0683985, 60.9362   ,  5.1935434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28731918371827075
{'currentTarget': array([ 4.56640161, 90.0212572 ]), 'previousTarget': array([ 4.48825582, 89.87782443]), 'currentState': array([ 0.999548 , 70.34189  ,  0.2638216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4257
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.92396423, 110.81781414]), 'previousTarget': array([ 11.94788792, 110.58520839]), 'currentState': array([16.02554  , 91.242905 ,  4.1948113], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2872517062209203
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.397065, 108.092865,   6.124966], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.146019328768507}
episode index:4258
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.31447343, 86.31115292]), 'previousTarget': array([ 9.37030688, 85.99657153]), 'currentState': array([ 8.907582, 66.31529 ,  4.418762], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2871842604105843
{'currentTarget': array([15.0709016 , 92.90442116]), 'previousTarget': array([14.96713394, 93.15976829]), 'currentState': array([18.75      , 73.24573   ,  0.33910912], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4259
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.86471843, 98.62389037]), 'previousTarget': array([ 5.83020719, 98.62981184]), 'currentState': array([ 2.0660777, 78.987946 ,  3.786279 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.28714925960135357
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.71405  , 118.03372  ,   3.1202383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9869616774828414}
episode index:4260
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.838697 , 116.14611  ,   1.7691694], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.642356368204613}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.28711088358035736
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.845038, 120.91421 ,   2.184978], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2449358209594574}
episode index:4261
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.66191736, 109.72871471]), 'previousTarget': array([ 10.3337034 , 109.98889814]), 'currentState': array([ 9.003967 , 89.73954  ,  3.6273596], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28704351828622776
{'currentTarget': array([  8.48358526, 114.53220492]), 'previousTarget': array([  8.54221671, 114.46775241]), 'currentState': array([ 3.1386182, 95.25965  ,  3.9476166], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4262
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.77339355, 103.98491374]), 'previousTarget': array([ 11.79136948, 103.87767469]), 'currentState': array([13.974593, 84.106415,  5.539326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.2870032149365743
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.093095, 118.35549 ,   1.508078], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.878000198928436}
episode index:4263
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.90627015, 112.96538849]), 'previousTarget': array([  8.93097322, 112.78406925]), 'currentState': array([ 5.8336196, 93.20283  ,  3.2907202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28693590649029466
{'currentTarget': array([  8.22080217, 115.21943364]), 'previousTarget': array([  8.25312423, 115.32189742]), 'currentState': array([ 1.2448108, 96.47549  ,  2.9579055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4264
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.3977648 , 110.59289848]), 'previousTarget': array([  8.39813833, 110.70920231]), 'currentState': array([ 5.039687 , 90.87683  ,  5.1512947], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.2869165413215387
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.824235 , 121.223206 ,   1.1185772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6966600370082177}
episode index:4265
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.42271106, 113.03752492]), 'previousTarget': array([ 12.45778872, 112.89972147]), 'currentState': array([18.995497 , 94.148415 ,  0.8239517], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2868492847483269
{'currentTarget': array([  8.88528402, 115.30955428]), 'previousTarget': array([  8.91826845, 115.32649105]), 'currentState': array([ 4.2609487, 95.85151  ,  4.5321217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4266
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.58255146, 110.87396438]), 'previousTarget': array([ 11.60186167, 110.70920231]), 'currentState': array([14.999764 , 91.16806  ,  5.8692656], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286782059699171
{'currentTarget': array([  8.62757128, 116.13011741]), 'previousTarget': array([  8.65433444, 116.25404143]), 'currentState': array([ 1.9426426, 97.2804   ,  5.1588507], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4267
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.90459  , 110.06804  ,   3.5702024], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.743194706106944}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.28679242779397457
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.80197  , 121.192024 ,   1.6091598], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6900292241716544}
episode index:4268
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.73040424, 84.87228067]), 'previousTarget': array([ 4.87879691, 84.79172879]), 'currentState': array([ 1.7633541, 65.09359  ,  3.0873742], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28672524755790196
{'currentTarget': array([ 7.70407025, 88.25351203]), 'previousTarget': array([ 7.7253083 , 88.40057601]), 'currentState': array([ 6.261423, 68.30561 ,  5.436536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4269
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1567719, 111.71746  ,   2.3485081], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.116234631119209}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.28676830643238155
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.146837 , 118.86992  ,   0.6800134], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6100680446773705}
episode index:4270
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.00034193, 77.03500235]), 'previousTarget': array([14.10381815, 76.90990945]), 'currentState': array([15.854463 , 57.12113  ,  5.0183096], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28670116330280243
{'currentTarget': array([11.47952282, 89.71742906]), 'previousTarget': array([11.41855277, 89.79013621]), 'currentState': array([12.455503 , 69.74126  ,  1.2669196], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4271
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.78733717, 88.92297514]), 'previousTarget': array([14.95885631, 87.76743395]), 'currentState': array([16.206823 , 69.06986  ,  2.4607768], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28663405160727273
{'currentTarget': array([  8.615993  , 114.42325142]), 'previousTarget': array([  8.68851417, 114.69358866]), 'currentState': array([ 3.7986376, 95.01209  ,  5.8658524], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4272
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.       , 124.       ,   2.6378508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.2867786217040201
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.446092 , 121.383316 ,   3.7936676], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0804313064537685}
episode index:4273
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.9326105, 113.859764 ,   3.6315472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.876112199515864}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2868308461131323
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.956513, 118.19432 ,   3.057729], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0433785026228946}
episode index:4274
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.34026037, 80.64069745]), 'previousTarget': array([11.32242309, 80.98851894]), 'currentState': array([12.020905, 60.652283,  4.908279], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.28677879894357067
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.799027 , 118.399765 ,   2.1802578], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6128056228316896}
episode index:4275
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.076384 , 121.16992  ,   3.7636147], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.09432279859485}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.2868697601807355
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.241876  , 120.18149   ,   0.19588795], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7674668718129514}
episode index:4276
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.138873 , 106.01232  ,   4.4212413], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.335538878522824}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.28687703614604726
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.842807, 120.84855 ,   5.429538], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0287858878559644}
episode index:4277
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.9541271 , 71.85704942]), 'previousTarget': array([ 7.8814955, 71.9805647]), 'currentState': array([ 7.1049776, 51.875084 ,  0.6657541], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2868099774653212
{'currentTarget': array([ 3.90823343, 83.19975359]), 'previousTarget': array([ 4.00998265, 83.17203233]), 'currentState': array([ 0.64196193, 63.46827   ,  2.5102553 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4278
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.79109898, 88.81150206]), 'previousTarget': array([10., 87.]), 'currentState': array([ 9.657142 , 68.81195  ,  2.2539263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.2868609416885794
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.635595, 120.23875 ,   4.670698], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3851354034405645}
episode index:4279
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.48034378, 108.08997434]), 'previousTarget': array([  7.40521743, 108.50882004]), 'currentState': array([ 3.3408139, 88.523056 ,  3.9335327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28679391810407273
{'currentTarget': array([  7.81217522, 110.99556686]), 'previousTarget': array([  6.96509508, 110.14660828]), 'currentState': array([3.0901215e+00, 9.1561005e+01, 6.9838226e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4280
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.30879504, 114.87309808]), 'previousTarget': array([  8.32455532, 114.97366596]), 'currentState': array([ 2.0434933, 95.87978  ,  5.5208383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.28680271315470557
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.269174 , 118.00538  ,   2.3782935], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.640885076713953}
episode index:4281
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.25138653, 82.65078976]), 'previousTarget': array([15.22022743, 82.80587954]), 'currentState': array([18.036043, 62.845596,  4.056239], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.2867640317215441
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.297873 , 121.55954  ,   0.6221848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.30854981550712}
episode index:4282
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.49957459, 106.70545339]), 'previousTarget': array([ 11.59337265, 106.85467564]), 'currentState': array([13.741283  , 86.83148   ,  0.15574378], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.28670973842861447
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.170975, 119.63406 ,   1.7508  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.40390805797661083}
episode index:4283
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.83229046, 111.80544072]), 'previousTarget': array([ 11.48418723, 111.68855151]), 'currentState': array([12.853218 , 91.90781  ,  2.9964268], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.28677577285066125
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.101541, 118.29541 ,   6.146815], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9268772381432033}
episode index:4284
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.573042 , 119.958534 ,   3.0026662], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.427152277438383}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.28687301250577185
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.251752 , 120.65834  ,   5.4181633], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.414317846863771}
episode index:4285
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.6294603, 120.55555  ,   5.7724156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.434767686337152}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2870370645327187
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.043374, 119.17563 ,   5.497917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.262822746384055}
episode index:4286
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.24095873, 78.35383   ]), 'previousTarget': array([10., 77.]), 'currentState': array([ 8.8765  , 58.35715 ,  1.836327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2869701093042296
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.396914, 103.44951 ,   2.65384 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.75394938693192}
episode index:4287
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.40700989, 90.21121156]), 'previousTarget': array([13.66317505, 88.86301209]), 'currentState': array([14.017811, 70.276184,  2.366277], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.28702092921548966
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.334719 , 119.60124  ,   1.8309716], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7123579311713422}
episode index:4288
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.13721269, 110.76324316]), 'previousTarget': array([ 10.35517412, 108.98960229]), 'currentState': array([10.434281 , 90.76545  ,  1.6113071], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 211
reward sum = 0.11995712819347787
running average episode reward sum: 0.28698197752488064
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.856783 , 121.8589   ,   2.9437134], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8644108174025977}
episode index:4289
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.53243255, 97.20309662]), 'previousTarget': array([11.60803474, 96.95150202]), 'currentState': array([12.873827, 77.24813 ,  2.110516], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.28703277097738933
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.326103 , 118.323944 ,   1.7405316], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8064607082329391}
episode index:4290
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.658924 , 123.15332  ,   5.5387735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9273483866031706}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2871965946150082
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.247456 , 121.79443  ,   5.7174644], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9458455748028445}
episode index:4291
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.75938552, 108.47075868]), 'previousTarget': array([  6.8507125, 107.40285  ]), 'currentState': array([ 3.9439325 , 88.83807   ,  0.74096584], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.28715936645635576
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.476054  , 120.20364   ,   0.96775466], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5374919914502068}
episode index:4292
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.93092339, 94.27439381]), 'previousTarget': array([10., 95.]), 'currentState': array([ 8.100502 , 74.29164  ,  3.2831588], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870924763174188
{'currentTarget': array([ 9.71909592, 78.63188222]), 'previousTarget': array([ 8.69229651, 77.06871958]), 'currentState': array([ 9.583292 , 58.632343 ,  0.9804822], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4293
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.45951277, 98.94667447]), 'previousTarget': array([ 8.45951277, 98.94667447]), 'currentState': array([ 7.       , 79.       ,  3.3048358], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870256173336467
{'currentTarget': array([  9.74992265, 118.21587095]), 'previousTarget': array([  9.78410253, 118.41206133]), 'currentState': array([ 6.973707 , 98.40949  ,  5.4701557], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4294
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.313583 , 119.79851  ,   2.5628548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.3197038915271535}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.28718929006535016
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.767471, 121.445984,   2.250809], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2835989675538104}
episode index:4295
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.10381815, 118.90990945]), 'currentState': array([ 11.157965 , 100.76119  ,   2.1848838], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.273624546020283}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.2873308517423039
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.166891 , 118.698814 ,   1.8386495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5450418857709793}
episode index:4296
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.66340774, 83.81612042]), 'previousTarget': array([15.82643809, 83.74660743]), 'currentState': array([18.756104, 64.05669 ,  4.504975], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.287263983962052
{'currentTarget': array([ 4.74061193, 90.42940172]), 'previousTarget': array([ 4.74150815, 90.3066967 ]), 'currentState': array([ 1.2384007, 70.73843  ,  3.0018032], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4297
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.3135728 , 86.05572022]), 'previousTarget': array([11.25976679, 85.98629668]), 'currentState': array([12.086952, 66.07068 ,  3.537539], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28719714729756574
{'currentTarget': array([ 11.31573584, 112.30379758]), 'previousTarget': array([ 11.3231979 , 110.46844787]), 'currentState': array([14.686021 , 92.58981  ,  1.0725571], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4298
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.042574  , 112.10175   ,   0.17100503], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.272324399330293}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.2872263988732429
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.573498 , 119.130005 ,   3.2861447], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0420130511023942}
episode index:4299
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.954389 , 112.11192  ,   1.6186024], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.86502808311838}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2873785509082494
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([9.73620701e+00, 1.20486084e+02, 1.18117511e-01], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5530500721483527}
episode index:4300
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.13727476, 115.35042742]), 'previousTarget': array([  9.83261089, 115.98266146]), 'currentState': array([10.727501, 95.35914 ,  6.123267], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.2873253975205189
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.184074 , 120.978325 ,   2.8183634], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0626937166379298}
episode index:4301
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.01123072, 108.01260261]), 'previousTarget': array([ 10.75243428, 107.96105157]), 'currentState': array([10.029968 , 88.01261  ,  3.3786354], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2872586087251864
{'currentTarget': array([  8.13259054, 115.3016317 ]), 'previousTarget': array([  8.17663586, 115.24815603]), 'currentState': array([ 0.745505 , 96.71586  ,  1.6067073], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4302
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.2823023 , 113.68071138]), 'previousTarget': array([  9.29248216, 113.86817872]), 'currentState': array([ 7.0253606, 93.808464 ,  3.7089713], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.28729585357878584
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.753571 , 121.06961  ,   2.9248495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3084094964024282}
episode index:4303
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.23484624, 66.90627534]), 'previousTarget': array([ 9.2739469 , 66.99812374]), 'currentState': array([ 8.946649 , 46.90835  ,  6.1296773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28722910268343765
{'currentTarget': array([ 8.50595672, 83.30854771]), 'previousTarget': array([ 8.42231889, 83.47115272]), 'currentState': array([ 7.692249, 63.325108,  4.650536], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4304
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.98561884, 113.39662785]), 'previousTarget': array([  9.13066247, 111.88618308]), 'currentState': array([ 5.948927 , 93.62851  ,  2.3090172], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2871623827989584
{'currentTarget': array([  8.54961631, 115.6549095 ]), 'previousTarget': array([  8.57460509, 115.72583586]), 'currentState': array([ 2.2171233, 96.68389  ,  4.6793003], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4305
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.91498147, 72.95581234]), 'previousTarget': array([ 3.66265197, 72.8219647 ]), 'currentState': array([ 1.3494167, 53.121048 ,  5.2694206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870956939037426
{'currentTarget': array([  8.94949361, 116.3932848 ]), 'previousTarget': array([  8.94433206, 116.39771062]), 'currentState': array([ 3.3566194, 97.19121  ,  4.4879785], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4306
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.9416408 , 102.90802855]), 'previousTarget': array([  6.53328126, 100.68542414]), 'currentState': array([ 3.4188836, 83.22072  ,  1.3001238], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870290359762052
{'currentTarget': array([  7.93898708, 113.70419732]), 'previousTarget': array([  7.94242578, 113.56906434]), 'currentState': array([ 1.7166559, 94.69676  ,  3.8814712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4307
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.1325293, 115.804436 ,   2.338585 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.9162691417787}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.28710284680052306
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.201857, 121.71239 ,   6.270357], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8892597478109654}
episode index:4308
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.83659259, 81.89819279]), 'previousTarget': array([ 8.01563705, 80.97419539]), 'currentState': array([ 5.1817837, 61.96677  ,  2.9081974], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.2870673111375055
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.76399  , 119.499214 ,   0.9650105], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8336980624661183}
episode index:4309
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.       , 101.       ,   2.9488618], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.235384061671347}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.28702370034965896
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.273594 , 119.69187  ,   5.4665723], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.753687856355374}
episode index:4310
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.8480053 , 115.62768   ,   0.93080354], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.029615280040541}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2871844696142496
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.508722  , 118.47603   ,   0.92182326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1322285222607618}
episode index:4311
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.1797317 , 103.85936129]), 'previousTarget': array([  8.20863052, 103.87767469]), 'currentState': array([ 5.93843 , 83.985344,  5.24866 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2871178683921684
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.777704 , 101.24053  ,   2.4913924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.034201733657987}
episode index:4312
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.86503421, 91.922942  ]), 'previousTarget': array([ 8.83261089, 91.98266146]), 'currentState': array([ 8.057229 , 71.93926  ,  6.0638275], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870512980540297
{'currentTarget': array([  7.53699254, 112.91722266]), 'previousTarget': array([  7.56628929, 112.8942377 ]), 'currentState': array([ 0.9679297, 94.02682  ,  3.0814335], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4313
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.30590268, 102.95915024]), 'previousTarget': array([ 12.32164208, 102.81984861]), 'currentState': array([14.987784 , 83.13978  ,  2.4712944], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.2870462713854376
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.857886 , 119.025826 ,   2.2585063], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5011461042656014}
episode index:4314
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.84887452, 103.28052903]), 'previousTarget': array([ 10.92049484, 102.97084547]), 'currentState': array([14.047117  , 83.4017    ,  0.13804206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.2870137377284843
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.997759, 121.03187 ,   1.807765], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.435365557452777}
episode index:4315
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.86152312, 75.34753961]), 'previousTarget': array([10.6923441 , 74.99763356]), 'currentState': array([11.247331 , 55.35126  ,  2.0912507], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2869472377892516
{'currentTarget': array([ 4.50066746, 83.25358117]), 'previousTarget': array([ 4.55254969, 83.24178111]), 'currentState': array([ 1.5405076, 63.473858 ,  4.9112787], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4316
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.850969, 109.17127 ,   3.714826], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.452787855867108}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.28701812479611105
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.495762 , 119.975296 ,   0.5997044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5048429230953093}
episode index:4317
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.24775708, 106.5830032 ]), 'previousTarget': array([ 13.28799949, 106.43700211]), 'currentState': array([17.953123 , 87.144394 ,  1.1496525], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2869516546421518
{'currentTarget': array([ 11.26936258, 114.35669297]), 'previousTarget': array([ 11.32556929, 114.14943229]), 'currentState': array([15.658351 , 94.844215 ,  5.1121235], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4318
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.662706  , 120.09271   ,   0.30355328], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6652891879250056}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2871167503461013
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.662706  , 120.09271   ,   0.30355328], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6652891879250056}
episode index:4319
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.14381197, 103.78958026]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([16.951618, 84.15541 ,  5.500678], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.2871104931032845
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.625767 , 118.45831  ,   1.2685416], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5864581193342053}
episode index:4320
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.42334635, 111.72709977]), 'previousTarget': array([ 10.66961979, 109.95570316]), 'currentState': array([11.445462, 91.753235,  1.534807], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.2870723728187883
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.384792 , 119.06022  ,   1.9236555], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1232404742380089}
episode index:4321
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.12002882, 84.94165008]), 'previousTarget': array([ 6.09369569, 82.89010906]), 'currentState': array([ 3.9200246, 65.06302  ,  1.6499939], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2870059516311856
{'currentTarget': array([ 5.84801674, 89.60987582]), 'previousTarget': array([ 5.81647457, 89.72062555]), 'currentState': array([ 3.1407113, 69.79396  ,  2.3232067], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4322
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.14709447, 82.23820581]), 'previousTarget': array([10., 82.]), 'currentState': array([10.225    , 62.238358 ,  5.7324066], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286939561172793
{'currentTarget': array([  8.91812737, 113.77408707]), 'previousTarget': array([  8.93678424, 113.6746881 ]), 'currentState': array([ 5.494053 , 94.069374 ,  4.8409495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4323
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.87507089, 82.65896976]), 'previousTarget': array([13.90630431, 82.89010906]), 'currentState': array([15.939487, 62.7658  ,  4.490268], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.2869051350053357
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.808816 , 119.268326 ,   0.6994487], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9511950926947255}
episode index:4324
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.4559276 , 112.98282151]), 'previousTarget': array([ 12.45778872, 112.89972147]), 'currentState': array([19.062733 , 94.10558  ,  2.0030475], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.28686081004703606
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.918295 , 118.45905  ,   1.9980248], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4605635296954604}
episode index:4325
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.50943281, 108.58629833]), 'previousTarget': array([ 12.59478257, 108.50882004]), 'currentState': array([16.804087 , 89.05284  ,  0.9117242], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.2868462071491931
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.464453, 119.58051 ,   3.206857], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6258479797300162}
episode index:4326
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.86393765, 109.82117726]), 'previousTarget': array([  6.9279843 , 110.10128274]), 'currentState': array([ 0.9751591, 90.70777  ,  5.2541347], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.28686199513337524
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.834811 , 118.23682  ,   1.2615416], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7709047747978781}
episode index:4327
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.769481 , 114.60597  ,   2.5602045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.532604510585483}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.28685890528954183
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.357319 , 118.28686  ,   1.0302693], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.750009613205784}
episode index:4328
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.14820691, 110.47748228]), 'previousTarget': array([ 10.66961979, 109.95570316]), 'currentState': array([10.459446, 90.479904,  3.346951], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28679264081615546
{'currentTarget': array([ 10.83391046, 117.34641392]), 'previousTarget': array([ 10.84243789, 117.28749113]), 'currentState': array([16.82996 , 98.26639 ,  3.034538], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4329
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.051725 , 114.29557  ,   2.7771585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.316483483413426}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.28694166453603787
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.488983 , 120.262695 ,   2.8832016], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5550795910458194}
episode index:4330
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.49839979, 69.84619036]), 'previousTarget': array([ 7.83261089, 67.98266146]), 'currentState': array([ 7.89987   , 49.85515   ,  0.82345355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2868754115541547
{'currentTarget': array([15.56530954, 91.21070496]), 'previousTarget': array([15.5898222 , 91.02101536]), 'currentState': array([19.361269 , 71.57424  ,  3.5771434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4331
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.69581223, 71.86380129]), 'previousTarget': array([ 3.62417438, 71.82709532]), 'currentState': array([ 1.0986779 , 52.033146  ,  0.05380695], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28680918915998244
{'currentTarget': array([11.24809623, 86.9463362 ]), 'previousTarget': array([10.15056397, 86.56520401]), 'currentState': array([12.002752  , 66.96058   ,  0.23405999], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4332
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.8318251 , 111.26726455]), 'previousTarget': array([  7.54459231, 109.47682419]), 'currentState': array([ 3.0125177, 91.85659  ,  1.7271692], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867429973323434
{'currentTarget': array([ 10.58453421, 118.2093204 ]), 'previousTarget': array([ 10.59084789, 118.14123463]), 'currentState': array([16.790867, 99.196655,  3.816356], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4333
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.016324  , 123.01182   ,   0.81784225], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.0201500054155215}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.2867405766286901
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.953522 , 119.92287  ,   3.0747292], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.95663640637887}
episode index:4334
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.9482374 , 122.66839   ,   0.98524034], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.851502497916173}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.28686121960252925
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.138781, 121.262314,   6.236622], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.528114885454882}
episode index:4335
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.35835034, 98.80370992]), 'previousTarget': array([ 8.45951277, 98.94667447]), 'currentState': array([ 6.813978 , 78.86343  ,  5.1454434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867950615721781
{'currentTarget': array([  9.03765322, 115.90312276]), 'previousTarget': array([  9.03765322, 115.90312276]), 'currentState': array([ 4.4641814, 96.43306  ,  2.218486 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4336
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.55146629, 111.38243512]), 'previousTarget': array([ 12.50557744, 111.23047895]), 'currentState': array([18.229374 , 92.20533  ,  5.7122564], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 257
reward sum = 0.07555183406752786
running average episode reward sum: 0.286746354348866
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.791716 , 120.6727   ,   2.5419984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0389117077391112}
episode index:4337
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.0162966, 88.0018843]), 'previousTarget': array([ 8.1519307 , 87.96679883]), 'currentState': array([ 6.7787848, 68.04021  ,  5.526193 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28668025329899305
{'currentTarget': array([  8.57995941, 116.21998693]), 'previousTarget': array([  8.64834616, 116.23134289]), 'currentState': array([ 1.5464809, 97.497536 ,  2.5015457], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4338
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.156611 , 115.04135  ,   0.6157273], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.961121233338173}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.28672483988651837
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.925428 , 118.29969  ,   5.3920937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.568721032833845}
episode index:4339
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.972531  , 111.83595   ,   0.35780248], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.221768941201603}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.2868208589775582
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.711199 , 118.00776  ,   3.6845639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.372768918854125}
episode index:4340
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.06568491, 92.51607946]), 'previousTarget': array([10., 91.]), 'currentState': array([ 8.386178, 72.527626,  2.6898  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286754786446119
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.046661 , 116.60222  ,   1.9727219], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.854718425332731}
episode index:4341
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.56442166, 85.06330082]), 'previousTarget': array([ 8.71383061, 83.98725709]), 'currentState': array([ 6.1735168, 65.111725 ,  2.8915694], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.2867284149914312
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.022971 , 118.43368  ,   2.2908769], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5223022602057505}
episode index:4342
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.45202228, 87.62942332]), 'previousTarget': array([ 8.74023321, 85.98629668]), 'currentState': array([ 7.4967036, 67.65225  ,  2.0517461], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866623941728746
{'currentTarget': array([  8.61826932, 114.81849884]), 'previousTarget': array([  8.57315825, 114.63787944]), 'currentState': array([ 3.4650266, 95.4938   ,  4.130492 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4343
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.05843027, 111.90570779]), 'previousTarget': array([  9.13066247, 111.88618308]), 'currentState': array([ 6.74751  , 92.039665 ,  5.1139035], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.28665508463478145
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.749126 , 119.279175 ,   1.3553114], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8918330384724473}
episode index:4344
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.84904284, 102.95050479]), 'previousTarget': array([ 10.92049484, 102.97084547]), 'currentState': array([11.843784, 82.97526 ,  3.628963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.28661994645071315
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.673182 , 119.675995 ,   2.8085446], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7042639958018033}
episode index:4345
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.744806, 110.08109 ,   4.427837], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.584384920325103}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.2867440947888137
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.679967 , 118.11519  ,   2.5496333], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9117884785743897}
episode index:4346
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.023345 , 106.797424 ,   1.1399853], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.23865022132408}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.28677802329859126
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0921545, 120.89368  ,   2.9591472], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8984156053037092}
episode index:4347
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.85209234, 110.8655808 ]), 'previousTarget': array([ 10., 109.]), 'currentState': array([ 9.528288 , 90.8682   ,  1.9091984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867120669914848
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.2355902, 117.17243  ,   3.3057241], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.209235576403325}
episode index:4348
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.87157418, 94.86140409]), 'previousTarget': array([11.72599692, 92.95938166]), 'currentState': array([13.356469 , 74.9166   ,  1.5466343], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28664614101609015
{'currentTarget': array([  8.58530319, 115.68120498]), 'previousTarget': array([  8.49115027, 115.49667669]), 'currentState': array([ 2.359462 , 96.67492  ,  2.7168045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4349
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.36495774, 108.20048785]), 'previousTarget': array([ 13.4237961 , 108.20692454]), 'currentState': array([18.849838 , 88.967285 ,  1.2788297], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.28661790235601936
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.721863 , 118.35785  ,   1.1335359], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3793845815468155}
episode index:4350
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.052792 , 117.31616  ,   1.1663795], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.524742750369401}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2867344264611188
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.835871 , 119.671974 ,   5.1837482], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8979314204634071}
episode index:4351
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.91089384, 69.94316161]), 'previousTarget': array([10.71431486, 69.9979595 ]), 'currentState': array([11.274777 , 49.946472 ,  6.1158695], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28666854079327386
{'currentTarget': array([10.88781136, 96.94696171]), 'previousTarget': array([11.04158185, 97.17918249]), 'currentState': array([11.6574745, 76.96178  ,  3.6707861], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4352
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.3666721 , 75.88079033]), 'previousTarget': array([ 9.31246186, 75.99755904]), 'currentState': array([ 9.079603  , 55.88285   ,  0.64555794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866026853968132
{'currentTarget': array([ 5.15680273, 88.30535927]), 'previousTarget': array([ 5.20826969, 88.08060282]), 'currentState': array([ 2.1357093, 68.53485  ,  5.242079 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4353
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.40819717, 79.46845833]), 'previousTarget': array([12.69133515, 78.95713898]), 'currentState': array([12.102643 , 59.48052  ,  2.7812202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28653686025087916
{'currentTarget': array([ 10.02612858, 103.22510391]), 'previousTarget': array([ 10.03214237, 103.27747465]), 'currentState': array([10.057281 , 83.22513  ,  4.6694484], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4354
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.4254406 , 82.82492935]), 'previousTarget': array([ 5.43782603, 82.85086911]), 'currentState': array([ 2.9827752, 62.974655 ,  3.3416288], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28647106533463323
{'currentTarget': array([ 5.82108354, 89.45807609]), 'previousTarget': array([ 5.85890758, 89.54819597]), 'currentState': array([ 3.1098332, 69.6427   ,  2.4191291], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4355
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.42264117, 82.42208414]), 'previousTarget': array([12.62395817, 81.95260657]), 'currentState': array([12.179268 , 62.4364   ,  2.6879628], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28640530062725617
{'currentTarget': array([ 6.0694871 , 89.28308745]), 'previousTarget': array([ 6.17550501, 89.09411093]), 'currentState': array([ 3.531    , 69.44484  ,  5.9788847], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4356
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.58104088, 100.99082954]), 'previousTarget': array([  8.53392998, 100.94108971]), 'currentState': array([ 7.0922623, 81.04632  ,  2.6573591], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28633956610794764
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.91166  , 110.982025 ,   1.7568434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.256621068311388}
episode index:4357
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.912077  , 122.08076   ,   0.15891713], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.5790700528827024}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2863932606826476
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.334265 , 118.31587  ,   3.7002916], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1486155797371502}
episode index:4358
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.00601719, 76.58400906]), 'previousTarget': array([ 3.74306117, 74.81099734]), 'currentState': array([ 1.2707748, 56.77193  ,  1.5973647], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.28635535647426824
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.620144 , 119.402435 ,   3.0610614], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.726832300183369}
episode index:4359
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.66506007, 115.03125219]), 'previousTarget': array([  9.59490445, 114.93630557]), 'currentState': array([ 8.319926 , 95.07654  ,  5.2585897], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.286437066622344
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.159705 , 118.36361  ,   1.6142428], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.462612346256336}
episode index:4360
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.87124278, 88.00331539]), 'previousTarget': array([11.8480693 , 87.96679883]), 'currentState': array([13.038896  , 68.03743   ,  0.45183715], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2863713851119973
{'currentTarget': array([13.59289144, 88.76975563]), 'previousTarget': array([13.59289144, 88.76975563]), 'currentState': array([15.878719 , 68.90081  ,  2.0466568], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4361
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.3503839 , 116.00946108]), 'previousTarget': array([ 10.5731765, 116.7042351]), 'currentState': array([12.099727, 96.08611 ,  3.31865 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.286318165137901
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.7200365, 121.48718  ,   5.4478297], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.652321006274954}
episode index:4362
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.31104363, 116.22054498]), 'previousTarget': array([ 10.3390904 , 115.93091516]), 'currentState': array([11.951468  , 96.28793   ,  0.27791274], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.28634165029786807
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.848619  , 119.49203   ,   0.79514766], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.917141151119093}
episode index:4363
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.28966068, 85.99477781]), 'previousTarget': array([ 7.424941, 83.949174]), 'currentState': array([ 7.285003  , 66.02003   ,  0.93326175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28627603580421596
{'currentTarget': array([14.60937647, 91.838553  ]), 'previousTarget': array([14.60937647, 91.838553  ]), 'currentState': array([17.839926, 72.10119 ,  2.507968], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4364
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.81070293, 113.19854957]), 'previousTarget': array([ 10.25976679, 112.98629668]), 'currentState': array([ 9.254281 , 93.20629  ,  2.7203565], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862104513744784
{'currentTarget': array([  8.08072847, 113.86083992]), 'previousTarget': array([  8.05373265, 113.76761956]), 'currentState': array([ 2.1130078, 94.771935 ,  4.6558824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4365
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.034997 , 109.92284  ,   4.8705835], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.266959871612721}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.28620387193228636
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.027562 , 119.92624  ,   2.3175485], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0302061141250245}
episode index:4366
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.79563783, 70.88778481]), 'previousTarget': array([ 5.73259233, 70.92481176]), 'currentState': array([ 4.089732 , 50.96067  ,  3.8871489], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.2861880352079311
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.379194 , 119.210686 ,   0.6067863], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8027834769680107}
episode index:4367
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.03225802, 105.54079777]), 'previousTarget': array([  7.03305841, 105.58914087]), 'currentState': array([ 3.0110974, 85.94921  ,  3.9300284], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 201
reward sum = 0.1326398781093821
running average episode reward sum: 0.28615288224156243
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.720581 , 119.946335 ,   5.6620927], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2805439419027997}
episode index:4368
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.076474 , 119.1644   ,   3.7854092], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.18793409899108}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.28630947325043365
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.773165 , 119.62653  ,   2.2734365], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.81206800314844}
episode index:4369
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.9044089, 124.023254 ,   4.256379 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.040197490527115}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.2863124643071984
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.783276 , 118.21384  ,   6.1743135], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.161202835675942}
episode index:4370
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.43413084, 113.17182649]), 'previousTarget': array([ 10.52256629, 112.94535509]), 'currentState': array([11.7031555, 93.21213  ,  5.3169036], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.2864015552668765
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.84809  , 119.73189  ,   2.0646508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8894611188777125}
episode index:4371
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.8992998, 111.01798  ,   3.0282068], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.786687718106737}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.2865367610005755
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.376975 , 120.737495 ,   0.7432246], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7827252888377694}
episode index:4372
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.9644938, 109.11272  ,   6.045884 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.302532299781378}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.2864894041953366
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.706671, 118.0062  ,   3.183727], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.624490954312116}
episode index:4373
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.19689876, 103.87017328]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([14.895999 , 84.05314  ,  3.3066134], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.2864415295812841
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.693108 , 118.37518  ,   0.5236365], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.085185285032019}
episode index:4374
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.8705198 , 104.90125827]), 'previousTarget': array([ 12.24863257, 103.80984546]), 'currentState': array([16.605942  , 85.25319   ,  0.54161495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.28642875113299543
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.019018 , 121.95864  ,   0.5479201], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1905707283766667}
episode index:4375
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.17946447, 64.36656365]), 'previousTarget': array([12.18985467, 65.98358488]), 'currentState': array([12.962373, 44.381893,  4.185011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2863632966651863
{'currentTarget': array([ 6.34469832, 58.4817977 ]), 'previousTarget': array([ 6.34469832, 58.4817977 ]), 'currentState': array([ 5.1584263, 38.51701  ,  1.2985625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4376
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.03207169, 74.8809527 ]), 'previousTarget': array([ 5.83833848, 74.91533358]), 'currentState': array([ 4.2799635, 54.957848 ,  5.6913676], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862978721057471
{'currentTarget': array([ 4.93124318, 80.31895353]), 'previousTarget': array([ 4.92335595, 80.16886424]), 'currentState': array([ 2.3970847, 60.480152 ,  2.7343867], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4377
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.28057326, 81.59990867]), 'previousTarget': array([13.28223316, 81.92609538]), 'currentState': array([14.983    , 61.672497 ,  3.6931937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28623247743418345
{'currentTarget': array([16.34863881, 72.08253709]), 'previousTarget': array([16.34863881, 72.08253709]), 'currentState': array([18.975506 , 52.2558   ,  4.2206326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4378
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.65105193, 109.11743804]), 'previousTarget': array([  7.31857996, 110.27985236]), 'currentState': array([ 3.4313269, 89.56766  ,  5.629694 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861671126300194
{'currentTarget': array([ 10.44616903, 116.58035621]), 'previousTarget': array([ 10.43427882, 116.56523573]), 'currentState': array([13.033686, 96.74844 ,  4.265122], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4379
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.223347 , 107.124954 ,   3.8969107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.898449289589031}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.28628296814851567
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.045019 , 118.790436 ,   1.4318879], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2104017099719149}
episode index:4380
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.917852 , 122.09955  ,   1.6070827], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.444960143028147}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.2863881699577334
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.557886, 118.97942 ,   3.819319], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8624138959407286}
episode index:4381
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.8201902, 109.153145 ,   5.9056087], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.483762101692163}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.2864967857825599
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.5088215, 118.82337  ,   2.296268 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8994912666750776}
episode index:4382
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.21315231, 100.39120134]), 'previousTarget': array([  7.11925147, 101.75525931]), 'currentState': array([ 2.4208264, 80.754036 ,  3.479865 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.2864571874410565
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.378662 , 120.840965 ,   1.8918141], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.61490922309827}
episode index:4383
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.79115739, 82.03290776]), 'previousTarget': array([ 8.68924552, 81.98811999]), 'currentState': array([ 8.1546955, 62.043037 ,  5.989133 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.28642120275895083
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.109785 , 118.56465  ,   2.5177202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8143451355745088}
episode index:4384
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.84892491, 118.11663962]), 'previousTarget': array([ 10.79270645, 118.2384301 ]), 'currentState': array([1.906759e+01, 9.988333e+01, 8.291049e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.2863917687975709
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.433106 , 118.52051  ,   6.1470532], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.155006407489653}
episode index:4385
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.83324941, 82.85862261]), 'previousTarget': array([15.98404021, 80.77129198]), 'currentState': array([18.936317 , 63.100815 ,  1.6966069], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28632647199665945
{'currentTarget': array([  9.22995544, 111.4698894 ]), 'previousTarget': array([  9.15450658, 111.50310907]), 'currentState': array([ 7.4317937, 91.55089  ,  3.9269602], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4386
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.18928276, 111.91455983]), 'previousTarget': array([ 11.17157288, 111.79898987]), 'currentState': array([14.099755 , 92.127464 ,  4.2740254], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862612049640639
{'currentTarget': array([16.61240569, 72.22802812]), 'previousTarget': array([16.65737097, 71.99177121]), 'currentState': array([19.354582 , 52.41691  ,  6.1276517], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4387
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.38379605, 108.64088873]), 'previousTarget': array([  7.40521743, 108.50882004]), 'currentState': array([ 2.8949623, 89.15114  ,  6.2463098], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861959676794322
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.537547, 116.30616 ,   2.117514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9728104151908328}
episode index:4388
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.0647135, 109.04452  ,   3.952842 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.125103610989159}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.28624464415749545
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.821654 , 118.63836  ,   1.1728474], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2743109909249077}
episode index:4389
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.85804276, 106.39503294]), 'previousTarget': array([  7.3792763, 104.7124451]), 'currentState': array([ 4.7475696, 86.63839  ,  1.3603399], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.2863412975932847
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.601584 , 118.58639  ,   2.5995438], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5362954638896296}
episode index:4390
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.4919414 , 68.89430942]), 'previousTarget': array([ 8.5631569 , 68.99206979]), 'currentState': array([ 7.9020257, 48.90301  ,  5.675575 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862760866396083
{'currentTarget': array([  7.15900577, 106.97353384]), 'previousTarget': array([  7.09076722, 106.91619511]), 'currentState': array([ 2.8973022, 87.43286  ,  4.032565 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4391
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.1273418 , 98.08370061]), 'previousTarget': array([11.04869701, 97.97736275]), 'currentState': array([12.154754 , 78.11011  ,  5.4197936], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286210905381266
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.898809, 112.96806 ,   1.3723  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.283790925694009}
episode index:4392
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.806606 , 109.89621  ,   3.6707115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.486353845322562}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.28633193794038636
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.2227125, 121.288704 ,   2.2690759], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.776452557246718}
episode index:4393
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.43727045, 81.92411901]), 'previousTarget': array([15.26725206, 81.81242258]), 'currentState': array([18.264606 , 62.124973 ,  5.0423717], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862667736395351
{'currentTarget': array([ 11.22155272, 117.25508982]), 'previousTarget': array([ 11.20169159, 117.2964912 ]), 'currentState': array([19.353174, 98.982796,  5.436711], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4394
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.98832127, 87.06108787]), 'previousTarget': array([ 4.98505385, 86.77598173]), 'currentState': array([ 1.9799298, 67.28864  ,  3.0120487], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28620163899251816
{'currentTarget': array([ 4.60076123, 90.73128254]), 'previousTarget': array([ 4.74550337, 90.95703928]), 'currentState': array([ 0.9725519, 71.06313  ,  1.7088877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4395
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.43298276, 114.94360001]), 'previousTarget': array([ 10.71202025, 115.72787848]), 'currentState': array([12.139351 , 95.016525 ,  3.5201254], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.2861712654923551
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.020418 , 121.23253  ,   1.8527961], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.331924354732474}
episode index:4396
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.26998478, 71.84849815]), 'previousTarget': array([ 7.17444044, 71.96548746]), 'currentState': array([ 6.1378756, 51.880566 ,  2.554017 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28610618219340306
{'currentTarget': array([12.26915981, 44.13527991]), 'previousTarget': array([12.38202648, 43.96546769]), 'currentState': array([12.867105 , 24.14422  ,  5.6939607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4397
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.60062516, 65.00529481]), 'previousTarget': array([16.61709559, 64.85753677]), 'currentState': array([18.983978, 45.14781 ,  2.778944], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2860411284912217
{'currentTarget': array([ 7.61712044, 82.4267051 ]), 'previousTarget': array([ 7.49778648, 82.35004622]), 'currentState': array([ 6.3512735, 62.466805 ,  2.6096957], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4398
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.21901106, 75.45947061]), 'previousTarget': array([14.18928508, 73.91786413]), 'currentState': array([17.546576  , 55.59537   ,  0.56721705], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285976104365627
{'currentTarget': array([ 3.02660921, 62.08653983]), 'previousTarget': array([ 3.08466179, 62.19659144]), 'currentState': array([ 0.6356689, 42.22997  ,  2.691744 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4399
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.34198363, 106.0491961 ]), 'previousTarget': array([ 14.0191792 , 104.36985865]), 'currentState': array([18.001257 , 86.59949  ,  2.6390886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.28596246164998346
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.259489 , 120.02353  ,   2.9697216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.259708818414952}
episode index:4400
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.81566754, 85.62048206]), 'previousTarget': array([12.575059, 83.949174]), 'currentState': array([12.870447 , 65.648315 ,  2.0929756], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858974849488587
{'currentTarget': array([  9.97489701, 116.98776226]), 'previousTarget': array([ 10.27012877, 116.75345342]), 'currentState': array([ 9.808229 , 96.98846  ,  2.8603244], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4401
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.33468551, 88.78303743]), 'previousTarget': array([12.46607002, 87.94108971]), 'currentState': array([12.189008 , 68.80129  ,  2.6623864], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858325377691793
{'currentTarget': array([ 6.04753431, 90.12436557]), 'previousTarget': array([ 6.07394124, 90.12025176]), 'currentState': array([ 3.4244442 , 70.29713   ,  0.09499268], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4402
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.6383684, 108.976555 ,   2.5778854], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.835939652497279}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.2859407617475072
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.01631   , 119.11104   ,   0.95092416], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3502364798431208}
episode index:4403
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.28453848, 86.50239927]), 'previousTarget': array([ 4.34829399, 86.71773129]), 'currentState': array([ 0.9206915, 66.787315 ,  3.904911 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28587583423575713
{'currentTarget': array([ 5.62253287, 89.76090143]), 'previousTarget': array([ 5.6050215 , 89.79349901]), 'currentState': array([ 2.757164, 69.967224,  6.072948], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4404
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.54260863, 97.78586955]), 'previousTarget': array([ 9.47605556, 97.99433347]), 'currentState': array([ 9.130894 , 77.79011  ,  2.0208118], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.2858429190436274
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.534382 , 118.26868  ,   1.4185846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7928414361634142}
episode index:4405
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.124158  , 72.97464507]), 'previousTarget': array([ 5.81071492, 73.91786413]), 'currentState': array([ 5.903336, 53.01194 ,  6.028053], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857780432108894
{'currentTarget': array([14.76046372, 88.13484449]), 'previousTarget': array([14.93205547, 88.08595757]), 'currentState': array([17.71555, 68.35436,  6.03884], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4406
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0870495, 104.844055 ,   5.9481974], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.156194810290339}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.2858634763848792
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.538238, 119.82889 ,   6.157167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4717434334850732}
episode index:4407
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.00918863, 109.08144392]), 'previousTarget': array([  7.27393758, 107.53800035]), 'currentState': array([ 4.421678, 89.40583 ,  0.601773], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.2859291504487612
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.206799 , 118.0456   ,   2.0450013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6524040613499995}
episode index:4408
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.165526 , 118.25963  ,   3.1978698], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.450830813280275}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.2859934897665268
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.514063 , 118.07037  ,   3.9784026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4354604111032967}
episode index:4409
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.906302 , 113.1869   ,   5.6324296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.900329197155932}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2860834129196433
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.03331  , 118.6308   ,   2.0394032], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3696067822721334}
episode index:4410
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.71768675, 80.51309172]), 'previousTarget': array([15.98404021, 80.77129198]), 'currentState': array([17.090307 , 60.654324 ,  3.0496504], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.2860412507027632
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.439691 , 119.025665 ,   2.0306969], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8395362447060737}
episode index:4411
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.596831, 111.39338 ,   2.286771], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.266374347454054}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2861834710102838
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.995001 , 119.15362  ,   2.1928267], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.167115843082106}
episode index:4412
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.25093523, 83.84554372]), 'previousTarget': array([15.17157288, 83.79898987]), 'currentState': array([18.1255   , 64.0532   ,  4.3745193], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861186209148815
{'currentTarget': array([  8.04211693, 112.51639745]), 'previousTarget': array([  8.03518754, 112.42374504]), 'currentState': array([ 2.9800298, 93.16762  ,  3.1818361], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4413
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.0104823, 111.912735 ,   2.9766407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.622126850639177}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.28618546454723387
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.569053 , 119.11719  ,   1.9564687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0503232270309162}
episode index:4414
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.36607909, 113.88696355]), 'previousTarget': array([ 10.61709559, 114.85753677]), 'currentState': array([11.561637 , 93.92273  ,  3.7427435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.2861994866650017
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.836083 , 118.15358  ,   1.6356686], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.603933596830513}
episode index:4415
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.04869701, 118.97736275]), 'currentState': array([ 12.560818, 100.32413 ,   0.768943], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.841818413058714}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.2863333909984107
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.273182 , 118.86968  ,   2.2021592], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3438313153259105}
episode index:4416
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.68010704, 89.58778833]), 'previousTarget': array([13.08575187, 87.90818058]), 'currentState': array([16.082731 , 69.73263  ,  1.2916172], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862685656891514
{'currentTarget': array([ 8.60533082, 88.0576465 ]), 'previousTarget': array([ 8.81068878, 87.74273733]), 'currentState': array([ 7.7329206, 68.07668  ,  2.2764308], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4417
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.78152874, 104.73223381]), 'previousTarget': array([ 10.45965677, 102.9926994 ]), 'currentState': array([11.803953 , 84.758385 ,  1.0690793], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862037697258899
{'currentTarget': array([  8.79196903, 116.38583191]), 'previousTarget': array([  8.79196903, 116.38583191]), 'currentState': array([ 2.4517891, 97.41738  ,  4.4699655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4418
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.590334 , 108.6784479]), 'previousTarget': array([ 10., 107.]), 'currentState': array([ 8.867115 , 88.69153  ,  1.8586504], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.2862772961956358
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.220349 , 119.37177  ,   1.3383279], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8872800611742413}
episode index:4419
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.35620205, 115.22669908]), 'previousTarget': array([ 11.4, 115.2]), 'currentState': array([16.822306  , 95.98815   ,  0.41820997], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286212527576587
{'currentTarget': array([  7.63014492, 111.12744047]), 'previousTarget': array([  7.81515158, 113.01498905]), 'currentState': array([ 2.4690864, 91.804825 ,  5.5857296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4420
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.9265547 , 104.65376282]), 'previousTarget': array([ 10.85900419, 104.96742669]), 'currentState': array([12.131893, 84.69012 ,  6.036194], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861477882579766
{'currentTarget': array([  7.75512182, 112.97080715]), 'previousTarget': array([  7.79882125, 113.1422079 ]), 'currentState': array([ 1.6705825, 93.918816 ,  5.2978644], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4421
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.40733367, 79.79255172]), 'previousTarget': array([ 5.31761399, 79.86526278]), 'currentState': array([ 3.137607 , 59.92176  ,  3.5074117], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2860830782199264
{'currentTarget': array([ 4.63932491, 90.33383273]), 'previousTarget': array([ 4.6682032 , 90.40411784]), 'currentState': array([ 1.0829221, 70.65257  ,  4.31943  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4422
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.5331745, 117.02177  ,   1.0180238], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.3497011664858234}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.286237773205633
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.9236965, 119.51151  ,   1.3891917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9847488708555112}
episode index:4423
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.51374973, 80.74695763]), 'previousTarget': array([ 4.68727346, 80.81864176]), 'currentState': array([ 1.745334 , 60.939487 ,  2.0302076], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861730720814906
{'currentTarget': array([ 10.60872857, 116.16868054]), 'previousTarget': array([ 10.27762647, 116.56809511]), 'currentState': array([13.747009, 96.416435,  5.951658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4424
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.33855188, 82.42398701]), 'previousTarget': array([10.66106563, 80.99712788]), 'currentState': array([12.05055   , 62.436665  ,  0.85162276], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28610840020079426
{'currentTarget': array([ 10.88920714, 117.57008546]), 'previousTarget': array([ 10.84371869, 117.66105064]), 'currentState': array([17.762297 , 98.78816  ,  1.8176938], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4425
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.63882622, 72.99060323]), 'previousTarget': array([ 6.4883985 , 72.94453985]), 'currentState': array([ 5.2124667, 53.04153  ,  2.2313259], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28604375754372224
{'currentTarget': array([ 4.55682604, 90.7918469 ]), 'previousTarget': array([ 4.45392313, 90.65025576]), 'currentState': array([ 0.8927478, 71.13035  ,  4.4987288], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4426
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.98756415, 69.91445098]), 'previousTarget': array([10., 70.]), 'currentState': array([ 9.982598 , 49.91445  ,  3.7609227], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28597914409047087
{'currentTarget': array([ 11.24814507, 115.71589102]), 'previousTarget': array([ 11.29774346, 115.71882655]), 'currentState': array([16.842417 , 96.51422  ,  1.4875721], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4427
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.42689511, 111.32641377]), 'previousTarget': array([  7.49442256, 111.23047895]), 'currentState': array([ 1.7387204, 92.15235  ,  4.666378 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859145598212544
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.674756 , 113.58668  ,   6.0553656], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.224122933455046}
episode index:4428
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.63515364, 80.10147561]), 'previousTarget': array([12.66961979, 79.95570316]), 'currentState': array([13.95321   , 60.144955  ,  0.52730983], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28585000471630495
{'currentTarget': array([15.1978568 , 74.45480275]), 'previousTarget': array([15.16220762, 74.58273793]), 'currentState': array([17.465641, 54.58379 ,  5.166359], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4429
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.19814312, 99.24425986]), 'previousTarget': array([ 5.2881459 , 98.53488686]), 'currentState': array([ 2.5946689 , 79.57156   ,  0.73014396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.2858825191652457
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.164367 , 118.086235 ,   1.8310814], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6518004880810486}
episode index:4430
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.41854111, 97.89413929]), 'previousTarget': array([ 9.47605556, 97.99433347]), 'currentState': array([ 8.892655, 77.901054,  5.69554 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858180004292572
{'currentTarget': array([11.80841939, 97.28862701]), 'previousTarget': array([11.80841939, 97.28862701]), 'currentState': array([13.395918 , 77.35173  ,  4.9727054], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4431
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.2233247 , 82.65700352]), 'previousTarget': array([15.12120309, 84.79172879]), 'currentState': array([17.99384 , 62.849827,  4.520689], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857535108082217
{'currentTarget': array([  7.61202375, 113.60573079]), 'previousTarget': array([  7.61202375, 113.60573079]), 'currentState': array([ 0.61492836, 94.86965   ,  4.038512  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4432
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.1705203, 123.83896  ,   0.7330959], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.422408960857425}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2858951222985613
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.471981, 121.96838 ,   5.042808], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.024179056127813}
episode index:4433
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.16961281, 110.65544144]), 'previousTarget': array([ 11.26725206, 110.81242258]), 'currentState': array([13.653534 , 90.81029  ,  5.2062645], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.2858647338936571
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.816333 , 118.56663  ,   1.3052561], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.649532050308593}
episode index:4434
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.2212577, 108.55613  ,   0.7312189], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.837304832076267}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28600833704146633
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.403327 , 120.661415 ,   1.3152833], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7282461170745103}
episode index:4435
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.837953 , 117.01816  ,   3.7138238], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.8601710076474784}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.2860816257931551
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.991699 , 118.90018  ,   5.4737196], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.275187114283609}
episode index:4436
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.996127 , 114.939766 ,   3.0911856], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.447867932064927}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.28621492293023104
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.374708 , 119.46196  ,   1.9002302], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7120340911503296}
episode index:4437
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.724986, 119.0708  ,   4.856271], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.374832023687812}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.28632394235854897
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.241935, 120.25452 ,   0.867597], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7996509137398192}
episode index:4438
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.928425 , 111.498634 ,   1.7666993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.717341364064275}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28646731265637937
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.406947 , 118.31463  ,   1.6628456], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7338058494983926}
episode index:4439
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.852486 , 123.84719  ,   6.1020675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.2699625991446934}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2866148380700606
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.061231, 121.811745,   4.194824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0996736256288884}
episode index:4440
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.85125301, 104.44440748]), 'previousTarget': array([ 13.41921333, 105.46834337]), 'currentState': array([16.45707  , 84.77214  ,  4.0365367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2865502997142691
{'currentTarget': array([  8.38489505, 115.61788463]), 'previousTarget': array([  8.38489505, 115.61788463]), 'currentState': array([ 1.4683695, 96.85191  ,  1.8943219], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4441
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.83885944, 113.8076798 ]), 'previousTarget': array([  7.77438937, 113.81774824]), 'currentState': array([ 1.2486193, 94.92465  ,  2.0310519], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28648579041671973
{'currentTarget': array([  8.57052074, 116.71663354]), 'previousTarget': array([  8.57052074, 116.71663354]), 'currentState': array([ 0.58694065, 98.37917   ,  3.3529835 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4442
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.11963182, 69.98281476]), 'previousTarget': array([ 4.99007438, 69.9007438 ]), 'currentState': array([ 3.1773791, 50.077347 ,  3.5966356], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864213101577918
{'currentTarget': array([15.64964548, 89.91900336]), 'previousTarget': array([15.562616  , 89.77814427]), 'currentState': array([19.341387 , 70.26268  ,  2.6154652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4443
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.58920287, 101.98434159]), 'previousTarget': array([  7.60909025, 101.82908591]), 'currentState': array([ 4.936513, 82.16104 ,  5.128871], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 251
reward sum = 0.08024793100055946
running average episode reward sum: 0.2863749165081165
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.586153 , 119.165764 ,   1.5830244], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.931246079136911}
episode index:4444
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.19077948, 104.03231716]), 'previousTarget': array([ 12.24863257, 103.80984546]), 'currentState': array([14.909328, 84.21794 ,  5.420722], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28631049020519
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.99451 , 106.15542 ,   2.228849], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.881046449725247}
episode index:4445
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.7839144, 122.20464  ,   4.793341 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.506733431500841}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.2863863370123619
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.485098, 121.391556,   6.033938], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.057025990003954}
episode index:4446
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.087254 , 111.89714  ,   2.6818128], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.13464296665241}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2865336484160922
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.95017  , 119.154686 ,   1.8155402], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2717617706775444}
episode index:4447
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.618018 , 119.22114  ,   3.2840548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8674884136177384}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2866940500239123
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.618018 , 119.22114  ,   3.2840548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8674884136177384}
episode index:4448
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.50890191, 113.42672538]), 'previousTarget': array([  8.49719013, 113.48782391]), 'currentState': array([ 4.0844593, 93.92226  ,  3.8911812], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.2866715679440498
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.235052, 120.07385 ,   5.188916], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7685046999409889}
episode index:4449
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.03556371, 69.63716638]), 'previousTarget': array([ 4.99007438, 69.9007438 ]), 'currentState': array([ 3.0736043, 49.73363  ,  5.830614 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866071473669837
{'currentTarget': array([14.01666661, 92.06594742]), 'previousTarget': array([14.06427759, 91.87843185]), 'currentState': array([16.86321 , 72.269554,  5.929958], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4450
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.8937496e+01, 1.1597245e+02, 7.5603843e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.803060491494225}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.28668005459955304
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.491761 , 119.80826  ,   3.7919238], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5278201001518668}
episode index:4451
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.24148542, 88.71948156]), 'previousTarget': array([ 9.37729134, 86.99644096]), 'currentState': array([10.395881  , 68.72008   ,  0.86395264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28661566105629166
{'currentTarget': array([ 4.80995717, 91.07392194]), 'previousTarget': array([ 4.80995717, 91.07392194]), 'currentState': array([ 1.2778741, 71.38828  ,  0.6829721], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4452
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.59910865, 105.01379084]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([18.269535  , 85.56676   ,  0.81536484], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28655129643445104
{'currentTarget': array([  8.86777425, 116.33751108]), 'previousTarget': array([  8.87752107, 116.41019993]), 'currentState': array([ 2.960774 , 97.22973  ,  4.5383863], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4453
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.41282568, 78.50523058]), 'previousTarget': array([10.67746131, 77.99739905]), 'currentState': array([ 9.129843 , 58.507233 ,  2.8718708], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864869607145511
{'currentTarget': array([ 6.58505262, 82.13960598]), 'previousTarget': array([ 6.37977073, 81.90349486]), 'currentState': array([ 4.7883782, 62.22047  ,  6.127549 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4454
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.3988109, 113.29121  ,   0.9735013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.411886639874718}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.28663187168810716
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.742732 , 118.563644 ,   1.4711361], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.908884513615182}
episode index:4455
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.91563962, 79.85942752]), 'previousTarget': array([ 3.96680906, 79.77872706]), 'currentState': array([9.1834956e-01, 6.0085297e+01, 5.1214695e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28656754676178575
{'currentTarget': array([  8.26952755, 115.66486849]), 'previousTarget': array([  8.34838558, 115.90782743]), 'currentState': array([ 0.85494  , 97.09005  ,  2.4412003], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4456
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.022846 , 109.02454  ,   5.9612913], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.384122228310261}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2866563928574215
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.592114 , 118.47422  ,   1.1312119], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6366439095654222}
episode index:4457
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.78081963, 79.04490847]), 'previousTarget': array([ 8.65538554, 78.98925886]), 'currentState': array([ 8.185709 , 59.053764 ,  0.1368038], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28659209128881286
{'currentTarget': array([ 4.93945961, 89.57235161]), 'previousTarget': array([ 4.93945961, 89.57235161]), 'currentState': array([ 1.658252  , 69.843346  ,  0.93586135], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4458
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.28728989, 110.29993294]), 'previousTarget': array([ 12.30718934, 110.44164417]), 'currentState': array([16.877434 , 90.83379  ,  5.6090074], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.2866132742954995
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.35637 , 121.87514 ,   0.768552], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.493523545343141}
episode index:4459
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.76511237, 76.43871065]), 'previousTarget': array([12.77155462, 74.9622374 ]), 'currentState': array([15.487342 , 56.513    ,  1.0960357], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 233
reward sum = 0.09616130339314856
running average episode reward sum: 0.28657057205987113
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.706806 , 121.52579  ,   1.8845799], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.289369868326626}
episode index:4460
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.74982857, 101.28284687]), 'previousTarget': array([14.6097561 , 99.51219512]), 'currentState': array([17.678596 , 81.67252  ,  2.6180563], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.286612887565351
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.239647 , 120.33745  ,   5.3527737], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7924045942631865}
episode index:4461
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.62448338, 99.55409196]), 'previousTarget': array([14.6097561 , 99.51219512]), 'currentState': array([19.03666  , 80.046844 ,  2.5171196], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 259
reward sum = 0.07404835256958406
running average episode reward sum: 0.28656524871842237
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.957031  , 121.28499   ,   0.37435415], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3411891231048054}
episode index:4462
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.23651349, 107.22769574]), 'previousTarget': array([  6.58078667, 105.46834337]), 'currentState': array([ 3.007069, 87.680016,  1.153906], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.28667357896648127
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.155578 , 118.94994  ,   2.1330462], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5614026679114856}
episode index:4463
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.54300816, 99.74614492]), 'previousTarget': array([ 6.44760664, 99.70060934]), 'currentState': array([ 3.178009, 80.03126 ,  6.279757], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.286659975601017
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.188743 , 120.02571  ,   1.8122425], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.189020654620401}
episode index:4464
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.0440719 , 81.82394855]), 'previousTarget': array([16.03319094, 79.77872706]), 'currentState': array([19.17154  , 62.06999  ,  1.6825932], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 262
reward sum = 0.07184904244991483
running average episode reward sum: 0.2866118656495834
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.623679 , 118.765465 ,   1.6722236], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8488743208734861}
episode index:4465
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.2980604 , 75.20268004]), 'previousTarget': array([11.38490648, 74.99053926]), 'currentState': array([11.877343, 55.21107 ,  4.987029], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28654768923542095
{'currentTarget': array([ 4.28647682, 89.47012223]), 'previousTarget': array([ 4.30928192, 89.77254952]), 'currentState': array([ 0.60744244, 69.81142   ,  2.297528  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4466
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.46661655, 110.70733593]), 'previousTarget': array([  8.39813833, 110.70920231]), 'currentState': array([ 5.210447 , 90.97418  ,  4.0102463], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.28650079829140807
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.251058 , 120.04726  ,   2.2992938], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2519498210964795}
episode index:4467
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.03126949, 117.02652786]), 'previousTarget': array([ 10., 117.]), 'currentState': array([1.0241581e+01, 9.7027634e+01, 2.1146949e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864366754627842
{'currentTarget': array([  8.80709321, 117.00394812]), 'previousTarget': array([  8.80709321, 117.00394812]), 'currentState': array([ 1.4087694, 98.422646 ,  1.4708011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4468
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 14.55763891, 100.39611035]), 'previousTarget': array([ 14.50280987, 100.48782391]), 'currentState': array([19.086584  , 80.91564   ,  0.26874065], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 292
reward sum = 0.0531467635277924
running average episode reward sum: 0.2863844736476276
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.179267 , 118.5856   ,   1.5144008], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.305556584196125}
episode index:4469
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.58917133, 87.70701012]), 'previousTarget': array([15.58917133, 87.70701012]), 'currentState': array([19.       , 68.       ,  1.5285051], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2863204055327176
{'currentTarget': array([ 4.64363947, 90.84662484]), 'previousTarget': array([ 4.57902082, 90.91557454]), 'currentState': array([ 1.0295262, 71.17588  ,  4.4180417], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4470
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.04645106, 83.66375743]), 'previousTarget': array([ 4.17356191, 83.74660743]), 'currentState': array([ 0.8126496, 63.926926 ,  0.2297718], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862563660772193
{'currentTarget': array([ 13.11787875, 109.96670363]), 'previousTarget': array([ 13.39766767, 108.07099424]), 'currentState': array([19.052977, 90.86763 ,  1.372778], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4471
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.33763836, 105.2920844 ]), 'previousTarget': array([  5.8507125, 103.40285  ]), 'currentState': array([ 1.5050812, 85.884705 ,  1.3537377], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861923552619069
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.40007  , 121.369774 ,   1.6520492], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.511019894506585}
episode index:4472
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.20819246, 115.29025212]), 'previousTarget': array([ 11.80053053, 113.31231517]), 'currentState': array([16.17788  , 95.917534 ,  1.5933924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28612837306757155
{'currentTarget': array([ 10.39633954, 101.17434109]), 'previousTarget': array([ 10.35501104, 101.29341171]), 'currentState': array([10.817309, 81.17877 ,  2.978129], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4473
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.71059605, 117.44746601]), 'previousTarget': array([ 10.73765188, 117.29527642]), 'currentState': array([16.074396 , 98.180145 ,  5.8819113], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2860644194750218
{'currentTarget': array([11.85754755, 97.33374863]), 'previousTarget': array([11.85754755, 97.33374863]), 'currentState': array([13.491114 , 77.40057  ,  0.5354204], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4474
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.7681208 , 112.20057626]), 'previousTarget': array([ 12.50557744, 111.23047895]), 'currentState': array([16.1899   , 92.6955   ,  3.0834568], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 206
reward sum = 0.12613920430197118
running average episode reward sum: 0.2860286819967709
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.323779  , 118.12707   ,   0.19243306], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.293527201602288}
episode index:4475
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.759286, 115.866486,   1.443781], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.303708738809692}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.2861751188751007
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.634153 , 120.49767  ,   2.5697188], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7082551546344593}
episode index:4476
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.95486338, 77.21623643]), 'previousTarget': array([11.36539906, 76.98992951]), 'currentState': array([ 9.9337635, 57.216248 ,  3.0145884], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861111976959908
{'currentTarget': array([  8.59639977, 116.53130745]), 'previousTarget': array([  8.55383709, 116.5159542 ]), 'currentState': array([ 1.0943558, 97.99164  ,  3.3257046], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4477
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.1956544, 111.98347  ,   5.73611  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.89723333251251}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28625336685560043
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.228534 , 118.56125  ,   1.3815566], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4567885047571367}
episode index:4478
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.9339905, 118.29872  ,   5.7703996], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.506388962316041}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.28640608970292003
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.5824  , 119.24995 ,   0.591674], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6037982911428081}
episode index:4479
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.61142961, 76.06682264]), 'previousTarget': array([ 6.51082225, 73.94285376]), 'currentState': array([ 5.0733957, 56.12605  ,  1.6879452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.2863593664334171
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.242914 , 119.30268  ,   0.2755043], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8903979316071706}
episode index:4480
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0338745, 117.196594 ,   5.5998044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.803610412910836}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.2864588852915338
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.785394 , 119.811844 ,   2.4522064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.795280882118172}
episode index:4481
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.07643304, 82.89877444]), 'previousTarget': array([ 8.03310171, 81.9732997 ]), 'currentState': array([ 8.578724  , 62.90497   ,  0.67691064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2863949721087378
{'currentTarget': array([ 4.64660783, 88.75811834]), 'previousTarget': array([ 4.68868881, 88.96764127]), 'currentState': array([ 1.2687775, 69.045425 ,  4.5873704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4482
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.90297423, 87.88777604]), 'previousTarget': array([11.86973376, 86.96803691]), 'currentState': array([14.7036495 , 67.969     ,  0.88295406], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28633108743951885
{'currentTarget': array([14.54404407, 91.81066293]), 'previousTarget': array([14.54404407, 91.81066293]), 'currentState': array([17.726902, 72.06555 ,  2.724727], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4483
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.3169956, 118.59098  ,   5.3199224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.829925577985415}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.28639554409931756
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.519662  , 120.66228   ,   0.12236625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8181294551713657}
episode index:4484
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.24545061, 103.68567524]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([17.147638  , 84.070045  ,  0.24485749], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2863316877907112
{'currentTarget': array([  6.34032147, 106.64253781]), 'previousTarget': array([  6.28787182, 106.89665959]), 'currentState': array([ 1.0554844, 87.35341  ,  3.3023107], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4485
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.921813 , 113.06123  ,   3.2916236], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.199989271299265}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.28637706420293
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.0879755, 118.99289  ,   1.8172735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.161043598096879}
episode index:4486
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.28723863, 69.33205155]), 'previousTarget': array([ 8.55534134, 67.99228841]), 'currentState': array([ 9.00592  , 49.33403  ,  1.3492701], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28631324047567286
{'currentTarget': array([ 5.21515929, 87.09620338]), 'previousTarget': array([ 4.85962525, 85.15535878]), 'currentState': array([ 2.3370492, 67.304375 ,  0.8388263], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4487
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.932816 , 116.884575 ,   5.2917047], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.600645044327594}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.2863677396518047
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.893868 , 120.466255 ,   2.2672846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9504183121593484}
episode index:4488
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.30556711, 73.67744731]), 'previousTarget': array([ 4.40662735, 73.85467564]), 'currentState': array([ 1.8653355, 53.826874 ,  1.0151305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.28634723639032794
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.982079 , 121.940285 ,   0.6897421], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1910885202116255}
episode index:4489
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.42890887, 70.80882817]), 'previousTarget': array([11.42053323, 70.99160369]), 'currentState': array([12.009625 , 50.81726  ,  3.4895768], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862834619501519
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.444073, 111.22963 ,   6.105309], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.32266081170989}
episode index:4490
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.7232704, 109.198685 ,   4.4520836], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.038655261038066}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.28629959751811707
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.813345, 118.287926,   1.957523], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8954493810304223}
episode index:4491
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.08726657, 76.77215285]), 'previousTarget': array([14.10381815, 76.90990945]), 'currentState': array([15.969904 , 56.86096  ,  6.2452264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.286235862077886
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.907587 , 119.32555  ,   2.0007792], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.984786475202168}
episode index:4492
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.71535886, 79.73340121]), 'previousTarget': array([12.66961979, 79.95570316]), 'currentState': array([14.060993, 59.77872 ,  2.049397], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28617215500864984
{'currentTarget': array([  8.33076382, 115.38338936]), 'previousTarget': array([  8.49144022, 115.82342118]), 'currentState': array([ 1.53021  , 96.57508  ,  3.5799637], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4493
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.00245546, 90.56324941]), 'previousTarget': array([14.17157288, 90.79898987]), 'currentState': array([16.697021 , 70.7456   ,  3.4059095], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28610847629146946
{'currentTarget': array([15.38069622, 91.81392099]), 'previousTarget': array([15.54901106, 91.89435305]), 'currentState': array([19.130955  , 72.16868   ,  0.37883902], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4494
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.89516672, 88.07032835]), 'previousTarget': array([10., 87.]), 'currentState': array([ 8.203539, 68.08229 ,  2.691612], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2860448259074224
{'currentTarget': array([15.35463227, 92.45800829]), 'previousTarget': array([15.35463227, 92.45800829]), 'currentState': array([19.171507, 72.8256  ,  5.740254], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4495
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.309087 , 117.47544  ,   1.9769933], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.61739551863105}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.2862013995671405
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.492276 , 119.49543  ,   1.4773494], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5899125681531558}
episode index:4496
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.235646, 113.23784 ,   5.268448], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.885640354759001}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.2863196351771094
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.47081  , 118.91749  ,   1.6053468], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1804634742286384}
episode index:4497
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.05681675, 86.19778517]), 'previousTarget': array([10., 87.]), 'currentState': array([ 8.498974 , 66.20557  ,  3.7142622], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28625598030045823
{'currentTarget': array([  7.08785161, 109.69066199]), 'previousTarget': array([  7.06394624, 109.56978232]), 'currentState': array([ 1.6510631, 90.44381  ,  6.209658 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4498
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.9346943, 123.12011  ,   1.4689717], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.741733733134164}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.28638545125677484
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.336659, 120.08659 ,   4.969534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3394614186688485}
episode index:4499
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.73150908, 81.73745048]), 'previousTarget': array([13.94201698, 81.89383588]), 'currentState': array([15.672775  , 61.831886  ,  0.53977704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.28633582094432497
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.475919 , 118.045586 ,   1.6238492], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4490961058588514}
episode index:4500
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.833653 , 114.908104 ,   1.2957816], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.533576140005504}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28648995650954506
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.414791 , 118.17657  ,   1.0847082], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8700158515242662}
episode index:4501
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.45243368, 113.35014611]), 'previousTarget': array([  8.49719013, 113.48782391]), 'currentState': array([ 3.9191384, 93.87069  ,  2.4211023], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864263203574994
{'currentTarget': array([  8.76765956, 115.92246297]), 'previousTarget': array([  8.69101795, 115.85876784]), 'currentState': array([ 2.9816046 , 96.77771   ,  0.23159087], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4502
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.43516965, 115.06208866]), 'previousTarget': array([ 10.84018998, 114.74881264]), 'currentState': array([12.19093 , 95.139305,  3.204007], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.28653371916395015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.595386 , 119.134    ,   1.8737643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9558578349384974}
episode index:4503
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.01489255, 102.9090752 ]), 'previousTarget': array([ 10.92049484, 102.97084547]), 'currentState': array([12.200443 , 82.944244 ,  5.9302278], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.2865590635098177
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.536385 , 118.22894  ,   5.8110514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.344593981748943}
episode index:4504
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.952194  , 114.96002   ,   0.21947163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.586872708002383}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.2866133024619698
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.905466 , 118.9029   ,   1.2338071], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4224962934163563}
episode index:4505
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  5.96404681, 104.16650991]), 'previousTarget': array([  5.9808208 , 104.36985865]), 'currentState': array([ 1.0240123, 84.78621  ,  4.538954 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 279
reward sum = 0.06056466128430853
running average episode reward sum: 0.2865631363187879
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.778841, 118.69284 ,   2.480849], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3257364196259411}
episode index:4506
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.00856115, 113.94760269]), 'previousTarget': array([  9.04114369, 113.76743395]), 'currentState': array([ 5.775466 , 94.210655 ,  3.4383478], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28649955452683784
{'currentTarget': array([  8.33675462, 113.19544919]), 'previousTarget': array([  8.38382564, 113.55582642]), 'currentState': array([ 3.5879333, 93.76741  ,  0.3970124], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4507
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.835957, 114.18181 ,   3.101739], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.579483977432664}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.2865362768195928
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.847279, 118.958015,   3.76975 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3429865363531304}
episode index:4508
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.135325, 111.98168 ,   5.962191], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.514680821347152}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.2866401098225611
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.519033 , 119.28554  ,   1.7471477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8612695219619947}
episode index:4509
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.13785486, 111.69792271]), 'previousTarget': array([ 12.67544468, 111.97366596]), 'currentState': array([17.125317, 92.32977 ,  4.134231], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.2866840901463863
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.210952 , 118.9004   ,   1.5565691], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3534108957704871}
episode index:4510
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.0528338, 122.72776  ,   0.3043375], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.463497478264223}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.286696167728157
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.505651  , 121.93837   ,   0.83864033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4475201344753494}
episode index:4511
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.1892605, 115.91789  ,   3.228326 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.813129626078982}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.2867017007320845
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.504902 , 118.76463  ,   5.6790996], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3308844148316326}
episode index:4512
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.38421812, 77.22559322]), 'previousTarget': array([10.6721752 , 78.99731309]), 'currentState': array([12.031097 , 57.236057 ,  5.4058075], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28663817276826176
{'currentTarget': array([16.9060861 , 59.03482642]), 'previousTarget': array([16.9060861 , 59.03482642]), 'currentState': array([19.157272 , 39.161926 ,  1.0093251], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4513
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.56786684, 106.65127057]), 'previousTarget': array([  7.03305841, 105.58914087]), 'currentState': array([ 1.5875868, 87.28127  ,  2.4779048], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.28662373183216755
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.976976, 119.97734 ,   2.894732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.977106246252991}
episode index:4514
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.4081207 , 90.14924807]), 'previousTarget': array([11.23133756, 87.98522349]), 'currentState': array([12.350513 , 70.17146  ,  1.6200788], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.28663657534302367
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.929445 , 119.99673  ,   0.9631539], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.07063060939721172}
episode index:4515
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.29248216, 113.86817872]), 'previousTarget': array([  9.29248216, 113.86817872]), 'currentState': array([ 7.       , 94.       ,  1.8228557], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.28670573376444153
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.826535 , 121.73608  ,   1.8572845], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.922790053112096}
episode index:4516
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.732899 , 106.76989  ,   1.2464224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.232806133525758}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.286797994548421
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.103358 , 120.97735  ,   2.4718385], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.473977348104899}
episode index:4517
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.73468635, 72.91474492]), 'previousTarget': array([10.71017536, 70.99789993]), 'currentState': array([11.046715 , 52.91718  ,  1.3751665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867345155766307
{'currentTarget': array([  5.96158897, 103.43245744]), 'previousTarget': array([  6.15429615, 103.69744162]), 'currentState': array([ 1.2251809, 84.00139  ,  3.8565462], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4518
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.8949   , 104.778946 ,   3.0009615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.22141692359161}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.28685205760407495
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.165825 , 119.09782  ,   1.8471985], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.228732371437024}
episode index:4519
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.29292664, 101.62635552]), 'previousTarget': array([11.00124766, 99.97504678]), 'currentState': array([12.696826 , 81.67569  ,  1.1499612], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28678859475947227
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.128235  , 115.79257   ,   0.34970117], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.296792367335701}
episode index:4520
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.5340236 , 74.90053209]), 'previousTarget': array([13.46607002, 74.94108971]), 'currentState': array([15.096447  , 54.961655  ,  0.22563642], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.28677666728063766
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.785891 , 119.68456  ,   2.0965097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.813534080696953}
episode index:4521
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.47610972, 72.32780644]), 'previousTarget': array([11.41201897, 71.99135509]), 'currentState': array([12.095088 , 52.337387 ,  3.8017502], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867132491764181
{'currentTarget': array([16.16391561, 62.87703579]), 'previousTarget': array([16.11072709, 62.86470393]), 'currentState': array([18.309582, 42.992466,  3.838953], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4522
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.18437335, 79.82155839]), 'previousTarget': array([11.35517412, 77.98960229]), 'currentState': array([11.773674 , 59.830242 ,  1.9866918], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866498591146944
{'currentTarget': array([ 8.95914707, 87.7239377 ]), 'previousTarget': array([ 8.95850403, 87.7559858 ]), 'currentState': array([ 8.314513 , 67.73433  ,  5.9996214], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4523
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.40864 , 103.73851 ,   1.285247], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.61489930013724}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.28669262914065746
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.732636 , 118.50964  ,   1.2810915], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6606981978900104}
episode index:4524
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.08807279, 114.08756278]), 'previousTarget': array([  9.04114369, 113.76743395]), 'currentState': array([ 6.0393476, 94.3213   ,  2.2865813], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.2867311991601663
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.303233 , 118.16043  ,   2.2574184], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9671040875859314}
episode index:4525
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.69124914, 107.06290268]), 'previousTarget': array([  9.60578253, 106.99082358]), 'currentState': array([ 9.214074 , 87.068596 ,  2.0676684], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28666784714974647
{'currentTarget': array([  6.67824435, 109.66893997]), 'previousTarget': array([  6.70354317, 109.72104184]), 'currentState': array([ 0.55629385, 90.62894   ,  4.064923  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4526
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.507854 , 103.75507  ,   1.9542303], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.252379600614624}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.28673419769862823
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.017938 , 121.277275 ,   2.5001686], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6332877039878644}
episode index:4527
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.0954971, 121.20294  ,   4.3432403], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.99551341704531}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.2867477486961534
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.638327 , 120.28255  ,   3.4309452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.662512255874952}
episode index:4528
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.112249 , 115.26839  ,   5.0614934], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.663414098978737}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.2867685699302908
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.022802 , 121.63641  ,   0.6830895], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5665463022443817}
episode index:4529
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.114452 , 120.51693  ,   4.1350675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.133207458832666}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.28691519718856223
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.336675, 119.96045 ,   3.025735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3372596949894517}
episode index:4530
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.40943094, 113.86816931]), 'previousTarget': array([  9.424941, 111.949174]), 'currentState': array([ 7.4920626, 93.96029  ,  2.2661476], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28685187447896426
{'currentTarget': array([  8.29317511, 115.78542323]), 'previousTarget': array([  8.32980129, 115.83646724]), 'currentState': array([ 0.78582513, 97.2479    ,  1.1832339 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4531
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.48152505, 99.9463776 ]), 'previousTarget': array([10.50015618, 99.99375293]), 'currentState': array([10.961624 , 79.95214  ,  1.7597057], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2867885797140748
{'currentTarget': array([ 12.89087591, 111.44417623]), 'previousTarget': array([ 12.93306079, 111.38274695]), 'currentState': array([19.29298 , 92.49654 ,  4.987688], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4532
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.1285935 , 73.98627622]), 'previousTarget': array([ 7.20990121, 73.96336993]), 'currentState': array([ 5.8829513, 54.025105 ,  1.3001604], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.2867369200441384
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.351637, 119.39942 ,   1.449646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6959474148110859}
episode index:4533
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.0928806 , 84.33218098]), 'previousTarget': array([10., 83.]), 'currentState': array([ 8.584396 , 64.338646 ,  2.4511156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866736785531715
{'currentTarget': array([ 4.71936295, 84.12361679]), 'previousTarget': array([ 4.62085401, 84.60136939]), 'currentState': array([ 1.8069468, 64.33681  ,  3.0657825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4534
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.25625615, 73.93955917]), 'previousTarget': array([ 7.20990121, 73.96336993]), 'currentState': array([ 6.0669975, 53.97495  ,  0.1927867], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.28663003106782686
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.545747 , 120.40553  ,   3.1939893], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6799238597886795}
episode index:4535
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.32326452, 68.9612187 ]), 'previousTarget': array([12.15568295, 68.98217027]), 'currentState': array([13.232715 , 48.981907 ,  5.3129935], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28656684102570434
{'currentTarget': array([ 5.7100434 , 90.01861951]), 'previousTarget': array([ 5.87373022, 89.97846887]), 'currentState': array([ 2.8771496, 70.22027  ,  5.3271217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4536
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.2227665 , 103.77257005]), 'previousTarget': array([  8.57404971, 101.93796297]), 'currentState': array([ 8.265938  , 83.79547   ,  0.43173683], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.2866481926632508
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.231072 , 120.79552  ,   4.8408985], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.828397073872236}
episode index:4537
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.19412222, 90.37506143]), 'previousTarget': array([ 9.40807829, 90.99583637]), 'currentState': array([ 6.9772205, 70.41212  ,  3.472442 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2865850264683052
{'currentTarget': array([  6.24492263, 106.08139897]), 'previousTarget': array([  6.16214019, 106.09636779]), 'currentState': array([ 1.0354136 , 86.77179   ,  0.47703028], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4538
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.059  , 114.69592,   2.76608], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.829649801209586}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.2867335197451353
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.166183 , 118.57373  ,   2.7643278], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4359184245741845}
episode index:4539
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.01722374, 81.40592447]), 'previousTarget': array([ 8.01563705, 80.97419539]), 'currentState': array([ 5.4761024, 61.46539  ,  2.8208735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2866703625821958
{'currentTarget': array([12.85850546, 80.25784554]), 'previousTarget': array([12.85919329, 80.41579258]), 'currentState': array([14.293324, 60.30938 ,  4.598154], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4540
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.70270363, 103.28602299]), 'previousTarget': array([ 13.66139084, 103.5237412 ]), 'currentState': array([18.028494 , 83.75944  ,  1.0207641], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.28662017440067133
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.207533, 118.19859 ,   3.081264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9680118923773338}
episode index:4541
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.74225438, 111.17684309]), 'previousTarget': array([ 12.87981639, 111.04057123]), 'currentState': array([18.678202 , 92.07803  ,  1.4860626], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28655707000296093
{'currentTarget': array([  8.13801963, 113.76959943]), 'previousTarget': array([  8.17924407, 113.88386506]), 'currentState': array([ 2.4112115, 94.60704  ,  3.704478 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4542
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.44928486, 89.18824506]), 'previousTarget': array([12.43617509, 88.93876756]), 'currentState': array([14.034123 , 69.25114  ,  4.9633884], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864939933861872
{'currentTarget': array([  8.25514652, 115.72573379]), 'previousTarget': array([  8.31224548, 115.86510185]), 'currentState': array([ 0.69625956, 97.20917   ,  2.2357934 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4543
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.98404021, 80.77129198]), 'previousTarget': array([15.98404021, 80.77129198]), 'currentState': array([19.       , 61.       ,  0.5169326], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864309445320089
{'currentTarget': array([  8.41048768, 115.96211649]), 'previousTarget': array([  8.41048768, 115.96211649]), 'currentState': array([ 1.0846617, 97.35211  ,  6.2106843], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4544
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.7117313e+01, 1.2159230e+02, 8.7779976e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.293255132804058}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.2864983890868757
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.54907  , 119.711136 ,   3.2394888], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6204198153313764}
episode index:4545
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.87904071, 80.88395252]), 'previousTarget': array([13.97653796, 80.89737675]), 'currentState': array([15.85271  , 60.981575 ,  5.2899895], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2864353670039265
{'currentTarget': array([  8.51126366, 115.56124682]), 'previousTarget': array([  8.39778773, 115.34625913]), 'currentState': array([ 2.1515326, 96.59934  ,  0.9542537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4546
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.3270132 , 76.24707503]), 'previousTarget': array([ 7.24756572, 75.96105157]), 'currentState': array([ 6.107432 , 56.284294 ,  2.3111491], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28637237264126897
{'currentTarget': array([11.81676432, 94.41544839]), 'previousTarget': array([11.68289565, 94.37334228]), 'currentState': array([13.233401, 74.46568 ,  4.473624], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4547
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.11443984, 106.63704403]), 'previousTarget': array([  7.82842712, 104.79898987]), 'currentState': array([ 5.3200507, 86.83322  ,  1.130686 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.28637724777275386
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.330029 , 118.96799  ,   3.8042843], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6834567438106094}
episode index:4548
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.42596955, 113.78853863]), 'previousTarget': array([ 11.3582148, 112.6656401]), 'currentState': array([15.900975, 94.29561 ,  0.414837], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.286429834987472
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.270377 , 119.40152  ,   3.2193792], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6567209357265542}
episode index:4549
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.33350056, 117.50421121]), 'previousTarget': array([ 10.75140711, 116.54352728]), 'currentState': array([12.982462 , 97.68041  ,  3.1930299], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 286
reward sum = 0.05645022209082803
running average episode reward sum: 0.28637929001760465
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.846168 , 118.7004   ,   1.5606761], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5507921563123073}
episode index:4550
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.10493717, 76.70617257]), 'previousTarget': array([10., 78.]), 'currentState': array([ 8.691543, 56.710445,  4.166011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28631636334434213
{'currentTarget': array([ 5.75328541, 88.45820915]), 'previousTarget': array([ 5.75841259, 88.23852046]), 'currentState': array([ 3.0846105 , 68.637054  ,  0.09710424], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4551
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.89030019, 114.8665958 ]), 'previousTarget': array([ 11.97753117, 114.72658355]), 'currentState': array([18.801336, 96.0986  ,  5.356615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862534643190029
{'currentTarget': array([  8.48054912, 115.64005265]), 'previousTarget': array([  8.52213275, 115.81168691]), 'currentState': array([ 1.8987483, 96.75408  ,  4.0170794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4552
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.9885035, 115.67673  ,   4.4176044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.618593845511113}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.2862701826483553
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.795204 , 119.571495 ,   1.7442515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2787296417632217}
episode index:4553
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.014329 , 117.78603  ,   3.0579371], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.481344905302457}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2864225387786477
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.109463 , 118.27136  ,   2.9647672], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.054043779657121}
episode index:4554
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.59480396, 115.08874345]), 'previousTarget': array([  8.6, 115.2]), 'currentState': array([ 3.0932152, 95.86031  ,  4.0055494], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 270
reward sum = 0.06629832272038531
running average episode reward sum: 0.28637421293538573
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.350116 , 118.10582  ,   1.9048637], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.511978692817376}
episode index:4555
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.33223748, 115.3790172 ]), 'previousTarget': array([ 11.4, 115.2]), 'currentState': array([16.872616 , 96.16173  ,  5.3300686], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.28637019021896376
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.979631 , 118.92722  ,   1.7688386], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2516195560898593}
episode index:4556
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.398857, 122.17775 ,   5.431676], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2591959263444976}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.286524596584946
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.12970028e+01, 1.21913536e+02, 9.09148976e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3116739695248008}
episode index:4557
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.966675 , 111.087074 ,   6.1111217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.312594871572786}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28646173467257546
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.664356 , 118.91785  ,   2.1420329], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.506789865066391}
episode index:4558
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.92554775, 81.55653367]), 'previousTarget': array([ 5.99007438, 79.9007438 ]), 'currentState': array([ 5.3311715, 61.620186 ,  0.9167585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.28641079458120267
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.481925 , 120.82471  ,   1.4366927], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7276265107333262}
episode index:4559
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.30997942, 76.67425764]), 'previousTarget': array([ 8.62469505, 75.99024152]), 'currentState': array([ 6.0706005, 56.712696 ,  3.084494 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28634798519642607
{'currentTarget': array([ 6.09826818, 89.27562357]), 'previousTarget': array([ 6.06013949, 89.21387191]), 'currentState': array([ 3.5786753, 69.43497  ,  0.9071577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4560
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.54135516, 70.87210652]), 'previousTarget': array([11.42053323, 70.99160369]), 'currentState': array([12.168533 , 50.881943 ,  3.5185454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28628520335358537
{'currentTarget': array([ 4.85311386, 59.28951666]), 'previousTarget': array([ 4.80877389, 59.16005   ]), 'currentState': array([ 3.1636233, 39.361004 ,  1.2216287], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4561
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.041135, 108.74171 ,   6.194564], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.25836829952315}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2862224490345688
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.051871, 106.90975 ,   4.870647], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.090350879880283}
episode index:4562
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.64714608, 68.26081441]), 'previousTarget': array([ 9.27775099, 67.99807127]), 'currentState': array([10.897284 , 48.26238  ,  6.2209888], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2861597222212805
{'currentTarget': array([12.34172491, 70.24281433]), 'previousTarget': array([12.4832018 , 70.31039574]), 'currentState': array([13.281945, 50.264927,  5.41065 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4563
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.43016588, 79.95961244]), 'previousTarget': array([13.3390904 , 79.93091516]), 'currentState': array([15.137266 , 60.0326   ,  6.1957355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28609702289564043
{'currentTarget': array([ 6.50084535, 88.87765222]), 'previousTarget': array([ 6.41459623, 89.01098268]), 'currentState': array([ 4.26628 , 69.00288 ,  5.492486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4564
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.34322944, 90.16727336]), 'previousTarget': array([13.61709559, 89.85753677]), 'currentState': array([13.909318, 70.22868 ,  2.92413 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2860343510395844
{'currentTarget': array([13.77461342, 91.22560879]), 'previousTarget': array([13.75017053, 90.97316652]), 'currentState': array([16.37592  , 71.3955   ,  1.5532833], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4565
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.57582249, 110.47922109]), 'previousTarget': array([  8.55942675, 108.83555733]), 'currentState': array([ 5.6170177, 90.699295 ,  1.9783218], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 256
reward sum = 0.0763149839065938
running average episode reward sum: 0.2859884203853722
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.570134  , 119.22866   ,   0.36531886], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6246479026392677}
episode index:4566
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.64362461, 84.00203703]), 'previousTarget': array([13.86933753, 83.88618308]), 'currentState': array([15.657684, 64.10371 ,  4.239939], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859257997546769
{'currentTarget': array([ 6.07335701, 88.31862159]), 'previousTarget': array([ 6.02102708, 88.66753642]), 'currentState': array([ 3.6133463, 68.47049  ,  5.1279483], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4567
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.03886369, 89.40669955]), 'previousTarget': array([11.77779873, 90.96262067]), 'currentState': array([11.717617 , 69.41822  ,  4.1453266], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28586320654107034
{'currentTarget': array([  8.06283043, 114.54997623]), 'previousTarget': array([  8.08436616, 114.71328024]), 'currentState': array([ 1.3645301, 95.70501  ,  3.8544273], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4568
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.31131504, 79.9359089 ]), 'previousTarget': array([15.44057325, 77.83555733]), 'currentState': array([17.939728 , 60.109375 ,  1.8017652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858006407265505
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.472889 , 117.46793  ,   0.8788191], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.9292937417386713}
episode index:4569
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.       , 102.       ,   2.6222122], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 18.24828759089466}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.28588448561010477
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.588555 , 119.37029  ,   1.9374042], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8619331256430517}
episode index:4570
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.203262, 102.96459 ,   4.378651], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.054029357450233}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.28599210689074117
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.922324, 120.41918 ,   2.149356], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9674967996612096}
episode index:4571
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.87238   , 96.68195907]), 'previousTarget': array([ 8.90815322, 95.9793708 ]), 'currentState': array([ 9.762921 , 76.68226  ,  6.2354026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.28599438039916386
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.397648  , 120.42427   ,   0.37740755], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7367724687822551}
episode index:4572
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.55713591, 87.68777363]), 'previousTarget': array([ 5.66824024, 87.82121323]), 'currentState': array([ 2.8328094, 67.87419  ,  4.858738 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859318406264984
{'currentTarget': array([ 6.5559796 , 87.96419935]), 'previousTarget': array([ 6.41931516, 87.79782789]), 'currentState': array([ 4.4181905, 68.07878  ,  1.5624001], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4573
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.0862403, 116.939766 ,   2.9624496], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.225521056678645}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.2860141209501445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.579598  , 119.04912   ,   0.93991715], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1136026375847243}
episode index:4574
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1731298, 106.75227  ,   4.4010305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.919041849979232}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.28612682400338796
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.1744795, 118.10248  ,   1.2487458], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.905526889667246}
episode index:4575
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.01855565, 119.79454546]), 'previousTarget': array([ 10., 119.]), 'currentState': array([11.817535  , 99.87562   ,  0.63455564], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.2861303811737695
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.726126 , 119.12338  ,   1.5228621], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.935966971027136}
episode index:4576
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.12001014, 114.15580258]), 'previousTarget': array([  8.09551454, 114.04848294]), 'currentState': array([ 1.995404, 95.11665 ,  3.500239], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28606786634283793
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.6780882, 114.18075  ,   2.900052 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.989164382782562}
episode index:4577
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.16417632, 70.79886398]), 'previousTarget': array([12.13125551, 70.98112317]), 'currentState': array([13.043053, 50.818184,  4.724317], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28600537882288535
{'currentTarget': array([ 5.10988911, 90.0228037 ]), 'previousTarget': array([ 4.99699318, 90.0701169 ]), 'currentState': array([ 1.889897, 70.283714,  6.270965], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4578
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.00475386, 114.0073103 ]), 'previousTarget': array([ 10., 114.]), 'currentState': array([10.020619, 94.00732 ,  4.37028 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859429185960186
{'currentTarget': array([ 12.25671548, 113.80819917]), 'previousTarget': array([ 12.67729627, 112.22078009]), 'currentState': array([19.105383, 95.01736 ,  2.18708 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4579
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.9800912, 106.97702  ,   3.9092479], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.841614681506616}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.2860520310899512
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.963565, 119.491714,   0.696595], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0282852811694205}
episode index:4580
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.99954064, 105.23186492]), 'previousTarget': array([ 10., 105.]), 'currentState': array([ 9.998919, 85.231865,  6.047962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28598958794847773
{'currentTarget': array([  8.80981385, 115.7262994 ]), 'previousTarget': array([  8.80981385, 115.7262994 ]), 'currentState': array([ 3.4441848, 96.45949  ,  1.961513 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4581
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.47761175, 86.86436902]), 'previousTarget': array([12.49484661, 86.94328241]), 'currentState': array([13.968885 , 66.920044 ,  4.4618154], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859271720628495
{'currentTarget': array([ 4.93828696, 91.05609005]), 'previousTarget': array([ 4.93828696, 91.05609005]), 'currentState': array([ 1.4929729 , 71.35508   ,  0.56189877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4582
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.83344431, 111.24786991]), 'previousTarget': array([  7.8507125, 111.40285  ]), 'currentState': array([ 3.0275824, 91.83386  ,  2.8141384], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.28590162001932296
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.697989 , 119.308586 ,   1.2333457], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9824675283202979}
episode index:4583
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.9223227 , 114.61161351]), 'previousTarget': array([  8.9223227 , 114.61161351]), 'currentState': array([ 5.       , 95.       ,  1.3188236], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.28596603205992044
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.081547 , 119.23564  ,   1.8868431], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0651166291457788}
episode index:4584
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.86303786, 106.31268572]), 'previousTarget': array([ 10.82555956, 105.96548746]), 'currentState': array([12.121615 , 86.352325 ,  5.8484197], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859036621510742
{'currentTarget': array([  8.52250668, 116.55252905]), 'previousTarget': array([  8.52250668, 116.55252905]), 'currentState': array([ 0.6440947, 98.16964  ,  2.2793097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4585
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.79249  , 105.96167  ,   1.4551051], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.060681057160624}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.28595592835809
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.251876 , 118.50949  ,   1.7519025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5116410406132292}
episode index:4586
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  5.90268197, 103.73489206]), 'previousTarget': array([  5.8507125, 103.40285  ]), 'currentState': array([ 1.0171419, 84.34078  ,  2.0085697], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.2859892097126685
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.479523  , 118.13062   ,   0.29977673], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.409651859052293}
episode index:4587
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.134975, 114.03673 ,   5.409789], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.869471823245098}
done in step count: 239
reward sum = 0.0905339582851764
running average episode reward sum: 0.285946608306516
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.647074 , 118.40793  ,   1.8183045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.089282464468474}
episode index:4588
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.44266383, 81.79128189]), 'previousTarget': array([10., 82.]), 'currentState': array([12.197275 , 61.805523 ,  5.7766614], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.2859014360225472
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.423338 , 119.41378  ,   1.1646643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5393324918841749}
episode index:4589
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.2894082, 116.08892  ,   2.5202148], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.645794811642313}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.28589196221209445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.908288, 121.968185,   5.168865], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2506863293137593}
episode index:4590
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.8664796, 108.20316  ,   5.630638 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.296068283589939}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.2859860241964957
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.946628 , 120.62869  ,   1.9664046], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.045632786693198}
episode index:4591
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.28858456, 85.83719027]), 'previousTarget': array([11.28616939, 83.98725709]), 'currentState': array([12.042427 , 65.8514   ,  1.3591152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859237450100418
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.249604 , 112.59672  ,   3.6766498], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.462025004827453}
episode index:4592
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.00648559, 82.96779297]), 'previousTarget': array([11.94882334, 82.97235659]), 'currentState': array([13.088542, 62.997086,  5.58653 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858614929427633
{'currentTarget': array([ 11.47263298, 115.77992533]), 'previousTarget': array([ 11.49194607, 115.7446933 ]), 'currentState': array([18.062126 , 96.89664  ,  3.7256515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4593
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.75079563, 112.18549026]), 'previousTarget': array([  7.68176659, 112.17596225]), 'currentState': array([ 2.2188935 , 92.96576   ,  0.14810199], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28579926797695077
{'currentTarget': array([ 12.09543366, 114.5043103 ]), 'previousTarget': array([ 12.03226059, 114.59714153]), 'currentState': array([1.9220797e+01, 9.5816635e+01, 6.2606983e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4594
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.63649913, 110.31163791]), 'previousTarget': array([ 11.0735161 , 108.90700027]), 'currentState': array([11.9476185, 90.35466  ,  2.8243577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.28583446361259535
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.822893 , 120.00623  ,   1.3583084], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8229167499063191}
episode index:4595
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.2463904, 104.8152812]), 'previousTarget': array([  9.14099581, 104.96742669]), 'currentState': array([ 8.255021, 84.83987 ,  4.958138], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28577227160571705
{'currentTarget': array([ 10.49497013, 103.74240801]), 'previousTarget': array([ 10.53425266, 105.57751691]), 'currentState': array([11.103598 , 83.75167  ,  4.9491525], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4596
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.189823 , 124.181946 ,   1.1801484], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.7205927712235365}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.28586312986618995
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.20047 , 121.35207 ,   4.897184], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8081015895506494}
episode index:4597
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.92222078, 84.83312757]), 'previousTarget': array([11.91071014, 84.97031416]), 'currentState': array([13.013791  , 64.86294   ,  0.78054667], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28580095867657135
{'currentTarget': array([15.16968527, 92.48285648]), 'previousTarget': array([15.13854554, 92.41834628]), 'currentState': array([18.862509, 72.82674 ,  4.027282], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4598
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.76921956, 71.31277123]), 'previousTarget': array([ 8.57119548, 69.99184173]), 'currentState': array([ 9.674419 , 51.312996 ,  0.5505613], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28573881452378236
{'currentTarget': array([16.84788655, 64.55647961]), 'previousTarget': array([16.74675611, 64.48778989]), 'currentState': array([19.299479 , 44.707306 ,  5.8343825], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4599
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.52646143, 106.82376881]), 'previousTarget': array([ 11.18928508, 106.91786413]), 'currentState': array([11.324932, 86.839714,  3.49558 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28567669739019025
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.11283  , 116.56208  ,   4.0577636], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.637779954745934}
episode index:4600
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.97928197, 73.65949351]), 'previousTarget': array([12.09184678, 73.9793708 ]), 'currentState': array([12.832738 , 53.67771  ,  2.4342875], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856146072581776
{'currentTarget': array([ 5.42899729, 86.80523011]), 'previousTarget': array([ 5.30339722, 86.95294083]), 'currentState': array([ 2.700693 , 66.992195 ,  2.5476234], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4601
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.8048906 , 113.66895325]), 'previousTarget': array([ 10.25976679, 112.98629668]), 'currentState': array([ 9.188826 , 93.678444 ,  2.1153436], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.28559150883930245
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.860104 , 118.22758  ,   1.8189446], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1073315673500335}
episode index:4602
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.845926 , 106.03962  ,   3.4670353], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.686714103088956}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.28564480321994895
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.966532 , 119.572525 ,   1.8812155], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.42878313664753953}
episode index:4603
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.00042101, 76.07701488]), 'previousTarget': array([ 7.24756572, 75.96105157]), 'currentState': array([ 4.1867466, 56.15942  ,  3.08042  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855827604738108
{'currentTarget': array([ 5.8640662 , 80.47740972]), 'previousTarget': array([ 5.91850987, 80.29843686]), 'currentState': array([ 3.7824862, 60.58603  ,  4.535117 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4604
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.264014  , 121.965256  ,   0.61657983], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.098548342527932}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.285737899939506
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.264014  , 121.965256  ,   0.61657983], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.098548342527932}
episode index:4605
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.066217 , 124.32521  ,   2.8298006], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.737822447400267}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.28574701519297935
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.416716 , 121.86894  ,   5.8596654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9578472462547594}
episode index:4606
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.01248537, 118.83786565]), 'previousTarget': array([ 10., 119.]), 'currentState': array([10.227343 , 98.83902  ,  5.4919324], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856849906617892
{'currentTarget': array([  8.92434415, 115.59242573]), 'previousTarget': array([  8.89809796, 115.41233882]), 'currentState': array([ 4.182567 , 96.16267  ,  2.3704774], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4607
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.52121538, 88.03982717]), 'previousTarget': array([12.46607002, 87.94108971]), 'currentState': array([14.094052 , 68.10177  ,  6.1958504], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856229930509685
{'currentTarget': array([13.88898617, 85.25579089]), 'previousTarget': array([13.81979453, 85.0802285 ]), 'currentState': array([16.113731 , 65.37991  ,  4.4136205], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4608
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.197236 , 109.97133  ,   1.7970457], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.060749339685868}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2857439130336834
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.896728 , 120.364784 ,   0.4177137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1620144641263668}
episode index:4609
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8684897, 118.038574 ,   5.851602 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.8966406532605644}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.28589240654495596
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.185447 , 119.26528  ,   0.8684408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0969539773734702}
episode index:4610
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.66251266, 101.76367118]), 'previousTarget': array([  6.62324859, 101.66906377]), 'currentState': array([ 3.0620503 , 82.09042   ,  0.46638215], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858304042880605
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.634763 , 114.164375 ,   2.3484225], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.296734293386671}
episode index:4611
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.973575, 117.902176,   1.546735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.4466331860249095}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.2859746279753137
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.758361  , 119.20439   ,   0.41644588], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4746730705822286}
episode index:4612
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.87097438, 108.7297996 ]), 'previousTarget': array([  8.9264839 , 108.90700027]), 'currentState': array([ 6.8773937, 88.82941  ,  5.4841676], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.2859460668055367
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.301015 , 120.41412  ,   1.4131219], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7487277294327936}
episode index:4613
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.73098062, 103.79220218]), 'previousTarget': array([  8.6609096 , 103.93091516]), 'currentState': array([ 7.1698217, 83.853226 ,  6.2830076], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28588409323232356
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.152638 , 118.38734  ,   1.1987152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6897091018780697}
episode index:4614
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.090493 , 114.739204 ,   3.1194413], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.660930321295779}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2860142125776072
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.340168 , 119.1358   ,   2.9525957], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8713306337007143}
episode index:4615
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.00124766, 119.97504678]), 'currentState': array([  8.984558 , 100.512634 ,   3.1434565], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.51380396154801}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859522510930799
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.663862 , 115.54704  ,   5.8997684], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.18892950242807}
episode index:4616
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.68523125, 80.69323424]), 'previousTarget': array([15.39931926, 78.83019061]), 'currentState': array([18.54819  , 60.899208 ,  1.5391892], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28589031644913515
{'currentTarget': array([13.12013131, 89.16871287]), 'previousTarget': array([12.64285585, 87.86610168]), 'currentState': array([15.133849  , 69.27035   ,  0.90463245], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4617
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.77901751, 102.49384508]), 'previousTarget': array([ 12.79857683, 102.74210955]), 'currentState': array([15.914658 , 82.74118  ,  0.4785655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858284086283363
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.9799008, 115.471664 ,   5.9712677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.533088143598374}
episode index:4618
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.165141 , 122.77631  ,   2.5883138], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.967914208439328}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.28580613716474695
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.645765  , 121.9286    ,   0.18769807], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.535355573551731}
episode index:4619
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.05952141, 85.10267347]), 'previousTarget': array([11.27320764, 84.98678996]), 'currentState': array([11.666464 , 65.111885 ,  5.3284855], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28574427436449484
{'currentTarget': array([14.20134998, 90.46168642]), 'previousTarget': array([14.15155601, 90.40880952]), 'currentState': array([17.017683  , 70.66097   ,  0.57307214], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4620
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.67363144, 112.52309035]), 'previousTarget': array([  8.6417852, 112.6656401]), 'currentState': array([ 5.1802664, 92.83054  ,  3.0502396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28568243833888035
{'currentTarget': array([  8.73510809, 112.58963619]), 'previousTarget': array([  8.70649351, 112.72173624]), 'currentState': array([ 5.369934 , 92.87478  ,  4.0780935], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4621
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.93357699, 75.12426512]), 'previousTarget': array([ 7.92209533, 74.9787322 ]), 'currentState': array([ 7.0135984, 55.145435 ,  3.854959 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856206290705249
{'currentTarget': array([ 4.66602922, 89.86967104]), 'previousTarget': array([ 4.66602922, 89.86967104]), 'currentState': array([ 1.1796397, 70.17589  ,  1.5260732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4622
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.57622786, 105.88545395]), 'previousTarget': array([ 11.79136948, 103.87767469]), 'currentState': array([13.79591  , 86.00901  ,  1.8156646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855588465420649
{'currentTarget': array([ 3.92393016, 70.99457361]), 'previousTarget': array([ 3.84168364, 71.22844551]), 'currentState': array([ 1.4630198, 51.146553 ,  4.7668986], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4623
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.084741, 106.937195,   3.15792 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.202465583418071}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.285546448534624
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.459828 , 119.3985   ,   1.5620513], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8084487582723403}
episode index:4624
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.90076724, 83.8987396 ]), 'previousTarget': array([10., 84.]), 'currentState': array([ 9.845793  , 63.898815  ,  0.30578583], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28548470876196785
{'currentTarget': array([  8.45360859, 116.36184342]), 'previousTarget': array([  8.46197149, 116.25236028]), 'currentState': array([ 0.6300465, 97.95554  ,  2.4694335], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4625
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.22076779, 72.32425728]), 'previousTarget': array([12.13125551, 70.98112317]), 'currentState': array([11.732713 , 52.33081  ,  2.5762696], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854229956818204
{'currentTarget': array([ 4.67660465, 88.41034427]), 'previousTarget': array([ 4.67660465, 88.41034427]), 'currentState': array([ 1.3531235, 68.688416 ,  3.4923382], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4626
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.50278214, 86.73021445]), 'previousTarget': array([13.8310498 , 84.88204353]), 'currentState': array([15.596891 , 66.84015  ,  1.2662643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28536130927687514
{'currentTarget': array([ 5.87488513, 89.84237711]), 'previousTarget': array([ 5.87488513, 89.84237711]), 'currentState': array([ 3.1644213, 70.02689  ,  2.0857294], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4627
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.29775054, 87.53097874]), 'previousTarget': array([ 4.41082867, 87.70701012]), 'currentState': array([ 0.83827084, 67.83245   ,  2.9359677 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852996495298404
{'currentTarget': array([11.9507314 , 94.59719614]), 'previousTarget': array([11.95356161, 94.2272907 ]), 'currentState': array([1.3482062e+01, 7.4655907e+01, 2.5474390e-03], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4628
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.78780542, 105.68573382]), 'previousTarget': array([  7.90987981, 105.78718271]), 'currentState': array([ 4.733174, 85.92038 ,  2.936415], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28523801642343943
{'currentTarget': array([ 10.75017096, 116.75500609]), 'previousTarget': array([ 10.72296745, 116.81333865]), 'currentState': array([15.254922  , 97.26893   ,  0.18709248], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4629
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.735231, 110.11893 ,   1.117658], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.188626584909176}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.2853548480881073
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.256622, 120.41893 ,   2.8652  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3246139177450076}
episode index:4630
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.62388486, 110.34294167]), 'previousTarget': array([  8.99007438, 109.9007438 ]), 'currentState': array([ 8.845531, 90.35809 ,  6.051308], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.2853797519544134
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.253188 , 119.2604   ,   2.1416962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4551599006850715}
episode index:4631
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.59840553, 74.83095071]), 'previousTarget': array([ 8.61509352, 74.99053926]), 'currentState': array([ 7.9781046, 54.840572 ,  5.08286  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28531814147255796
{'currentTarget': array([ 3.66071383, 78.98537378]), 'previousTarget': array([ 3.66071383, 78.98537378]), 'currentState': array([ 0.6057569, 59.22007  ,  2.692559 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4632
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.41050599, 81.80948235]), 'previousTarget': array([ 7.99875234, 79.97504678]), 'currentState': array([ 7.5788236, 61.826782 ,  2.0178182], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852565575870685
{'currentTarget': array([14.04948613, 90.32590071]), 'previousTarget': array([13.21498473, 89.29044735]), 'currentState': array([16.753729  , 70.50957   ,  0.92829716], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4633
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.03961795, 90.58166226]), 'previousTarget': array([10.58342373, 91.99566113]), 'currentState': array([11.745959, 70.59414 ,  5.582639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.2852395422611148
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.862188 , 119.666794 ,   2.2443023], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8917641943451622}
episode index:4634
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.3918413e+01, 1.2190732e+02, 1.1121848e-01], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.357961002883097}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.2852308315290272
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.990827  , 121.77594   ,   0.83020365], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.667836811656159}
episode index:4635
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.16109569, 85.21350081]), 'previousTarget': array([11.27320764, 84.98678996]), 'currentState': array([11.8282795, 65.22463  ,  5.2404604], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 296
reward sum = 0.0510525689892109
running average episode reward sum: 0.285180318530205
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.784266 , 118.844986 ,   1.6410234], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1749886737063089}
episode index:4636
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.45175501, 73.66756418]), 'previousTarget': array([ 7.17444044, 71.96548746]), 'currentState': array([ 4.9245806, 53.725956 ,  1.7494636], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851188174910568
{'currentTarget': array([12.11331372, 73.8325683 ]), 'previousTarget': array([12.11331372, 73.8325683 ]), 'currentState': array([13.027856  , 53.85349   ,  0.24042815], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4637
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.969849 , 116.520485 ,   3.2990973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.7901100843458995}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.2851491045341567
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.201107 , 118.58824  ,   0.8163789], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8535695105251835}
episode index:4638
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.0461062 , 106.06887025]), 'previousTarget': array([ 12.3142293 , 107.65744374]), 'currentState': array([14.952386, 86.28116 ,  3.969341], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285087636738396
{'currentTarget': array([  8.73552853, 116.35329195]), 'previousTarget': array([  8.71648757, 116.31077268]), 'currentState': array([ 2.183372 , 97.457016 ,  5.1467376], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4639
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8668256, 106.81378  ,   3.8467207], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.357648937941164}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.28511884393166875
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.334103, 119.42631 ,   3.193969], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4522233674888887}
episode index:4640
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.88178061, 116.66184728]), 'previousTarget': array([  9.86874449, 116.98112317]), 'currentState': array([ 9.173932 , 96.67438  ,  1.3077501], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.28519727181023735
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.798929, 119.070175,   2.301394], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.951316810323653}
episode index:4641
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.836051 , 112.9999   ,   1.4910896], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.508159478457259}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2853326272552338
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.162331, 118.93633 ,   3.09114 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5755635839179707}
episode index:4642
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.07919231, 77.49527045]), 'previousTarget': array([15.51930531, 75.84555753]), 'currentState': array([17.452251 , 57.636555 ,  1.8031665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28527117288795933
{'currentTarget': array([ 5.43839619, 88.39925946]), 'previousTarget': array([ 5.32065692, 88.16211886]), 'currentState': array([ 2.5809896, 68.60443  ,  4.7575927], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4643
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.793686 , 118.25897  ,   5.3650084], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.753209461064655}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2854250765975011
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.793686 , 118.25897  ,   5.3650084], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.753209461064655}
episode index:4644
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.18417559, 83.80394799]), 'previousTarget': array([ 8.06989443, 83.97136265]), 'currentState': array([ 7.1821084, 63.829067 ,  6.1006603], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853636287876846
{'currentTarget': array([ 5.49061669, 89.79239367]), 'previousTarget': array([ 5.49116446, 89.77500148]), 'currentState': array([ 2.5377424, 70.01158  ,  5.1336555], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4645
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.09089981, 83.01935596]), 'previousTarget': array([ 4.11925147, 82.75525931]), 'currentState': array([ 0.93515295, 63.269894  ,  2.0889416 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285302207429788
{'currentTarget': array([ 4.57162349, 89.82732282]), 'previousTarget': array([ 4.49579588, 89.79450206]), 'currentState': array([ 1.0302731, 70.14335  ,  1.897753 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4646
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.06544058, 88.87884217]), 'previousTarget': array([ 5.09935538, 88.75839053]), 'currentState': array([ 1.9333754, 69.12561  ,  1.4374447], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852408125067345
{'currentTarget': array([ 5.4712564 , 88.48293404]), 'previousTarget': array([ 5.51361762, 88.60908088]), 'currentState': array([ 2.6266375, 68.686264 ,  5.8794513], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4647
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.76648933, 71.86764674]), 'previousTarget': array([ 7.8814955, 71.9805647]), 'currentState': array([ 6.8394165, 51.889145 ,  6.164098 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28517944400146195
{'currentTarget': array([  7.00788426, 111.01315801]), 'previousTarget': array([  7.11263763, 111.19521903]), 'currentState': array([ 0.6899773, 92.03728  ,  3.1098027], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4648
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.001858 , 113.991455 ,   4.5784435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.006614794751858}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.28529403369679335
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.716775 , 118.12508  ,   2.8461819], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0072631164950256}
episode index:4649
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.10038049, 72.47367921]), 'previousTarget': array([ 5.73259233, 70.92481176]), 'currentState': array([ 3.0493956, 52.57912  ,  1.4321829], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28523268014115966
{'currentTarget': array([ 5.60433471, 86.42463038]), 'previousTarget': array([ 5.7052177 , 86.57713325]), 'currentState': array([ 3.008103 , 66.59386  ,  5.7026553], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4650
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.061025 , 108.82573  ,   2.2354393], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.213652339360115}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.28534720911502676
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.188942  , 121.58785   ,   0.49652916], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4085694368826505}
episode index:4651
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.89253754, 112.44985598]), 'previousTarget': array([ 11.98075682, 112.35993796]), 'currentState': array([16.755344 , 93.05003  ,  4.9216595], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 239
reward sum = 0.0905339582851764
running average episode reward sum: 0.28530533180401435
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([8.96654701e+00, 1.20544106e+02, 8.17314386e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.167936601562213}
episode index:4652
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.48333457, 87.64192662]), 'previousTarget': array([11.25976679, 85.98629668]), 'currentState': array([10.7820425, 67.64416  ,  2.2680118], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852440153776649
{'currentTarget': array([  7.02310604, 109.06044529]), 'previousTarget': array([  6.98766497, 109.0921843 ]), 'currentState': array([ 1.77163 , 89.76221 ,  2.354296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4653
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.326183  , 109.27819   ,   0.58390427], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.803516035598696}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.2852216438559091
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.424537, 121.01674 ,   5.21326 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.875058017111359}
episode index:4654
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.76941317, 107.00757089]), 'previousTarget': array([ 10.79009879, 106.96336993]), 'currentState': array([11.951744 , 87.04255  ,  6.2628946], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.2852060700770419
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.025648, 118.52713 ,   6.216785], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7659861989005634}
episode index:4655
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.61716172, 71.89990908]), 'previousTarget': array([ 6.46662886, 71.94615251]), 'currentState': array([ 5.2140446, 51.94919  ,  5.7816663], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851448144777986
{'currentTarget': array([ 7.0088021 , 88.08494383]), 'previousTarget': array([ 7.0088021 , 88.08494383]), 'currentState': array([ 5.1425066, 68.17221  ,  6.071893 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4656
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.83799338, 111.4371026 ]), 'previousTarget': array([ 11.80941823, 111.55604828]), 'currentState': array([16.035315 , 91.8825   ,  1.6988318], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2850835851854478
{'currentTarget': array([  8.50170238, 115.8404548 ]), 'previousTarget': array([  8.42181749, 115.687492  ]), 'currentState': array([ 1.7238637, 97.02395  ,  3.22587  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4657
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.44370234, 98.74376595]), 'previousTarget': array([ 6.36592926, 98.71472851]), 'currentState': array([ 3.1434507, 79.01794  ,  5.130255 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.28508109604114484
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.100849 , 118.224495 ,   1.8150313], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7783668886034445}
episode index:4658
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.29402281, 69.69969913]), 'previousTarget': array([12.13125551, 70.98112317]), 'currentState': array([11.808372 , 49.706314 ,  4.1011844], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28501990670951977
{'currentTarget': array([16.30320438, 65.28531061]), 'previousTarget': array([16.36642032, 65.23943428]), 'currentState': array([18.592093  , 45.416718  ,  0.05510109], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4659
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.53395004, 66.90467208]), 'previousTarget': array([11.45226033, 66.99249812]), 'currentState': array([12.111519, 46.913013,  4.137864], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28495874363941043
{'currentTarget': array([ 6.96473501, 78.25901531]), 'previousTarget': array([ 8.20572413, 77.08620936]), 'currentState': array([ 5.5142317, 58.311684 ,  2.638666 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4660
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.81458046, 66.87732975]), 'previousTarget': array([10.7260531 , 66.99812374]), 'currentState': array([11.121223, 46.87968 ,  4.385577], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2848976068139139
{'currentTarget': array([ 8.14853716, 97.04246868]), 'previousTarget': array([ 8.14853716, 97.04246868]), 'currentState': array([ 6.540811, 77.10719 ,  2.156087], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4661
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.9989357, 111.86774  ,   4.053966 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.730731840256261}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2848364962161417
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.949772 , 119.90602  ,   2.6284688], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.051102462148621}
episode index:4662
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.28522556, 74.26289735]), 'previousTarget': array([13.46607002, 74.94108971]), 'currentState': array([13.283268 , 54.287815 ,  3.5757198], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28477541182921995
{'currentTarget': array([ 3.85016155, 79.04385528]), 'previousTarget': array([ 3.75988195, 78.98608085]), 'currentState': array([ 0.880322  , 59.265583  ,  0.10514402], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4663
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.02888  , 122.90698  ,   5.3809934], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.0649014052225128}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.28492661778723255
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.911312 , 120.94573  ,   5.0726438], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.94988145554196}
episode index:4664
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.979681, 123.10939 ,   4.616584], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.564088794660966}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.28506533991587557
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.414838, 119.257065,   4.092852], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5980359158340451}
episode index:4665
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.95287979, 104.57507542]), 'previousTarget': array([  6.9223227 , 104.61161351]), 'currentState': array([ 3.0768802, 84.954254 ,  4.22243  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.28503021421429847
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.9976425, 119.7362   ,   0.739149 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.2638121084097185}
episode index:4666
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.56893099, 68.9351588 ]), 'previousTarget': array([ 3.5150853 , 68.84122844]), 'currentState': array([ 1.0698858, 49.091904 ,  6.257588 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2849691406736483
{'currentTarget': array([ 6.97990717, 77.0525622 ]), 'previousTarget': array([ 6.874611  , 76.94434875]), 'currentState': array([ 5.576958 , 57.10183  ,  5.4719396], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4667
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.949735 , 114.91805  ,   4.0224714], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.08219590655047}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.2850229750962133
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.19127  , 118.2794   ,   1.8390707], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7311949164863596}
episode index:4668
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.46936562, 87.93651669]), 'previousTarget': array([ 7.53392998, 87.94108971]), 'currentState': array([ 5.8957443, 67.99852  ,  6.133422 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28496192926732145
{'currentTarget': array([  7.99741162, 114.72086708]), 'previousTarget': array([  8.07038649, 114.77398842]), 'currentState': array([ 0.90383565, 96.0211    ,  3.8872695 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4669
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.72444614, 104.48703863]), 'previousTarget': array([ 12.6207237, 104.7124451]), 'currentState': array([13.934069 , 84.609474 ,  3.7921185], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2849009095822535
{'currentTarget': array([17.12934839, 45.11415918]), 'previousTarget': array([17.26853322, 44.96964429]), 'currentState': array([19.024836, 25.204184,  4.797086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4670
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.55359498, 99.60651258]), 'previousTarget': array([ 5.3902439 , 99.51219512]), 'currentState': array([ 1.2930735, 80.06558  ,  5.5005546], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2848399160242183
{'currentTarget': array([ 4.47389884, 76.41986204]), 'previousTarget': array([ 4.47930637, 76.62490692]), 'currentState': array([ 1.9579809, 56.57874  ,  4.0411205], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4671
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.16936124, 92.30859067]), 'previousTarget': array([15.23855458, 92.64310384]), 'currentState': array([18.839508 , 72.648224 ,  6.0351276], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28477894857643915
{'currentTarget': array([14.68927541, 91.92409231]), 'previousTarget': array([14.61085192, 92.16735076]), 'currentState': array([17.984062, 72.19735 ,  5.964511], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4672
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.630482 , 116.91531  ,   0.8382269], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.14845940753563}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2849256466400864
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.937472 , 118.69454  ,   1.0259163], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6071948121629134}
episode index:4673
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.14289121, 89.08318391]), 'previousTarget': array([13.12154811, 86.91159005]), 'currentState': array([15.165594 , 69.18573  ,  2.0035408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2848646869381951
{'currentTarget': array([13.94574407, 91.96416678]), 'previousTarget': array([13.94574407, 91.96416678]), 'currentState': array([16.73306  , 72.15935  ,  3.3639112], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4674
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.19132018, 97.6963086 ]), 'previousTarget': array([11.07077201, 96.97840172]), 'currentState': array([10.362873 , 77.697044 ,  2.8297758], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.28493712773134017
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.812106 , 121.37542  ,   5.9205055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.817380247066566}
episode index:4675
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.8357997, 106.6502   ,   1.307166 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.524086879990492}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.28503438248875185
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.709349 , 118.96402  ,   2.2497358], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9987816115762114}
episode index:4676
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.064011  , 116.12879   ,   0.55743665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.612703018854254}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.28506816773552063
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.800982 , 118.27699  ,   2.6543317], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9000859834129302}
episode index:4677
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.165048 , 120.826866 ,   2.3062794], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.317571776113425}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28521674230419625
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.540722, 121.65657 ,   2.844847], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7425859435327191}
episode index:4678
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.557495, 111.44707 ,   4.290827], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.199922079105852}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.2852763038447281
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.878592 , 121.59607  ,   3.3435578], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.821911198396806}
episode index:4679
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.53684023, 64.93666779]), 'previousTarget': array([10.73335703, 64.99822246]), 'currentState': array([10.731821 , 44.93762  ,  5.3381004], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28521534736954757
{'currentTarget': array([ 4.32116758, 33.07329543]), 'previousTarget': array([ 4.26294166, 33.24738985]), 'currentState': array([ 3.0173674, 13.115838 ,  2.7346306], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4680
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.90304146, 92.78357975]), 'previousTarget': array([10.59192171, 90.99583637]), 'currentState': array([ 9.831792 , 72.78371  ,  2.1905491], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.2851870338435715
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.43073  , 118.974144 ,   1.5295478], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8748304785180148}
episode index:4681
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.973235 , 113.90248  ,   5.0241385], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.783794228219509}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.28528096662915764
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.168858 , 119.40752  ,   2.4592206], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9246088705696178}
episode index:4682
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.57276467, 112.06941636]), 'previousTarget': array([  8.33860916, 112.5237412 ]), 'currentState': array([ 1.7195692, 92.94508  ,  3.547008 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28522004820792574
{'currentTarget': array([  8.15281665, 114.95861654]), 'previousTarget': array([  8.24680212, 115.22740539]), 'currentState': array([ 1.272071 , 96.1795   ,  1.8454604], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4683
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.49811903, 90.99455007]), 'previousTarget': array([ 9.40807829, 90.99583637]), 'currentState': array([ 9.152111, 70.99754 ,  4.268512], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851591557979753
{'currentTarget': array([ 6.36308116, 57.40365233]), 'previousTarget': array([ 6.32569721, 57.48936634]), 'currentState': array([ 5.203015 , 37.437325 ,  2.3124657], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4684
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.020095, 113.8057  ,   5.198702], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.891320493977867}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.2852800306358188
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.960086, 118.97266 ,   2.279516], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.028118825102762}
episode index:4685
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.057165 , 107.083336 ,   2.0263731], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.07945490139475}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.2854160666289456
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.213926 , 119.021286 ,   1.3444563], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0018211123499223}
episode index:4686
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.408858 , 114.022    ,   1.3204684], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.9737183907387505}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.28556219057461896
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.808492  , 120.140594  ,   0.93924165], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1997744876172034}
episode index:4687
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  5.87696222, 100.49844396]), 'previousTarget': array([  6.01888287, 100.59205401]), 'currentState': array([ 1.7399911 , 80.930984  ,  0.70159537], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.2855187424300964
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.887003 , 121.373634 ,   0.3762514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.378274149080823}
episode index:4688
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.890401 , 115.841934 ,   3.1889534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.303570768050804}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2856468861983382
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.562565 , 118.10242  ,   1.3309001], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3805547845860175}
episode index:4689
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.96087557, 114.00633639]), 'previousTarget': array([  8.09551454, 114.04848294]), 'currentState': array([ 1.5191998 , 95.07211   ,  0.69184643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855859806788929
{'currentTarget': array([  8.71436172, 115.25313027]), 'previousTarget': array([  8.50338753, 115.74685184]), 'currentState': array([ 3.4859471, 95.94863  ,  5.661313 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4690
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.19903151, 71.66790755]), 'previousTarget': array([ 9.28982464, 70.99789993]), 'currentState': array([ 7.454301 , 51.681778 ,  2.5124743], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855251011264139
{'currentTarget': array([  7.14766227, 111.90358395]), 'previousTarget': array([  7.20312198, 111.85329944]), 'currentState': array([ 0.5020776, 93.03996  ,  0.5086156], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4691
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.97719764, 119.85048302]), 'previousTarget': array([  9.96680906, 119.77872706]), 'currentState': array([  6.9619246, 100.07909  ,   4.0933933], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.2855577295153064
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.563035  , 119.59712   ,   0.16340327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4923735814615722}
episode index:4692
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.24416184, 83.72101929]), 'previousTarget': array([ 4.17356191, 83.74660743]), 'currentState': array([ 1.110261 , 63.96808  ,  5.8786435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854968819275128
{'currentTarget': array([ 5.16327292, 90.099658  ]), 'previousTarget': array([ 5.16327292, 90.099658  ]), 'currentState': array([ 1.9695559, 70.3563   ,  1.2953703], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4693
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.53364773, 89.77762535]), 'previousTarget': array([ 8.76866244, 87.98522349]), 'currentState': array([ 7.5644126, 69.801125 ,  2.3148732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854360602654064
{'currentTarget': array([ 5.8589961 , 99.58024386]), 'previousTarget': array([ 5.81269135, 99.48798024]), 'currentState': array([ 1.8840288, 79.97923  ,  2.7207713], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4694
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.34461245, 83.8717749 ]), 'previousTarget': array([ 6.13066247, 83.88618308]), 'currentState': array([ 4.331328, 63.973366,  4.822811], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28537526451242123
{'currentTarget': array([ 11.08115629, 117.54697115]), 'previousTarget': array([ 11.08314719, 117.52340863]), 'currentState': array([1.9147326e+01, 9.9245689e+01, 4.2456325e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4695
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.88864388, 72.95919132]), 'previousTarget': array([ 5.0186243 , 70.89786813]), 'currentState': array([ 2.7282019, 53.07622  ,  1.9169759], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853144946520055
{'currentTarget': array([ 4.75020676, 90.04547674]), 'previousTarget': array([ 4.8193106 , 90.28261321]), 'currentState': array([ 1.2976538, 70.34573  ,  3.6568024], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4696
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.075222 , 109.0561   ,   5.4796467], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.000933657548924}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.2854112342472229
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.116014 , 119.320724 ,   1.6898744], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6891112831575797}
episode index:4697
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.63040273, 111.68175672]), 'previousTarget': array([ 10.66961979, 109.95570316]), 'currentState': array([12.14178  , 91.738945 ,  1.5687432], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.2854537161833717
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.552195 , 119.274124 ,   1.3085433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6195790335474234}
episode index:4698
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.81998544, 118.76967465]), 'previousTarget': array([  9.82842712, 118.79898987]), 'currentState': array([ 6.9245224 , 98.98038   ,  0.14032477], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.285570562330481
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.32662   , 120.35158   ,   0.61686146], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7099143849164549}
episode index:4699
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.72316402, 74.1774033 ]), 'previousTarget': array([10.69700447, 73.99770471]), 'currentState': array([11.038761 , 54.179893 ,  6.1848607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855098026363681
{'currentTarget': array([ 3.57934405, 70.33556818]), 'previousTarget': array([ 3.60524755, 70.42084648]), 'currentState': array([ 1.0150689, 50.500637 ,  5.124469 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4700
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.12160014, 118.68708928]), 'previousTarget': array([ 10.26740767, 116.92481176]), 'currentState': array([11.966081 , 98.77232  ,  1.6117604], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.2856213146690388
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.7375   , 119.70972  ,   2.6441393], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7925723117485683}
episode index:4701
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.10451742, 116.6765522 ]), 'previousTarget': array([ 11.67544468, 114.97366596]), 'currentState': array([17.412117 , 97.69724  ,  1.8597571], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.28566581006353714
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0814295, 119.177055 ,   1.3251938], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8269635075355259}
episode index:4702
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.51963726, 71.94461903]), 'previousTarget': array([ 8.59674911, 72.99109528]), 'currentState': array([ 6.488716 , 51.971207 ,  3.1421027], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856050688749206
{'currentTarget': array([ 5.26882131, 89.86879494]), 'previousTarget': array([ 5.23485542, 90.08626555]), 'currentState': array([ 2.1664484, 70.11088  ,  5.305964 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4703
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.20680235, 113.69879645]), 'previousTarget': array([ 11.22305213, 113.64012894]), 'currentState': array([14.968816 , 94.0558   ,  4.7030582], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28554435351163937
{'currentTarget': array([  8.37119936, 116.18291818]), 'previousTarget': array([  8.37119936, 116.18291818]), 'currentState': array([ 0.5216956, 97.78767  ,  0.7979169], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4704
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.26467689, 79.98526092]), 'previousTarget': array([12.71235444, 77.95850618]), 'currentState': array([14.89101  , 60.051495 ,  0.9455348], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854836639572267
{'currentTarget': array([13.77685942, 89.22878419]), 'previousTarget': array([13.53480981, 89.43468331]), 'currentState': array([16.213375 , 69.377754 ,  5.7713213], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4705
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.4453221 , 88.22461371]), 'previousTarget': array([ 7.53392998, 87.94108971]), 'currentState': array([ 5.8425336, 68.28894  ,  3.6841187], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854230001952298
{'currentTarget': array([ 4.76182525, 90.96020359]), 'previousTarget': array([ 4.83363804, 90.92291902]), 'currentState': array([ 1.2115365, 71.27784  ,  2.2844808], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4706
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.13781  , 116.0234   ,   0.7271952], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.170782140131509}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.28555067427033515
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.922408 , 118.31348  ,   2.6297135], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6883073794693182}
episode index:4707
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.16570615, 102.88797836]), 'previousTarget': array([ 10.48734798, 100.99342862]), 'currentState': array([10.359369 , 82.888916 ,  2.1102643], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.28559512796305603
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.070474 , 119.78501  ,   2.0614305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.94146645586305}
episode index:4708
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.8200688 , 84.56593924]), 'previousTarget': array([15.77022864, 84.73749166]), 'currentState': array([19.061647, 64.83038 ,  4.466765], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28553447917818386
{'currentTarget': array([  8.5237985 , 110.38529099]), 'previousTarget': array([  8.4662616 , 110.20823505]), 'currentState': array([ 5.4886494, 90.616936 ,  4.4312625], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4709
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.10381815, 118.90990945]), 'currentState': array([ 11.518175 , 100.88546  ,   1.3598179], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.174735955125453}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.2856564587695773
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.393601 , 120.291435 ,   1.5443823], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6326208702050051}
episode index:4710
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.48300078, 108.78240704]), 'previousTarget': array([ 11.44057325, 108.83555733]), 'currentState': array([14.104256, 88.954926,  4.832573], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.28562454973670476
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.824762 , 119.17246  ,   2.1708157], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.437359357206918}
episode index:4711
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.9278517, 120.99383  ,   5.96333  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.152940678463085}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2857698541616333
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0913315 , 121.00588   ,   0.43499154], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.01002008246348}
episode index:4712
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.70786086, 96.93760785]), 'previousTarget': array([ 9.47605556, 97.99433347]), 'currentState': array([ 7.589056 , 76.968925 ,  3.9978185], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857092197771305
{'currentTarget': array([  8.18946286, 115.48483954]), 'previousTarget': array([  8.20899415, 115.48269795]), 'currentState': array([ 0.7457989, 96.92165  ,  2.6713982], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4713
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.22749116, 105.64697759]), 'previousTarget': array([  6.45649603, 104.49717013]), 'currentState': array([ 3.4343004, 86.00998  ,  0.5437953], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.2856965424194209
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.661733  , 120.92738   ,   0.39938232], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6281890092274875}
episode index:4714
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.278423, 120.77769 ,   2.298675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.3264047004771}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.2857836509402332
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.894645 , 121.2915   ,   3.6798973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.699933201639021}
episode index:4715
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.81182802, 102.99402444]), 'previousTarget': array([  6.71783336, 102.65140491]), 'currentState': array([ 3.126558, 83.33649 ,  5.26254 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.28582903969743406
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.237512 , 118.302345 ,   1.9416635], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4471201032442313}
episode index:4716
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.21793196, 84.79624797]), 'previousTarget': array([ 4.22977136, 84.73749166]), 'currentState': array([ 0.97644746, 65.06068   ,  5.4176636 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28576844418340025
{'currentTarget': array([ 5.23894002, 88.79358957]), 'previousTarget': array([ 5.26207382, 88.9344092 ]), 'currentState': array([ 2.2225096, 69.02237  ,  4.503227 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4717
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.87561541, 104.54941602]), 'previousTarget': array([ 10.45965677, 102.9926994 ]), 'currentState': array([ 9.714611 , 84.550064 ,  2.4626746], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285707874356316
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.8125033, 121.67767  ,   1.337586 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.6020442538058077}
episode index:4718
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.32753898, 82.07887755]), 'previousTarget': array([ 4.73274794, 81.81242258]), 'currentState': array([ 1.368743 , 62.29895  ,  3.4451113], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.28567779108926733
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.183354 , 119.3446   ,   1.9670948], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.352730888090022}
episode index:4719
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([16.10951963, 75.66777512]), 'previousTarget': array([16.21490337, 75.80513158]), 'currentState': array([18.839956 , 55.855034 ,  4.8607516], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28561726613352806
{'currentTarget': array([11.55985289, 66.70622913]), 'previousTarget': array([11.73998457, 66.79160668]), 'currentState': array([12.144981, 46.71479 ,  5.366932], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4720
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.86808693e+00, 1.12108154e+02, 1.31208263e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.331780032874615}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28575222216578694
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.723427 , 118.646576 ,   0.9493108], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1913367425640953}
episode index:4721
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.172764, 115.85264 ,   5.651154], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.150958548210021}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.2857215428330336
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.838673 , 119.740814 ,   1.9554304], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.189898531865177}
episode index:4722
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.136911 , 119.79867  ,   4.8630342], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.1433656634207874}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.2858307761691983
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.754123 , 119.83907  ,   2.4535615], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7614891447877339}
episode index:4723
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.96680906, 119.77872706]), 'previousTarget': array([  9.96680906, 119.77872706]), 'currentState': array([  7.       , 100.       ,   3.5919607], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.2858485363007564
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.130943, 119.76924 ,   2.327581], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1542453406671462}
episode index:4724
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.37800166, 81.72039854]), 'previousTarget': array([ 4.0667466 , 81.76347807]), 'currentState': array([ 2.9805522 , 61.864613  ,  0.18385667], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28578803925603663
{'currentTarget': array([13.37996877, 90.61074624]), 'previousTarget': array([13.37996877, 90.61074624]), 'currentState': array([15.665046, 70.741714,  3.009388], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4725
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.11404148, 78.9928845 ]), 'previousTarget': array([ 3.91921747, 78.78580727]), 'currentState': array([ 1.2724627, 59.195778 ,  3.7207344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28572756781311326
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.305896, 104.339615,   4.293189], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.751750743956325}
episode index:4726
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.87856  , 111.06796  ,   5.0570188], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.27368026641819}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.285822041221584
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.629694, 118.035164,   5.719646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.999426799258222}
episode index:4727
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.41298275, 90.7296987 ]), 'previousTarget': array([15.38696959, 90.67094332]), 'currentState': array([19.049932 , 71.06316  ,  5.9380903], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 296
reward sum = 0.0510525689892109
running average episode reward sum: 0.2857723860878631
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.940992 , 119.672615 ,   2.4507687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1084575301243729}
episode index:4728
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.84319168, 101.80211293]), 'previousTarget': array([ 10.94882334, 101.97235659]), 'currentState': array([11.76889 , 81.82355 ,  5.963819], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28571195631706847
{'currentTarget': array([  8.63909321, 115.91259296]), 'previousTarget': array([  8.62434402, 115.91033663]), 'currentState': array([ 2.3210657, 96.93675  ,  5.642486 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4729
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.180329 , 103.98944  ,   5.9974337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.03152713057151}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.2857236802293723
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.213834 , 118.63859  ,   0.9882344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5721005201307134}
episode index:4730
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.72823419, 105.59939407]), 'previousTarget': array([  8.75787621, 105.922597  ]), 'currentState': array([ 6.968815 , 85.67693  ,  4.0177207], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28566328629992194
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.971359  , 121.03461   ,   0.13176264], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.1463291812858887}
episode index:4731
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.4097857 , 116.69153147]), 'previousTarget': array([ 10.41321632, 116.83200822]), 'currentState': array([12.868193, 96.8432  ,  4.907208], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.28562930796643904
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.574856 , 119.85235  ,   2.0912936], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5817622517750933}
episode index:4732
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.071793  , 113.90386   ,   0.66620296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.604043463932834}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.2857103015118866
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.366018 , 121.77794  ,   3.6050367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8875908800775594}
episode index:4733
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.2414076, 122.28016  ,   3.5562894], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.050528454404295}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.28582801019204546
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.819916 , 119.822426 ,   5.9382567], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.25290889831756425}
episode index:4734
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.12161403, 117.3829006 ]), 'previousTarget': array([ 11.12161403, 117.3829006 ]), 'currentState': array([19.       , 99.       ,  0.9558295], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857676452479711
{'currentTarget': array([  8.76184564, 115.74135643]), 'previousTarget': array([  8.76184564, 115.74135643]), 'currentState': array([ 3.1782641, 96.536575 ,  1.5044651], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4735
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.53368513, 88.58302581]), 'previousTarget': array([15.52429332, 88.69567118]), 'currentState': array([19.003016 , 68.88623  ,  2.9916115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 235
reward sum = 0.0942476934556249
running average episode reward sum: 0.28572720606896085
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.690398 , 120.62044  ,   0.8908037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4491375624445872}
episode index:4736
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.52093637, 116.34857067]), 'previousTarget': array([  8.57265691, 116.51093912]), 'currentState': array([ 1.0122652, 97.811584 ,  2.0012126], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.28568349143757066
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.630305 , 119.45122  ,   1.1879579], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7201910956336601}
episode index:4737
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.003632 , 112.13818  ,   3.5246787], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.892004321450894}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.2857157696162056
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.616154, 118.3034  ,   2.132061], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.343162549740508}
episode index:4738
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.87984795, 100.12575604]), 'previousTarget': array([ 10., 100.]), 'currentState': array([ 9.758938 , 80.12612  ,  2.7080784], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2856554793082047
{'currentTarget': array([  8.59777975, 116.349615  ]), 'previousTarget': array([  8.53728063, 116.19710042]), 'currentState': array([ 1.4261026, 97.679665 ,  3.2262378], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4739
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.04398758, 106.40514189]), 'previousTarget': array([  7.3792763, 104.7124451]), 'currentState': array([ 5.1957407, 86.60899  ,  0.7242962], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 206
reward sum = 0.12613920430197118
running average episode reward sum: 0.2856218260856295
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.017116 , 119.331116 ,   2.1609101], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0926625977863114}
episode index:4740
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.56846227, 74.60703248]), 'previousTarget': array([ 7.1919076 , 72.96445232]), 'currentState': array([ 5.060839 , 54.663937 ,  2.4436464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.2856495626504571
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.323805 , 118.06352  ,   5.1397266], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.345720577371877}
episode index:4741
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.83422255, 90.0647568 ]), 'previousTarget': array([ 8.79936077, 89.98401917]), 'currentState': array([ 9.723467, 70.06506 ,  6.026955], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.28562564876761043
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.050686, 119.00991 ,   0.399338], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3716684534639048}
episode index:4742
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.52716106, 111.47147737]), 'previousTarget': array([ 11.00992562, 109.9007438 ]), 'currentState': array([11.761037 , 91.509575 ,  2.6383088], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855654283061372
{'currentTarget': array([  8.75835447, 116.98784801]), 'previousTarget': array([  8.75835447, 116.98784801]), 'currentState': array([ 1.1362854, 98.4972   ,  2.5223143], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4743
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.63678478, 108.7967979 ]), 'previousTarget': array([  6.8507125, 107.40285  ]), 'currentState': array([ 3.5088038, 89.22744  ,  0.5652435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.2856476719024176
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.166179 , 118.90543  ,   1.5303091], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1071092292588585}
episode index:4744
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.926035, 113.9903  ,   4.302166], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.86521322754319}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.2856579414882948
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.255484 , 121.877045 ,   5.5319586], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8943517640710736}
episode index:4745
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.      , 101.      ,   2.543303], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.2856277370100592
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.340793 , 118.89691  ,   0.7410947], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7362398791392837}
episode index:4746
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.16207698, 85.48406624]), 'previousTarget': array([11.93010557, 83.97136265]), 'currentState': array([11.8350525, 65.49539  ,  2.1630905], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855675668526946
{'currentTarget': array([  8.76175834, 116.40091123]), 'previousTarget': array([  8.25387417, 114.58962521]), 'currentState': array([ 2.2552059, 97.488884 ,  1.555542 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4747
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.004689  , 113.868416  ,   0.82086647], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.213350531305726}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.28558147621236296
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.089195 , 118.459564 ,   2.5111413], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4544077112954232}
episode index:4748
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.52169122, 97.30107271]), 'previousTarget': array([ 9.47605556, 97.99433347]), 'currentState': array([ 7.2219086, 77.34335  ,  3.6099598], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.2855675060698391
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.068791, 121.4419  ,   1.668376], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4101138019343837}
episode index:4749
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.15177459, 90.16738613]), 'previousTarget': array([ 8.19784581, 89.96409691]), 'currentState': array([ 6.915082 , 70.20566  ,  5.7662544], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 228
reward sum = 0.10111704470857531
running average episode reward sum: 0.28552867439376306
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.009027 , 119.55404  ,   1.7744436], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0866966621840213}
episode index:4750
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.18733914, 76.07392077]), 'previousTarget': array([ 7.20990121, 73.96336993]), 'currentState': array([ 5.9093227, 56.114796 ,  1.7151172], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854685757462375
{'currentTarget': array([ 4.52586337, 89.82271506]), 'previousTarget': array([ 4.54860763, 89.81252436]), 'currentState': array([ 0.95613533, 70.14387   ,  4.698187  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4751
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.47406169, 101.91642513]), 'previousTarget': array([  8.57404971, 101.93796297]), 'currentState': array([ 6.792387, 81.98725 ,  5.136989], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28540850239275556
{'currentTarget': array([  9.83928451, 118.93431564]), 'previousTarget': array([  9.83353562, 118.88887828]), 'currentState': array([ 6.856817, 99.15794 ,  4.059771], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4752
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.28854833, 77.86990506]), 'previousTarget': array([ 7.28764556, 77.95850618]), 'currentState': array([ 6.0040255, 57.911198 ,  4.552524 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28534845431735206
{'currentTarget': array([ 4.79655989, 90.14313189]), 'previousTarget': array([ 4.7544309 , 89.96505796]), 'currentState': array([ 1.3627282, 70.44012  ,  4.5021095], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4753
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.67138085, 111.50028957]), 'previousTarget': array([  9.05798302, 110.89383588]), 'currentState': array([ 8.89871   , 91.51522   ,  0.49171847], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852884315040754
{'currentTarget': array([ 10.89230053, 117.67139028]), 'previousTarget': array([ 10.89544733, 117.70193318]), 'currentState': array([18.04869  , 98.995575 ,  1.8141452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4754
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.23187024, 78.03131084]), 'previousTarget': array([ 7.28764556, 77.95850618]), 'currentState': array([ 5.91559  , 58.074673 ,  0.1399152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28522843393698727
{'currentTarget': array([  8.11276699, 113.80641047]), 'previousTarget': array([  8.16190097, 113.8873886 ]), 'currentState': array([ 2.2832377, 94.67485  ,  1.4565492], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4755
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.173363, 122.88732 ,   5.324435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.074798693143841}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.28518326800186705
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.463625, 121.63558 ,   3.846035], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7212863129135345}
episode index:4756
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.44722837, 87.69930071]), 'previousTarget': array([ 4.41082867, 87.70701012]), 'currentState': array([ 1.0587586, 67.988434 ,  3.310088 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851233177668446
{'currentTarget': array([ 4.89632656, 90.97390243]), 'previousTarget': array([ 4.89632656, 90.97390243]), 'currentState': array([ 1.432848 , 71.27608  ,  3.7207208], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4757
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.402741, 103.38466 ,   2.96712 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.674448904541475}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.28507819290949665
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.545824 , 118.836105 ,   1.8870926], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.28552505170927}
episode index:4758
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.57506981, 115.86444706]), 'previousTarget': array([  8.5704125 , 115.88993593]), 'currentState': array([ 2.0598435, 96.95541  ,  3.944943 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28501828994817924
{'currentTarget': array([  8.77284844, 115.76187151]), 'previousTarget': array([  8.81610044, 115.91232183]), 'currentState': array([ 3.210328 , 96.55098  ,  4.4673595], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4759
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.19018866, 79.88881997]), 'previousTarget': array([ 9.33328705, 79.9972228 ]), 'currentState': array([ 8.786488 , 59.892895 ,  4.6141305], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 203
reward sum = 0.1300003445350054
running average episode reward sum: 0.28498572315292436
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.068681 , 118.2292   ,   1.4892452], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6202516278090613}
episode index:4760
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.96682331, 109.95331753]), 'previousTarget': array([  7.15325301, 109.32469879]), 'currentState': array([ 3.999784 , 90.3507   ,  5.7657924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.2850152556881556
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.299747 , 119.57462  ,   1.3612225], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3675850900048414}
episode index:4761
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.0694826, 112.009705 ,   1.214267 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.257794079195728}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.2851061240789389
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.275713  , 119.3639    ,   0.69723016], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8378764813851125}
episode index:4762
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.101759 , 106.17798  ,   3.5106685], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.851177382663792}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.2851682824434233
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.215925 , 118.26694  ,   1.2005496], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4872533661903193}
episode index:4763
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.16292865, 86.99854909]), 'previousTarget': array([ 8.13026624, 86.96803691]), 'currentState': array([ 7.051322, 67.029465,  4.685174], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28510842344207077
{'currentTarget': array([ 4.7965364 , 89.98225585]), 'previousTarget': array([ 4.8002377 , 90.00433662]), 'currentState': array([ 1.3805547, 70.27614  ,  5.1188216], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4764
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.96323197, 78.00897858]), 'previousTarget': array([10., 77.]), 'currentState': array([ 8.469578 , 58.01507  ,  2.6953323], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28504858956516793
{'currentTarget': array([  8.81986094, 116.20523612]), 'previousTarget': array([  8.81571367, 116.00726186]), 'currentState': array([ 2.8806138, 97.10745  ,  5.673541 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4765
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.61163897, 116.00366502]), 'previousTarget': array([ 11.0776773 , 114.61161351]), 'currentState': array([13.637405 , 96.23387  ,  2.9953134], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.28501472058585015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.427081, 119.42206 ,   2.037327], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6757358604420602}
episode index:4766
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.43874525, 91.99599967]), 'previousTarget': array([ 9.41657627, 91.99566113]), 'currentState': array([ 9.037987 , 72.000015 ,  5.8712606], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28495493146888223
{'currentTarget': array([ 4.24840148, 86.19466904]), 'previousTarget': array([ 4.7058479 , 88.13021177]), 'currentState': array([ 0.8938318, 66.478004 ,  4.5476747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4767
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.660948 , 108.00615  ,   1.3803556], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.068368390182423}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2850021275290292
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.031345 , 118.577644 ,   1.4151175], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7208682103988495}
episode index:4768
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.073185, 104.07044 ,   5.1774  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.92972591579263}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.28502146796085864
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.529491  , 120.41172   ,   0.92051715], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5270589562982735}
episode index:4769
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.40290844, 109.44526141]), 'previousTarget': array([ 12.72606242, 107.53800035]), 'currentState': array([16.84254  , 89.944244 ,  1.8159401], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.2850120279213667
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.053974, 118.35202 ,   4.79519 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5500693743396408}
episode index:4770
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.752052, 113.047615,   4.735644], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.169752001446751}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.28507657506420475
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.580617 , 118.893585 ,   1.7467183], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9293790825483335}
episode index:4771
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.058222 , 121.81432  ,   1.0974736], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.287678965883167}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2851831420609733
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.4262705, 120.57057  ,   3.1571329], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5361639849206254}
episode index:4772
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.05588295, 119.18735284]), 'previousTarget': array([ 10.04869701, 118.97736275]), 'currentState': array([11.427974 , 99.234474 ,  0.7481407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2852403561813351
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.211631 , 118.46447  ,   1.2875898], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.357141696609823}
episode index:4773
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.63805039, 101.65322932]), 'previousTarget': array([  6.62324859, 101.66906377]), 'currentState': array([ 3.0331788, 81.98079  ,  2.549351 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851806074682682
{'currentTarget': array([  8.52819691, 115.74535303]), 'previousTarget': array([  8.52819691, 115.74535303]), 'currentState': array([ 1.9897915, 96.844315 ,  1.711399 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4774
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.2684464, 110.91995  ,   2.154396 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.925778168307893}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.28529390338792626
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.252706 , 118.37372  ,   5.8748417], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7897601099992366}
episode index:4775
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.37196542, 83.90331464]), 'previousTarget': array([ 4.17356191, 83.74660743]), 'currentState': array([ 1.2908796, 64.14207  ,  4.662511 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28523416848353184
{'currentTarget': array([ 4.80762743, 91.04005513]), 'previousTarget': array([ 4.695161  , 89.22185504]), 'currentState': array([ 1.2780119, 71.35397  ,  1.7182426], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4776
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.811123 , 108.30613  ,   5.2205467], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.120872307936892}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.2853063036023762
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.552242 , 118.425674 ,   1.2878194], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6683742111527153}
episode index:4777
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.2159147, 120.93346  ,   5.236306 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.9364045533930354}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2854416654785387
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.542637, 119.32893 ,   5.837249], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.60444297079907}
episode index:4778
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.48286768, 114.9440344 ]), 'previousTarget': array([ 11.4, 115.2]), 'currentState': array([17.111586, 95.75243 ,  2.646772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.285449148091865
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.725642 , 119.24503  ,   1.3448011], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8032726637291003}
episode index:4779
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.48069469, 115.84555753]), 'previousTarget': array([  9.48069469, 115.84555753]), 'currentState': array([ 7.      , 96.      ,  2.560596], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28538943069686673
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.918044, 111.56254 ,   6.138393], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.92780733629804}
episode index:4780
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.959084 , 103.14182  ,   3.5940301], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.11590819971002}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.28536103746844316
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.954589, 118.71088 ,   2.726491], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.289923120886786}
episode index:4781
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.04114369, 113.76743395]), 'previousTarget': array([  9.04114369, 113.76743395]), 'currentState': array([ 6.       , 94.       ,  1.9562505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.2854529695676672
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.502517 , 119.756996 ,   1.5524312], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5220405521421692}
episode index:4782
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.88461426, 87.88917521]), 'previousTarget': array([ 8.76866244, 87.98522349]), 'currentState': array([ 8.190323 , 67.90123  ,  6.0887585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853932888297271
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.669234 , 100.52213  ,   2.8321671], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.760599972725327}
episode index:4783
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.38906772, 82.78014211]), 'previousTarget': array([ 6.71776684, 81.92609538]), 'currentState': array([ 2.930191 , 62.93187  ,  2.8175743], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853336330419282
{'currentTarget': array([15.25131658, 92.60671439]), 'previousTarget': array([15.19300355, 92.78728473]), 'currentState': array([19.01677  , 72.96438  ,  2.3039527], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4784
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.15446507, 66.69189831]), 'previousTarget': array([11.45965677, 65.9926994 ]), 'currentState': array([10.212417 , 46.691982 ,  3.0050747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852740021886279
{'currentTarget': array([  8.66665775, 116.63743433]), 'previousTarget': array([  8.6648109 , 116.60139309]), 'currentState': array([ 1.2945684, 98.04571  ,  3.041021 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4785
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.40972542, 75.95493878]), 'previousTarget': array([15.51930531, 75.84555753]), 'currentState': array([17.847855, 56.104107,  4.048562], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28521439625419653
{'currentTarget': array([  8.76879854, 113.43906028]), 'previousTarget': array([  8.83954335, 113.62987733]), 'currentState': array([ 5.0800605, 93.78217  ,  4.572325 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4786
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.68675396, 111.96352094]), 'previousTarget': array([ 12.67544468, 111.97366596]), 'currentState': array([19.028147, 92.995476,  6.20171 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.2852613507873061
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.755024 , 118.54442  ,   1.4569969], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.915380727946199}
episode index:4787
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.27185955, 93.24919616]), 'previousTarget': array([10., 91.]), 'currentState': array([10.475102, 73.25023 ,  1.599018], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.2852327132853763
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.093946 , 118.8286   ,   2.0701337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.602779287227837}
episode index:4788
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.03539479, 88.7364793 ]), 'previousTarget': array([ 5.09935538, 88.75839053]), 'currentState': array([ 1.8987241, 68.98398  ,  5.294532 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28517315331183585
{'currentTarget': array([ 4.77744931, 89.57850901]), 'previousTarget': array([ 4.88296495, 89.57151205]), 'currentState': array([ 1.3934913, 69.86687  ,  1.5326926], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4789
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.890369 , 113.9867   ,   3.3747709], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.3034315876338205}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.28528437122087247
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.478429 , 119.28002  ,   1.9433882], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6833144662546682}
episode index:4790
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0407915, 100.82992  ,   5.877004 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.17012549182622}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.28531637563134815
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.344135 , 119.0021   ,   1.6606762], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.055574407110952}
episode index:4791
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.38019226, 80.80902691]), 'previousTarget': array([ 3.91921747, 78.78580727]), 'currentState': array([ 1.5413213, 61.01153  ,  1.6271242], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285256835486183
{'currentTarget': array([16.62557156, 71.65327538]), 'previousTarget': array([16.62557156, 71.65327538]), 'currentState': array([19.341047 , 51.838478 ,  2.9650383], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4792
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.2590604 , 100.69419975]), 'previousTarget': array([  6.22665585, 102.54828331]), 'currentState': array([ 2.4543746, 81.059425 ,  4.9286413], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.2852650117211877
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.275323 , 121.00943  ,   1.1888342], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9983642904734191}
episode index:4793
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.17815287, 78.04113321]), 'previousTarget': array([ 9.31742033, 76.99748095]), 'currentState': array([ 7.3105736, 58.05996  ,  2.4568653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852055071296731
{'currentTarget': array([ 5.41251016, 90.26455075]), 'previousTarget': array([ 5.2515145 , 90.13317559]), 'currentState': array([2.3630514e+00, 7.0498398e+01, 3.2712158e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4794
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.072653 , 112.91496  ,   0.5155013], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.511332751017822}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.28519658348815374
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.832786  , 118.478325  ,   0.45181167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9177811607795996}
episode index:4795
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.5929875, 116.11465  ,   6.1195087], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.364197644886433}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.2853374090566508
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.031017  , 119.08499   ,   0.18366158], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.171205539908005}
episode index:4796
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.55422331, 113.80686042]), 'previousTarget': array([ 10.84018998, 114.74881264]), 'currentState': array([12.336897 , 93.88647  ,  3.8809705], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.2852964240500756
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.128286  , 121.19339   ,   0.68075407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2197953463161992}
episode index:4797
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.34721313, 108.5919009 ]), 'previousTarget': array([ 13.4237961 , 108.20692454]), 'currentState': array([18.977982 , 89.4009   ,  4.6360803], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.2852535205543775
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.293965 , 119.90279  ,   2.7250988], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.3096225656529375}
episode index:4798
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.022358 , 120.96224  ,   2.7563486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.113706013664947}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28539831040214697
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.890314, 121.883545,   2.825937], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0833628280376297}
episode index:4799
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.79205497, 74.21599053]), 'previousTarget': array([10.69700447, 73.99770471]), 'currentState': array([11.138    , 54.218983 ,  3.6522965], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.28539640485158385
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.373806 , 120.1225   ,   2.0566344], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.37925653046365}
episode index:4800
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.6935806 , 82.04296469]), 'previousTarget': array([11.94882334, 82.97235659]), 'currentState': array([11.058975 , 62.046303 ,  3.6584995], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28533695965165645
{'currentTarget': array([  9.60387189, 116.96797784]), 'previousTarget': array([  9.60387189, 116.96797784]), 'currentState': array([ 7.0129275, 97.13651  ,  2.5574064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4801
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.70497494, 109.44493993]), 'previousTarget': array([ 11.71202025, 109.72787848]), 'currentState': array([14.894265 , 89.70087  ,  6.1152244], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852775392102463
{'currentTarget': array([ 9.72504061, 96.80760797]), 'previousTarget': array([ 9.0559112 , 95.08437457]), 'currentState': array([ 9.487946 , 76.80901  ,  0.7686078], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4802
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.20750987, 80.41841295]), 'previousTarget': array([ 4.60068074, 78.83019061]), 'currentState': array([ 1.3114955, 60.629196 ,  2.2187097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285218143511889
{'currentTarget': array([  8.51735981, 116.38128295]), 'previousTarget': array([  8.56464741, 116.51436498]), 'currentState': array([ 0.93482065, 97.87439   ,  1.1025262 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4803
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.8628881, 107.0766   ,   1.10762  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.827226674023304}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2853432822979432
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.147895  , 118.120255  ,   0.26071858], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.202522574321601}
episode index:4804
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.768149 , 116.9123   ,   0.4642481], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.0963923077411657}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.2853779754026192
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.190454 , 120.943306 ,   3.0124896], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5188830623722414}
episode index:4805
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.7749224, 118.00945  ,   2.6149137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.670498800806728}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.2853970887341883
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.070987 , 121.19668  ,   5.4814186], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5149601464633933}
episode index:4806
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.2179088 , 89.73293211]), 'previousTarget': array([ 9.39992002, 89.9960012 ]), 'currentState': array([ 7.042367 , 69.76751  ,  3.2503045], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.2854327060961624
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.98046  , 119.00534  ,   2.2679949], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.216206182242768}
episode index:4807
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.33693108, 86.5589263 ]), 'previousTarget': array([11.91071014, 84.97031416]), 'currentState': array([12.135867 , 66.57489  ,  2.1894772], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28537333989273145
{'currentTarget': array([ 8.01782374, 78.30712835]), 'previousTarget': array([ 8.0948656 , 78.51275941]), 'currentState': array([ 7.06805   , 58.329693  ,  0.09334486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4808
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.57509242, 114.92887128]), 'previousTarget': array([  9.59490445, 114.93630557]), 'currentState': array([ 7.9051533 , 94.99871   ,  0.11776751], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.2853702999508764
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.017952 , 119.071785 ,   1.1334407], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9283886089023179}
episode index:4809
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.44783651, 69.97294918]), 'previousTarget': array([ 3.55043485, 69.83671551]), 'currentState': array([ 0.85057 , 50.14231 ,  5.503737], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28531097140618805
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.497888 , 107.473656 ,   3.8976867], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.615584362262803}
episode index:4810
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([8.89383221e+00, 1.15376633e+02, 1.13137364e-01], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.75385448516043}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.28539642188356146
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.471973 , 119.632774 ,   1.9429377], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5980080135047264}
episode index:4811
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.6107945, 96.8865064]), 'previousTarget': array([11.60803474, 96.95150202]), 'currentState': array([11.139128 , 76.893486 ,  2.9898653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28533711256895555
{'currentTarget': array([ 13.37682633, 105.31705826]), 'previousTarget': array([ 13.51551232, 105.06987616]), 'currentState': array([17.859465 , 85.82588  ,  0.6973373], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4812
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.69119408, 92.38585319]), 'previousTarget': array([ 9.43467991, 93.99527578]), 'currentState': array([ 7.744333, 72.40828 ,  4.089086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28527782789981593
{'currentTarget': array([  8.71709081, 116.19889205]), 'previousTarget': array([  8.65541516, 116.09308479]), 'currentState': array([ 2.32136 , 97.2491  ,  5.151662], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4813
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.95499641, 119.68480298]), 'previousTarget': array([  9.96680906, 119.77872706]), 'currentState': array([ 7.128081 , 99.8856   ,  5.3333573], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.28524903390599216
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.096742  , 121.89917   ,   0.91154873], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.103026864172878}
episode index:4814
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.12560993, 68.88699059]), 'previousTarget': array([12.15568295, 68.98217027]), 'currentState': array([12.956621 , 48.904263 ,  4.4628563], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851897921544021
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.503725 , 114.61731  ,   2.8099048], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.406208948028257}
episode index:4815
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.69048994, 80.50209823]), 'previousTarget': array([10.6721752 , 78.99731309]), 'currentState': array([ 9.533772 , 60.502712 ,  2.5354428], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.28515074374026916
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.221407 , 120.44683  ,   3.7672086], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8338623186384306}
episode index:4816
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.24738537, 104.35432363]), 'previousTarget': array([  7.67835792, 102.81984861]), 'currentState': array([ 6.020929 , 84.47864  ,  1.4835525], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 294
reward sum = 0.05208914293358933
running average episode reward sum: 0.28510236059706656
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.142448 , 119.73438  ,   1.4175894], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8764462263182322}
episode index:4817
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.71039395, 117.601502  ]), 'previousTarget': array([  9.6609096 , 115.93091516]), 'currentState': array([ 7.3129125, 97.74572  ,  1.613378 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2852271597899098
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.757303 , 118.913345 ,   1.5769964], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1134271754959715}
episode index:4818
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.05764499, 109.05604105]), 'previousTarget': array([ 11.0735161 , 108.90700027]), 'currentState': array([12.98152  , 89.14879  ,  4.9711924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.28525195846450796
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.704398  , 120.91794   ,   0.16576186], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.5878270493850466}
episode index:4819
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.50547242, 104.07480245]), 'previousTarget': array([ 10.44465866, 103.99228841]), 'currentState': array([11.139961 , 84.08487  ,  5.9757733], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851927775602622
{'currentTarget': array([ 10.1836598, 118.003341 ]), 'previousTarget': array([ 10.16475307, 117.99372554]), 'currentState': array([1.2015597e+01, 9.8087418e+01, 4.5307674e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4820
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.965061 , 123.89535  ,   1.0462906], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.030487716475468}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.2852255208093943
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.6290865, 121.7292   ,   4.536338 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.84007888684792}
episode index:4821
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.03319094, 119.77872706]), 'previousTarget': array([ 10.03319094, 119.77872706]), 'currentState': array([ 13.       , 100.       ,   1.3456361], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851663699340709
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.1107755, 114.332405 ,   4.4415255], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.17340795606134}
episode index:4822
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.33038021, 94.95570316]), 'previousTarget': array([ 8.33038021, 94.95570316]), 'currentState': array([ 7.       , 75.       ,  2.5170276], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28510724358741235
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.367182 , 120.17481  ,   2.2463489], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.6370218009474122}
episode index:4823
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.65743387, 79.55265588]), 'previousTarget': array([ 6.6609096 , 79.93091516]), 'currentState': array([ 5.01025 , 59.6206  ,  6.197466], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28504814175416454
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.649452, 106.68909 ,   4.593247], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.37925209105055}
episode index:4824
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.84239898, 102.58387105]), 'previousTarget': array([ 12.79857683, 102.74210955]), 'currentState': array([16.063877 , 82.845024 ,  0.9461371], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.284989064419086
{'currentTarget': array([  8.01248211, 113.59815017]), 'previousTarget': array([  8.01248211, 113.59815017]), 'currentState': array([ 2.0824924, 94.49749  ,  3.671145 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4825
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.22893399, 114.26288879]), 'previousTarget': array([  8.82323232, 116.13347761]), 'currentState': array([ 2.3295677, 95.15275  ,  4.1597433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.28496464354685136
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.99308 , 118.97162 ,   3.124565], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4296070645466328}
episode index:4826
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.39689372, 94.0506841 ]), 'previousTarget': array([ 9.43467991, 93.99527578]), 'currentState': array([ 8.932185 , 74.05608  ,  3.8988786], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.2849356881423908
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.87     , 118.615875 ,   1.7347214], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7868132522508504}
episode index:4827
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.07253108, 97.95258857]), 'previousTarget': array([11.04869701, 97.97736275]), 'currentState': array([12.044313 , 77.97621  ,  5.6161714], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.28491942299097717
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.326391 , 118.64813  ,   1.7422118], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3907111279398925}
episode index:4828
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.57957356, 89.85541141]), 'previousTarget': array([11.80215419, 89.96409691]), 'currentState': array([12.626136, 69.88281 ,  5.348979], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.2849205762465827
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.040066 , 120.42549  ,   1.4112889], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0500079091409742}
episode index:4829
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.6311064, 115.42172  ,   0.1587508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.055894302684239}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.28501320208372716
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.1015294e+01, 1.1829727e+02, 8.9782536e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9824494004304523}
episode index:4830
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.84551019, 98.52353041]), 'previousTarget': array([ 5.83020719, 98.62981184]), 'currentState': array([ 2.0470517 , 78.88755   ,  0.52138025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28495420535384025
{'currentTarget': array([  8.81197074, 116.00221661]), 'previousTarget': array([  8.83943732, 116.06489858]), 'currentState': array([ 3.1147726, 96.83083  ,  2.6698585], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4831
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.09870394, 114.08924898]), 'previousTarget': array([  8.09551454, 114.04848294]), 'currentState': array([ 1.9743987, 95.05     ,  3.4209437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2848952330431296
{'currentTarget': array([  8.48695356, 116.25046694]), 'previousTarget': array([  8.59422516, 116.4614494 ]), 'currentState': array([ 1.0027455, 97.70359  ,  5.0968122], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4832
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.961634 , 120.95417  ,   3.2498355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.01228711687801}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.28503504284593467
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.524414 , 118.63666  ,   3.5851767], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0451260157855358}
episode index:4833
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.70549422, 86.04106506]), 'previousTarget': array([10.62969312, 85.99657153]), 'currentState': array([11.120903 , 66.04538  ,  6.2026157], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.2850192086338719
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.831273 , 119.20161  ,   2.0020337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1525826272334894}
episode index:4834
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.973128 , 114.78004  ,   5.5445433], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.2200322392070415}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.2850892202546077
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.873416 , 118.84503  ,   3.7356849], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.200826889126754}
episode index:4835
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.66642645, 102.18410972]), 'previousTarget': array([ 9.49984382, 99.99375293]), 'currentState': array([ 9.292025 , 82.187614 ,  1.5869386], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28503026880294213
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 13.132097  , 103.09541   ,   0.45239064], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.19229733792185}
episode index:4836
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.464195 , 114.13411  ,   2.6120448], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.063610114278774}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.28509896011411423
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.716006, 119.27113 ,   6.070786], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7822396824862695}
episode index:4837
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.0071493 , 99.69068022]), 'previousTarget': array([10.52394444, 97.99433347]), 'currentState': array([10.01419 , 79.69068 ,  2.205049], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28504003101942343
{'currentTarget': array([  8.58005145, 114.51209132]), 'previousTarget': array([  8.55262427, 114.30519004]), 'currentState': array([ 3.5702064, 95.14972  ,  2.3689883], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4838
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.1880882, 105.87358  ,   4.9258986], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.683043656426317}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.2850343365734107
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.280306 , 118.01898  ,   1.8354712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.107698325445378}
episode index:4839
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.76108587, 94.67999675]), 'previousTarget': array([10.57456437, 92.9954746 ]), 'currentState': array([11.361988 , 74.689026 ,  1.3838639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28497544518155665
{'currentTarget': array([  9.38055672, 112.1736031 ]), 'previousTarget': array([  9.44214826, 112.08638704]), 'currentState': array([ 7.8025327, 92.235954 ,  2.9777384], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4840
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.695357 , 108.10148  ,   1.4069505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.902420729520278}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.28510718846791205
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.631183 , 118.2608   ,   2.8430424], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.777872788698225}
episode index:4841
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.47364295, 105.94649083]), 'previousTarget': array([ 10.41201897, 105.99135509]), 'currentState': array([11.147317 , 85.95784  ,  0.3786859], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2850483063554652
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.747522 , 123.76504  ,   3.0894845], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9678971641891088}
episode index:4842
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.88544947, 75.23118761]), 'previousTarget': array([ 5.83833848, 74.91533358]), 'currentState': array([ 4.0550313, 55.315125 ,  3.2496505], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28498944855939756
{'currentTarget': array([15.22635794, 91.16623498]), 'previousTarget': array([15.1487893, 91.2491983]), 'currentState': array([1.879340e+01, 7.148690e+01, 6.549278e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4843
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.7790895, 113.841545 ,   3.6624289], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.466100224215613}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.28508486198750904
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.319694  , 119.411835  ,   0.85785973], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4448285389072426}
episode index:4844
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.72778661, 78.59780291]), 'previousTarget': array([ 3.91921747, 78.78580727]), 'currentState': array([ 0.7320744, 58.823433 ,  5.757801 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28502602094272317
{'currentTarget': array([  8.58273617, 116.6712164 ]), 'previousTarget': array([  8.59766798, 116.80523355]), 'currentState': array([ 0.74807507, 98.26964   ,  4.876208  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4845
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.866585 , 120.199486 ,   1.0193548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8772142415774298}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2851735599396396
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.866585 , 120.199486 ,   1.0193548], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8772142415774298}
episode index:4846
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.322395, 106.018555,   3.199976], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.985161842889813}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2852299025388986
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.899471 , 119.64525  ,   0.6469521], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1562924134707404}
episode index:4847
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.8425886, 105.27755  ,   5.1637993], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.338071548919228}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.2852553955817789
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.879305  , 120.22295   ,   0.14678639], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.25352653884650866}
episode index:4848
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.40141113, 73.75577297]), 'previousTarget': array([ 5.75787621, 71.922597  ]), 'currentState': array([ 3.422345 , 53.85393  ,  1.9599266], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28519656790688064
{'currentTarget': array([ 5.35745904, 90.04607688]), 'previousTarget': array([ 5.52534781, 90.09243885]), 'currentState': array([ 2.2942443, 70.28205  ,  5.993233 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4849
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.90594591, 85.2558676 ]), 'previousTarget': array([ 6.84396421, 85.91481348]), 'currentState': array([ 3.5654507, 65.39329  ,  3.2649052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 287
reward sum = 0.05588571986991975
running average episode reward sum: 0.2851492873196565
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.555783 , 119.15017  ,   1.6081716], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9589267943693358}
episode index:4850
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.97257248, 70.0184411 ]), 'previousTarget': array([ 4.99007438, 69.9007438 ]), 'currentState': array([ 2.9709597, 50.118855 ,  5.788639 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 239
reward sum = 0.0905339582851764
running average episode reward sum: 0.285109168719567
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.657895, 118.04648 ,   5.583122], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5621989125059037}
episode index:4851
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.058181, 119.35767 ,   2.096848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0452982464073175}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.28525650813244424
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.058181, 119.35767 ,   2.096848], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0452982464073175}
episode index:4852
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.3316542 , 73.46394383]), 'previousTarget': array([15.59337265, 73.85467564]), 'currentState': array([17.60817  , 53.59393  ,  3.8785157], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851977287159735
{'currentTarget': array([  7.8733465 , 112.83831524]), 'previousTarget': array([  7.95838928, 112.98823305]), 'currentState': array([ 2.180081 , 93.665764 ,  3.9306712], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4853
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.988168 , 113.093376 ,   1.2948995], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.976956958387636}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.2852388893343415
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.831905 , 119.79205  ,   1.1716257], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1864598343586052}
episode index:4854
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.7790341, 118.78062  ,   1.1433684], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.339346318427026}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.28525477669346117
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.789898, 118.36504 ,   6.200063], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0340671930447987}
episode index:4855
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10.00124766, 119.97504678]), 'currentState': array([ 11.219452 , 100.06019  ,   0.7147454], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.977065696425022}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.2853098485016302
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.072629 , 118.570915 ,   1.2220995], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3993837480841123}
episode index:4856
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.57587032, 102.96875572]), 'previousTarget': array([ 10.45965677, 102.9926994 ]), 'currentState': array([11.251736 , 82.98018  ,  3.3975654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.2852669775923916
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.426811, 119.129944,   2.1611  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7977543356138481}
episode index:4857
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.233516, 114.10526 ,   4.88876 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.899360708777577}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.2853732689904868
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.648209 , 118.043816 ,   2.4564252], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3778135538259644}
episode index:4858
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.756149  , 118.90508   ,   0.62479675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.859359910414061}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.2853680646670432
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.634148 , 120.54991  ,   3.7514963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7241929066346924}
episode index:4859
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.17157288, 90.79898987]), 'previousTarget': array([14.17157288, 90.79898987]), 'currentState': array([17.       , 71.       ,  2.7920055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28530934695826393
{'currentTarget': array([ 7.86656036, 69.71255151]), 'previousTarget': array([ 7.86656036, 69.71255151]), 'currentState': array([ 7.018825 , 49.730526 ,  4.2238855], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4860
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.4820103 , 99.25277224]), 'previousTarget': array([ 8.424941, 97.949174]), 'currentState': array([ 8.982832 , 79.259    ,  1.0848475], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 284
reward sum = 0.05759639025694116
running average episode reward sum: 0.28526250207928816
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.0775385, 121.86363  ,   3.2331836], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8652445422415598}
episode index:4861
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.94210731, 107.50926195]), 'previousTarget': array([ 12.96694159, 105.58914087]), 'currentState': array([17.527487 , 88.042    ,  0.9986696], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.2852433994077157
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.228683  , 121.424866  ,   0.69268435], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2732805746544886}
episode index:4862
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.02758232, 107.98215093]), 'previousTarget': array([ 11.13318583, 107.91268452]), 'currentState': array([12.731459 , 88.05486  ,  4.6095014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851847435575393
{'currentTarget': array([  8.53774591, 116.58409254]), 'previousTarget': array([  8.67598532, 116.73573941]), 'currentState': array([ 0.6671195, 98.19787  ,  2.639609 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4863
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.60936768, 102.29588409]), 'previousTarget': array([  9.02429504, 100.97375327]), 'currentState': array([ 9.168185 , 82.30075  ,  0.7857914], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.28518233903604767
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.050229, 119.44215 ,   2.296077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.028005773817073}
episode index:4864
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.11925147, 82.75525931]), 'previousTarget': array([ 4.11925147, 82.75525931]), 'currentState': array([ 1.       , 63.       ,  1.9964749], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851237198502232
{'currentTarget': array([15.1649602 , 90.89969821]), 'previousTarget': array([15.28498086, 90.68414868]), 'currentState': array([18.660099 , 71.207466 ,  3.6108117], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4865
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.537039 , 113.90348  ,   1.4023126], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.186054891676292}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.28521561866851425
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.153942  , 121.09743   ,   0.21828139], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3856986630396582}
episode index:4866
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0512017, 114.20814  ,   4.4970284], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.659580764305371}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.2853168320937723
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.783297 , 120.564964 ,   5.9744844], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.341473761896042}
episode index:4867
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 6.52446811, 85.09157048]), 'previousTarget': array([ 5.52508559, 84.83995823]), 'currentState': array([ 4.5430365 , 65.189964  ,  0.11140859], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.28529854423056955
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.810201 , 119.09689  ,   1.7114294], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2132711372951408}
episode index:4868
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.80114397, 81.09930117]), 'previousTarget': array([12.66961979, 79.95570316]), 'currentState': array([12.726174, 61.120705,  2.187037], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.2852622305177491
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.49524  , 119.063705 ,   1.4113116], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7722724158809235}
episode index:4869
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0843842, 110.87107  ,   5.0721917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.760311604755481}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.2852993185145976
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.070716, 121.68469 ,   2.707217], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.561313246534869}
episode index:4870
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.92930801, 117.82802293]), 'previousTarget': array([  9.90815322, 117.9793708 ]), 'currentState': array([ 9.278707, 97.83861 ,  4.605117], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852407475192138
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.770519 , 108.794464 ,   1.0411619], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.542955059405687}
episode index:4871
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.06175116, 102.1511712 ]), 'previousTarget': array([ 10., 102.]), 'currentState': array([10.130944 , 82.15129  ,  4.5852656], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.28525436994512493
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.001062 , 118.11781  ,   1.1512364], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.7456108689123884}
episode index:4872
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.47930969, 84.92240436]), 'previousTarget': array([12.59993437, 82.95093522]), 'currentState': array([13.889406 , 64.972176 ,  1.7162055], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28519583221273315
{'currentTarget': array([  8.08622381, 114.97084956]), 'previousTarget': array([  8.03648244, 114.98367215]), 'currentState': array([ 0.97310346, 96.27851   ,  3.317686  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4873
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.055168 , 121.945885 ,   4.9696193], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.28686920605263}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.28517337662579806
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.778963 , 118.06915  ,   3.3739169], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.943457769564884}
episode index:4874
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.16089052, 117.25013211]), 'previousTarget': array([ 11.12161403, 117.3829006 ]), 'currentState': array([18.939394 , 98.824745 ,  3.8849883], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.2851626465922231
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.370985  , 119.88889   ,   0.68749696], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.6387523527900959}
episode index:4875
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.463151 , 110.78501  ,   1.3815835], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.71270439744785}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.28518800703681507
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.95241  , 121.478676 ,   4.3613133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.758853765247199}
episode index:4876
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.64853798, 104.65605106]), 'previousTarget': array([ 11.85036314, 102.88414095]), 'currentState': array([13.785022 , 84.77049  ,  2.1389508], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851295309230081
{'currentTarget': array([  8.41362057, 116.19254537]), 'previousTarget': array([  8.34475428, 116.04371071]), 'currentState': array([ 0.7215608, 97.7309   ,  5.83024  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4877
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.38489292, 107.78359405]), 'previousTarget': array([ 11.66317505, 105.86301209]), 'currentState': array([13.637731 , 87.91088  ,  2.5485682], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2850710787846475
{'currentTarget': array([  8.91710549, 116.48264824]), 'previousTarget': array([  8.91710549, 116.48264824]), 'currentState': array([ 3.032249 , 97.368034 ,  3.1987102], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4878
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.61709559, 89.85753677]), 'previousTarget': array([13.61709559, 89.85753677]), 'currentState': array([16.       , 70.       ,  1.9355707], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28501265060699127
{'currentTarget': array([  9.07646102, 114.99750686]), 'previousTarget': array([  9.06112019, 115.07485128]), 'currentState': array([ 5.4455047, 95.329865 ,  3.788963 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4879
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.982012 , 123.88876  ,   0.7336167], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.878907212777092}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2851414425325807
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.438562 , 120.211464 ,   2.9833877], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4540215789442446}
episode index:4880
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([1.79336185e+01, 1.20285286e+02, 4.42067944e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.938746204477955}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.28513170090427825
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.306672 , 119.8844   ,   1.0933274], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3117756906551679}
episode index:4881
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.025752, 112.244   ,   5.180921], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.016180905723242}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.28520625455595056
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.488563 , 119.10113  ,   1.4624015], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.738904860443054}
episode index:4882
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.39499304, 115.93739269]), 'previousTarget': array([ 10.3390904 , 115.93091516]), 'currentState': array([12.330397, 96.03126 ,  4.816102], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.2851739398074604
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.848957  , 118.56506   ,   0.13516688], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4428640945996203}
episode index:4883
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([12.29846139, 75.18972243]), 'previousTarget': array([13.44224665, 75.93924283]), 'currentState': array([13.322978, 55.21598 ,  4.321072], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28511555038489544
{'currentTarget': array([ 4.36399005, 90.37297889]), 'previousTarget': array([ 4.40675503, 90.3713015 ]), 'currentState': array([ 0.6263762, 70.72533  ,  4.9184   ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4884
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.57979867, 68.93784158]), 'previousTarget': array([11.4368431 , 68.99206979]), 'currentState': array([12.198277, 48.947407,  4.621234], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2850571848679282
{'currentTarget': array([15.34557677, 92.71953596]), 'previousTarget': array([15.34557677, 92.71953596]), 'currentState': array([19.191418 , 73.09278  ,  2.6775398], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4885
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.23513123, 106.22999693]), 'previousTarget': array([  6.26234812, 106.29527642]), 'currentState': array([ 0.96050894, 86.93807   ,  3.640616  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28499884324188074
{'currentTarget': array([  8.35523693, 115.52437926]), 'previousTarget': array([  8.35523693, 115.52437926]), 'currentState': array([ 1.4564579, 96.75188  ,  1.1262337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4886
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.0161457, 105.85022  ,   3.119695 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 16.246790796079107}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.2850830287083853
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.197289 , 118.95734  ,   1.8697044], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0611646727702997}
episode index:4887
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.49208393, 67.91654928]), 'previousTarget': array([12.18985467, 65.98358488]), 'currentState': array([12.064808 , 47.92475  ,  1.3694857], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28502470566650556
{'currentTarget': array([  7.54809281, 111.61126883]), 'previousTarget': array([  7.46193706, 111.5177293 ]), 'currentState': array([ 1.9371397, 92.41447  ,  5.7645617], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4888
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.985819, 111.05732 ,   2.786284], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.160513553154509}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.28498419469359326
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.577729 , 119.91934  ,   1.7018062], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5833324643374369}
episode index:4889
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.2657741 , 113.80486308]), 'previousTarget': array([ 10.28616939, 111.98725709]), 'currentState': array([11.122994 , 93.82324  ,  1.1444882], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.2849703011766156
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.106326 , 118.857704 ,   2.2572067], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4503423092324594}
episode index:4890
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.48852029, 105.19562725]), 'previousTarget': array([  8.27093182, 104.87065345]), 'currentState': array([ 6.4571433, 85.29906  ,  6.1079774], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.2849346682799529
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.902849 , 121.08466  ,   2.1612337], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.19028074456686}
episode index:4891
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.934898 , 105.011536 ,   4.6493616], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 15.112838762535766}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.2849894002523328
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.451894 , 118.82864  ,   0.7723241], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9413160836165746}
episode index:4892
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.35054149, 108.47024077]), 'previousTarget': array([  7.40521743, 108.50882004]), 'currentState': array([ 2.871419, 88.97826 ,  4.50093 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28493115594408586
{'currentTarget': array([  8.44599176, 115.38406159]), 'previousTarget': array([  8.47495071, 115.47978974]), 'currentState': array([ 2.0646908, 96.429405 ,  1.2712653], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4893
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.470976 , 116.93727  ,   2.0317538], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.320634142949189}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.28505772948700875
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.086349 , 121.96528  ,   2.4721694], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.743060701213106}
episode index:4894
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.288512, 111.58239 ,   2.343882], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.765697015815512}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.2851334396996925
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.427735  , 118.201416  ,   0.61148447], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.2963737756722025}
episode index:4895
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.00482098, 104.51320597]), 'previousTarget': array([  5.9808208 , 104.36985865]), 'currentState': array([ 1.0089172, 85.14723  ,  4.6492105], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.2851118265959129
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.162918 , 119.00458  ,   0.5282251], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.300604399499071}
episode index:4896
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.9672482, 115.132225 ,   5.12763  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.392568048861193}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.2852240181284541
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.800188, 120.52367 ,   5.888533], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3091123557025033}
episode index:4897
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.95655001, 105.10809734]), 'previousTarget': array([  9.07950516, 102.97084547]), 'currentState': array([ 7.558612 , 85.15701  ,  1.9255534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.28521942208295015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.059325, 119.27327 ,   2.124098], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2846427186969551}
episode index:4898
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.027005, 116.833275,   4.074743], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.122979511594876}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.28530918752770923
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.027451 , 119.825    ,   1.7392309], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.042247918923024}
episode index:4899
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.63826833, 110.83026927]), 'previousTarget': array([ 11.60186167, 110.70920231]), 'currentState': array([15.15578   , 91.14202   ,  0.21222275], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.2853959720256163
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.578008 , 118.17382  ,   2.9363036], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.413511938525347}
episode index:4900
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.73244177, 90.63821029]), 'previousTarget': array([14.77736202, 90.73865762]), 'currentState': array([17.914907, 70.893036,  2.086646], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853377398338135
{'currentTarget': array([  8.74113238, 115.27253986]), 'previousTarget': array([  8.74113238, 115.27253986]), 'currentState': array([ 3.5947056, 95.94602  ,  1.7969189], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4901
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.59634304, 115.02919444]), 'previousTarget': array([ 10.61709559, 114.85753677]), 'currentState': array([12.978642 , 95.171585 ,  4.9910026], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.2853934167817356
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.868486 , 119.60051  ,   1.6125816], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9107155697254474}
episode index:4902
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.991646, 114.745605,   3.719095], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.564259756524967}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.285527230106765
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.48763  , 118.34726  ,   2.9335837], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.730339336226465}
episode index:4903
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  0.79407877, 109.37154   ,   3.4724572 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 14.061053563796476}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.28558515709949955
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.212753 , 118.041595 ,   1.3080754], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9699269811813505}
episode index:4904
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 13.16059014, 103.58163846]), 'previousTarget': array([ 13.18260682, 103.63230779]), 'currentState': array([16.941244 , 83.94222  ,  5.7182846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855269338258809
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.541846 , 113.98157  ,   5.8275003], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.03584592273989}
episode index:4905
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 4.07510832, 74.01683844]), 'previousTarget': array([ 4.40662735, 73.85467564]), 'currentState': array([ 1.5192541, 54.18082  ,  4.9370213], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854687342877998
{'currentTarget': array([ 5.40607872, 87.50859139]), 'previousTarget': array([ 5.5064524 , 87.59084561]), 'currentState': array([ 2.6061506, 67.70555  ,  3.30886  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4906
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.18239875, 72.22377481]), 'previousTarget': array([ 7.17444044, 71.96548746]), 'currentState': array([ 6.0049453, 52.258465 ,  3.1284738], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285410558470745
{'currentTarget': array([  8.2990806 , 115.97528856]), 'previousTarget': array([  8.3834583 , 116.14467855]), 'currentState': array([ 0.51343715, 97.55292   ,  1.9131148 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4907
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.59907731, 112.92956168]), 'previousTarget': array([ 10.52256629, 112.94535509]), 'currentState': array([12.287624 , 93.00097  ,  5.5343776], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.2854064744632627
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.615388, 121.09718 ,   2.204776], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9527644906073687}
episode index:4908
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.26194531, 105.55863118]), 'previousTarget': array([  6.11785121, 105.33410456]), 'currentState': array([ 1.2502451, 86.19674  ,  5.8899894], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.2854291412654544
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.861266, 120.43818 ,   6.231539], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.4596171614891695}
episode index:4909
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.81906876, 75.01360568]), 'previousTarget': array([14.85853601, 74.88502281]), 'currentState': array([16.949337 , 55.12738  ,  4.5578732], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.28538411072675207
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.228425  , 121.6772    ,   0.75756687], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.439565287780777}
episode index:4910
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.273022 , 102.08501  ,   4.9885073], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 17.998039851544092}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.2854171262231962
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.638085 , 121.0407   ,   3.5614495], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.220743745670752}
episode index:4911
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.13628624, 73.45517108]), 'previousTarget': array([10., 72.]), 'currentState': array([11.624395  , 53.46113   ,  0.73330027], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.28542573893313405
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.227872 , 119.75486  ,   1.3242865], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.789002984295107}
episode index:4912
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.46024441, 84.51103837]), 'previousTarget': array([ 6.13066247, 83.88618308]), 'currentState': array([ 6.0326023, 64.56206  ,  0.2920826], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.28538570363771015
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.219334 , 118.23817  ,   1.3128864], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9270433686611497}
episode index:4913
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 7.78093176, 82.07734667]), 'previousTarget': array([ 8.67757691, 80.98851894]), 'currentState': array([ 6.6126175, 62.1115   ,  2.9773133], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853276275889438
{'currentTarget': array([15.84318543, 36.88905636]), 'previousTarget': array([15.98317068, 37.04237508]), 'currentState': array([17.24584 , 16.938303,  2.254824], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4914
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.920366, 115.81354 ,   5.800292], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.460377066918118}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.28543267396980854
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.7268   , 120.016525 ,   2.7193727], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.7268790355134815}
episode index:4915
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.072948, 121.88089 ,   4.397769], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0969433358072886}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.28557802940634847
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.072948, 121.88089 ,   4.397769], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0969433358072886}
episode index:4916
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.16215898, 79.64508537]), 'previousTarget': array([11.35517412, 77.98960229]), 'currentState': array([11.737889 , 59.653374 ,  1.9215232], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.28553610697850307
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.672709, 118.15685 ,   6.029924], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.489005593037118}
episode index:4917
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.28168705, 89.40088705]), 'previousTarget': array([ 6.33682495, 88.86301209]), 'currentState': array([ 2.2337554, 69.6345   ,  2.6808767], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854780475830215
{'currentTarget': array([  9.81755859, 118.09407676]), 'previousTarget': array([  9.82436407, 118.24389326]), 'currentState': array([ 7.9118023, 98.18508  ,  4.7734704], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4918
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.28617276, 94.77354528]), 'previousTarget': array([10., 96.]), 'currentState': array([ 8.720464 , 74.78155  ,  4.5742655], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2854200117937182
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.162973, 117.46487 ,   5.133587], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.598360951390281}
episode index:4919
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.53382535, 104.12466823]), 'previousTarget': array([  9.55534134, 103.99228841]), 'currentState': array([ 8.946784 , 84.133286 ,  4.4994397], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.28540189624539075
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.606833 , 120.953255 ,   2.1616108], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.130018304363777}
episode index:4920
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.60811047, 111.40935157]), 'previousTarget': array([  8.05211208, 110.58520839]), 'currentState': array([ 2.2435777, 92.142235 ,  3.1731958], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.2853880053696394
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.605943 , 118.8435   ,   1.4699252], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3056274091081774}
episode index:4921
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.41164247, 88.42081558]), 'previousTarget': array([ 9.38454428, 87.9963028 ]), 'currentState': array([ 9.0390835, 68.424286 ,  3.5776994], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28533002324745943
{'currentTarget': array([  9.87070582, 118.99604575]), 'previousTarget': array([  9.84728211, 118.73764488]), 'currentState': array([ 7.316105 , 99.15987  ,  5.5688176], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4922
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.71585517, 117.88028964]), 'previousTarget': array([ 10.67544468, 117.97366596]), 'currentState': array([17.115063 , 98.93167  ,  5.0346665], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.2853228136259301
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.102329 , 118.55805  ,   1.7212775], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4455743250719855}
episode index:4923
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.85302896, 68.95392265]), 'previousTarget': array([10.71833779, 68.99801656]), 'currentState': array([11.1872015, 48.956715 ,  4.3810434], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28526486829416203
{'currentTarget': array([ 7.4208512 , 81.56830762]), 'previousTarget': array([ 7.48356121, 81.57232484]), 'currentState': array([ 6.0816646, 61.613194 ,  4.90681  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4924
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.7421801 , 96.82080585]), 'previousTarget': array([11.60803474, 96.95150202]), 'currentState': array([13.241179, 76.87706 ,  5.354014], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28520694649349315
{'currentTarget': array([  9.85805542, 119.07727525]), 'previousTarget': array([  9.85805542, 119.07727525]), 'currentState': array([ 6.8171854, 99.3098   ,  2.3386345], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4925
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.45526538, 106.37312582]), 'previousTarget': array([ 13.28799949, 106.43700211]), 'currentState': array([16.001722 , 86.69007  ,  3.2874558], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.2852484981229107
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.46936   , 118.40631   ,   0.28101283], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.1676864070728996}
episode index:4926
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.72555267, 87.63274136]), 'previousTarget': array([ 5.66824024, 87.82121323]), 'currentState': array([ 3.1070704 , 67.80489   ,  0.92575675], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851906031567806
{'currentTarget': array([ 5.31621552, 90.22734742]), 'previousTarget': array([ 5.31621552, 90.22734742]), 'currentState': array([ 2.2080753, 70.47034  ,  2.324817 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4927
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.0177445, 110.844734 ,   1.7827917], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.423135875259625}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.28525550077528317
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.276782, 118.325806,   5.731956], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.402583365157503}
episode index:4928
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.132574 , 120.94871  ,   1.7282265], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.195391518213678}
done in step count: 288
reward sum = 0.05532686267122055
running average episode reward sum: 0.2852088526442009
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.020662 , 118.42195  ,   1.2764267], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.5314058186480004}
episode index:4929
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.7071169 , 106.05321765]), 'previousTarget': array([ 12.42229124, 106.6773982 ]), 'currentState': array([14.137026 , 86.20138  ,  3.4207747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 250
reward sum = 0.08105851616218128
running average episode reward sum: 0.2851674428396407
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.936146  , 118.482834  ,   0.86927515], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.4597669768391484}
episode index:4930
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.46312196, 89.07434132]), 'previousTarget': array([ 9.3920815 , 88.99615643]), 'currentState': array([ 9.115969 , 69.077354 ,  4.6966534], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28510961127548745
{'currentTarget': array([ 4.58212619, 90.80481017]), 'previousTarget': array([ 4.7156926 , 90.73974377]), 'currentState': array([ 0.9329453, 71.14054  ,  5.866975 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:4931
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.8243394, 117.01581  ,   1.7957469], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.974350509917478}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.2851769633699049
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.419947  , 121.68838   ,   0.35643968], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.312398473989929}
episode index:4932
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 18.812109 , 119.8931   ,   1.7151489], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.8127574107967}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.285310006788926
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.797297, 121.41485 ,   2.675811], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6240312612370509}
episode index:4933
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.80727238, 112.81723035]), 'previousTarget': array([  7.8507125, 111.40285  ]), 'currentState': array([ 1.9677917, 93.688705 ,  1.8943152], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852521814936708
{'currentTarget': array([ 11.81210434, 114.94273817]), 'previousTarget': array([ 11.32466259, 116.68533644]), 'currentState': array([18.55844 , 96.114914,  4.43324 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4934
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.15777842, 85.43069557]), 'previousTarget': array([12.575059, 83.949174]), 'currentState': array([14.97713   , 65.51362   ,  0.87038213], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2851943796331858
{'currentTarget': array([ 5.97185327, 89.48387144]), 'previousTarget': array([ 6.00131866, 89.52970424]), 'currentState': array([ 3.3545454, 69.65587  ,  3.6278033], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4935
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.45910331, 88.62622572]), 'previousTarget': array([15.52429332, 88.69567118]), 'currentState': array([18.88763 , 68.92229 ,  6.189979], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.2851784179552045
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.216502 , 118.92376  ,   2.2893586], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.0978009369142654}
episode index:4936
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.97481  , 119.121605 ,   2.7956507], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3121858727635525}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2853232066086468
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.97481  , 119.121605 ,   2.7956507], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3121858727635525}
episode index:4937
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 12.064134 , 113.509636 ,   5.3947654], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.810688182773436}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.28539043361033445
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.954411, 118.333626,   2.989183], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.568369834316214}
episode index:4938
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.879932 , 117.088486 ,   4.5978503], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.400610102864809}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.285427930311686
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.7313175, 118.111725 ,   2.733505 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.561843747065744}
episode index:4939
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.07200451, 95.31964755]), 'previousTarget': array([11.09184678, 95.9793708 ]), 'currentState': array([10.130354 , 75.31973  ,  3.4930923], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.2853992495438402
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.179181 , 118.11243  ,   2.1460476], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.622654040297138}
episode index:4940
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.3142293 , 91.65744374]), 'previousTarget': array([15.3142293 , 91.65744374]), 'currentState': array([19.       , 72.       ,  1.0048262], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28534148810900034
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.642709, 113.12473 ,   5.018759], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.884544555340484}
episode index:4941
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.57200063, 69.02154921]), 'previousTarget': array([10.7260531 , 66.99812374]), 'currentState': array([10.796395 , 49.022808 ,  1.7752752], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2852837500498929
{'currentTarget': array([  9.97508043, 118.56352718]), 'previousTarget': array([  9.99452899, 118.52566491]), 'currentState': array([ 9.628178 , 98.566536 ,  1.0329376], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4942
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.627005 , 107.259926 ,   2.9453053], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 12.959189571046174}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.2854089975362289
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.548611 , 119.76722  ,   1.0255554], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5959533766325176}
episode index:4943
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.90656202, 66.14175734]), 'previousTarget': array([ 7.79936077, 64.98401917]), 'currentState': array([ 8.500603 , 46.145878 ,  1.0674157], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28535126917912207
{'currentTarget': array([15.2136166 , 92.96678271]), 'previousTarget': array([15.21097416, 92.88291723]), 'currentState': array([19.001017  , 73.32867   ,  0.64253783], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4944
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.44596803, 85.93104084]), 'previousTarget': array([ 4.22977136, 84.73749166]), 'currentState': array([ 2.7961168, 66.10736  ,  0.7514267], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285293564170188
{'currentTarget': array([ 4.68409469, 89.66327176]), 'previousTarget': array([ 4.68409469, 89.66327176]), 'currentState': array([ 1.2320915 , 69.96343   ,  0.69654953], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4945
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.068229  , 113.855     ,   0.56980735], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.3696315155304974}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.28525437186904407
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.934957 , 121.216934 ,   3.3597243], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2186712056261915}
episode index:4946
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.76472708, 115.47097072]), 'previousTarget': array([ 10.16738911, 115.98266146]), 'currentState': array([ 8.727171 , 95.4979   ,  3.4853082], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.2852977481896485
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.97168  , 118.0417   ,   0.7952467], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.778929791742561}
episode index:4947
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.9730167, 107.05866  ,   1.9547642], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.099117513839406}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.28537393741616307
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.292229, 118.64611 ,   2.191783], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.52772932787121}
episode index:4948
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.84075218, 109.43858087]), 'previousTarget': array([  7.9223227 , 109.61161351]), 'currentState': array([ 3.834684, 89.8439  ,  5.867414], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.2854325312356338
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.627817 , 119.62494  ,   1.5456779], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.5283851351057757}
episode index:4949
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.13235509, 117.90133876]), 'previousTarget': array([ 10.09184678, 117.9793708 ]), 'currentState': array([11.391183, 97.940994,  5.550854], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.28539634853915896
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.261477 , 118.165375 ,   3.4036832], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8531649289698306}
episode index:4950
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10.39097356, 116.84634315]), 'previousTarget': array([ 10.41321632, 116.83200822]), 'currentState': array([12.851629, 96.99829 ,  5.63551 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2853387043564607
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.444691 , 112.86609  ,   4.4561296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.301484441948417}
episode index:4951
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.868648, 122.01278 ,   2.627963], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.409956872708283}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.28543974220711715
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.74338   , 121.81433   ,   0.47772425], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.9607168064605727}
episode index:4952
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.00553667, 106.18235927]), 'previousTarget': array([ 11.72906818, 104.87065345]), 'currentState': array([12.457137, 86.23511 ,  2.590198], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285382112539803
{'currentTarget': array([  8.55306677, 116.24077793]), 'previousTarget': array([  8.58163757, 116.32392651]), 'currentState': array([ 1.3688202, 97.57566  ,  5.889414 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4953
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 16.940481 , 114.82402  ,   4.2301016], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.658004623423688}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.28544911052696537
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.182091 , 118.649475 ,   3.3310359], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3627452280102819}
episode index:4954
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.8567963, 122.17259  ,   5.9485097], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.583251863424818}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.2855111725523689
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.895048, 119.09103 ,   5.706998], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4307871210473544}
episode index:4955
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  6.204841, 118.96663 ,   5.014913], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.9333301871354487}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.28563422079322975
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.229143  , 118.32958   ,   0.39996403], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8397053766958749}
episode index:4956
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.62701002, 65.82421155]), 'previousTarget': array([ 8.54034323, 65.9926994 ]), 'currentState': array([ 8.120308 , 45.83063  ,  1.5354846], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2855765983964589
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.2217293, 118.74142  ,   3.7481408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 4.9412450281804645}
episode index:4957
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.193871 , 109.94259  ,   1.3177202], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.613038088276971}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.2857051115259529
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.891388 , 119.288826 ,   1.8290334], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7194199426940241}
episode index:4958
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  2.6437082, 121.74311  ,   0.2669185], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.559990964330674}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.28576949970010324
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.542846 , 121.31464  ,   6.2194514], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.3918543202093734}
episode index:4959
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 19.00907 , 123.074585,   0.616778], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.519264935187865}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.2858537090136717
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.324776, 121.7683  ,   3.233224], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8928346649803454}
episode index:4960
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 14.075186  , 113.91208   ,   0.24693602], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.325975903944853}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.2859626215141397
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.133409 , 118.13053  ,   1.9327052], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.6417942451096033}
episode index:4961
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  8.52817995, 116.47672914]), 'previousTarget': array([  8.57265691, 116.51093912]), 'currentState': array([ 0.81895816, 98.02225   ,  5.690572  ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28590499099791356
{'currentTarget': array([  8.79736527, 115.53547414]), 'previousTarget': array([  8.80148212, 115.68760149]), 'currentState': array([ 3.5952864, 96.22386  ,  2.1775384], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4962
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.27412291, 108.75551563]), 'previousTarget': array([  9.28764556, 108.95850618]), 'currentState': array([ 7.9857235, 88.79706  ,  3.8688364], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28584738370575197
{'currentTarget': array([ 12.11875952, 111.56169823]), 'previousTarget': array([ 12.0970147 , 111.53021342]), 'currentState': array([16.98934  , 92.163826 ,  0.7479334], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4963
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.028806, 113.92136 ,   4.496908], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.249186808631688}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.28582556161531136
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.345038 , 119.56138  ,   2.2692115], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.7882661465613269}
episode index:4964
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.55534134, 67.99228841]), 'previousTarget': array([ 8.55534134, 67.99228841]), 'currentState': array([ 8.       , 48.       ,  2.6078386], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28576799352636567
{'currentTarget': array([  8.77928084, 110.98247171]), 'previousTarget': array([  8.85062113, 111.15066925]), 'currentState': array([ 6.0963163, 91.163246 ,  4.115182 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4965
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 12.94845504, 102.60426448]), 'previousTarget': array([ 12.79857683, 102.74210955]), 'currentState': array([16.290648 , 82.8855   ,  1.2196836], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857104486223128
{'currentTarget': array([  8.6864277 , 115.90429052]), 'previousTarget': array([  8.61377572, 115.69869939]), 'currentState': array([ 2.5784914 , 96.85979   ,  0.30479223], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4966
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([2.8924878e+00, 1.1934350e+02, 3.1486511e-02], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.1377674908778115}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2858482759932365
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.815746 , 119.95945  ,   5.9744062], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.1849477329445555}
episode index:4967
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.68514108, 65.61164495]), 'previousTarget': array([12.20063923, 64.98401917]), 'currentState': array([10.937065, 45.61323 ,  2.835137], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28579073809549227
{'currentTarget': array([ 4.51749875, 90.31372095]), 'previousTarget': array([ 4.51749875, 90.31372095]), 'currentState': array([ 0.8852951, 70.64631  ,  3.3382928], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4968
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.25898005, 106.14209164]), 'previousTarget': array([  9.17444044, 105.96548746]), 'currentState': array([ 8.1910515, 86.17062  ,  4.9883904], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.28575316797623757
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.191802 , 120.12362  ,   1.2930677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.22818784711694462}
episode index:4969
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.68079431, 80.92661056]), 'previousTarget': array([ 8.67757691, 80.98851894]), 'currentState': array([ 8.005934, 60.938   ,  4.604945], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28569567236899884
{'currentTarget': array([ 4.44552395, 85.78882937]), 'previousTarget': array([ 4.59201412, 85.75692405]), 'currentState': array([ 1.2403232, 66.04733  ,  2.739269 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4970
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.76575043, 81.98851934]), 'previousTarget': array([13.94201698, 81.89383588]), 'currentState': array([15.7374735, 62.08595  ,  3.915023 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285638199894171
{'currentTarget': array([ 4.84101367, 91.07994664]), 'previousTarget': array([ 4.76746855, 91.01711203]), 'currentState': array([ 1.3287036, 71.39077  ,  2.1400673], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4971
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 3.56192166, 68.84521366]), 'previousTarget': array([ 3.5150853 , 68.84122844]), 'currentState': array([ 1.0645255, 49.00175  ,  2.6958296], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28558075053779647
{'currentTarget': array([ 5.5188894 , 88.73134936]), 'previousTarget': array([ 5.5292931 , 88.75183443]), 'currentState': array([ 2.6816761, 68.93362  ,  1.9896656], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4972
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 9.95240891, 95.34941951]), 'previousTarget': array([10., 95.]), 'currentState': array([ 9.913796  , 75.34946   ,  0.46214032], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.2856097688894929
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.364936, 119.160645,   1.801355], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9152572292738255}
episode index:4973
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.63807531, 86.63165248]), 'previousTarget': array([14.47491441, 84.83995823]), 'currentState': array([15.805784 , 66.74947  ,  2.2103696], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.28557622401374344
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.744588 , 121.186104 ,   2.7813365], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4004475755790033}
episode index:4974
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  7.286476 , 118.335945 ,   0.6522408], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 3.183125881414739}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.28571781673253466
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.474607  , 119.931305  ,   0.73838747], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.526938567906998}
episode index:4975
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  5.9237084, 123.8489   ,   3.5198894], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 5.60626282649486}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.2858214969521501
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.299635, 120.61386 ,   5.248101], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8077795191616515}
episode index:4976
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  7.16093151, 106.63245817]), 'previousTarget': array([  7.15008417, 106.5646825 ]), 'currentState': array([ 3.0059047, 87.068825 ,  3.2627652], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 267
reward sum = 0.06832772446471172
running average episode reward sum: 0.2857777971786947
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.36008  , 119.290924 ,   6.2480226], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.9551369420378921}
episode index:4977
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 15.310781 , 110.27673  ,   3.0661502], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.07909306721377}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.28578556490321955
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.461128 , 118.852005 ,   1.5592991], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.2371466193633787}
episode index:4978
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.09586755, 95.40259171]), 'previousTarget': array([ 8.86874449, 93.98112317]), 'currentState': array([ 6.5522475, 75.46225  ,  2.1153793], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 284
reward sum = 0.05759639025694116
running average episode reward sum: 0.2857397345809367
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.86745  , 120.72406  ,   1.7348399], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.002905783157819}
episode index:4979
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.78192351, 88.04171532]), 'previousTarget': array([11.82555956, 88.96548746]), 'currentState': array([11.271117 , 68.0477   ,  3.5445685], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28568235712419354
{'currentTarget': array([  8.6901516 , 113.57476794]), 'previousTarget': array([  8.65907657, 113.55511721]), 'currentState': array([ 4.6951194, 93.97784  ,  1.8940974], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4980
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.860505 , 110.36507  ,   2.6600118], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 9.673283415563477}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.2856955927896089
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.95036  , 118.842384 ,   1.6936291], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.268034239745177}
episode index:4981
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.1737875, 113.118744 ,   2.9852114], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 11.19168053087334}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.28573559595650666
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.198305 , 120.117256 ,   0.1569745], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8055064144698802}
episode index:4982
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.1492875, 115.40285  ]), 'previousTarget': array([ 11.1492875, 115.40285  ]), 'currentState': array([16.      , 96.      ,  2.305077], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.28575395859968683
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.346278 , 120.38045  ,   1.0212562], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.6969195731227469}
episode index:4983
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 5.81215317, 99.70164101]), 'previousTarget': array([ 5.9223227 , 99.61161351]), 'currentState': array([ 1.7709737, 80.114174 ,  5.5626297], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.2857754186859144
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.247802  , 120.84644   ,   0.48980302], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.945935417830959}
episode index:4984
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  1.0083085, 110.25515  ,   4.8677797], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 13.25943513539123}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.2858757000745044
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  8.263155  , 118.68068   ,   0.29317218], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.181109273865924}
episode index:4985
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 8.32204211, 86.68514464]), 'previousTarget': array([ 8.72679236, 84.98678996]), 'currentState': array([ 7.3159842, 66.710464 ,  1.9225025], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28581836439458574
{'currentTarget': array([ 10.73968406, 117.86692282]), 'previousTarget': array([ 10.73625621, 117.91836598]), 'currentState': array([17.292267, 98.970795,  1.837677], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4986
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([15.20483341, 85.5535569 ]), 'previousTarget': array([15.06902678, 85.78406925]), 'currentState': array([18.192902 , 65.77803  ,  5.6372657], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28576105170872357
{'currentTarget': array([11.94585079, 90.73695116]), 'previousTarget': array([11.94585079, 90.73695116]), 'currentState': array([13.272823 , 70.78102  ,  1.3887496], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:4987
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  4.964553, 114.01537 ,   4.843389], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 7.82122019134246}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.28585814114218316
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.131561 , 118.23178  ,   1.0450689], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.0992925808575764}
episode index:4988
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  6.54862348, 105.33216121]), 'previousTarget': array([  6.58078667, 105.46834337]), 'currentState': array([ 1.9676855, 85.86385  ,  3.6443367], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858008434590519
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.47354 , 109.736725,   5.42758 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 10.276768779229894}
episode index:4989
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.91431954, 118.51544864]), 'previousTarget': array([  9.73259233, 116.92481176]), 'currentState': array([ 8.761943 , 98.548676 ,  0.5746459], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.28587106082229935
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.362687, 118.0623  ,   3.609112], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3688797168836553}
episode index:4990
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([11.18435261, 90.98336106]), 'previousTarget': array([11.18435261, 90.98336106]), 'currentState': array([12.       , 71.       ,  0.4290437], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.2858322912577899
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 10.291006, 119.9817  ,   3.121464], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.29158110396739056}
episode index:4991
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([13.04857152, 88.9045705 ]), 'previousTarget': array([13.04857152, 88.9045705 ]), 'currentState': array([15.       , 69.       ,  1.0873995], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2857750331866245
{'currentTarget': array([14.64767513, 91.68316812]), 'previousTarget': array([14.75519865, 91.84637067]), 'currentState': array([17.886957, 71.947235,  5.899368], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4992
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.74228903, 117.11023253]), 'previousTarget': array([  9.73259233, 116.92481176]), 'currentState': array([ 7.965729, 97.18929 ,  4.828948], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.285717798050797
{'currentTarget': array([ 10.94747246, 115.34315458]), 'previousTarget': array([ 10.56120499, 116.52117319]), 'currentState': array([14.934938 , 95.74468  ,  5.7732544], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4993
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([14.79864756, 87.97024029]), 'previousTarget': array([14.95885631, 87.76743395]), 'currentState': array([17.761944, 68.19099 ,  6.004909], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.28566058583652976
{'currentTarget': array([ 3.65205607, 77.99743405]), 'previousTarget': array([ 3.65205607, 77.99743405]), 'currentState': array([ 0.66335005, 58.222004  ,  2.9204378 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4994
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  3.2232406, 120.82656  ,   5.964764 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 6.826981091267784}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.2857937849284343
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.264978, 118.70316 ,   5.916864], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.4906517242066413}
episode index:4995
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.897202, 123.042305,   1.256208], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.462943447855885}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.2858922692307704
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.66773 , 120.81732 ,   4.134121], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 1.8572397115239612}
episode index:4996
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 17.872488, 118.10751 ,   5.702339], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 8.096763123643093}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.28596236993455926
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([ 11.556765 , 121.8106   ,   2.5245984], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 2.3878419974514835}
episode index:4997
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([10.10654062, 99.6796186 ]), 'previousTarget': array([10.52394444, 97.99433347]), 'currentState': array([10.2114   , 79.67989  ,  2.3544486], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2859051545744283
{'currentTarget': array([  8.50969552, 116.3687837 ]), 'previousTarget': array([  8.4603422 , 116.30186415]), 'currentState': array([ 0.91606194, 97.86644   ,  2.8616457 ], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4998
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([ 11.95039783, 110.84109778]), 'previousTarget': array([ 12.68142004, 110.27985236]), 'currentState': array([16.116014 , 91.27972  ,  3.0515997], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 299
reward sum = 0.0
running average episode reward sum: 0.2858479621050195
{'currentTarget': array([  8.72047828, 116.34047408]), 'previousTarget': array([  8.61225891, 115.95405674]), 'currentState': array([ 2.1195016, 97.4612   ,  3.8558493], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
episode index:4999
map used: OneDtrapMapUltraSmall
at step 0:
{'currentTarget': array([  9.95797391, 119.59284313]), 'previousTarget': array([  9.70226409, 117.81660336]), 'currentState': array([ 7.9045153, 99.69854  ,  1.1647873], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.285908199868986
{'currentTarget': array([ 10., 120.]), 'previousTarget': array([ 10., 120.]), 'currentState': array([  9.591305 , 119.28168  ,   0.6728135], dtype=float32), 'targetState': array([ 10, 120], dtype=int32), 'currentDistance': 0.8264498547429198}
episode index:0
map used: OneDtrapMapUltraSmall
done in step count: 48
reward sum = 1.0
episode index:1
map used: OneDtrapMapUltraSmall
done in step count: 104
reward sum = 1.0
episode index:2
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:3
map used: OneDtrapMapUltraSmall
done in step count: 53
reward sum = 1.0
episode index:4
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:5
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:6
map used: OneDtrapMapUltraSmall
done in step count: 22
reward sum = 1.0
episode index:7
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:8
map used: OneDtrapMapUltraSmall
done in step count: 130
reward sum = 1.0
episode index:9
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:10
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:11
map used: OneDtrapMapUltraSmall
done in step count: 119
reward sum = 1.0
episode index:12
map used: OneDtrapMapUltraSmall
done in step count: 226
reward sum = 1.0
episode index:13
map used: OneDtrapMapUltraSmall
done in step count: 118
reward sum = 1.0
episode index:14
map used: OneDtrapMapUltraSmall
done in step count: 89
reward sum = 1.0
episode index:15
map used: OneDtrapMapUltraSmall
done in step count: 48
reward sum = 1.0
episode index:16
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:17
map used: OneDtrapMapUltraSmall
done in step count: 185
reward sum = 1.0
episode index:18
map used: OneDtrapMapUltraSmall
done in step count: 54
reward sum = 1.0
episode index:19
map used: OneDtrapMapUltraSmall
done in step count: 164
reward sum = 1.0
episode index:20
map used: OneDtrapMapUltraSmall
done in step count: 16
reward sum = 1.0
episode index:21
map used: OneDtrapMapUltraSmall
done in step count: 149
reward sum = 1.0
episode index:22
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:23
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:24
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:25
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:26
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:27
map used: OneDtrapMapUltraSmall
done in step count: 29
reward sum = 1.0
episode index:28
map used: OneDtrapMapUltraSmall
done in step count: 24
reward sum = 1.0
episode index:29
map used: OneDtrapMapUltraSmall
done in step count: 58
reward sum = 1.0
episode index:30
map used: OneDtrapMapUltraSmall
done in step count: 0
reward sum = 1.0
episode index:31
map used: OneDtrapMapUltraSmall
done in step count: 112
reward sum = 1.0
episode index:32
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:33
map used: OneDtrapMapUltraSmall
done in step count: 99
reward sum = 1.0
episode index:34
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:35
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:36
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:37
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:38
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:39
map used: OneDtrapMapUltraSmall
done in step count: 209
reward sum = 1.0
episode index:40
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:41
map used: OneDtrapMapUltraSmall
done in step count: 25
reward sum = 1.0
episode index:42
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:43
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:44
map used: OneDtrapMapUltraSmall
done in step count: 55
reward sum = 1.0
episode index:45
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:46
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:47
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:48
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:49
map used: OneDtrapMapUltraSmall
done in step count: 8
reward sum = 1.0
episode index:50
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:51
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:52
map used: OneDtrapMapUltraSmall
done in step count: 40
reward sum = 1.0
episode index:53
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:54
map used: OneDtrapMapUltraSmall
done in step count: 30
reward sum = 1.0
episode index:55
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:56
map used: OneDtrapMapUltraSmall
done in step count: 10
reward sum = 1.0
episode index:57
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:58
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:59
map used: OneDtrapMapUltraSmall
done in step count: 205
reward sum = 1.0
episode index:60
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:61
map used: OneDtrapMapUltraSmall
done in step count: 191
reward sum = 1.0
episode index:62
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:63
map used: OneDtrapMapUltraSmall
done in step count: 144
reward sum = 1.0
episode index:64
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:65
map used: OneDtrapMapUltraSmall
done in step count: 105
reward sum = 1.0
episode index:66
map used: OneDtrapMapUltraSmall
done in step count: 14
reward sum = 1.0
episode index:67
map used: OneDtrapMapUltraSmall
done in step count: 9
reward sum = 1.0
episode index:68
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:69
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:70
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:71
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:72
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:73
map used: OneDtrapMapUltraSmall
done in step count: 243
reward sum = 1.0
episode index:74
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:75
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:76
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:77
map used: OneDtrapMapUltraSmall
done in step count: 173
reward sum = 1.0
episode index:78
map used: OneDtrapMapUltraSmall
done in step count: 109
reward sum = 1.0
episode index:79
map used: OneDtrapMapUltraSmall
done in step count: 109
reward sum = 1.0
episode index:80
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:81
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:82
map used: OneDtrapMapUltraSmall
done in step count: 293
reward sum = 1.0
episode index:83
map used: OneDtrapMapUltraSmall
done in step count: 141
reward sum = 1.0
episode index:84
map used: OneDtrapMapUltraSmall
done in step count: 0
reward sum = 1.0
episode index:85
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:86
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:87
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:88
map used: OneDtrapMapUltraSmall
done in step count: 47
reward sum = 1.0
episode index:89
map used: OneDtrapMapUltraSmall
done in step count: 6
reward sum = 1.0
episode index:90
map used: OneDtrapMapUltraSmall
done in step count: 26
reward sum = 1.0
episode index:91
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:92
map used: OneDtrapMapUltraSmall
done in step count: 46
reward sum = 1.0
episode index:93
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:94
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:95
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:96
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:97
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:98
map used: OneDtrapMapUltraSmall
reward sum = 0.0
episode index:99
map used: OneDtrapMapUltraSmall
reward sum = 0.0

Process finished with exit code 0
