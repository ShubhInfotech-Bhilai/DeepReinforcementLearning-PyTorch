/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/DynamicMaze/StochAgent/FreeSpace/DQNHER_FixedTarget/DQNHER_CNNDynMazeStochAgentCPU.py
episode index:0
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.067333, 7.485413, 3.664972], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.574212853622848}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.822076 , 11.666681 ,  4.8825817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5353244465466713}
episode index:1
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.82352941, 11.70588235]), 'previousTarget': array([ 8.82352941, 11.70588235]), 'currentState': array([0.       , 7.       , 4.4280043], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([12.38717749,  8.88144664]), 'previousTarget': array([12.38717749,  8.88144664]), 'currentState': array([ 8.459943  , -0.31511945,  4.566536  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:2
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.24462346, 16.8119483 ]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([26.97546   , 21.687653  ,  0.14931554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([21.01005089,  9.90198458]), 'previousTarget': array([21.01005089,  9.90198458]), 'currentState': array([28.636032,  3.433259,  4.974651], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:3
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.16387525, 13.22593594]), 'previousTarget': array([16.42507074, 14.14495755]), 'currentState': array([26.886238 ,  8.335089 ,  5.5083637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.450989 , 11.697823 ,  1.1458919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.24704297479538}
episode index:4
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.07106781, 16.92893219]), 'previousTarget': array([13.07106781, 16.92893219]), 'currentState': array([ 6.      , 24.      ,  4.809406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([20.20157532, 10.5303491 ]), 'previousTarget': array([20.20157532, 10.5303491 ]), 'currentState': array([27.786097 ,  4.0130615,  1.1188991], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:5
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.19131191, 16.75304952]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([25.       , 23.       ,  4.0153384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.807289 , 11.2307415,  2.3582654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.360652545336291}
episode index:6
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        , 25.        ,  0.31379092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([12.00131828, 20.15217276]), 'previousTarget': array([11.17672205, 20.03982319]), 'currentState': array([ 6.9710617, 28.794888 ,  1.0757589], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:7
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.18189592, 14.08010732]), 'previousTarget': array([18.92040615, 13.19058177]), 'currentState': array([26.396437, 10.195234,  3.278501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([11.19463817, 16.8474128 ]), 'previousTarget': array([11.19463817, 16.8474128 ]), 'currentState': array([ 2.1987152, 21.21472  ,  5.63159  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:8
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.81908317, 13.88337958]), 'previousTarget': array([15., 15.]), 'currentState': array([13.219722,  4.012106,  5.329053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.025105350614887476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.837435, 16.296494,  3.017528], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5434352807389027}
episode index:9
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 19.       ,  4.7748203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.08432382964762752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0566845, 15.685421 ,  5.1202765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1660386260086855}
episode index:10
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.       ,  4.       ,  1.3902795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.07665802695238866
{'scaleFactor': 1.0, 'currentTarget': array([14.42789415, 13.87972064]), 'previousTarget': array([14.42789415, 13.87972064]), 'currentState': array([9.879818 , 4.9738235, 2.9483247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:11
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.31793849, 16.51127389]), 'previousTarget': array([ 9.94427191, 17.52786405]), 'currentState': array([ 2.0668564, 20.30831  ,  5.555752 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 434
reward sum = 0.012754823560906535
running average episode reward sum: 0.07133276000309847
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.556479 , 15.944806 ,  3.3663614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8207921041933013}
episode index:12
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.80580676, 16.03883865]), 'previousTarget': array([ 9.80580676, 16.03883865]), 'currentState': array([ 0.      , 18.      ,  5.872858], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.13068740872470064
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.508129 , 15.308001 ,  6.1669154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5233328824486814}
episode index:13
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45299804, 15.67949706]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([21.      , 24.      ,  4.948805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.12135259381579346
{'scaleFactor': 1.0, 'currentTarget': array([ 9.45784305, 17.43333172]), 'previousTarget': array([ 9.45784305, 17.43333172]), 'currentState': array([ 0.30151808, 21.453495  ,  2.171852  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:14
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38476052, 16.25278872]), 'previousTarget': array([19.38476052, 16.25278872]), 'currentState': array([29.      , 19.      ,  4.720319], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.12313875366457919
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.616901 , 13.865109 ,  0.5941323], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.975435826535507}
episode index:15
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.78280103, 20.65196555]), 'previousTarget': array([ 9.78280103, 20.65196555]), 'currentState': array([ 3.      , 28.      ,  2.372283], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.11544258156054299
{'scaleFactor': 1.0, 'currentTarget': array([21.61814375,  8.02714802]), 'previousTarget': array([21.61814375,  8.02714802]), 'currentState': array([28.502321  ,  0.77400386,  4.8824043 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:16
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.7151203 , 20.27441048]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 6.428593, 28.762793,  3.007406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.10865184146874635
{'scaleFactor': 1.0, 'currentTarget': array([14.21205017,  9.50913026]), 'previousTarget': array([14.21205017,  9.50913026]), 'currentState': array([12.791583 , -0.3894693,  3.4302635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:17
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.97785158, 15.33480989]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([ 0.       , 16.       ,  4.1972733], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.14805490236477326
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.735735 , 15.487047 ,  0.6773402], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3548361943761378}
episode index:18
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.50070868, 16.29381101]), 'previousTarget': array([14.14495755, 16.42507074]), 'currentState': array([10.900419 , 25.623222 ,  1.0941846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.17766007735990583
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.80314 , 15.307089,  4.643234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8598470324109392}
episode index:19
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.38103758, 11.84232075]), 'previousTarget': array([15.33480989,  9.97785158]), 'currentState': array([16.579048  ,  1.9143416 ,  0.49405968], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 473
reward sum = 0.008618804795743068
running average episode reward sum: 0.1692080137316977
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.362376 , 13.321098 ,  5.9448123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7959050417876827}
episode index:20
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07106781, 19.92893219]), 'previousTarget': array([10.07106781, 19.92893219]), 'currentState': array([ 3.       , 27.       ,  5.4174104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.20421630236394087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.457963 , 13.313243 ,  6.1318994], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.285394410979453}
episode index:21
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.48084  , 15.300829 ,  2.5956335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5110880767643344}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.24038828862012537
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.48084  , 15.300829 ,  2.5956335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5110880767643344}
episode index:22
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.94762   , 13.454723  ,  0.03128028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5461641320326673}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2734148847670764
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.94762   , 13.454723  ,  0.03128028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5461641320326673}
episode index:23
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.531949, 21.055538,  4.255406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.9783178529994165}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.2738854217881647
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.76645   , 14.612734  ,  0.60232174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.808403659673764}
episode index:24
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.032368 , 12.641634 ,  3.2950842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.557572759817}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.28029925798831057
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.806042, 16.387436,  4.701346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8304411471557889}
episode index:25
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.94427191, 13.47213595]), 'previousTarget': array([11.94427191, 13.47213595]), 'currentState': array([3.      , 9.      , 5.444507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 349
reward sum = 0.02996973580906778
running average episode reward sum: 0.2706711994429551
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.363152, 16.104822,  4.817547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7546549950549808}
episode index:26
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.47942816, 18.11628302]), 'previousTarget': array([15.47942816, 18.11628302]), 'currentState': array([17.       , 28.       ,  1.8921841], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 379
reward sum = 0.022168624768315548
running average episode reward sum: 0.2614674003809314
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.486181 , 16.733818 ,  1.9984812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.283606748135834}
episode index:27
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.76768  , 7.5752425, 1.9445521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.083181957265522}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.2555983014276856
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.459936 , 14.133832 ,  4.7526135], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6975454577798412}
episode index:28
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.49803   , 10.325106  ,  0.41768253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.701347744483138}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.26019090889825597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.054716, 15.597932,  5.387292], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.600430149282054}
episode index:29
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.971536, 11.251824,  5.293662], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.261864377564317}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2515178786016474
{'scaleFactor': 1.0, 'currentTarget': array([ 9.63113768, 21.23636422]), 'previousTarget': array([ 9.63113768, 21.23636422]), 'currentState': array([ 3.1068416, 28.814857 ,  3.6302938], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:30
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.70823068, 15.20395823]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.512214 , 20.933296 ,  1.2642498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2434043986467556
{'scaleFactor': 1.0, 'currentTarget': array([10.10348751, 19.12407404]), 'previousTarget': array([ 8.67192401, 20.49933254]), 'currentState': array([ 2.4549105, 25.566067 ,  5.0062065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:31
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.025413 , 14.7124705]), 'previousTarget': array([19.025413 , 14.7124705]), 'currentState': array([29.       , 14.       ,  4.8382654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.23579801118904448
{'scaleFactor': 1.0, 'currentTarget': array([18.51306564, 11.64550169]), 'previousTarget': array([18.51306564, 11.64550169]), 'currentState': array([25.745472 ,  4.7395406,  0.9904556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:32
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.76620873, 15.25695843]), 'previousTarget': array([15.25842724, 15.14357069]), 'currentState': array([25.247252 , 18.436554 ,  5.8233695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.23279504130321277
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.718521 , 15.709058 ,  3.2830234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8590529332301238}
episode index:33
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.8944951 ,  9.00758443]), 'previousTarget': array([12.74099823, 10.804711  ]), 'currentState': array([7.2932777 , 0.12902606, 4.2986817 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 426
reward sum = 0.013822700512858787
running average episode reward sum: 0.22635467833879058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.805635 , 16.29309  ,  3.7630374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.760281381611908}
episode index:34
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.74885442, 19.47780851]), 'previousTarget': array([18.29411765, 21.17647059]), 'currentState': array([22.980549, 28.000095,  4.788314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.245987894600072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.436273, 14.296271,  3.226131], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8279902470271028}
episode index:35
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.03385913, 14.64192609]), 'previousTarget': array([14.99503719, 14.9503719 ]), 'currentState': array([1.5975250e+01, 4.6863356e+00, 1.1956056e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.24488844022555373
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.845974 , 15.786868 ,  2.2033467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.006684153512074}
episode index:36
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84288535, 21.17127813]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([12.       , 31.       ,  1.8755946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.2523330911018984
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.540176, 16.347067,  5.431124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4513372148734187}
episode index:37
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.503817,  9.064498,  4.969472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.123042462147041}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.25494562045600694
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.18541  , 15.31679  ,  2.2080472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2270091520969113}
episode index:38
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.1914503 , 12.93919299]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.       , 9.       , 4.1590962], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.2589970063708244
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.305098 , 15.799467 ,  5.6413555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0592624775033836}
episode index:39
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.49698247, 12.05865958]), 'previousTarget': array([11.72672794, 11.39940073]), 'currentState': array([3.838648 , 5.6282697, 2.5365036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 303
reward sum = 0.047584330476474465
running average episode reward sum: 0.2537116894734657
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.666597  , 16.218655  ,  0.94453347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2634383362936275}
episode index:40
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.39247183, 17.56906875]), 'previousTarget': array([18.26042701, 17.6676221 ]), 'currentState': array([24.207542 , 24.887184 ,  2.4309313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.25250758635816745
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.226007 , 15.146162 ,  4.5696716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7876723871597808}
episode index:41
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99738954, 12.21872008]), 'previousTarget': array([14.52057184, 11.88371698]), 'currentState': array([14.988004 ,  2.2187245,  5.8239064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 333
reward sum = 0.035198147020879485
running average episode reward sum: 0.24733355208823204
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.899339 , 16.53019  ,  2.8193011], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5334968673225862}
episode index:42
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.63663603, 19.58258088]), 'previousTarget': array([16.63663603, 19.58258088]), 'currentState': array([20.      , 29.      ,  4.327473], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.24520439388354554
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0300865, 16.515139 ,  5.7303176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7989933908031381}
episode index:43
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 13.19058177]), 'previousTarget': array([11.07959385, 13.19058177]), 'currentState': array([2.      , 9.      , 4.517524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.25338170463931803
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.659327, 14.844069,  2.969567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6666371035224514}
episode index:44
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.70388481, 14.3775642 ]), 'previousTarget': array([13.10366477, 14.13802944]), 'currentState': array([ 3.0522273, 11.761172 ,  1.4625397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.25943092203679013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.485964, 16.199755,  4.977718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9098428874191442}
episode index:45
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.50452943, 16.58848379]), 'previousTarget': array([12.07106781, 17.92893219]), 'currentState': array([ 6.6498446, 23.869507 ,  5.6940784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2740534095557075
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.086192, 15.533074,  4.628834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5399975712584278}
episode index:46
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.84709893, 16.7979129 ]), 'previousTarget': array([18.26042701, 17.6676221 ]), 'currentState': array([24.012932 , 23.772928 ,  2.8636994], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.27072397491287176
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.927956 , 16.373657 ,  2.6665514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.367265738667792}
episode index:47
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 19.       ,  3.0719557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.2727865928865562
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.749178  , 14.061174  ,  0.46172392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5639531210583915}
episode index:48
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.783371, 11.905313,  5.903901], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.376034513380203}
done in step count: 414
reward sum = 0.015594467994581931
running average episode reward sum: 0.26753777401120976
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.130293 , 16.42413  ,  1.1956022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.350308375752888}
episode index:49
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.32050294, 15.45299804]), 'previousTarget': array([14.32050294, 15.45299804]), 'currentState': array([ 6.       , 21.       ,  4.6874504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 325
reward sum = 0.03814505489267701
running average episode reward sum: 0.2629499196288391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3490505, 14.467976 ,  1.4258331], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7345558867007362}
episode index:50
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.684566 , 15.078071 ,  1.4239539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.684917402006521}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.26597287963480737
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.679824, 15.910785,  2.417218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1365251032380088}
episode index:51
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.90470911, 19.22197586]), 'previousTarget': array([15.90470911, 19.22197586]), 'currentState': array([18.       , 29.       ,  3.0853636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.26284205753224527
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.447769, 15.318238,  4.999663], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5493385242679977}
episode index:52
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.64598284, 17.59848486]), 'previousTarget': array([15.58578644, 19.10050506]), 'currentState': array([18.058548, 27.303099,  4.806945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.27564654983165576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.93287  , 16.675972 ,  3.9959514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9181054103613242}
episode index:53
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.00862428, 22.61739737]), 'previousTarget': array([ 8.32793492, 21.19548901]), 'currentState': array([ 1.2467747, 29.984716 ,  2.2450647], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2705419840940325
{'scaleFactor': 1.0, 'currentTarget': array([10.10739749, 20.71766228]), 'previousTarget': array([10.10739749, 20.71766228]), 'currentState': array([ 3.6058066, 28.315643 ,  2.7027245], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:54
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.47482046, 19.81022639]), 'previousTarget': array([16.52786405, 18.05572809]), 'currentState': array([22.049747 , 28.70236  ,  0.9676197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.28019814055666914
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.84197  , 13.492645 ,  2.4579406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7265666883263708}
episode index:55
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.93919299, 19.8085497 ]), 'previousTarget': array([12.93919299, 19.8085497 ]), 'currentState': array([ 9.      , 29.      ,  2.675138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.28104679442954655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.718347 , 14.186035 ,  0.5176904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8613172510629281}
episode index:56
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        , 18.        ,  0.13609889], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.29105400454648217
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.205349 , 16.64908  ,  5.1350813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0426286726138025}
episode index:57
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([27.       , 31.       ,  2.0033553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.28829985755996157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.081661 , 16.699282 ,  3.0835462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5627293904324255}
episode index:58
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 3.        , 31.        ,  0.82824177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.29205724550385315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.115826, 13.5683  ,  5.794287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6827146097742094}
episode index:59
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51658317, 17.75902574]), 'previousTarget': array([10.51658317, 17.75902574]), 'currentState': array([ 2.        , 23.        ,  0.45770654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 413
reward sum = 0.015751987873315082
running average episode reward sum: 0.2874521578766775
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.208352 , 16.012365 ,  5.0842338], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0335831737196868}
episode index:60
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 16.80941823]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 2.       , 21.       ,  2.5203779], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.2949885502775778
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.544207 , 16.174889 ,  5.8448853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2602027034588998}
episode index:61
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.       , 16.       ,  4.6848397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.29194564113899124
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.59586763e+01, 1.32633085e+01, 1.40962005e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9837231662854038}
episode index:62
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.3605876, 17.46588  ,  1.3077788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.027526597830251}
done in step count: 266
reward sum = 0.06901790349970881
running average episode reward sum: 0.28840710562090743
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.253325 , 15.079438 ,  3.4378853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2558394615392832}
episode index:63
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91227901, 15.6783628 ]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 0.       , 17.       ,  3.3670259], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.2958123495072542
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.699658 , 13.922444 ,  0.9884453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1186291937410797}
episode index:64
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47409319, 16.35636161]), 'previousTarget': array([13.47409319, 16.35636161]), 'currentState': array([ 6.        , 23.        ,  0.12261254], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.29651009892274327
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.560892 , 14.24175  ,  3.3411868], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.735317718730715}
episode index:65
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 14.       ,  1.5559022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.30146487613438655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.293785, 14.083973,  3.524864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5852398758260497}
episode index:66
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.8363379 , 18.76580117]), 'previousTarget': array([16.63124508, 17.56338512]), 'currentState': array([23.852598, 26.75358 ,  6.167611], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.30259579808646286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.200779, 13.305163,  1.26278 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8738264128711826}
episode index:67
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.08280237, 12.49610581]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([10.643221,  3.106256,  4.275321], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.3057979263888723
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.090187 , 14.094458 ,  1.7371911], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1136206867754335}
episode index:68
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.092224 ,  6.0021276,  4.766868 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.514386837245963}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.3112581679630972
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.413175 , 13.418743 ,  1.0235493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6866349022572966}
episode index:69
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07912183,  8.43120107]), 'previousTarget': array([11.40757591,  9.41178475]), 'currentState': array([4.083584 , 0.4278565, 2.8613   ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 427
reward sum = 0.013684473507730199
running average episode reward sum: 0.3070071151851634
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.666788 , 15.88554  ,  5.3003697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9461560527936789}
episode index:70
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.91333207, 12.97463401]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([5.7376466 , 6.009755  , 0.33430833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3156794754561881
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.95351   , 13.083603  ,  0.07690191], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7365635852335086}
episode index:71
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.06331086, 12.39492615]), 'previousTarget': array([16.52590681, 13.64363839]), 'currentState': array([25.681158 ,  5.9166236,  5.753626 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.3192061661092571
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.899127  , 13.131498  ,  0.64692575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.168690806261006}
episode index:72
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.74721128, 10.61523948]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([11.      ,  1.      ,  5.516092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 353
reward sum = 0.02878880863894463
running average episode reward sum: 0.31522784614391036
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.848166 , 14.498177 ,  1.5359095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9855014478991653}
episode index:73
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.31756858, 18.03861062]), 'previousTarget': array([20.31756858, 18.03861062]), 'currentState': array([29.       , 23.       ,  0.7854927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.31281530774948724
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.119681 , 15.989124 ,  4.4064536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3241328437668172}
episode index:74
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.43294146, 17.31035268]), 'previousTarget': array([12.43294146, 17.31035268]), 'currentState': array([ 5.      , 24.      ,  4.741296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.3176541576336312
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4438715, 16.644691 ,  5.6022935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7361707672856783}
episode index:75
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.36363916, 10.77918251]), 'previousTarget': array([19.18761806,  9.13733471]), 'currentState': array([24.595875 ,  2.9587452,  1.6386384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.3197921484091284
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.897425 , 14.069604 ,  1.7038195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4426743092482963}
episode index:76
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42981745, 21.60082264]), 'previousTarget': array([16.83772234, 20.51316702]), 'currentState': array([18.54684 , 31.374165,  2.753653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.3175629645537836
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.349619, 15.71907 ,  6.007369], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.800227770685964}
episode index:77
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 11.       ,  5.7580104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 203
reward sum = 0.1300003445350054
running average episode reward sum: 0.31515831557918383
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.333791  , 16.006361  ,  0.81587225], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9465394025207905}
episode index:78
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.       , 17.       ,  1.3959608], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.32025416642752674
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.407063 , 13.161604 ,  3.6051786], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8829232652240184}
episode index:79
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.42761929,  9.04463611]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([4.3377676, 1.11282  , 4.0559444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.31625098934718265
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.483679 ,  7.896781 ,  1.4758402], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.417415140166344}
episode index:80
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.39552332, 15.65464471]), 'previousTarget': array([ 9.6623494 , 16.42337349]), 'currentState': array([ 0.49508607, 17.062246  ,  4.747589  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.3140513575415072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.129355 , 16.325108 ,  4.5263953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.331406365962481}
episode index:81
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.      , 16.      ,  5.442139], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.3154115863933017
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.408924, 14.874751,  1.509604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5959980576144415}
episode index:82
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.3353119 , 8.19357109]), 'previousTarget': array([8.07106781, 8.07106781]), 'currentState': array([2.9383583, 0.5072856, 0.3700177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.32156528563959424
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.993757, 13.163666,  3.241722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0879839239374323}
episode index:83
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.80580676, 13.96116135]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 0.       , 12.       ,  3.6976376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.3250123327097522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.254707 , 16.232761 ,  1.1710836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1367608918832524}
episode index:84
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.66519011,  9.97785158]), 'previousTarget': array([14.66519011,  9.97785158]), 'currentState': array([14.        ,  0.        ,  0.36023873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.3317220494338577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.518408, 14.549791,  2.204573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5837456089760564}
episode index:85
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.21383285, 14.03619602]), 'previousTarget': array([17.13606076, 14.64398987]), 'currentState': array([27.79238  , 11.163663 ,  4.5712967], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.33690925071252387
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.993559 , 14.708311 ,  2.0156348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.2917600272249671}
episode index:86
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.14357069, 14.74157276]), 'previousTarget': array([15.14357069, 14.74157276]), 'currentState': array([20.      ,  6.      ,  4.259094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.34153903373178546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.305668  , 14.249056  ,  0.46984857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8107712062634582}
episode index:87
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.14211329, 15.00272177]), 'previousTarget': array([16.1613009 , 15.21114562]), 'currentState': array([25.14028  , 15.194208 ,  4.7554116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.3449603129176142
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.226702 , 13.142049 ,  1.1258171], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.568378722118363}
episode index:88
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.36221099, 13.03871026]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([27.        ,  8.        ,  0.43567044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.3450350195877312
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.533468, 15.258663,  4.48719 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5551307026389556}
episode index:89
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      , 14.      ,  4.933175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.35124998687018755
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.580212 , 13.508854 ,  0.5110894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1726908385714316}
episode index:90
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36875492, 17.56338512]), 'previousTarget': array([13.36875492, 17.56338512]), 'currentState': array([ 8.     , 26.     ,  1.72512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.35310812462601304
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.043879, 16.854574,  0.740852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.086531498270367}
episode index:91
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96116135, 15.19419324]), 'previousTarget': array([14.96116135, 15.19419324]), 'currentState': array([13.       , 25.       ,  2.1113806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.3543339143058368
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.002506  , 13.179684  ,  0.11552113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7025049014680116}
episode index:92
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.03883865, 20.19419324]), 'previousTarget': array([16.03883865, 20.19419324]), 'currentState': array([18.       , 30.       ,  0.3340961], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.35197906581767757
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.956807 , 15.117542 ,  6.0103936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.12522702563535132}
episode index:93
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.999941, 15.267916,  4.988616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.26791573223262966}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3588729055430214
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.999941, 15.267916,  4.988616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.26791573223262966}
episode index:94
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.        , 10.        ,  0.06153029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.3636187472554239
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.933043 , 13.9439   ,  3.0206172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4092250838498512}
episode index:95
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.85504245, 21.42507074]), 'previousTarget': array([18.85504245, 21.42507074]), 'currentState': array([24.       , 30.       ,  2.4043539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.36135879719684766
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.707178 , 15.853233 ,  2.7802246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9085241018320656}
episode index:96
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 13.       ,  2.6831539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.36108063286378644
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.624391  , 13.273112  ,  0.43011656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8363019257675164}
episode index:97
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.54057759, 20.36613715]), 'previousTarget': array([19.54057759, 20.36613715]), 'currentState': array([26.       , 28.       ,  6.1446457], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.35915378895896716
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.4145945e+01, 1.3064652e+01, 4.1788421e-04], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1154150422276015}
episode index:98
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.75863552, 19.064533  ]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 1.8562925, 25.192581 ,  4.5541115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.36447935545146376
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6711235, 15.712368 ,  6.097416 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5077734988041458}
episode index:99
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.88586135, 14.03932796]), 'previousTarget': array([12.43294146, 12.68964732]), 'currentState': array([6.31246  , 7.509122 , 1.2321068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.3660379671234522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.543548 , 16.89136  ,  5.9055285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.967913651219725}
episode index:100
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 11.       ,  4.5260873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.36966435724752095
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.888631 , 13.619499 ,  3.2793274], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3849857488507546}
episode index:101
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.      , 8.      , 1.979667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.37527039442549626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.202959 , 14.165617 ,  1.0587614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9813003675492376}
episode index:102
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72672794, 18.60059927]), 'previousTarget': array([11.72672794, 18.60059927]), 'currentState': array([ 5.       , 26.       ,  1.6590718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.37477694913848586
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.808904 , 15.756362 ,  5.5029593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7801289731242904}
episode index:103
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 10.22192192]), 'previousTarget': array([11.69209979, 10.22192192]), 'currentState': array([6.       , 2.       , 4.1769376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.377108806751984
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.994549 , 13.218625 ,  1.2778955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6742328903869965}
episode index:104
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.67949706, 17.45299804]), 'previousTarget': array([18.67949706, 17.45299804]), 'currentState': array([27.       , 23.       ,  5.8077497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.38077770111003273
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.91771  , 13.963845 ,  4.8260317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.179730674435399}
episode index:105
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 18.       ,  2.5079694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.3828930630536885
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.193241 , 13.104722 ,  2.1338396], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0598394358575356}
episode index:106
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 22.       ,  0.9014715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.3805174301381367
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.632083, 15.906801,  5.023112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.641184189776841}
episode index:107
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.54700196, 21.67949706]), 'previousTarget': array([10.54700196, 21.67949706]), 'currentState': array([ 5.       , 30.       ,  5.2903824], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.37843652568580877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.744951 , 15.574037 ,  1.6295652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8369466172335491}
episode index:108
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.80122092, 19.7179866 ]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 4.1531496, 27.188137 ,  4.2527466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.3808011950470754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.369568 , 13.411219 ,  4.9816837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2765182025895667}
episode index:109
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0000107 , 13.26790762]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([15.0000725,  3.2679076,  3.751907 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.38318128965650855
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.043246 , 13.648047 ,  4.5408874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.378373576187665}
episode index:110
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57492926, 11.14495755]), 'previousTarget': array([ 8.57492926, 11.14495755]), 'currentState': array([0.       , 6.       , 3.1976042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.3845604512380462
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.644106 , 13.44154  ,  1.1238993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.265365876267289}
episode index:111
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 11.69209979]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([2.        , 6.        , 0.79738957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.388961884914483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.746291, 15.199229,  6.227801], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2694400908543906}
episode index:112
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       ,  7.       ,  4.8912945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.38981170336899346
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.535929 , 13.919286 ,  2.0932496], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8780362855212718}
episode index:113
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.804711  , 12.74099823]), 'previousTarget': array([10.804711  , 12.74099823]), 'currentState': array([2.       , 8.       , 2.5442467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.39119192213234233
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.41459 , 16.73661 ,  6.263352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.239839940090559}
episode index:114
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.37621714, 21.01947422]), 'previousTarget': array([15.37621714, 21.01947422]), 'currentState': array([16.       , 31.       ,  1.5250765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.39127446761772666
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.468039 , 13.347633 ,  2.9721854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2532690393771504}
episode index:115
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.52576695, 14.58504608]), 'currentState': array([8.965556 , 7.6304193, 5.7489986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.52497942307189}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.39225387642091064
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.359116  , 16.15385   ,  0.32907218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.005959300804309}
episode index:116
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.1711491, 19.7843135]), 'previousTarget': array([13.42535625, 21.298575  ]), 'currentState': array([12.464142 , 29.637543 ,  5.1227493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.39232593433997587
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.505127, 15.88399 ,  3.737614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0130834918278255}
episode index:117
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.39031544, 17.99951461]), 'previousTarget': array([15., 19.]), 'currentState': array([16.680698  , 27.91591   ,  0.47198027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.3906479908191523
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.260963 , 16.828806 ,  4.1849217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9724872985123691}
episode index:118
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.30790021, 19.77807808]), 'previousTarget': array([18.30790021, 19.77807808]), 'currentState': array([24.       , 28.       ,  2.6166935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.3901192073453595
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.83947   , 13.288187  ,  0.41422692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5127580868802077}
episode index:119
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 11.      ,  5.546431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.39244297860556215
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.320013, 16.283667,  1.285727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.114274374860709}
episode index:120
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 12.       ,  2.8005455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.3934569797528204
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.071482 , 13.94503  ,  2.6519344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0573887113320084}
episode index:121
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 24.       ,  5.6779594], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.3974973723098967
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.945103 , 15.975684 ,  5.2661877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.176093764698593}
episode index:122
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 12.       ,  4.3028493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.39954294349736424
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.44616  , 13.38153  ,  1.2313006], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.170442965120819}
episode index:123
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.231758 , 10.575681 ,  0.9817984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.955335031516476}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4036878975617697
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.33244  , 16.788588 ,  3.1432436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4453630803819495}
episode index:124
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 22.       ,  1.5447849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.40165552562610446
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.523575, 16.018307,  5.091057], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8325470964399118}
episode index:125
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.35501771, 10.6357623 ]), 'previousTarget': array([17.28609324,  9.28476691]), 'currentState': array([19.320206 ,  1.0854921,  2.9880545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.40276687126372945
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.977288 , 14.621548 ,  1.3790009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3791331796534544}
episode index:126
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.66762542,  9.98700176]), 'previousTarget': array([18.85504245,  8.57492926]), 'currentState': array([24.572283 ,  1.9163747,  1.1639009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.40166408162582784
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.393203, 15.332631,  5.089774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6408658558551672}
episode index:127
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13940614, 13.89352217]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([8.      , 6.      , 6.100559], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.40407727807619087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.09168  , 14.312843 ,  3.7234585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2899421194506373}
episode index:128
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.47213595, 13.94427191]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([10.      ,  5.      ,  5.089979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.40741399500156994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.202131 , 14.178181 ,  1.4872618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.456195922377217}
episode index:129
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.51261336, 12.13527631]), 'previousTarget': array([11.39940073, 11.72672794]), 'currentState': array([5.9563475, 4.5844235, 5.7140894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.40893393401800043
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.385567 , 13.438959 ,  1.8751403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6079522109411466}
episode index:130
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.59818034, 12.33761505]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([8.939247 , 3.489206 , 0.5489588], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4095519213385058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.80965  , 16.26766  ,  5.3605785], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2094788706388133}
episode index:131
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.41495392, 14.52576695]), 'previousTarget': array([15.41495392, 14.52576695]), 'currentState': array([22.      ,  7.      ,  4.417741], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.40957766186725875
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.225396, 13.585054,  2.70106 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.613097074715872}
episode index:132
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.1613009 , 14.78885438]), 'previousTarget': array([16.1613009 , 14.78885438]), 'currentState': array([26.      , 13.      ,  6.155398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.40933449634136587
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.915962 , 13.447589 ,  1.5167537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8024890974806786}
episode index:133
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 7.988345 , 20.2156   ,  6.2820177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.738751475147515}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.41344838823434077
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.50031  , 16.339453 ,  5.5566564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4296236581923658}
episode index:134
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.94681661, 12.8693082 ]), 'previousTarget': array([17.19131191, 13.24695048]), 'currentState': array([23.69216 ,  5.486873,  4.177129], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4134446940335967
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.045763 , 13.054557 ,  3.1931598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.166868082084022}
episode index:135
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.258702 , 14.142454 ,  4.3534923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.872298398374801}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.41423069277342545
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.672589, 16.467203,  6.124697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5032906638135064}
episode index:136
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.44220991, 16.05914151]), 'previousTarget': array([18.44220991, 16.05914151]), 'currentState': array([28.      , 19.      ,  5.495625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 425
reward sum = 0.013962323750362412
running average episode reward sum: 0.4113090258462498
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.206528 , 13.328279 ,  2.8195815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0616405480819937}
episode index:137
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.      ,  6.      ,  5.043421], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4103103311011481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.008034 , 16.389225 ,  2.6740177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7164143333252655}
episode index:138
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 15.00496281]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([25.       , 16.       ,  1.1137192], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 269
reward sum = 0.06696800274786396
running average episode reward sum: 0.407840242407959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.765349 , 15.044873 ,  4.8318253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2354657991474034}
episode index:139
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52786405, 16.05572809]), 'previousTarget': array([15.52786405, 16.05572809]), 'currentState': array([20.      , 25.      ,  5.843673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.41132237106403585
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.339344, 16.602905,  3.180904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.733716134022109}
episode index:140
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.56705854, 12.68964732]), 'previousTarget': array([17.56705854, 12.68964732]), 'currentState': array([25.       ,  6.       ,  4.9525204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.41339418187208926
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.139727 , 15.301553 ,  3.2946122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1789448188071538}
episode index:141
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74157276, 14.85642931]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([ 6.      , 10.      ,  4.958277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.41151579708166686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.860334, 14.229946,  4.370943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0134117857554825}
episode index:142
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.60431  ,  8.906518 ,  1.1713423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.985956907798112}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.41471320977878096
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.851358, 14.498398,  2.649785], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9181065444269378}
episode index:143
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.74391196, 14.24859507]), 'previousTarget': array([11.74391196, 14.24859507]), 'currentState': array([ 2.     , 12.     ,  5.53819], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4173444672361759
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.584099, 13.129876,  5.660001], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.915812375653351}
episode index:144
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.418242,  9.91352 ,  2.852289], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.70418725544585}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.42045757996398836
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.220537 , 13.83223  ,  2.3954668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1284209126494353}
episode index:145
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.       , 26.       ,  0.5389276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.41881832909523475
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.824795 , 13.202693 ,  3.6871264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.561286484223091}
episode index:146
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.5226134 , 12.96276447]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([28.640274,  8.855665,  5.660892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.4176687966283179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.525393 , 13.734102 ,  2.0177648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9434413547291924}
episode index:147
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.      , 21.      ,  3.479559], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42133722374569416
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.736582 , 16.339806 ,  3.2431962], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.841549509519291}
episode index:148
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 12.       ,  4.8484483], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.4200880922389473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.70309  , 14.792019 ,  2.2859793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7332061701607071}
episode index:149
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.07106781, 13.07106781]), 'previousTarget': array([13.07106781, 13.07106781]), 'currentState': array([6.       , 6.       , 3.2763247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 317
reward sum = 0.04133868785485247
running average episode reward sum: 0.41756309620972
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.223936, 14.121838,  1.295888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5063826548853956}
episode index:150
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.57492926, 14.14495755]), 'currentState': array([ 6.606158 , 10.191746 ,  5.9320908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.673463499417597}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.41994891252223276
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.965759 , 14.060958 ,  1.1353842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3470304492250236}
episode index:151
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 15.       ,  5.3320756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.999999999999999}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4231359728017497
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.339268 , 14.321319 ,  3.6809053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7587562190647847}
episode index:152
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.88091135, 14.44737414]), 'previousTarget': array([17.56338512, 13.36875492]), 'currentState': array([24.351995 ,  9.133173 ,  1.6373696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4264013892829665
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.00477  , 16.837132 ,  2.6531544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0939473399805664}
episode index:153
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 3.       , 15.       ,  2.8877902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4281094912088503
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.291845, 16.539938,  4.677827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0100431079554344}
episode index:154
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.42173715, 14.87347886]), 'currentState': array([23.512676, 10.662889,  3.46409 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.553857476444813}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.431482914168148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.788542, 16.141636,  2.811894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1218421865190797}
episode index:155
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      , 19.      ,  4.395359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.65685424949238}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4324424237832119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.199232, 16.674406,  5.484286], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8560347122610357}
episode index:156
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 18.       ,  2.1579082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4322919635962013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.685101 , 16.833393 ,  5.8836207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9572156291233298}
episode index:157
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.03861062, 20.31756858]), 'previousTarget': array([18.03861062, 20.31756858]), 'currentState': array([23.       , 29.       ,  2.2115877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4324458736224526
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.862323 , 14.963191 ,  3.2778325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.863108060503165}
episode index:158
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.98280454, 14.37042732]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([ 3.4369264 , 11.39113   ,  0.98940396], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.43368724316698587
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.13361 , 14.776804,  5.591125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.879688525220242}
episode index:159
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.06080701, 10.1914503 ]), 'previousTarget': array([17.06080701, 10.1914503 ]), 'currentState': array([21.       ,  1.       ,  4.5796185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.43511596065334207
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.568186 , 13.684116 ,  3.3578155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0471337200442417}
episode index:160
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([ 4.       , 15.       ,  3.3414917], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4379188731444153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.641535 , 13.251333 ,  1.1739969], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.785030180389888}
episode index:161
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.752553 , 14.963617 ,  6.0594273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2479774643110744}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4413885097299436
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.752553 , 14.963617 ,  6.0594273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2479774643110744}
episode index:162
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.88171698, 17.91263916]), 'previousTarget': array([12.88171698, 17.91263916]), 'currentState': array([ 7.      , 26.      ,  4.787748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4429530784926407
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.65617 , 15.481513,  4.789687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5916704617355095}
episode index:163
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.30535035, 13.19958708]), 'previousTarget': array([19.8085497 , 12.93919299]), 'currentState': array([27.087103 ,  8.416197 ,  2.6711216], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4457666699348124
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.673129 , 15.343322 ,  1.6432415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7556273017697838}
episode index:164
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.626239 , 16.54646  ,  3.4223547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.668445718153964}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4491256598139953
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.626239 , 16.54646  ,  3.4223547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.668445718153964}
episode index:165
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.36170579, 20.34196481]), 'previousTarget': array([12.51123442, 21.63670822]), 'currentState': array([ 7.9335155, 29.308075 ,  2.897986 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.451058897681412
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.456434, 13.801068,  5.790972], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9544903253537433}
episode index:166
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.701425  , 15.57464375]), 'previousTarget': array([12.701425  , 15.57464375]), 'currentState': array([ 3.       , 18.       ,  2.4277554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4535079962260819
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.025396, 15.343822,  4.668131], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0334725549327983}
episode index:167
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.47213595,  9.94427191]), 'previousTarget': array([12.47213595,  9.94427191]), 'currentState': array([8.       , 1.       , 4.2550907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.45248628610295316
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.13715  , 16.987196 ,  2.7954197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.991923163259845}
episode index:168
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.480075 , 12.654869 ,  5.7966394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.87711322798911}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4540133094234818
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.739601 , 13.069285 ,  0.9236305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9481956943198007}
episode index:169
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.10593621, 20.02063204]), 'previousTarget': array([14.62378286, 21.01947422]), 'currentState': array([12.352738 , 29.865747 ,  3.8080509], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.45539822575551464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.166654 , 14.568592 ,  5.3509507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9383916921733603}
episode index:170
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.66519011,  9.97785158]), 'previousTarget': array([14.66519011,  9.97785158]), 'currentState': array([14.      ,  0.      ,  3.869223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4567669442357109
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.257033 , 13.712744 ,  1.6193788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.166785956038742}
episode index:171
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       ,  6.       ,  4.6600933], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.45788517495741254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.183925 , 15.754761 ,  3.0764048], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7768475165801788}
episode index:172
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 22.       ,  2.7617502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4601109438500514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.407211 , 16.104013 ,  3.6213567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7885998254504716}
episode index:173
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.86318339, 10.17821552]), 'previousTarget': array([ 8.86318339, 10.17821552]), 'currentState': array([1.       , 4.       , 2.2172818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4596565540469163
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.130766 , 14.465326 ,  2.3387198], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0205115630405182}
episode index:174
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 15.       ,  6.0028996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.461474638648929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.860282 , 13.156037 ,  2.8776684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.167753577458787}
episode index:175
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 14.       ,  1.0367116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4623953815821238
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.9353285, 16.678827 ,  5.596191 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9879603852459513}
episode index:176
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.24695048, 22.19131191]), 'previousTarget': array([ 9.24695048, 22.19131191]), 'currentState': array([ 3.        , 30.        ,  0.38025767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.4607960612545113
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.582923, 14.963249,  5.615524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.417553536058441}
episode index:177
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.31756858, 18.03861062]), 'previousTarget': array([20.31756858, 18.03861062]), 'currentState': array([29.       , 23.       ,  5.5364313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4617102709940432
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.644257 , 16.867586 ,  3.4245842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3077950870384085}
episode index:178
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897, 20.65196555]), 'previousTarget': array([20.21719897, 20.65196555]), 'currentState': array([27.      , 28.      ,  5.671268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.4613693457535823
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.211879 , 15.701039 ,  4.3895907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.054794379490165}
episode index:179
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25608804, 14.24859507]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([28.       , 12.       ,  1.7074444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4606825960599452
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.674953  , 13.711608  ,  0.82459056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1131548011640553}
episode index:180
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.80868809, 9.24695048]), 'previousTarget': array([7.80868809, 9.24695048]), 'currentState': array([0.      , 3.      , 1.726184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.46274796161458687
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.602677, 14.41726 ,  5.793373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.513967077215886}
episode index:181
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25608804, 14.24859507]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([28.      , 12.      ,  5.983562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4617080780399035
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7817955, 15.690961 ,  4.5292606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.400517455444507}
episode index:182
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.63117406, 16.35236179]), 'currentState': array([12.969261 , 24.250563 ,  5.1448293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.470839976988785}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4642783363451881
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.58363  , 16.667477 ,  5.1046743], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7186746914458964}
episode index:183
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89949494, 14.41421356]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([ 1.       , 13.       ,  1.3737994], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.4624980193267718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.977585 , 16.838339 ,  5.0905414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.082105149048327}
episode index:184
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.05555434, 14.42210984]), 'previousTarget': array([17.56338512, 13.36875492]), 'currentState': array([24.827047 ,  9.619932 ,  1.6302912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4644638063781708
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.684862 , 14.694317 ,  2.6656365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7123675422602038}
episode index:185
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.117414, 14.324818,  3.76057 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.145445902237602}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4668289583600559
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.655919, 13.063583,  2.720857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.044489956522977}
episode index:186
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.41495392, 14.52576695]), 'currentState': array([20.002628 ,  6.897506 ,  2.9771783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.522431168101305}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.46926701042458996
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.793632, 14.555399,  2.151159], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9096818919694272}
episode index:187
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.97101622, 12.79876498]), 'previousTarget': array([19.48341683, 12.24097426]), 'currentState': array([26.005964 ,  6.8456473,  3.555263 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.4673196865941484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.517868 , 13.785687 ,  5.4483113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3065244744325564}
episode index:188
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.18346962, 11.7527251 ]), 'previousTarget': array([18., 11.]), 'currentState': array([25.184034 ,  4.6118503,  1.8591988], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.46939766896476814
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.206293, 16.789808,  2.893151], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1583689937085193}
episode index:189
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.0056719 , 17.63871065]), 'previousTarget': array([20.05572809, 17.52786405]), 'currentState': array([27.356586 , 23.139822 ,  3.0616632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.46891261095402464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.725793 , 14.5507555,  2.383587 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8535783102518147}
episode index:190
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.158578 , 23.219511 ,  2.6492407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.539377829461436}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4685765866698587
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.567047 , 16.733406 ,  2.4698389], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.336735600437863}
episode index:191
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.91263916, 12.88171698]), 'previousTarget': array([17.91263916, 12.88171698]), 'currentState': array([26.       ,  7.       ,  5.0212026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.46772701604099637
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.128994 , 13.462659 ,  3.6351478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.907366020899908}
episode index:192
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.719603 ,  9.021257 ,  0.6260444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.114309495901849}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.4660711506291134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.644171  , 15.945278  ,  0.10648271], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6528229606979008}
episode index:193
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 14.8304548 ]), 'previousTarget': array([12.96545758, 14.8304548 ]), 'currentState': array([ 3.      , 14.      ,  4.928809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4654630164222234
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.729412  , 13.729527  ,  0.23481482], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7968010758212782}
episode index:194
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00496281, 15.0496281 ]), 'previousTarget': array([15.00496281, 15.0496281 ]), 'currentState': array([16.     , 25.     ,  5.65349], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4674424767025959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.87849  , 15.25285  ,  4.5899215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9141544600078176}
episode index:195
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 19.77807808]), 'previousTarget': array([11.69209979, 19.77807808]), 'currentState': array([ 6.       , 28.       ,  1.9263115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4665396502618172
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.752077, 13.816877,  4.089549], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1141318177571216}
episode index:196
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.78885438, 16.1613009 ]), 'previousTarget': array([14.78885438, 16.1613009 ]), 'currentState': array([13.       , 26.       ,  3.1881542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.46828172243420013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.118228 , 13.365873 ,  4.9628706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4922752590562687}
episode index:197
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.008957 ,  8.726849 ,  0.8714839], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.586982338026267}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.469162176371829
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.43038 , 13.858021,  3.214036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9410883270602424}
episode index:198
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.       , 8.       , 2.2086248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4690534378662611
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.238575 , 13.595853 ,  0.9997275], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8723507147100322}
episode index:199
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51658317, 12.24097426]), 'previousTarget': array([10.51658317, 12.24097426]), 'currentState': array([2.       , 7.       , 1.3039805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.46906110388485805
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.305814 , 13.347478 ,  0.3531468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6805806502847156}
episode index:200
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.811297 , 17.008923 ,  4.4151216], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.334262999657659}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4716528396864259
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.22548  , 15.096642 ,  4.7921305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7771495533779222}
episode index:201
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 10.       ,  3.3696592], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.47384028724977845
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.528484  , 16.8521    ,  0.89091307], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9111783806137792}
episode index:202
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.61308449, 21.6887162 ]), 'previousTarget': array([15.75965265, 21.07722123]), 'currentState': array([1.8957521e+01, 3.1410013e+01, 1.6082272e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4753764343116378
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.595068 , 15.20945  ,  3.9880466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6087607216369655}
episode index:203
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.57826285, 14.87347886]), 'previousTarget': array([14.57826285, 14.87347886]), 'currentState': array([ 5.       , 12.       ,  2.3134894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.4742344745652306
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.28093  , 16.655611 ,  0.5922448], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8050236008732226}
episode index:204
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.58648061, 18.53945027]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 1.2166722, 24.011772 ,  5.2254143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.4731771581369317
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.406171 , 14.7584505,  5.86594  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.612029010984434}
episode index:205
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25608804, 14.24859507]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([28.       , 12.       ,  0.4545573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.47208085487361584
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.806037 , 13.528035 ,  2.6142313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6782061977549236}
episode index:206
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96116135,  9.80580676]), 'previousTarget': array([13.96116135,  9.80580676]), 'currentState': array([12.       ,  0.       ,  3.9927762], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.4709365832521994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.296167 , 15.872759 ,  5.5274215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5626125231776886}
episode index:207
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.49357515, 19.31877137]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([24.782759 , 27.093485 ,  5.5041976], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.47152326528657107
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.08735  , 14.641267 ,  3.9720495], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1449984075395863}
episode index:208
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.        , 22.        ,  0.71497047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4731806034315981
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.233866 , 16.394592 ,  4.8282733], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.414065353930955}
episode index:209
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 0.       , 15.       ,  3.6324382], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.47204744164973533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.880244 , 13.155485 ,  6.0748773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8483983495223177}
episode index:210
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8304548 , 12.96545758]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([14.       ,  3.       ,  3.9106867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.47304510588367205
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.71587  , 14.368181 ,  0.9082364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8284978768935058}
episode index:211
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33277453, 14.22931931]), 'previousTarget': array([16.21147869, 12.77895573]), 'currentState': array([19.29694  ,  5.0486116,  3.0126622], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.47369638953296067
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.232597, 13.993701,  1.995624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5912051864022767}
episode index:212
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.52057184, 18.11628302]), 'previousTarget': array([14.52057184, 18.11628302]), 'currentState': array([13.      , 28.      ,  3.473809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4755510816608293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.684228 , 15.581557 ,  3.2357578], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8979848224454127}
episode index:213
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.66654394, 16.41020921]), 'previousTarget': array([18.66654394, 16.41020921]), 'currentState': array([28.       , 20.       ,  3.0069206], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.47664922252817254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.477456 , 14.685191 ,  2.9721935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5718994053174108}
episode index:214
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.        , 17.        ,  0.45388478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4757432293793925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.535818  , 13.63866   ,  0.47992477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0523115403338883}
episode index:215
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.58979079, 11.33345606]), 'previousTarget': array([13.58979079, 11.33345606]), 'currentState': array([10.      ,  2.      ,  4.658675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.47406357209047545
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.367035 , 13.565962 ,  1.7213507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.981224344101906}
episode index:216
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.67949706, 12.54700196]), 'previousTarget': array([18.67949706, 12.54700196]), 'currentState': array([27.       ,  7.       ,  0.4472137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4746951558114081
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.212298 , 13.348067 ,  1.7113249], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4340827295288503}
episode index:217
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.04402655, 11.53260059]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([28.284727 ,  5.8677216,  1.5243745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.47523768466732597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.477323, 13.657825,  2.323679], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.029773139840483}
episode index:218
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 10.      ,  4.332843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4770345254349134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.861515 , 13.775971 ,  1.4984012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.227887844126852}
episode index:219
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.63117406, 16.35236179]), 'currentState': array([12.070944 , 24.001259 ,  4.7384987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.465834929863918}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4787755428403969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.255748, 13.658915,  5.982778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5337605698643693}
episode index:220
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.23076923, 16.15384615]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 3.       , 20.       ,  1.2364497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.47957592147267625
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.31321  , 15.287201 ,  5.5807514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.711065881291848}
episode index:221
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 20.       ,  4.1978226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4798071359838584
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.862263, 15.404284,  4.608147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9056410017203698}
episode index:222
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 20.       ,  1.8883467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4817934030620829
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.886796, 16.207521,  4.422926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2128162147787467}
episode index:223
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.08636336,  8.06404996]), 'previousTarget': array([20.08636336,  8.06404996]), 'currentState': array([26.       ,  0.       ,  5.6741962], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.4817645934145098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.759573 , 16.440205 ,  1.5558922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2738263852519345}
episode index:224
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 16.       ,  1.7739482], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4833323674946679
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.466222 , 15.370182 ,  4.4711075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5778182020684932}
episode index:225
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.96138938,  9.68243142]), 'previousTarget': array([11.96138938,  9.68243142]), 'currentState': array([7.      , 1.      , 2.528492], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4832141258143548
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.615334 , 13.355647 ,  0.5414388], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.149697108940075}
episode index:226
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.52786405,  9.94427191]), 'previousTarget': array([17.52786405,  9.94427191]), 'currentState': array([22.       ,  1.       ,  3.8858018], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4839163173397749
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.8785   , 13.642473 ,  2.4847426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7608637767874558}
episode index:227
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.      , 20.      ,  6.109498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.48523983410936894
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.100203 , 14.550407 ,  2.8482838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1885197395276033}
episode index:228
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.17575448, 14.07838415]), 'previousTarget': array([18.92040615, 13.19058177]), 'currentState': array([26.383749 , 10.1780205,  2.3147721], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.48658644742611257
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.147142 , 16.611826 ,  3.5831523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.618528269573416}
episode index:229
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.      , 9.      , 3.362382], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4857594915129521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.21486  , 14.780815 ,  2.4704833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8151606517649267}
episode index:230
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28609324, 14.28476691]), 'previousTarget': array([15.28609324, 14.28476691]), 'currentState': array([19.       ,  5.       ,  3.4204462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.4840005562756257
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.043316  , 16.667532  ,  0.92662364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9224742788173084}
episode index:231
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.7088  , 12.527351,  1.809903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.31853199992945}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4855844235808811
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.1842165, 16.32237  ,  4.0990615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.335140267932528}
episode index:232
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([19.312233 , 22.926949 ,  3.7630584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.023960686437807}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.48648926819233473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.074033, 14.220049,  5.26372 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.077901208070026}
episode index:233
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.81238194,  9.13733471]), 'previousTarget': array([10.81238194,  9.13733471]), 'currentState': array([5.     , 1.     , 3.08052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.48607172396106074
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.24937  , 13.712308 ,  2.5616145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7941788374388905}
episode index:234
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      , 11.      ,  3.018546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.48715099480968715
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.245768, 13.403729,  6.127624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3717948986020203}
episode index:235
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.       , 8.       , 1.4850931], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.899494936611664}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.4865324145838582
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.08234   , 13.983513  ,  0.37467164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0198166421196424}
episode index:236
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.49612146, 17.53840437]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([24.507555 , 24.668608 ,  3.8844624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.48841230037847055
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.751137, 16.062752,  4.179327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.639847757410741}
episode index:237
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 20.       ,  1.4120803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.48735843590120087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.057547 , 15.483198 ,  4.6976614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1627060297818828}
episode index:238
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.55779009, 13.94085849]), 'previousTarget': array([11.55779009, 13.94085849]), 'currentState': array([ 2.       , 11.       ,  1.2542243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.48891784978714264
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.503834, 15.203724,  5.402292], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5363621514623836}
episode index:239
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.12652114, 15.42173715]), 'previousTarget': array([15.12652114, 15.42173715]), 'currentState': array([18.      , 25.      ,  3.058016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4880669744683345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.132789, 16.146471,  2.548791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1541354494135017}
episode index:240
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.7858   , 16.160837 ,  5.2642703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.34185593124274}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4896105071661476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.14135 , 16.627459,  5.970547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8400819502705659}
episode index:241
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17821552,  8.86318339]), 'previousTarget': array([10.17821552,  8.86318339]), 'currentState': array([4.      , 1.      , 3.956782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4894552722202812
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.89712  , 13.284898 ,  0.6421727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9355621121202422}
episode index:242
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.44692322, 14.40223305]), 'previousTarget': array([15.52786405, 13.94427191]), 'currentState': array([21.434895 ,  6.3932257,  0.6147209], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4903069510097021
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.79506  , 15.202099 ,  1.9549359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8064001466896293}
episode index:243
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.63778901, 16.96128974]), 'previousTarget': array([11.63778901, 16.96128974]), 'currentState': array([ 3.       , 22.       ,  2.4317634], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.48918702455750174
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.107595 , 14.248904 ,  1.2882411], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3382497993131994}
episode index:244
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 17.       ,  0.3359114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4910719348650221
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.805584, 16.690228,  6.22936 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7013719844994764}
episode index:245
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([27.        , 31.        ,  0.42324865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4912122785750243
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.453651 , 16.98517  ,  3.9010527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.460488538966586}
episode index:246
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.999884 , 17.978424 ,  5.8064513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.819980536948066}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4931915810909149
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.481389 , 16.01441  ,  5.9008226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8262548882783276}
episode index:247
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.61523948, 13.74721128]), 'previousTarget': array([10.61523948, 13.74721128]), 'currentState': array([ 1.      , 11.      ,  3.278266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.49319822253651685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.27917  , 16.566893 ,  1.1259861], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7247458744352184}
episode index:248
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.75902574, 19.48341683]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([23.       , 28.       ,  2.1500897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.49481324274423655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.010284, 15.520176,  4.423556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1363351702617712}
episode index:249
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.741034 , 11.554028 ,  1.6637533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.861068608729108}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4965249685509713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.71653 , 16.711746,  2.308072], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4241596111314645}
episode index:250
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       ,  7.       ,  2.3653982], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.49703094634515543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.616929  , 16.720812  ,  0.57643205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.762934131350442}
episode index:251
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.701425  , 14.42535625]), 'previousTarget': array([12.701425  , 14.42535625]), 'currentState': array([ 3.      , 12.      ,  2.369777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4984715312987115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.055122 , 15.9330225,  6.2075124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1570998952584883}
episode index:252
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.96128974, 18.36221099]), 'previousTarget': array([16.96128974, 18.36221099]), 'currentState': array([22.       , 27.       ,  1.3523452], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.49772083145418955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.928949 , 13.144035 ,  3.6563861], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0754642198123165}
episode index:253
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.528118 , 17.645908 ,  4.3133054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.410039522621061}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.49939415374936175
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.834473 , 14.025425 ,  4.9630256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0772786140598964}
episode index:254
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.878305, 21.203175,  4.64039 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.555986793276171}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.49978459238746686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.642995, 13.534523,  5.350506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9972695615129383}
episode index:255
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.94427191, 16.52786405]), 'previousTarget': array([11.94427191, 16.52786405]), 'currentState': array([ 3.       , 21.       ,  0.9453492], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.49853276071249514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.565527 , 13.904839 ,  3.3959992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9105635935197396}
episode index:256
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.28609324,  9.28476691]), 'previousTarget': array([17.28609324,  9.28476691]), 'currentState': array([21.       ,  0.       ,  4.7703085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.49922224043369273
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.08837  , 14.009121 ,  1.1201518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.346443454772721}
episode index:257
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.75140493, 11.74391196]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([18.      ,  2.      ,  2.598358], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5001832863790335
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.7765665, 15.976728 ,  6.247779 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.247819691124141}
episode index:258
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.19419324, 15.03883865]), 'currentState': array([23.878803 , 18.656176 ,  2.9813373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.602123063396725}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5008610692465287
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.083952, 16.465342,  4.300875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.822684213174693}
episode index:259
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28609324, 14.28476691]), 'previousTarget': array([15.28609324, 14.28476691]), 'currentState': array([19.       ,  5.       ,  5.6499476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5021767697239802
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.120937 , 14.449878 ,  1.2879854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5632586831101146}
episode index:260
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.10050506, 15.58578644]), 'previousTarget': array([19.10050506, 15.58578644]), 'currentState': array([2.9000000e+01, 1.7000000e+01, 2.7368244e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5021109252816438
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.577654, 15.989862,  4.768927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0761988176287531}
episode index:261
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 20.       ,  2.0826526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5026969874774166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.399822 , 14.393818 ,  5.6563935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7111474558504598}
episode index:262
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8304548 , 12.96545758]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([14.       ,  3.       ,  5.4005084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5028042443423508
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.28904  , 14.666674 ,  0.7391301], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.74312704605419}
episode index:263
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.45299804, 18.67949706]), 'previousTarget': array([17.45299804, 18.67949706]), 'currentState': array([23.       , 27.       ,  5.7022676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5032853783834905
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.251251 , 16.520275 ,  5.0592723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6946566503945328}
episode index:264
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3177872, 18.598156 ]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([27.       , 25.       ,  5.3062186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5034928529501486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.608312 , 13.968747 ,  2.3689709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7321313224353723}
episode index:265
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74355791, 17.50934076]), 'previousTarget': array([15., 19.]), 'currentState': array([13.726903 , 27.457527 ,  4.5459814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5051040277432193
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.331277, 15.617154,  4.900643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7791897075186986}
episode index:266
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.97785158, 15.33480989]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([ 0.       , 16.       ,  2.2878513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5049925521412061
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.2213955, 15.465719 ,  4.408301 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.838566938841472}
episode index:267
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.132927 , 12.283036 ,  3.4243796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.632859153759312}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5066567219089628
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.304409 , 15.622695 ,  1.5494657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4454175641030522}
episode index:268
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.745952 , 18.144318 ,  4.5398884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.123051312601701}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5082381666152752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.674175 , 13.71932  ,  4.8744507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8433533941582547}
episode index:269
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.005077 , 14.857579 ,  3.2348602], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.007103261924886}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5099858030352186
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.341797 , 15.768815 ,  1.9290419], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5464460612626219}
episode index:270
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.77807808, 18.30790021]), 'previousTarget': array([19.77807808, 18.30790021]), 'currentState': array([28.       , 24.       ,  2.3817182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5087395452018469
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.710485, 13.537453,  3.438344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9498441897680387}
episode index:271
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       ,  8.       ,  1.1482488], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5099682350848692
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.387799 , 14.953696 ,  1.1543314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.39055384463989046}
episode index:272
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.8085497 , 17.06080701]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([29.        , 21.        ,  0.17675975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5110662557190684
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.559592, 15.627228,  3.630135], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8405701487293006}
episode index:273
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.83800603,  9.60477128]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([9.118312  , 0.32232141, 3.7302635 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5123085605197101
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.594387 , 13.190734 ,  2.4857533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.411537655775415}
episode index:274
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.01377474, 14.53499687]), 'previousTarget': array([17.51316702, 14.16227766]), 'currentState': array([25.103212, 10.365809,  3.712339], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5116136242071467
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.742495 , 16.358562 ,  3.7312632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2095201641359057}
episode index:275
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.39793423, 15.58256937]), 'previousTarget': array([13.39793423, 15.58256937]), 'currentState': array([ 4.       , 19.       ,  1.6132618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5105868736126836
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.65064  , 13.470155 ,  5.6811056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.039901987504311}
episode index:276
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94085849, 11.55779009]), 'previousTarget': array([13.94085849, 11.55779009]), 'currentState': array([11.       ,  2.       ,  5.0850105], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5110869304890581
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.089446, 13.471524,  0.525617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4467232130004515}
episode index:277
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 18.       ,  5.8398347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5118302527988035
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.338921 , 16.886082 ,  4.2796803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9162910350076494}
episode index:278
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13802944, 16.89633523]), 'previousTarget': array([14.13802944, 16.89633523]), 'currentState': array([10.       , 26.       ,  1.3635623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5109958866188259
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.762093 , 16.654495 ,  2.7583036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4170901500833213}
episode index:279
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.     , 13.     ,  1.88278], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5126016013451872
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.655315  , 15.196057  ,  0.35899174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.396542490326381}
episode index:280
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.01725007, 21.45520022]), 'previousTarget': array([17.01725007, 21.45520022]), 'currentState': array([20.       , 31.       ,  0.9241179], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5130414016466772
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.125351, 13.641523,  2.673946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3642476459035637}
episode index:281
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.07106781, 15.92893219]), 'currentState': array([ 7.754499, 21.147778,  5.354835], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.502234170764641}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5126864664320929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.882512 , 13.987989 ,  5.3980412], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5076288090191414}
episode index:282
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.57826285, 14.87347886]), 'previousTarget': array([14.57826285, 14.87347886]), 'currentState': array([ 5.      , 12.      ,  5.058379], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.5111979928500442
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.591641 , 16.901775 ,  3.0262554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.366479113161067}
episode index:283
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.32793492, 8.80451099]), 'previousTarget': array([8.32793492, 8.80451099]), 'currentState': array([1.       , 2.       , 3.0705147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5121094194449566
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.586618, 14.704058,  6.234548], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4440323160112476}
episode index:284
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 14.99503719]), 'previousTarget': array([15.0496281 , 14.99503719]), 'currentState': array([25.      , 14.      ,  4.283417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5125224517669155
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.74536   , 13.8881035 ,  0.84388703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0694427673949036}
episode index:285
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.67949706, 12.54700196]), 'previousTarget': array([18.67949706, 12.54700196]), 'currentState': array([27.      ,  7.      ,  6.252686], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5132399625390531
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.555469 , 13.624453 ,  2.4794028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4455929151621272}
episode index:286
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.        , 20.        ,  0.85258645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5147320886256801
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.864527 , 16.680729 ,  5.5228777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.028336555243227}
episode index:287
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.03323209, 15.03016538]), 'previousTarget': array([16.52590681, 16.35636161]), 'currentState': array([22.437689, 21.751328,  4.132268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5159012750231426
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.880241 , 15.205425 ,  4.2467413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1384458151655559}
episode index:288
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.0530405, 14.9771627]), 'previousTarget': array([17.13606076, 14.64398987]), 'currentState': array([26.05069  , 14.760344 ,  1.9427748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5156961140290969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.483822, 16.331135,  4.024056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.993400909028838}
episode index:289
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.76696499, 18.02945514]), 'previousTarget': array([14.76696499, 18.02945514]), 'currentState': array([14.      , 28.      ,  2.359328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5156241917724456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.874058  , 15.947931  ,  0.12082928], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4718422591242593}
episode index:290
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.218962 , 12.987978 ,  2.2365787], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3524668471618604}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5172543491890351
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.983612 , 14.560845 ,  2.3965418], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.43946029304682604}
episode index:291
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.91263916, 17.11828302]), 'previousTarget': array([17.91263916, 17.11828302]), 'currentState': array([26.      , 23.      ,  5.699032], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5164385537760079
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.538032 , 13.332935 ,  2.4368558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.751737018537251}
episode index:292
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.99802 , 20.088972,  6.098451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.869015110173178}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5177011691955987
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.892956 , 16.359234 ,  5.6189966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.626310763947419}
episode index:293
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 15.       ,  5.7809196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5178201566376627
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.6186   , 13.222026 ,  2.1462922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8184218556343321}
episode index:294
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85277334, 15.44128997]), 'previousTarget': array([14.13802944, 16.89633523]), 'currentState': array([11.687981 , 24.927284 ,  4.7371955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5186490466637964
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.507939 , 14.448927 ,  5.5381517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5905742831788579}
episode index:295
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.2999894 , 22.52001696]), 'previousTarget': array([10.2999894 , 22.52001696]), 'currentState': array([ 5.       , 31.       ,  2.0768502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5176155218548951
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.578644, 16.343895,  5.987189], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9560948622398047}
episode index:296
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.57464375, 12.701425  ]), 'previousTarget': array([15.57464375, 12.701425  ]), 'currentState': array([18.       ,  3.       ,  3.2964616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.5161684830917141
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.375095 , 14.022452 ,  1.2481533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.687153421858592}
episode index:297
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.48341683, 17.75902574]), 'previousTarget': array([19.48341683, 17.75902574]), 'currentState': array([28.       , 23.       ,  1.9368347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.5151290168300553
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.956091, 14.046849,  2.646504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3500393370720234}
episode index:298
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.      , 10.      ,  3.375557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.5137477774058696
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.151995 , 13.0885   ,  1.7845573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0911588037513984}
episode index:299
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.55689083, 8.54930538]), 'previousTarget': array([7.55689083, 8.54930538]), 'currentState': array([0.       , 2.       , 3.6705444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5139920740876417
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.36461 , 13.536596,  0.479877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5953906118690224}
episode index:300
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.89675823, 16.46605242]), 'previousTarget': array([15., 15.]), 'currentState': array([21.114805 , 24.996702 ,  1.2905239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.5124981774834876
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.216309, 15.271373,  4.857152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8042167903700466}
episode index:301
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.14527  ,  7.748313 ,  2.8396497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.35285720082461}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5131304606540706
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.574032 , 13.217987 ,  2.4241183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2823135934189636}
episode index:302
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58256937, 13.39793423]), 'previousTarget': array([15.58256937, 13.39793423]), 'currentState': array([19.     ,  4.     ,  3.38891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5125185540427959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.980321 , 13.377446 ,  2.6612444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.916357566078251}
episode index:303
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      , 11.      ,  2.161802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5139608944896946
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.530377, 14.878277,  0.619444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.474654926769051}
episode index:304
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 16.       ,  2.6983345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5154252719175972
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.530062 , 16.169209 ,  2.6764827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9256524741561216}
episode index:305
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.09464254, 16.04106794]), 'previousTarget': array([15.09464254, 16.04106794]), 'currentState': array([16.       , 26.       ,  3.1404285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5159939771919485
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.284084 , 13.576027 ,  5.8575864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5938113449506366}
episode index:306
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 11.07106781]), 'previousTarget': array([18.92893219, 11.07106781]), 'currentState': array([26.       ,  4.       ,  1.3831317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5172590850024269
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.92513  , 14.589882 ,  3.1125112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9683306697296423}
episode index:307
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.      , 14.      ,  4.428074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5179572807318165
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.007662 , 14.205745 ,  1.6291871], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2830525145581837}
episode index:308
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.        ,  9.        ,  0.09280959], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5169905493034497
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.663422 , 13.468534 ,  2.3033116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.261052452465545}
episode index:309
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.29496509, 21.1288252 ]), 'previousTarget': array([12.47213595, 20.05572809]), 'currentState': array([ 6.1215553, 29.686619 ,  2.5136063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.5157059267473467
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.407396 , 15.43742  ,  5.6571493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.651581864141981}
episode index:310
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.57826285, 15.12652114]), 'currentState': array([ 6.8053007, 17.13925  ,  6.245193 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.469326190727992}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5163788348155478
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.323033 , 16.10072  ,  4.2904797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.005941303881423}
episode index:311
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36336397, 19.58258088]), 'previousTarget': array([13.36336397, 19.58258088]), 'currentState': array([10.       , 29.       ,  4.4982815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5157847779357576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.949056, 16.636202,  5.764625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8915240389158468}
episode index:312
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52786405, 13.94427191]), 'previousTarget': array([15.52786405, 13.94427191]), 'currentState': array([20.      ,  5.      ,  5.305742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5162957819968584
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.214935 , 14.068785 ,  1.9983399], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0133598493013563}
episode index:313
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.0618314 , 12.22885465]), 'previousTarget': array([ 9.0618314 , 12.22885465]), 'currentState': array([0.      , 8.      , 2.186067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.5149284914780731
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.621414 , 16.404072 ,  1.3930364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.967718600850974}
episode index:314
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.013221 , 17.770414 ,  2.8171031], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.704661480123501}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5151203208701329
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.726117 , 13.348455 ,  5.3582664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.085755744518764}
episode index:315
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.16513  , 15.795772 ,  2.8909063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.2091850559443245}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5163521618642427
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.139681 , 15.859778 ,  3.9798336], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.04939166535135}
episode index:316
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24859507, 11.74391196]), 'previousTarget': array([14.24859507, 11.74391196]), 'currentState': array([12.       ,  2.       ,  5.7734523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5159497383822553
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.678496  , 16.211933  ,  0.40380982], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2538526692295733}
episode index:317
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 23.       ,  1.1836919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5169252694685866
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.148143, 15.485922,  4.493608], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9807041949692274}
episode index:318
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.7124705, 10.974587 ]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([14.       ,  1.       ,  3.5796995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5164181708643123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.12816  , 16.593393 ,  6.0434475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9523443220587886}
episode index:319
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.85504245, 16.42507074]), 'previousTarget': array([15.85504245, 16.42507074]), 'currentState': array([21.       , 25.       ,  0.3693611], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5152315515958507
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.996451, 16.7831  ,  4.682896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0426358989183213}
episode index:320
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.        , 22.        ,  0.39888945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 256
reward sum = 0.0763149839065938
running average episode reward sum: 0.5138642102634854
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.101387, 13.72058 ,  4.317875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.289464241005052}
episode index:321
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.16227766, 17.51316702]), 'previousTarget': array([14.16227766, 17.51316702]), 'currentState': array([11.      , 27.      ,  4.056231], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5148340997466286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4917  , 16.453285,  4.807637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5396124958875312}
episode index:322
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.      , 17.      ,  6.145842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5154847072890782
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.219743 , 16.89047  ,  3.0788598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0451601567095383}
episode index:323
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.71332067, 14.7940775 ]), 'previousTarget': array([11.97054486, 14.76696499]), 'currentState': array([ 3.8389788, 13.21377  ,  5.09821  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5153029944509759
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.7831545, 16.752714 ,  5.7541375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9197233831929998}
episode index:324
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 17.       ,  1.3646213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5149627759224447
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.086471 , 13.127039 ,  5.6295605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.083871164676218}
episode index:325
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.97054486, 14.76696499]), 'previousTarget': array([11.97054486, 14.76696499]), 'currentState': array([ 2.      , 14.      ,  3.910241], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.5137982674223974
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.3406756e+01, 1.3125667e+01, 4.6622115e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.459989998177841}
episode index:326
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94427191, 15.52786405]), 'previousTarget': array([13.94427191, 15.52786405]), 'currentState': array([ 5.       , 20.       ,  3.1904817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5147032509110789
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.74691  , 15.370972 ,  4.9928637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7858652994819344}
episode index:327
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 20.       ,  1.0135002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5155293938680794
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.406959 , 15.140984 ,  4.2338285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5992677499549128}
episode index:328
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.3216372 , 20.08772099]), 'previousTarget': array([14.3216372 , 20.08772099]), 'currentState': array([13.       , 30.       ,  0.0927197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5152050497360261
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.91094  , 15.625314 ,  3.9696164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.255813923198569}
episode index:329
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 13.       ,  4.8473287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5164120563958673
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.918977 , 14.985831 ,  1.3873715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.919086003728632}
episode index:330
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.81852598, 12.12863427]), 'previousTarget': array([18.598156 , 10.6822128]), 'currentState': array([24.823626 ,  4.992208 ,  1.8644114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5162175899120928
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.588045 , 16.440548 ,  2.4616249], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1440768830266443}
episode index:331
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.39940073, 18.27327206]), 'previousTarget': array([11.39940073, 18.27327206]), 'currentState': array([ 4.       , 25.       ,  3.0557153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5167815360117538
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.462532, 16.750288,  5.973492], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8309505512199045}
episode index:332
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       ,  9.       ,  3.2817848], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5180854654828898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.851705 , 14.559855 ,  2.4520044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2297599548569094}
episode index:333
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36875492, 17.56338512]), 'previousTarget': array([13.36875492, 17.56338512]), 'currentState': array([ 8.       , 26.       ,  2.5821376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5180612148265026
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.191711 , 13.413404 ,  5.8396297], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.59813599650974}
episode index:334
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.30735482, 10.12819161]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([10.02543  ,  0.682083 ,  3.6924675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.518980938435485
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.059751 , 15.640588 ,  1.3761896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0432622541825056}
episode index:335
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74157276, 14.85642931]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([ 6.      , 10.      ,  6.065407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5197982995819379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.064148 , 14.679533 ,  1.5841227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.32682419498355664}
episode index:336
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.54700196, 11.32050294]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([7.       , 3.       , 1.3562225], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5209939862135284
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.69387  , 14.08904  ,  1.4953468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9232903843766258}
episode index:337
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.        , 13.        ,  0.43300456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5222661639167429
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4844055, 14.555072 ,  5.991386 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6810277131633281}
episode index:338
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 20.       ,  3.5157104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5213722733723468
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.679345, 13.606309,  3.366105], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.550446576244582}
episode index:339
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.52576695, 15.41495392]), 'previousTarget': array([14.52576695, 15.41495392]), 'currentState': array([ 7.      , 22.      ,  4.713072], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5212368844565626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.670676  , 13.0804    ,  0.18050927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.033388544953674}
episode index:340
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.80580676, 14.96116135]), 'currentState': array([ 6.973179  , 13.326444  ,  0.18192191], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.19942975696123}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5225253276399744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.6894   , 13.914742 ,  0.2233507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.128829677715811}
episode index:341
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 10.       ,  4.4139605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5234622219842548
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.98145  , 13.336976 ,  1.2576852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9501519406458006}
episode index:342
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.23303501, 11.97054486]), 'previousTarget': array([15.23303501, 11.97054486]), 'currentState': array([16.      ,  2.      ,  5.210531], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.52411443735553
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.04469  , 15.812037 ,  2.3265746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3231705592896572}
episode index:343
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.04745394, 15.0622916 ]), 'previousTarget': array([18.02945514, 15.23303501]), 'currentState': array([26.029818, 15.655938,  3.316997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.525355354833857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.274155 , 14.6310835,  2.6092525], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.32648765465317}
episode index:344
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.267078 , 19.00151  ,  3.5870512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.010413708643364}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5258309888368576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.824579, 15.230016,  5.736303], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.197714996589223}
episode index:345
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64398987, 17.13606076]), 'previousTarget': array([14.64398987, 17.13606076]), 'currentState': array([13.      , 27.      ,  3.285617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5259574923444886
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.183029 , 14.004134 ,  4.5694385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.071987382856557}
episode index:346
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91227901, 15.6783628 ]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 0.       , 17.       ,  0.9652668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5262568183930729
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.083902, 13.927501,  0.750579], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5248275375220217}
episode index:347
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      ,  9.      ,  6.261454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5262549208905801
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.751399, 14.355278,  6.152069], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4052298013222508}
episode index:348
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.601642  , 10.54522032]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([26.786432 ,  3.589734 ,  2.0073678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5265883211232281
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.822454, 16.988695,  2.889921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6974522442219726}
episode index:349
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.17403643, 20.90614089]), 'previousTarget': array([21.45069462, 22.44310917]), 'currentState': array([26.763527, 28.428017,  4.517771], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5267779729097375
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.303793 , 15.398    ,  2.2757006], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.500693489400481}
episode index:350
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.55725948,  8.6954142 ]), 'previousTarget': array([21.67206508,  8.80451099]), 'currentState': array([27.169722 ,  1.1937248,  2.6433082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5257984246060319
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.742383, 13.315777,  2.552124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1019534281054177}
episode index:351
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 23.      ,  5.406934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.526205167032065
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.917547, 14.08161 ,  5.462905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1261298312244405}
episode index:352
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.13606076, 15.35601013]), 'previousTarget': array([17.13606076, 15.35601013]), 'currentState': array([27.      , 17.      ,  5.329775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5256153197141813
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.487955 , 14.734831 ,  2.3353038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5113983706108147}
episode index:353
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.31414516, 12.98170776]), 'previousTarget': array([11.39940073, 11.72672794]), 'currentState': array([4.319722 , 6.974279 , 0.6607258], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5263277661539694
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.550062 , 15.444847 ,  0.0201259], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6327187296141943}
episode index:354
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.75304952, 22.19131191]), 'previousTarget': array([20.75304952, 22.19131191]), 'currentState': array([27.       , 30.       ,  5.0405207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5265323527463981
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.293615, 16.52168 ,  3.763718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5497491806030717}
episode index:355
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 15.       ,  1.1865158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5275431744288973
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.79419  , 16.430073 ,  0.8072152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.870584118344994}
episode index:356
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([20.76147  ,  6.985725 ,  2.1638343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.870316645363731}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5287293001304971
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.927084, 14.074265,  2.168584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4170867921442187}
episode index:357
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.40743398,  9.50791373]), 'previousTarget': array([21.40743398,  9.50791373]), 'currentState': array([29.       ,  3.       ,  5.8340216], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5288921701914104
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.06882  , 14.71154  ,  2.1034544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9748360048722308}
episode index:358
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([14.884092 ,  6.6597443,  1.8984265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.341061104643245}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5285018965086328
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.905565, 16.324175,  1.043614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7179134179322828}
episode index:359
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.      , 20.      ,  3.094414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5293989961602612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.767574 , 16.35839  ,  5.3890224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3781318111039589}
episode index:360
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 18.       ,  5.4828887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5289163419180031
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.373358 , 16.254503 ,  6.2621603], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4023048475431399}
episode index:361
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      , 11.      ,  0.406801], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5291950913359181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.788572 , 13.397738 ,  4.0033655], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7858018768506874}
episode index:362
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 18.       ,  1.7222551], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5301790852212631
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.970064, 15.516974,  5.218218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0992211159827556}
episode index:363
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.      , 16.      ,  2.451197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.530287387741197
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.765165 , 14.456014 ,  4.0371637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8470868294472855}
episode index:364
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.4573102 , 10.94366714]), 'previousTarget': array([ 9.68243142, 11.96138938]), 'currentState': array([1.387497 , 5.0378976, 4.5233145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 314
reward sum = 0.04260407137887648
running average episode reward sum: 0.5289512690662317
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.4441403e+01, 1.4986852e+01, 1.1822015e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5587513327450193}
episode index:365
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 11.       ,  3.3108876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5292974656550511
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.427946 , 16.637194 ,  1.0146247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2697481443894474}
episode index:366
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.36889329, 18.76364357]), 'previousTarget': array([16.42337349, 20.3376506 ]), 'currentState': array([19.786976, 28.16134 ,  5.143495], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.529287381245979
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.269365 , 16.247444 ,  3.5454304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7797205416481803}
episode index:367
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 7.4726152, 18.646706 ,  5.6292167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.36420846393234}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5304594155632453
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.907229 , 15.833579 ,  1.1126006], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3744095415045305}
episode index:368
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.95893206, 15.09464254]), 'previousTarget': array([13.95893206, 15.09464254]), 'currentState': array([ 4.       , 16.       ,  3.9794898], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.5292599185270038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.63347  , 13.142148 ,  1.1678097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.473831032442877}
episode index:369
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.05482  ,  9.465051 ,  3.1929526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.8612842656537545}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5294811545297223
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.120871 , 13.265129 ,  2.2372053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.557519191587513}
episode index:370
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.       , 17.       ,  2.1153808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5304431591582571
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.560841 , 15.266726 ,  3.5782855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5138126959380585}
episode index:371
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.      , 18.      ,  4.438237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5313525748937699
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.527419 , 13.526014 ,  4.9564333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.083537447935831}
episode index:372
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.92893219, 20.92893219]), 'previousTarget': array([20.92893219, 20.92893219]), 'currentState': array([28.       , 28.       ,  2.7504568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5306539198927464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.858287 , 15.478516 ,  4.2977614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4990589324885506}
episode index:373
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.93400984, 12.4911795 ]), 'previousTarget': array([15.2875295, 10.974587 ]), 'currentState': array([14.671068 ,  2.494637 ,  1.2191031], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5317022909476532
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.901207, 13.426031,  0.307562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4681908357891444}
episode index:374
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.49229675, 13.29980283]), 'previousTarget': array([10.51658317, 12.24097426]), 'currentState': array([2.4936447, 8.938122 , 0.9282272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.530772302220617
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.57731   , 13.602711  ,  0.15695351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4598234442113722}
episode index:375
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.323364 , 17.117935 ,  4.0931726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.497387112233533}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5319936524806685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.16257  , 15.489268 ,  4.2938156], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5155700505659019}
episode index:376
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.28609324,  9.28476691]), 'previousTarget': array([17.28609324,  9.28476691]), 'currentState': array([21.       ,  0.       ,  5.3979487], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5311245538315056
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.94542 , 13.938062,  1.078121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4218060348560666}
episode index:377
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.84214392, 21.5516486 ]), 'previousTarget': array([ 7.31055268, 22.1768175 ]), 'currentState': array([ 1.9934534 , 28.83831   ,  0.22964782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5313361746931493
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.72468 , 16.033054,  5.115835], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0104034950784393}
episode index:378
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36875492, 17.56338512]), 'previousTarget': array([13.36875492, 17.56338512]), 'currentState': array([ 8.       , 26.       ,  1.4205934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5312530634931653
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.25552  , 13.613894 ,  4.7517166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.228115682697062}
episode index:379
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.74099823, 19.195289  ]), 'previousTarget': array([12.74099823, 19.195289  ]), 'currentState': array([ 8.      , 28.      ,  6.202353], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5312521489654335
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.954415 , 16.458946 ,  5.3954597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7433968859393505}
episode index:380
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.055271  , 12.698942  ,  0.91576326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.531494473182135}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5318386768873186
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.079432 , 14.698796 ,  1.7399747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1206677124753102}
episode index:381
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 14.99503719]), 'previousTarget': array([15.0496281 , 14.99503719]), 'currentState': array([25.        , 14.        ,  0.38728842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5320951820033288
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.018074 , 15.098955 ,  3.0747821], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0228718710765388}
episode index:382
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.77114535,  9.0618314 ]), 'previousTarget': array([17.77114535,  9.0618314 ]), 'currentState': array([22.     ,  0.     ,  5.03422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.5311208935625885
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.331415 , 16.252527 ,  4.3111625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.086384382813724}
episode index:383
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.27988761, 16.40669876]), 'previousTarget': array([17.56705854, 17.31035268]), 'currentState': array([23.009697 , 23.803297 ,  2.9484265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5306720065941491
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.452198 , 13.366472 ,  5.1904063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7229335241595443}
episode index:384
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.64763821, 15.36882594]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 4.       , 18.       ,  0.6848469], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5314180713500013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.4406792e+01, 1.3407217e+01, 4.6789008e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.699662821161098}
episode index:385
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99503719, 14.9503719 ]), 'previousTarget': array([14.99503719, 14.9503719 ]), 'currentState': array([14.       ,  5.       ,  3.6845648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5317061374918011
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.383373  , 13.571902  ,  0.02135766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.157068642111055}
episode index:386
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06368358, 11.21483268]), 'previousTarget': array([11.96138938,  9.68243142]), 'currentState': array([8.509449  , 2.3120832 , 0.80242574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5321682746746964
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.822277, 13.728843,  2.654326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7328797112352248}
episode index:387
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.      , 15.      ,  6.235829], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5324529224257532
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.9189   , 15.781715 ,  2.2318568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0720169848550203}
episode index:388
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 13.       ,  5.2866845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5335288533447101
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.543186 , 14.272233 ,  0.8468286], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7061853365742439}
episode index:389
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.687166 , 12.975382 ,  1.8290672], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6354522348341303}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5346992921822877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.176348 , 14.909048 ,  1.8628442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1798585675970408}
episode index:390
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.38366053, 19.8197379 ]), 'previousTarget': array([18.85504245, 21.42507074]), 'currentState': array([24.129498 , 28.004196 ,  5.2678843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5345474782432171
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.559946, 15.392556,  2.971942], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.683842052673303}
episode index:391
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.38168568, 19.91031418]), 'previousTarget': array([ 9.34803445, 20.21719897]), 'currentState': array([ 0.3506775, 25.868746 ,  3.4059217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 349
reward sum = 0.02996973580906778
running average episode reward sum: 0.5332602901247626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.179255, 13.333219,  6.139484], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.468455869579831}
episode index:392
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.55689083, 21.45069462]), 'previousTarget': array([ 7.55689083, 21.45069462]), 'currentState': array([ 0.     , 28.     ,  4.72828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5338431970566261
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.091023, 13.370362,  5.400636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.866000393860292}
episode index:393
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.551592, 19.738033,  5.316504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.499022569920135}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5349263260234873
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.357067  , 15.248245  ,  0.09004324], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6891940236473667}
episode index:394
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.14495755, 13.57492926]), 'previousTarget': array([14.14495755, 13.57492926]), 'currentState': array([9.       , 5.       , 4.9856505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5352319789210839
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.380281 , 13.227092 ,  0.2690789], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2468601203588268}
episode index:395
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.6284586 , 10.75724629]), 'previousTarget': array([19.6284586 , 10.75724629]), 'currentState': array([27.        ,  4.        ,  0.08339209], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.5341409136467922
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.41052 , 15.192298,  3.289837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4235683081920447}
episode index:396
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.32793492, 8.80451099]), 'previousTarget': array([8.32793492, 8.80451099]), 'currentState': array([1.       , 2.       , 6.0165257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5347944989616458
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.212153  , 15.701361  ,  0.45283693], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.920495294421687}
episode index:397
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.3234678 , 14.81905724]), 'previousTarget': array([19.10050506, 14.41421356]), 'currentState': array([27.293282 , 14.042646 ,  2.4356945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5357003877940505
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.530074 , 16.719    ,  2.5151658], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7820748752402293}
episode index:398
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 11.       ,  4.0496864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5354031960450258
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.816996 , 16.540543 ,  4.0214124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.382172243138788}
episode index:399
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.94427191, 17.52786405]), 'previousTarget': array([ 9.94427191, 17.52786405]), 'currentState': array([ 1.       , 22.       ,  3.0953183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5355924811537455
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.377411 , 13.9947815,  6.1145377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1824049416619073}
episode index:400
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 12.       ,  6.0304565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5362161361653501
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.06926   , 13.558889  ,  0.18331522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7944681603087078}
episode index:401
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.298575  , 14.42535625]), 'previousTarget': array([17.298575  , 14.42535625]), 'currentState': array([27.       , 12.       ,  3.7841856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5372242556012597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.766125 , 13.283781 ,  2.7053232], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.879455924863513}
episode index:402
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.03883865, 15.19419324]), 'currentState': array([18.454891 , 23.62767  ,  3.9180977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.293705818303058}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5379828633873209
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.267222, 15.418577,  5.304711], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7826174752459076}
episode index:403
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.641043, 17.856768,  5.674347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.879231107057002}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5391017176858672
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.281668 , 16.712938 ,  5.7887173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1393526513079117}
episode index:404
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91227901, 14.3216372 ]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([ 0.       , 13.       ,  1.5572239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5396719434343099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.412941 , 15.546974 ,  5.7832947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8023833560390432}
episode index:405
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.43941288,  9.55935459]), 'previousTarget': array([15.6783628 ,  9.91227901]), 'currentState': array([18.99708   , -0.10803077,  0.27482003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5395253166193766
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.915998 , 14.574184 ,  1.0221664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.434022083564924}
episode index:406
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.14495755,  8.57492926]), 'previousTarget': array([11.14495755,  8.57492926]), 'currentState': array([6.      , 0.      , 5.855021], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5400540487338342
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6522255, 13.907231 ,  1.2962373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2726120617667214}
episode index:407
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       ,  9.       ,  5.4004054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292885}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5395417424583125
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.875645, 14.954982,  2.568371], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8761848570943043}
episode index:408
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.84390353,  9.38838112]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([4.8922367, 1.3523583, 6.2178125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5403897696692118
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.272623, 14.912606,  2.348486], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7295862958748378}
episode index:409
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       ,  9.       ,  4.6688366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5402311117968618
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.3105   , 13.093415 ,  3.8654115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9317028005413228}
episode index:410
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.39793423, 14.41743063]), 'previousTarget': array([13.39793423, 14.41743063]), 'currentState': array([ 4.      , 11.      ,  5.501145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5398626271406023
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.641868  , 14.083517  ,  0.22208375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6384335403922876}
episode index:411
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.88171698, 12.08736084]), 'previousTarget': array([12.88171698, 12.08736084]), 'currentState': array([7.       , 4.       , 3.5720727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5404026273522685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.057694 , 16.87981  ,  1.0275235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1027664317763244}
episode index:412
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 23.       ,  1.8079101], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5399715605011989
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.623993, 15.262028,  4.571087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6449959090334096}
episode index:413
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52786405, 13.94427191]), 'previousTarget': array([15.52786405, 13.94427191]), 'currentState': array([20.       ,  5.       ,  1.4385343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5405272889681166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.12209 , 15.870398,  3.661236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.420097361481455}
episode index:414
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91227901, 15.6783628 ]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 0.        , 17.        ,  0.07314253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5414707541703789
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.150644 , 13.3253355,  5.7218695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4949183704348386}
episode index:415
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13205979, 15.62267585]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([17.206757 , 25.40509  ,  4.7878246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5423431371531636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.36024  , 13.853514 ,  4.1825004], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.778956491976003}
episode index:416
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17821552, 21.13681661]), 'previousTarget': array([10.17821552, 21.13681661]), 'currentState': array([ 4.       , 29.       ,  4.9744062], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5417604926787252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.421257 , 16.983624 ,  3.4564912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.535190645311729}
episode index:417
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.85258  ,  7.9945593,  2.128209 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.2462575825787825}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5427857044187282
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.849636  , 13.376168  ,  0.46686107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4612970502433837}
episode index:418
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.92893219,  8.07106781]), 'previousTarget': array([21.92893219,  8.07106781]), 'currentState': array([29.       ,  1.       ,  2.5357761], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5429934321676172
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.239983 , 13.013291 ,  1.3548111], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0011503879331194}
episode index:419
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.19131191, 16.75304952]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([25.      , 23.      ,  4.855534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5427139004800482
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.976616 , 14.261238 ,  1.6769276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7391318985786326}
episode index:420
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.       , 29.       ,  3.2358599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5425767923322907
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.838577 , 15.023394 ,  3.9740868], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.16310904195416015}
episode index:421
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 13.       ,  5.9341683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5436135771845839
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.838358, 13.787682,  0.646097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4739606384065975}
episode index:422
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.57258043, 14.79900301]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([ 1.5897317, 14.213571 ,  6.132937 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.544382211311261
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.501511 , 13.479146 ,  6.0970626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.135056743885946}
episode index:423
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 1.       , 15.       ,  1.6394522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5451679160558074
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.239223  , 15.546212  ,  0.08900755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9365522556430673}
episode index:424
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.90104096, 16.96857764]), 'previousTarget': array([11.39940073, 18.27327206]), 'currentState': array([ 5.6070623, 23.809475 ,  5.8329215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5460782629542807
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.009766, 16.371439,  5.930898], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6915699789179086}
episode index:425
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.31756858, 11.96138938]), 'previousTarget': array([20.31756858, 11.96138938]), 'currentState': array([29.       ,  7.       ,  6.0498624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5452858080218406
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.50633  , 13.688652 ,  3.9983416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9971642449677314}
episode index:426
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.112097 , 21.3377   ,  5.1450715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.4352021690731585}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5446688873837109
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.429088 , 15.327171 ,  1.3422937], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6046204294934612}
episode index:427
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.32050294, 15.45299804]), 'previousTarget': array([14.32050294, 15.45299804]), 'currentState': array([ 6.      , 21.      ,  4.231368], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.54538568384098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.055022  , 13.44857   ,  0.04524868], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.81656743120912}
episode index:428
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.35636161, 16.52590681]), 'previousTarget': array([16.35636161, 16.52590681]), 'currentState': array([23.       , 24.       ,  0.3269306], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5447780430238056
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.026943 , 15.6734705,  4.1419697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2280777094600064}
episode index:429
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 17.       ,  6.1472564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5444811659003391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.146248, 16.779146,  3.422546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9733864019427367}
episode index:430
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.54401549, 20.94313758]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([13.779018 , 30.913834 ,  6.0735836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5450965874834502
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.099499 , 16.869215 ,  3.8696203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1686083690112454}
episode index:431
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 13.       ,  3.2692103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.544902433270336
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.206437 , 13.729364 ,  0.9030724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.287296050289509}
episode index:432
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.75304952, 22.19131191]), 'previousTarget': array([20.75304952, 22.19131191]), 'currentState': array([27.       , 30.       ,  2.7819176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.5441303686350621
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.104263 , 15.819962 ,  5.8531103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3754033547258975}
episode index:433
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.      , 20.      ,  3.086407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5448188774478475
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.591512 , 16.060211 ,  0.8540845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7629200114022654}
episode index:434
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.14495755,  8.57492926]), 'previousTarget': array([11.14495755,  8.57492926]), 'currentState': array([6.       , 0.       , 3.4438887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.5440222231470223
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.4016    , 14.84677   ,  0.26481247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4099509140401927}
episode index:435
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.24695048, 12.80868809]), 'previousTarget': array([13.24695048, 12.80868809]), 'currentState': array([7.      , 5.      , 4.540072], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5447078217026114
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.192127 , 14.152746 ,  1.1155152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9965577780734973}
episode index:436
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.86318339, 10.17821552]), 'previousTarget': array([ 8.86318339, 10.17821552]), 'currentState': array([1.       , 4.       , 4.8817015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5445600725833181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.702001  , 14.118661  ,  0.03549248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9303559499457958}
episode index:437
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.11263345, 19.9926328 ]), 'previousTarget': array([20.49208627, 21.40743398]), 'currentState': array([25.470676 , 27.711136 ,  3.9121506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5444928968866068
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.344159 , 16.458061 ,  5.7151566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.598771330690659}
episode index:438
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.24275371, 19.6284586 ]), 'previousTarget': array([19.24275371, 19.6284586 ]), 'currentState': array([26.       , 27.       ,  1.3801291], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5446446607650721
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504314 , 15.831823 ,  5.1324487], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9683151736779532}
episode index:439
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.4019482, 7.1893624]), 'previousTarget': array([7.07106781, 7.07106781]), 'currentState': array([ 1.9487686 , -0.44977847,  0.19694203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5454011979519674
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.983955 , 14.203763 ,  1.3550122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1377727452116986}
episode index:440
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.11628302, 14.52057184]), 'previousTarget': array([18.11628302, 14.52057184]), 'currentState': array([28.      , 13.      ,  5.807651], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5451590580514185
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.845554, 16.349201,  4.118861], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2861353306420877}
episode index:441
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.8386991 , 14.78885438]), 'previousTarget': array([13.8386991 , 14.78885438]), 'currentState': array([ 4.      , 13.      ,  5.588475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5453363574560335
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.689142 , 14.520665 ,  1.3213596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8394515410138464}
episode index:442
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.      , 12.      ,  5.559456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5461882950112749
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.682758, 13.669688,  6.2402  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.145088581110673}
episode index:443
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.07106781, 16.92893219]), 'previousTarget': array([13.07106781, 16.92893219]), 'currentState': array([ 6.       , 24.       ,  2.9278574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.546692472603153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.657272 , 16.25746  ,  5.1815467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3033291975549863}
episode index:444
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       ,  7.       ,  0.2754624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5468651308554857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.115182 , 13.915392 ,  2.0946622], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0907069075764821}
episode index:445
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.9769976 , 15.59103606]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([14.588104, 25.583471,  3.880592], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5473311715647865
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.594321, 13.272387,  5.846058], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.227236174079304}
episode index:446
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.94087889, 15.67656293]), 'previousTarget': array([17.56338512, 16.63124508]), 'currentState': array([24.05978 , 21.514666,  3.600252], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5480502199791136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.707362, 16.466784,  5.449254], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9550873258343517}
episode index:447
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.98584335, 8.3469222 ]), 'previousTarget': array([8.54930538, 7.55689083]), 'currentState': array([3.967155  , 0.36097264, 0.08324026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5476117355741558
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.646399, 13.858056,  3.008462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3121993068735813}
episode index:448
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.73462344, 16.80768079]), 'previousTarget': array([13.73462344, 16.80768079]), 'currentState': array([ 8.        , 25.        ,  0.08959068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5484472210059014
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.040455 , 16.192312 ,  5.1738048], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5304689954860093}
episode index:449
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42337349,  9.6623494 ]), 'previousTarget': array([16.42337349,  9.6623494 ]), 'currentState': array([19.      ,  0.      ,  4.811622], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5486564751860765
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.180756 , 15.163882 ,  0.8588802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8354751675876065}
episode index:450
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13940614, 13.89352217]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([8.       , 6.       , 3.9242532], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.549199618885539
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.079605  , 16.172306  ,  0.18544358], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.490445660679878}
episode index:451
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72672794, 11.39940073]), 'previousTarget': array([11.72672794, 11.39940073]), 'currentState': array([5.      , 4.      , 4.206186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5496047156792755
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.596191 , 13.563152 ,  2.4251447], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.147640165473651}
episode index:452
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.      , 14.      ,  4.381585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5499443249051481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4051285 , 16.179888  ,  0.57541513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.321365687491025}
episode index:453
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80226425, 12.29349187]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([22.344849 ,  3.9700456,  1.4617277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5496894504599645
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.685064, 13.144526,  0.853517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8820122214266068}
episode index:454
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.60059927, 18.27327206]), 'previousTarget': array([18.60059927, 18.27327206]), 'currentState': array([26.        , 25.        ,  0.29820877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5497206938445632
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.98855 , 15.075657,  3.862331], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.0765183858495409}
episode index:455
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.1914503 , 12.93919299]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.       , 9.       , 4.0881977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5496911928168496
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.122398 , 15.6138   ,  5.9062996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2792687815437092}
episode index:456
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 15.00496281]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([25.      , 16.      ,  4.947524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5497600663864367
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.8805   , 14.244033 ,  2.8595176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0267633190018253}
episode index:457
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.289766 , 13.03687  ,  2.1723442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0161082785707274}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5507212889489118
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.157928 , 14.685791 ,  2.2407289], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.199802325418871}
episode index:458
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.351369  ,  8.474381  ,  0.09885025], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.730652419467981}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.5501054402081013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.191111  , 13.580829  ,  0.31190857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4319813332999844}
episode index:459
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.701425  , 15.57464375]), 'previousTarget': array([12.701425  , 15.57464375]), 'currentState': array([ 3.       , 18.       ,  3.8240247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5496587157366651
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.35004  , 15.611414 ,  4.8284874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.482037854942115}
episode index:460
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.82352941, 11.70588235]), 'previousTarget': array([ 8.82352941, 11.70588235]), 'currentState': array([0.      , 7.      , 2.695879], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5493443410228727
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.851636  , 13.645556  ,  0.24110582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3625451283660395}
episode index:461
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.25278872, 19.38476052]), 'previousTarget': array([16.25278872, 19.38476052]), 'currentState': array([19.       , 29.       ,  0.5829093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5484511714642877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1828785, 13.361316 ,  2.0445755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8311126947704246}
episode index:462
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 18.30790021]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 2.       , 24.       ,  0.6374959], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5485998517439378
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.807981, 16.285563,  5.317111], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.518388464616091}
episode index:463
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.     , 15.     ,  5.46281], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5494877744987999
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.771605, 14.768151,  0.934348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.32545094028248606}
episode index:464
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.80451099, 8.32793492]), 'previousTarget': array([8.80451099, 8.32793492]), 'currentState': array([2.      , 1.      , 3.478498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.549788766566263
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.933901, 13.346492,  6.141272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6548288195929566}
episode index:465
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3177872, 18.598156 ]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([27.       , 25.       ,  2.4282286], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.5488562377511287
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.848422 , 16.502506 ,  4.1190524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7254984860509817}
episode index:466
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897, 20.65196555]), 'previousTarget': array([20.21719897, 20.65196555]), 'currentState': array([27.       , 28.       ,  3.9571202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5495981692639929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.552439 , 15.233652 ,  2.9236436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5699233547206664}
episode index:467
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.45069462,  7.55689083]), 'previousTarget': array([21.45069462,  7.55689083]), 'currentState': array([28.       ,  0.       ,  3.5294929], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5491903703930908
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.127823 , 14.844962 ,  1.3979865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1384292616555178}
episode index:468
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.24097426, 19.48341683]), 'previousTarget': array([12.24097426, 19.48341683]), 'currentState': array([ 7.        , 28.        ,  0.97196007], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.548608411538733
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.37      , 16.621414  ,  0.05915755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.73950685601448}
episode index:469
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 20.       ,  1.5032597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5491297006283179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.301679, 14.885316,  5.566056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7076758730293459}
episode index:470
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 14.28476691]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([11.       ,  5.       ,  3.5305538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5488145306757133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.040935 , 13.39872  ,  0.7893259], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5302244379344945}
episode index:471
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.993854  , 12.156679  ,  0.12426172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.645168800468024}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5497075062463156
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.732799  , 13.135281  ,  0.35610104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.883766176435203}
episode index:472
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.09464254, 16.04106794]), 'previousTarget': array([15.09464254, 16.04106794]), 'currentState': array([16.        , 26.        ,  0.05992019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.548964518403488
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.343287 , 14.667473 ,  4.2676387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3838327109946522}
episode index:473
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96784564, 12.82300799]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([14.820161 ,  2.8240986,  1.5127821], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5492608993475082
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.695379 , 14.8075695,  2.5309358], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.318736053352711}
episode index:474
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.46455784, 16.14803963]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([27.956976 , 19.293512 ,  4.3660398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5490658442915007
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.802398 , 13.250137 ,  2.4633439], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.925061422617291}
episode index:475
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.32274222, 11.83542625]), 'previousTarget': array([15.6783628 ,  9.91227901]), 'currentState': array([16.33734  ,  1.8870298,  1.8024849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5489009720169087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.071263, 13.68301 ,  5.062617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3354844465072366}
episode index:476
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.24695048, 12.80868809]), 'previousTarget': array([13.24695048, 12.80868809]), 'currentState': array([7.      , 5.      , 4.912966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.5480094165915825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.497716 , 13.459421 ,  5.9256806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.620392679504784}
episode index:477
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.52576695, 15.41495392]), 'previousTarget': array([14.52576695, 15.41495392]), 'currentState': array([ 7.      , 22.      ,  4.737659], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5484418640196411
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.520728, 14.856823,  6.034957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.486184707034273}
episode index:478
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.31055268, 22.1768175 ]), 'previousTarget': array([ 7.31055268, 22.1768175 ]), 'currentState': array([ 0.      , 29.      ,  4.023665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5490924203674943
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.049418 , 16.59406  ,  6.2772818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.519086844774383}
episode index:479
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 13.       ,  1.4920449], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5493848300872892
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.7815485, 16.494843 ,  5.706216 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5107201081670494}
episode index:480
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 22.       ,  6.0031953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5501610460214693
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504162, 16.515772,  4.396638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5948102819576429}
episode index:481
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 17.       ,  1.8650323], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5509148970618473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.391947 , 16.001198 ,  3.9678965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8942629654999614}
episode index:482
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.9329442 , 13.34488277]), 'previousTarget': array([13.47213595, 11.94427191]), 'currentState': array([8.514407 , 4.9401546, 2.3774397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5508515960796287
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.604368  , 16.360321  ,  0.10469454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.416685518611468}
episode index:483
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.51021464, 20.66019226]), 'previousTarget': array([18.52786405, 22.05572809]), 'currentState': array([23.780582 , 29.158617 ,  5.1566334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5509509853572869
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.104354, 14.238929,  3.057044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7681921092559756}
episode index:484
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42535625, 12.701425  ]), 'previousTarget': array([14.42535625, 12.701425  ]), 'currentState': array([12.      ,  3.      ,  3.914197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5514678505205483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.9027    , 13.982636  ,  0.77475756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.022005771421282}
episode index:485
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.08736084, 12.88171698]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([4.      , 7.      , 5.683328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5511411903926413
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.577551 , 13.974007 ,  1.2674332], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1773815765220663}
episode index:486
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.      , 19.      ,  1.243501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5516390817545531
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.837332 , 15.778847 ,  4.6877413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.399428331241869}
episode index:487
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.26042701, 17.6676221 ]), 'previousTarget': array([18.26042701, 17.6676221 ]), 'currentState': array([26.      , 24.      ,  5.584893], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5516412219090773
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.318634 , 15.641437 ,  2.9231935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4663685060836318}
episode index:488
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96116135, 14.80580676]), 'previousTarget': array([14.96116135, 14.80580676]), 'currentState': array([13.      ,  5.      ,  6.083598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5516547697958641
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.683813 , 16.598385 ,  3.3698077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.070551163411412}
episode index:489
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.71412881, 19.56630132]), 'previousTarget': array([15.37621714, 21.01947422]), 'currentState': array([17.259258 , 29.446209 ,  5.7327075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5512182384307683
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.481665 , 13.390483 ,  3.0056758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6800434903395625}
episode index:490
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.0676113, 17.7181909]), 'previousTarget': array([13.36336397, 19.58258088]), 'currentState': array([10.823006 , 27.177183 ,  4.9469023], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5508486078792796
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.732198, 15.910301,  4.649366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5607596887556647}
episode index:491
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.00363  , 16.120451 ,  3.8764002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.295635488583353}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5517411920096063
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.519711 , 14.779562 ,  3.9578755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5645280857439309}
episode index:492
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.14034336, 13.37879396]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([25.111742 ,  7.340847 ,  3.1782956], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5511712387996719
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.945557 , 14.4481   ,  1.0300808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1901446587530444}
episode index:493
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80941823, 18.92040615]), 'previousTarget': array([16.80941823, 18.92040615]), 'currentState': array([21.        , 28.        ,  0.49171078], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5506800914956942
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.825546, 13.642096,  2.056768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3690648480657424}
episode index:494
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80768079, 16.26537656]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([25.       , 22.       ,  1.3962849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5510322132016784
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.523308, 14.304308,  4.020904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6746504411634864}
episode index:495
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 14.       ,  1.7388186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5517630298030533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.261376, 14.64554 ,  3.558844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4404081468212533}
episode index:496
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.13547393, 16.52963297]), 'previousTarget': array([ 9.28476691, 17.28609324]), 'currentState': array([ 1.8373458, 20.20996  ,  5.9338117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.550953931967642
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.555748  , 13.006006  ,  0.69239056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0428830029239404}
episode index:497
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28476691, 14.71390676]), 'previousTarget': array([14.28476691, 14.71390676]), 'currentState': array([ 5.       , 11.       ,  6.2683697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5517381211592752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.425564 , 13.43244  ,  1.5330164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.669497499738083}
episode index:498
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 12.       ,  5.7854023], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499368}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5513808589534536
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.314044  , 14.018793  ,  0.38244838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9506959288766164}
episode index:499
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86059386, 13.89352217]), 'previousTarget': array([15.86059386, 13.89352217]), 'currentState': array([22.      ,  6.      ,  4.534183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5509536060373444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.626576 , 13.995424 ,  1.5890753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7016065103681308}
episode index:500
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33480989, 20.02214842]), 'previousTarget': array([15.33480989, 20.02214842]), 'currentState': array([16.      , 30.      ,  5.543545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5516054372089245
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.295965, 16.267088,  4.162644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8124672754158218}
episode index:501
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74758757, 12.50372316]), 'previousTarget': array([15., 11.]), 'currentState': array([13.741562 ,  2.5544562,  2.2017589], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5523633254772473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.378892 , 13.840343 ,  1.7753202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9931870412834458}
episode index:502
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.93018278, 15.06757842]), 'previousTarget': array([13.47409319, 16.35636161]), 'currentState': array([ 7.7448487, 22.022503 ,  5.9038806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5531749212715272
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.486011 , 16.886759 ,  5.1049023], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4190954582233104}
episode index:503
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 24.       ,  3.6698706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5533270020452012
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.284277, 15.750877,  3.671964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0373409791749815}
episode index:504
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.832573 , 18.181534 ,  4.8752613], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.8496619955093103}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5541917010510523
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.156879 , 16.208002 ,  4.7784243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.203715613310805}
episode index:505
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84615385, 12.23076923]), 'previousTarget': array([13.84615385, 12.23076923]), 'currentState': array([10.      ,  3.      ,  4.694455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5539808937639235
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.864056, 16.445986,  5.654462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8388160648432752}
episode index:506
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.25900177, 10.804711  ]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([22.      ,  2.      ,  4.892932], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5535029454161625
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.014889 , 16.060932 ,  1.8315434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4681880836703392}
episode index:507
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.595857, 24.090824,  4.45851 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.110330865491209}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5540727096838156
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.664297 , 15.29307  ,  4.9851193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6899037785628082}
episode index:508
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.486312 , 17.067095 ,  4.3449087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.129966284990894}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5549291483681302
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.767782 , 15.2006235,  4.292619 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.248443219555598}
episode index:509
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.      ,  8.      ,  5.012846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5544460411568882
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.963484 , 15.36848  ,  1.6584082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9977602419365044}
episode index:510
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.31020426, 19.0309806 ]), 'previousTarget': array([15.37621714, 21.01947422]), 'currentState': array([16.077486 , 29.001501 ,  4.8803864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5551850221877103
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.003624, 15.76548 ,  5.112892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1381012551876353}
episode index:511
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80941823, 11.07959385]), 'previousTarget': array([16.80941823, 11.07959385]), 'currentState': array([21.      ,  2.      ,  4.651245], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.5545419814325273
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.74841  , 16.638014 ,  2.8005707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3958347853887103}
episode index:512
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36613715, 19.54057759]), 'previousTarget': array([20.36613715, 19.54057759]), 'currentState': array([28.      , 26.      ,  3.739912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5551544645345476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.104551, 15.550249,  4.618112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.05100067491538}
episode index:513
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.105185, 21.002768,  4.772017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.745478465135869}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5557309301115133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.311557 , 14.46213  ,  5.5067873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.873646720531613}
episode index:514
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.        , 16.        ,  0.58189195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5558032320848919
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.053873 , 16.645449 ,  3.3146539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9540086501976244}
episode index:515
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 10.      ,  5.986119], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5555592122814791
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.36939  , 15.505132 ,  1.4067266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.459584997108987}
episode index:516
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.39191359, 19.55399694]), 'previousTarget': array([18.30790021, 19.77807808]), 'currentState': array([22.041876, 28.407124,  2.736667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5561649890715903
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.784198, 16.963425,  4.467518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9752486098411377}
episode index:517
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.34457408, 21.07847167]), 'previousTarget': array([ 9.66528823, 22.75958076]), 'currentState': array([ 4.264155, 29.01752 ,  4.287468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5560659946694966
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.717574 , 16.267612 ,  5.7197933], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.803179824821106}
episode index:518
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 11.       ,  5.3544536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5563914558280484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.085551 , 14.770516 ,  1.7230239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9281537012147896}
episode index:519
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  7.       ,  0.4201972], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5570782361966168
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.956789 , 13.479655 ,  2.1496408], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.520958677893891}
episode index:520
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 12.07106781]), 'previousTarget': array([17.92893219, 12.07106781]), 'currentState': array([25.       ,  5.       ,  3.7057717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5570698009585472
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.233919 , 13.953461 ,  1.8952814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.052872648611033}
episode index:521
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 23.       ,  3.0367153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5571851655945315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.679514 , 14.475418 ,  0.9557459], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4208693244408361}
episode index:522
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.49258221, 17.18491021]), 'previousTarget': array([19., 18.]), 'currentState': array([25.012514 , 23.776619 ,  3.0168064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5578664888868624
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.10666  , 16.039654 ,  3.2790203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0451106688943208}
episode index:523
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.363617, 19.850107,  3.738998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.395389220293523}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5586167246903225
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.614395 , 15.4196205,  3.807053 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.447750737978225}
episode index:524
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.53083962, 15.29359841]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([24.281582 , 20.133486 ,  4.6098914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5582025329509392
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.854696 , 14.168419 ,  1.9284295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0325908147209333}
episode index:525
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 10.       ,  3.5905395], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.558326720901396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.882103 , 14.206327 ,  1.6494865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.042603170196441}
episode index:526
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.16049759, 11.66015515]), 'previousTarget': array([11.32050294, 12.54700196]), 'currentState': array([3.6155586, 5.0970845, 5.8081765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5589324026890574
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.573347 , 13.897558 ,  1.8024051], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2426201281335432}
episode index:527
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([7.      , 9.      , 4.080936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5590908860212462
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.86287  , 14.916064 ,  1.9588077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.866943030623052}
episode index:528
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51658317, 12.24097426]), 'previousTarget': array([10.51658317, 12.24097426]), 'currentState': array([2.       , 7.       , 2.6525722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5591437138018062
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.744247 , 15.339342 ,  6.0653653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.42492640104490487}
episode index:529
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05572809, 17.52786405]), 'previousTarget': array([20.05572809, 17.52786405]), 'currentState': array([29.       , 22.       ,  4.8749986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5596012362088576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.795304, 16.93087 ,  3.619692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2758626731850833}
episode index:530
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 25.       ,  6.1691656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5601032463550473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.40639   , 13.756704  ,  0.03110617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.021231393994081}
episode index:531
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3177872, 11.401844 ]), 'previousTarget': array([19.3177872, 11.401844 ]), 'currentState': array([27.       ,  5.       ,  3.3829303], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5606349003908159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.617793 , 15.144785 ,  3.3521547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6345320865263985}
episode index:532
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64398987, 12.86393924]), 'previousTarget': array([14.64398987, 12.86393924]), 'currentState': array([13.       ,  3.       ,  1.5906029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5612628616551083
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.931067  , 13.717834  ,  0.78583056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3179659871051315}
episode index:533
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 20.       ,  5.8796015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5618717043705784
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4299   , 16.330313 ,  4.2941628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.447323658863654}
episode index:534
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.417   , 12.222338,  2.1144  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.682023335228679}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5626719441754933
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.382553 , 13.934038 ,  2.0565543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7457742452902392}
episode index:535
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.74421706, 13.26366525]), 'previousTarget': array([ 9.94427191, 12.47213595]), 'currentState': array([2.9205945, 8.557958 , 5.9040008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5625832597972252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.891026 , 13.5866   ,  3.0310616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3608634663457684}
episode index:536
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64398987, 12.86393924]), 'previousTarget': array([14.64398987, 12.86393924]), 'currentState': array([13.       ,  3.       ,  2.1564047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5621709372678338
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.689733 , 13.044511 ,  1.3317364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9799504385396263}
episode index:537
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 24.       ,  6.1204443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.562823997323997
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.399015 , 16.613182 ,  3.4422355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2727753932694696}
episode index:538
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.11828302, 12.08736084]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([23.        ,  4.        ,  0.27295226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5624726805951107
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.989384 , 14.3568   ,  1.6265091], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0907782364103853}
episode index:539
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 12.       ,  4.4006352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5629155656116736
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.414098 , 13.41433  ,  0.6213124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6388495414579172}
episode index:540
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42618187, 17.74494057]), 'previousTarget': array([17.45299804, 18.67949706]), 'currentState': array([21.036688 , 26.61868  ,  3.6088562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5631494538191734
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.460098 , 16.847948 ,  2.8173354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3551643326088376}
episode index:541
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.      , 12.      ,  4.941099], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5636813879838887
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.478704 , 15.912643 ,  1.8170826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0305707098286037}
episode index:542
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91363664, 21.93595004]), 'previousTarget': array([ 9.91363664, 21.93595004]), 'currentState': array([ 4.       , 30.       ,  1.0929335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5634926966016313
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.58689 , 16.159042,  5.132096], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9650945865732086}
episode index:543
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       ,  8.       ,  2.8365304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042628}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5637115603854709
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.152477  , 16.816969  ,  0.93524164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.004911677798441}
episode index:544
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.57662651,  9.6623494 ]), 'previousTarget': array([13.57662651,  9.6623494 ]), 'currentState': array([11.       ,  0.       ,  4.8024707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.5629602033054862
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.287111 , 14.099146 ,  1.4250919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5710485612947338}
episode index:545
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 13.       ,  3.7843566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.5621180969446732
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.495008  , 14.586808  ,  0.38596678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.551056988318952}
episode index:546
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.366797 ,  8.897118 ,  1.8757437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.6467231845692725}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5619950998014475
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.241876 , 13.371073 ,  0.5671847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3967488726810955}
episode index:547
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.67206508,  8.80451099]), 'previousTarget': array([21.67206508,  8.80451099]), 'currentState': array([29.      ,  2.      ,  6.018038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5623747495204323
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.154651 , 16.276995 ,  2.5065405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2863251192685377}
episode index:548
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 16.80941823]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 2.      , 21.      ,  3.883326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5619594528125443
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.289437, 16.595901,  5.719879], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7469404461985318}
episode index:549
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.5000000e+01, 2.4000000e+01, 2.3920234e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.561415367602431
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.334501 , 16.360577 ,  2.8358245], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4010923818204537}
episode index:550
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.52576695, 15.41495392]), 'currentState': array([ 8.978689, 22.291185,  6.210377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.456086297042418}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5620711377055626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.860948 , 16.642067 ,  4.3639803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.647944008156974}
episode index:551
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91363664, 21.93595004]), 'previousTarget': array([ 9.91363664, 21.93595004]), 'currentState': array([ 4.      , 30.      ,  4.510578], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.5612746188760145
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.182508, 14.047916,  5.548138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2548926494847492}
episode index:552
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.56338512, 16.63124508]), 'previousTarget': array([17.56338512, 16.63124508]), 'currentState': array([26.      , 22.      ,  1.618372], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5613319277865487
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.897127 , 13.223552 ,  3.3214428], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9901270085642115}
episode index:553
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.54700196, 15.67949706]), 'previousTarget': array([14.54700196, 15.67949706]), 'currentState': array([ 9.       , 24.       ,  4.3383436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5612119038367539
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.74828  , 15.968576 ,  4.1060176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0007514563205928}
episode index:554
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.71390676,  9.28476691]), 'previousTarget': array([12.71390676,  9.28476691]), 'currentState': array([9.       , 0.       , 2.9337792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.5605018534424803
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.139479 , 13.682617 ,  1.8688467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5735292850373148}
episode index:555
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.26537656, 13.19231921]), 'previousTarget': array([16.26537656, 13.19231921]), 'currentState': array([22.       ,  5.       ,  4.9739594], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5601860157184712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.536129, 14.08888 ,  2.29904 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7242559906286758}
episode index:556
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.19548901, 21.67206508]), 'previousTarget': array([21.19548901, 21.67206508]), 'currentState': array([28.      , 29.      ,  5.491195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5604180858623681
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.049454 , 13.439149 ,  3.2635133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8808533409022794}
episode index:557
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.7124705, 19.025413 ]), 'previousTarget': array([14.7124705, 19.025413 ]), 'currentState': array([14.      , 29.      ,  3.734992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5599611699843501
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.112903 , 16.237057 ,  5.1221666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2421981987742357}
episode index:558
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.54700196, 14.32050294]), 'currentState': array([8.751406 , 7.98449  , 1.4534458], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.394802405630307}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5604226114290959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.242851 , 16.64455  ,  6.1580443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4066818382092108}
episode index:559
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36875492, 12.43661488]), 'previousTarget': array([13.36875492, 12.43661488]), 'currentState': array([8.       , 4.       , 3.4828455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5606407042569196
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.648167, 14.122406,  1.254665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6117148032967639}
episode index:560
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 15.00496281]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([25.      , 16.      ,  0.672599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5607983903961558
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.967564 , 14.986938 ,  3.5429554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.967606982644137}
episode index:561
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96804777, 14.9712038 ]), 'previousTarget': array([13.47409319, 13.64363839]), 'currentState': array([7.539639 , 8.276524 , 1.3234646], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5609212111093356
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.214443  , 15.851458  ,  0.35596186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1584815548170553}
episode index:562
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.      , 21.      ,  4.082591], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5610894846607829
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.14054 , 16.44305 ,  5.565402], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6796028377163448}
episode index:563
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       ,  9.       ,  3.7691483], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5608881260244408
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.406026 , 14.667421 ,  2.1052628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5248481542290292}
episode index:564
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.09709272, 19.2994916 ]), 'previousTarget': array([13.94085849, 18.44220991]), 'currentState': array([ 9.049882 , 28.443893 ,  2.8906944], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5603650784558094
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.390868, 15.800153,  4.316918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7970947835073892}
episode index:565
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.28609324,  9.28476691]), 'previousTarget': array([17.28609324,  9.28476691]), 'currentState': array([21.       ,  0.       ,  3.7828157], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5602581384406917
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.548706, 14.028372,  1.288569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.115858267110585}
episode index:566
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.80580676, 15.03883865]), 'previousTarget': array([14.80580676, 15.03883865]), 'currentState': array([ 5.      , 17.      ,  3.353863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5597772309820078
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.932411, 15.792469,  5.742748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.088592870128223}
episode index:567
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.70588235,  8.82352941]), 'previousTarget': array([11.70588235,  8.82352941]), 'currentState': array([7.       , 0.       , 3.9929214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.560042681679702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.486425 , 13.063738 ,  0.9342922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.441018539060962}
episode index:568
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.36230891, 12.39974003]), 'previousTarget': array([18., 11.]), 'currentState': array([24.0866   ,  4.998124 ,  1.0359831], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5598609014794633
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.581799 , 15.9122   ,  3.0472295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8259779452186224}
episode index:569
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.        , 19.        ,  0.04806155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5598483095069772
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.067637 , 16.69311  ,  3.0728679], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6944599861042016}
episode index:570
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.      , 22.      ,  5.955195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5602577070098435
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.767389, 14.15476 ,  3.663567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.141628828345277}
episode index:571
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.81367491, 20.75358483]), 'previousTarget': array([19.45299804, 21.67949706]), 'currentState': array([26.23047  , 28.423313 ,  5.7972293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5603904478823155
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.73712  , 15.761283 ,  2.9175367], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.059668391697579}
episode index:572
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 17.       ,  2.8193614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5605567110109225
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.41118   , 13.427964  ,  0.06799143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2350943983365164}
episode index:573
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.96116135, 15.19419324]), 'currentState': array([12.523908 , 23.057491 ,  3.5855238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.429365299878233}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5605926164170326
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.818244, 15.684222,  4.865451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3655428730234993}
episode index:574
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.10366477, 15.86197056]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.       , 20.       ,  1.9968164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.5598392611496618
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.295183 , 15.828731 ,  4.0117264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8955724104503966}
episode index:575
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 21.      ,  5.080237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5603907919862057
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.904924 , 15.523728 ,  4.1482153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9756083488751506}
episode index:576
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.03883865, 14.80580676]), 'previousTarget': array([15.03883865, 14.80580676]), 'currentState': array([17.       ,  5.       ,  5.5906262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5606144632061066
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.992957, 13.88871 ,  2.318348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1113122952125993}
episode index:577
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.10647783, 14.13940614]), 'currentState': array([22.005417 ,  8.147089 ,  3.0046992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.799910950418873}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5600394043772645
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.325112 , 14.482691 ,  2.3759763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8503423702577348}
episode index:578
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      ,  7.      ,  4.370569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.5593466639710858
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.20341   , 13.556204  ,  0.76111424], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8795587343485467}
episode index:579
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.03454242, 15.1695452 ]), 'previousTarget': array([17.03454242, 15.1695452 ]), 'currentState': array([27.      , 16.      ,  5.304073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5596705009199832
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.27652  , 13.340473 ,  2.2525883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.093688615743958}
episode index:580
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 14.       ,  2.3493164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5595336178832386
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.371181 , 16.012081 ,  3.8923109], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7042437970286437}
episode index:581
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.42507074, 14.14495755]), 'currentState': array([23.587866 , 10.41629  ,  1.6302152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.734569033747631}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.559856020763734
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.36435  , 15.488744 ,  1.9328921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4492479687415762}
episode index:582
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 17.92893219]), 'previousTarget': array([17.92893219, 17.92893219]), 'currentState': array([25.      , 25.      ,  2.050547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.55934634077486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.963095  , 14.508769  ,  0.01570338], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1473798143456082}
episode index:583
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1695452 , 17.03454242]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([16.       , 27.       ,  1.9983132], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5594779146537796
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.812034, 16.020548,  4.222883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.037713494169049}
episode index:584
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.4375  , 16.08073 ,  3.896702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6033851487347346}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5602138498424057
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.981108, 14.709993,  4.236407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.002221669935235}
episode index:585
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 11.       ,  1.8369269], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5608807034261218
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.271669  , 15.512463  ,  0.44187507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8905523080683351}
episode index:586
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.77114535,  9.0618314 ]), 'previousTarget': array([17.77114535,  9.0618314 ]), 'currentState': array([22.        ,  0.        ,  0.49106422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5611853365947116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.847891 , 15.036394 ,  2.8174522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8486715689908413}
episode index:587
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.13940614, 16.10647783]), 'currentState': array([ 9.612907, 22.817406,  5.812735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.493818961086765}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5610640864865646
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.2280035, 16.677326 ,  6.110937 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.846456547640259}
episode index:588
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.04106794, 14.90535746]), 'previousTarget': array([16.04106794, 14.90535746]), 'currentState': array([26.       , 14.       ,  2.9200168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5609432480935554
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.051386 , 14.421187 ,  1.5916998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2001817748236596}
episode index:589
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.      ,  5.      ,  1.043436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5604333196414948
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.126538 , 13.664928 ,  2.3274727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.595415764737872}
episode index:590
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.41178475, 11.40757591]), 'previousTarget': array([ 9.41178475, 11.40757591]), 'currentState': array([1.       , 6.       , 5.1244273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5607117409888998
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.430063 , 14.391964 ,  1.7434527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7447564866964823}
episode index:591
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.13681661, 10.17821552]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([29.      ,  4.      ,  5.050538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5604763059908692
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.567947 , 15.743162 ,  2.5318615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7351510019101448}
episode index:592
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.07106781, 22.92893219]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([ 0.       , 30.       ,  1.7836809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 317
reward sum = 0.04133868785485247
running average episode reward sum: 0.5596008631272334
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.33494 , 15.722402,  5.631977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5178698188491995}
episode index:593
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.80868809, 20.75304952]), 'previousTarget': array([ 7.80868809, 20.75304952]), 'currentState': array([ 0.       , 27.       ,  4.7610993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5596875910336402
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.540683, 14.949726,  0.852596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.462060344003157}
episode index:594
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.04106794, 14.90535746]), 'currentState': array([24.219343, 14.910637,  2.397478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.21977627353011}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5602822627251528
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.772789 , 16.559303 ,  2.7942867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3609759791758833}
episode index:595
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.        , 13.        ,  0.04619798], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5599625435555632
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.665404 , 13.938476 ,  3.0232577], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7052799367742453}
episode index:596
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.85504245, 21.42507074]), 'previousTarget': array([18.85504245, 21.42507074]), 'currentState': array([24.       , 30.       ,  2.3539758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5603406266330366
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.842606 , 15.9928875,  3.5768518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5249220963696943}
episode index:597
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 13.       ,  5.6166916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280517}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5609779836945216
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.973283 , 16.45856  ,  1.4990346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7836894926472477}
episode index:598
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.54057759, 20.36613715]), 'previousTarget': array([19.54057759, 20.36613715]), 'currentState': array([26.        , 28.        ,  0.39997876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5606587043188207
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3185215, 15.390914 ,  4.4721684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7263208490633157}
episode index:599
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.298575  , 15.57464375]), 'previousTarget': array([17.298575  , 15.57464375]), 'currentState': array([27.       , 18.       ,  5.7089825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.560969559968842
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.565294 , 15.395241 ,  3.3884385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6144229357493831}
episode index:600
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 12.       ,  3.4253893], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5609465767449184
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.671082 , 16.109203 ,  2.5072777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2964113867618687}
episode index:601
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 15.       ,  1.3921077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5612190580725147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.375417, 14.167435,  5.375799], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8254960173019055}
episode index:602
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.42262566, 10.93046615]), 'previousTarget': array([13.57662651,  9.6623494 ]), 'currentState': array([9.808558 , 1.6063834, 1.8635616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5613018079588503
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.636787  , 14.049176  ,  0.29669398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.017835571303219}
episode index:603
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.27478111, 14.04675947]), 'previousTarget': array([15., 15.]), 'currentState': array([8.219948 , 6.0881796, 3.8391218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5613842838389397
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.605751 , 13.551442 ,  1.0153645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5701127905545225}
episode index:604
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 20.       ,  3.4789782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.560992649534848
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.688202 , 15.7847595,  4.8658767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0437764931431586}
episode index:605
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80941823, 18.92040615]), 'previousTarget': array([16.80941823, 18.92040615]), 'currentState': array([21.       , 28.       ,  2.2808466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5611380455395237
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.539122 , 13.584549 ,  3.5064166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0910277519866542}
episode index:606
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 13.      ,  5.432407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5606779510584708
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.595432 , 14.565527 ,  2.3635995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7370932250066639}
episode index:607
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07106781, 18.92893219]), 'previousTarget': array([11.07106781, 18.92893219]), 'currentState': array([ 4.       , 26.       ,  2.5698295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5609362612254771
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.074427, 13.840191,  5.409038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.483860910573686}
episode index:608
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.03861062, 20.31756858]), 'previousTarget': array([18.03861062, 20.31756858]), 'currentState': array([23.      , 29.      ,  2.263267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5605218247877254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.1173   , 15.3951235,  4.048281 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9237156051320714}
episode index:609
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 13.       ,  1.7355211], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5607218785094019
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.79548  , 15.554966 ,  5.7950997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.969935720899189}
episode index:610
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.10950813, 17.70192115]), 'previousTarget': array([14.7124705, 19.025413 ]), 'currentState': array([15.514473, 27.693718,  4.814694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5613296419617712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157629 , 16.410133 ,  4.0515876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3200877034328458}
episode index:611
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24859507, 18.25608804]), 'previousTarget': array([14.24859507, 18.25608804]), 'currentState': array([12.       , 28.       ,  2.2365313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.5607331744324594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.503803 , 14.790358 ,  5.9654875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5108125778535613}
episode index:612
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.93306109, 14.81269219]), 'previousTarget': array([14.78885438, 13.8386991 ]), 'currentState': array([11.567768 ,  5.395962 ,  2.3058581], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5613086786299327
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.487388 , 13.111797 ,  1.4088918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9500912904055763}
episode index:613
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.6623494 , 13.57662651]), 'previousTarget': array([ 9.6623494 , 13.57662651]), 'currentState': array([ 0.      , 11.      ,  5.990237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5609020864521138
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.762355 , 14.362873 ,  1.1064923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6800043620920438}
episode index:614
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.41495392, 23.47423305]), 'previousTarget': array([22.41495392, 23.47423305]), 'currentState': array([29.       , 31.       ,  2.1661968], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5609738002418463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.699506, 15.827368,  4.789654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5413703631444986}
episode index:615
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.29411765,  8.82352941]), 'previousTarget': array([18.29411765,  8.82352941]), 'currentState': array([23.       ,  0.       ,  5.1203756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.5602988377515443
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.003215, 13.109321,  2.857581], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8906820926239127}
episode index:616
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.5301154 , 14.90537885]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([ 3.5507708 , 14.262977  ,  0.87163883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5606262994640168
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.307827 , 14.86947  ,  5.3995795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7043732262567377}
episode index:617
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.25842724, 14.85642931]), 'previousTarget': array([15.25842724, 14.85642931]), 'currentState': array([24.      , 10.      ,  5.948138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.5599359327575345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.616325 , 15.588146 ,  3.5241723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8519230810855443}
episode index:618
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([28.        , 15.        ,  0.13376588], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.559464383135821
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.109083, 14.341006,  0.831359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6679609745638271}
episode index:619
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       ,  8.       ,  5.9280834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5599632241513584
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.298435 , 14.062405 ,  0.8729346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9839454498610701}
episode index:620
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        , 22.        ,  0.33335328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5597967934325058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.620152, 16.278622,  5.415442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0639196523094636}
episode index:621
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.08434694, 20.92655211]), 'previousTarget': array([10.9026124 , 22.28424463]), 'currentState': array([ 7.669986 , 29.899479 ,  6.2613096], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5598125561480116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.198006, 16.774393,  3.205194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7854066920720635}
episode index:622
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.869482 , 18.004263 ,  4.5768886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5384401672328116}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.560503065688705
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.59931  , 16.022594 ,  5.0367126], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8982865003262774}
episode index:623
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 15.1695452 ]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.       , 16.       ,  2.4466891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5602093695047864
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.579408, 16.243828,  5.499331], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.313013912704208}
episode index:624
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.96138938, 20.31756858]), 'previousTarget': array([11.96138938, 20.31756858]), 'currentState': array([ 7.       , 29.       ,  0.7369725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5602810442209988
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.231153, 13.325174,  5.150214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.435952047059418}
episode index:625
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.480392 , 22.068678 ,  4.7514277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.491231809349521}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5605097449410923
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.202527 , 16.125076 ,  3.6205277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.143159600249099}
episode index:626
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.83304977, 17.91853042]), 'previousTarget': array([13.74721128, 19.38476052]), 'currentState': array([10.12041 , 27.203804,  4.294149], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5611023376092995
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.637127 , 15.05011  ,  5.8627543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3637939813181392}
episode index:627
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.64763821, 14.63117406]), 'previousTarget': array([13.64763821, 14.63117406]), 'currentState': array([ 4.      , 12.      ,  4.404691], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5614350459026051
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.01677  , 15.12117  ,  1.5078229], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9869277725181527}
episode index:628
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.26537656, 13.19231921]), 'previousTarget': array([16.26537656, 13.19231921]), 'currentState': array([22.       ,  5.       ,  5.1555195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.561595374988585
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.460816 , 13.598018 ,  2.7711997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.50208966536929}
episode index:629
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       ,  9.       ,  4.4595637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 245
reward sum = 0.08523592457219176
running average episode reward sum: 0.5608392488768129
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.501751, 14.888792,  2.972134], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5139272542658193}
episode index:630
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.298575  , 15.57464375]), 'previousTarget': array([17.298575  , 15.57464375]), 'currentState': array([27.       , 18.       ,  1.3534287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.5602841921372249
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.51482  , 15.547981 ,  1.9750971], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.610889007874718}
episode index:631
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.      , 12.      ,  4.660264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5599267438219601
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.601408  , 14.374792  ,  0.06915778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5319740495913678}
episode index:632
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.86559264, 15.060069  ]), 'previousTarget': array([13.57492926, 15.85504245]), 'currentState': array([ 5.735876 , 19.140299 ,  5.2258825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5600573676106848
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.392461  , 14.632552  ,  0.33354774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6489997362459288}
episode index:633
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.79497533, 18.56577087]), 'previousTarget': array([11.09710761, 20.07376011]), 'currentState': array([ 5.1101336, 26.003035 ,  5.131557 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5606441309865464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.318897, 15.311827,  5.313841], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7097784533733338}
episode index:634
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47409319, 13.64363839]), 'previousTarget': array([13.47409319, 13.64363839]), 'currentState': array([6.       , 7.       , 3.2616405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 300
reward sum = 0.04904089407128572
running average episode reward sum: 0.5598384565977035
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.97188  , 16.271273 ,  4.0915933], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.34615531186957}
episode index:635
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 20.       ,  2.1021883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5598538068270736
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.494735 , 16.394127 ,  4.3784285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0516854587533184}
episode index:636
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 20.       ,  5.8860397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5600903836252608
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.735566 , 15.86224  ,  3.2464087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9018773776689502}
episode index:637
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.07106781, 9.07106781]), 'previousTarget': array([9.07106781, 9.07106781]), 'currentState': array([2.      , 2.      , 5.520737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 381
reward sum = 0.02172746913542607
running average episode reward sum: 0.5592465546056843
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.973728 , 14.160805 ,  1.0054092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2854552650614208}
episode index:638
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.        , 18.        ,  0.42593038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5598447292454266
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.711626 , 16.525288 ,  5.4994664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.683126254255511}
episode index:639
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.42173715, 14.87347886]), 'currentState': array([23.06446 , 11.496325,  3.190246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.79268269607688}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5598335397890468
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.376978 , 13.893254 ,  2.4893835], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9644558429698196}
episode index:640
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.39373 , 17.808405,  4.18439 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.405058754635793}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.560489181692652
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.575064 , 14.2526045,  4.1042037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7433948871987373}
episode index:641
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.52738198,  9.90987035]), 'previousTarget': array([10.17821552,  8.86318339]), 'currentState': array([5.891715  , 1.6491647 , 0.09145853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5602037416073419
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.709857 , 13.722136 ,  1.7117385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.815876251938413}
episode index:642
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2537806 , 16.41333708]), 'previousTarget': array([15.75140493, 18.25608804]), 'currentState': array([17.021128 , 26.255922 ,  3.5946512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.560035529956734
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0908375, 16.797318 ,  0.571993 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0141813978904786}
episode index:643
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84288535, 21.17127813]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([12.       , 31.       ,  1.8552014], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5599190017584691
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.195532, 13.40891 ,  3.125008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.990192057211153}
episode index:644
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42507074, 15.85504245]), 'previousTarget': array([16.42507074, 15.85504245]), 'currentState': array([25.       , 21.       ,  1.1022564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5598495724804309
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.235082 , 13.705295 ,  3.8862476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3158744021099653}
episode index:645
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 13.       ,  4.7218037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5603970456615504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.924916  , 13.542861  ,  0.26400384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7258980883306565}
episode index:646
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57464375, 21.298575  ]), 'previousTarget': array([16.57464375, 21.298575  ]), 'currentState': array([19.      , 31.      ,  1.901535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5603681245337379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.255802 , 16.892916 ,  3.8077214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.910121641911975}
episode index:647
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.133527 , 11.647761 ,  1.3805498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.354896929497389}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5610311366872353
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.511729 , 13.611677 ,  1.0655158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4796307297272964}
episode index:648
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.98274993, 21.45520022]), 'previousTarget': array([12.98274993, 21.45520022]), 'currentState': array([10.       , 31.       ,  1.4818053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5611371343675372
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.96604  , 14.205122 ,  3.4600558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.120646830478981}
episode index:649
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       ,  7.       ,  2.9023654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5606620936424939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.019861 , 15.19229  ,  6.2712946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9894534788918403}
episode index:650
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 14.       ,  3.3148136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5608923411595903
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.440075, 13.08187 ,  2.503769], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3985491803958223}
episode index:651
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.67206508, 21.19548901]), 'previousTarget': array([21.67206508, 21.19548901]), 'currentState': array([29.       , 28.       ,  0.5119654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5607466178988698
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.174981 , 13.326962 ,  2.3738475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8653982765916577}
episode index:652
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.89633523, 14.13802944]), 'previousTarget': array([16.89633523, 14.13802944]), 'currentState': array([26.      , 10.      ,  6.229285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5605732283060137
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.467699, 13.620392,  2.601285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4567295773815732}
episode index:653
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 19.       ,  1.9973179], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5608585476730252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.399786 , 16.894207 ,  6.0534773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9359362086102863}
episode index:654
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.41741912, 13.36336397]), 'previousTarget': array([10.41741912, 13.36336397]), 'currentState': array([ 1.       , 10.       ,  2.9335477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5604413340267563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.566443, 13.209377,  2.189328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.293777254892421}
episode index:655
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.82178448, 21.13681661]), 'previousTarget': array([19.82178448, 21.13681661]), 'currentState': array([26.      , 29.      ,  5.956645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5607259845760015
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.800456 , 16.297224 ,  4.9643393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3124816233237109}
episode index:656
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.67949706, 14.54700196]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([24.      ,  9.      ,  4.216427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5610804568729081
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.493673 , 13.179973 ,  2.8537915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8857923828396017}
episode index:657
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71512849, 17.12640838]), 'previousTarget': array([14.41421356, 19.10050506]), 'currentState': array([13.387307 , 27.03786  ,  4.9482923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5611472131195109
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.37218 , 15.36208 ,  3.798596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7247479718201071}
episode index:658
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.23259377, 22.91684196]), 'previousTarget': array([20.75304952, 22.19131191]), 'currentState': array([28.97741 , 30.299759,  6.144828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5609480352749046
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.717388 , 15.768395 ,  4.9783587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.881449865337949}
episode index:659
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.80868809, 9.24695048]), 'previousTarget': array([7.80868809, 9.24695048]), 'currentState': array([0.      , 3.      , 3.963619], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5610056231100428
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.749835 , 13.2420225,  2.0457704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7756878554871696}
episode index:660
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.80451099, 8.32793492]), 'previousTarget': array([8.80451099, 8.32793492]), 'currentState': array([2.       , 1.       , 2.4777863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5614321549864026
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.827102 , 15.041952 ,  0.5940907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.173648323018876}
episode index:661
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.5078087 , 12.77909938]), 'previousTarget': array([12.68964732, 12.43294146]), 'currentState': array([7.930852 , 4.4786444, 5.239297 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5614105907679802
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.320447 , 14.297759 ,  1.2394632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8204506942892171}
episode index:662
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 10.       ,  4.5049067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5618099241511892
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.728734, 13.47972 ,  1.733434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9817588470183405}
episode index:663
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 16.      ,  4.573846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 310
reward sum = 0.044351705540476356
running average episode reward sum: 0.5610306196050887
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.202332 , 16.749086 ,  2.4124417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1224759843014005}
episode index:664
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.75140493, 11.74391196]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([18.       ,  2.       ,  4.4976077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5612133624252471
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.777317 , 14.322616 ,  1.2209108], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9020266768743428}
episode index:665
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 14.       ,  3.4856446], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5615623127574069
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.646461  , 13.401636  ,  0.65961295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7241456731111358}
episode index:666
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.8085497 , 12.93919299]), 'previousTarget': array([19.8085497 , 12.93919299]), 'currentState': array([29.       ,  9.       ,  5.3982882], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5615005109731384
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.828793 , 15.640268 ,  4.7074237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.334792256638477}
episode index:667
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86059386, 13.89352217]), 'previousTarget': array([15.86059386, 13.89352217]), 'currentState': array([22.       ,  6.       ,  2.5579312], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5615933625957702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.542631 , 14.708427 ,  3.0972712], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.616005785870007}
episode index:668
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.7124705, 19.025413 ]), 'previousTarget': array([14.7124705, 19.025413 ]), 'currentState': array([14.       , 29.       ,  6.2705193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5615474914154404
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7390375, 16.718029 ,  5.524677 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.131114758278419}
episode index:669
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.07106781, 14.07106781]), 'previousTarget': array([14.07106781, 14.07106781]), 'currentState': array([7.       , 7.       , 4.5707016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5618023509352
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.868412 , 15.664866 ,  0.5684702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3124552389899522}
episode index:670
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.99291461, 19.09685391]), 'previousTarget': array([13.16227766, 20.51316702]), 'currentState': array([11.605788, 28.807756,  6.024904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5615387052242586
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.751007, 16.690044,  5.444284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4335726574016268}
episode index:671
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.45942241,  9.63386285]), 'previousTarget': array([10.45942241,  9.63386285]), 'currentState': array([4.        , 2.        , 0.29534692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5619324997460018
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.070472 , 14.493375 ,  1.8414009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9949306462755105}
episode index:672
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6725855, 16.096567 ,  6.087599 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7217690935290055}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5625834172798116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6725855, 16.096567 ,  6.087599 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7217690935290055}
episode index:673
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.008118 , 14.819988 ,  3.5266263], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0161698414715623}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5632175665123341
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.154546 , 14.068808 ,  3.1489205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9439299134876891}
episode index:674
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.6676221 , 11.73957299]), 'previousTarget': array([17.6676221 , 11.73957299]), 'currentState': array([24.       ,  4.       ,  3.9169147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5630873775871391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.4445095, 13.87355  ,  3.312165 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2109819852432087}
episode index:675
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.330593 , 13.493158 ,  1.1203222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6488412211265606}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5637336980344954
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.330593 , 13.493158 ,  1.1203222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6488412211265606}
episode index:676
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.        ,  7.        ,  0.05627927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5640499279626559
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.742893 , 14.028009 ,  1.8955092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5890510239811455}
episode index:677
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([7.       , 9.       , 1.3728162], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5636254467528278
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.585626  , 15.5483265 ,  0.61116093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.516943225158314}
episode index:678
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.45942241, 20.36613715]), 'previousTarget': array([10.45942241, 20.36613715]), 'currentState': array([ 4.       , 28.       ,  3.8459728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5635168529770567
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.529711, 16.772335,  6.070954], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3028074081778946}
episode index:679
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 15.00496281]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([25.      , 16.      ,  6.056772], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5636051008328128
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.822561, 14.252185,  2.199811], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9700144753354827}
episode index:680
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32793492, 21.19548901]), 'previousTarget': array([ 8.32793492, 21.19548901]), 'currentState': array([ 1.       , 28.       ,  3.2952187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5636839334908297
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.8167515 , 16.847609  ,  0.09032791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.856673755470059}
episode index:681
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.10158486,  9.93790583]), 'previousTarget': array([12.24097426, 10.51658317]), 'currentState': array([5.0000567, 2.0150683, 3.6693504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.563520238060882
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.871886  , 14.639611  ,  0.45081538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1842806595866253}
episode index:682
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.33471177, 22.75958076]), 'previousTarget': array([20.33471177, 22.75958076]), 'currentState': array([26.      , 31.      ,  4.064338], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5635043716466821
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.961645 , 15.966124 ,  5.4888616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9668846208143583}
episode index:683
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94427191, 15.52786405]), 'previousTarget': array([13.94427191, 15.52786405]), 'currentState': array([ 5.       , 20.       ,  4.3103094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5634645527191391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.352718 , 13.524782 ,  0.7601144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.00152805561242}
episode index:684
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 19.      ,  5.560517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5640026560697783
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.641285 , 15.615995 ,  3.6518176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7530734737790628}
episode index:685
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.6716943, 11.902076 ,  6.197908 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.956205081242777}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5645807804924171
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.4656925 , 13.122025  ,  0.07657364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9348543527105635}
episode index:686
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.76696499, 11.97054486]), 'previousTarget': array([14.76696499, 11.97054486]), 'currentState': array([14.       ,  2.       ,  5.5181856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5645163841927925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.915084 , 16.224077 ,  1.2801738], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6356674565064968}
episode index:687
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.      , 18.      ,  3.776715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5649972299341673
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.367462, 13.756022,  6.093739], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0524765386167956}
episode index:688
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([27.       , 24.       ,  4.4919543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.564794897413782
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.364897 , 15.473471 ,  3.9035256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5977659643683337}
episode index:689
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 16.52786405]), 'previousTarget': array([18.05572809, 16.52786405]), 'currentState': array([27.        , 21.        ,  0.84367716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5643457885199885
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.698706, 15.216259,  4.161643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7314079395448544}
episode index:690
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.84378558, 15.57572034]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([24.104181 , 21.211842 ,  4.0182576], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.564905331590003
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.100386 , 15.501382 ,  3.8443701], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.20922801711028}
episode index:691
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9895252 , 14.63059298]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 1.0316789 , 13.71337   ,  0.64657706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5652474201130507
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.264391, 16.994675,  5.914399], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.125993360909818}
episode index:692
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.70739852, 19.39987354]), 'previousTarget': array([21.13681661, 19.82178448]), 'currentState': array([27.013071, 26.228281,  2.574688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5651246120848521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.769426 , 16.3958   ,  5.4880056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5938235560609835}
episode index:693
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.12652114, 14.57826285]), 'previousTarget': array([15.12652114, 14.57826285]), 'currentState': array([18.      ,  5.      ,  5.284491], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5652742477426111
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.852662 , 16.885061 ,  1.7310607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0689341707528826}
episode index:694
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.96128974, 18.36221099]), 'previousTarget': array([16.96128974, 18.36221099]), 'currentState': array([22.      , 27.      ,  1.400555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5649719262562263
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.055302 , 15.885725 ,  4.5248632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3777410570487811}
episode index:695
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.24097426, 10.51658317]), 'previousTarget': array([12.24097426, 10.51658317]), 'currentState': array([7.       , 2.       , 4.9811807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5648162334710075
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.364412 , 15.278206 ,  1.7767038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3924867871879734}
episode index:696
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.00180224, 11.24363695]), 'previousTarget': array([13.24695048, 12.80868809]), 'currentState': array([5.7636003, 3.427958 , 4.0630317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5651676131478371
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.531968, 13.047198,  5.965884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.023962592041492}
episode index:697
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.28609324, 20.71523309]), 'previousTarget': array([17.28609324, 20.71523309]), 'currentState': array([21.       , 30.       ,  1.0882273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5651739649950138
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.313587 , 13.683741 ,  4.1039677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3530985519564247}
episode index:698
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 13.       ,  2.2848554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5649050990178014
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.519653, 15.316089,  5.817538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.513716731572782}
episode index:699
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15384615, 12.23076923]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([20.        ,  3.        ,  0.81169873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5653144599779115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.690915 , 14.020862 ,  2.3013122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1983637353359629}
episode index:700
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57464375, 21.298575  ]), 'previousTarget': array([16.57464375, 21.298575  ]), 'currentState': array([1.9000000e+01, 3.1000000e+01, 2.5034603e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5653287827881812
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.505116 , 16.70186  ,  4.1034846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2651723347180015}
episode index:701
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 15.       ,  1.0868919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.56579888175039
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.53808  , 16.876091 ,  2.7148864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.42598602680996}
episode index:702
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.116344 , 15.327809 ,  3.8939288], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.126835233344164}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5654509474585185
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.576582 , 13.314724 ,  2.8311894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3077621114451325}
episode index:703
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 3.       , 15.       ,  1.7368137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.565617287866973
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.184206, 16.427425,  5.455078], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.309686352397387}
episode index:704
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.38349617, 10.88492064]), 'previousTarget': array([8.59256602, 9.50791373]), 'currentState': array([1.3169196, 4.974731 , 1.2533946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5660849771810038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.486315 , 14.472724 ,  1.4635496], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.602892069410108}
episode index:705
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45299804, 15.67949706]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([21.       , 24.       ,  3.4023414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5665641515405333
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.17432  , 15.4921055,  4.8277216], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9612049234830712}
episode index:706
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.10738167, 15.10599328]), 'previousTarget': array([19., 15.]), 'currentState': array([27.094757 , 15.60832  ,  2.6890502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.567054891421641
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.760353 , 15.917527 ,  4.1448555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.191634583053683}
episode index:707
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6497659 , 20.96720534]), 'previousTarget': array([ 9.24695048, 22.19131191]), 'currentState': array([ 4.7587986 , 29.04783   ,  0.08306807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5669742852843923
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.572406  , 14.857815  ,  0.40733942], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4346573775536364}
episode index:708
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.07376011, 11.09710761]), 'previousTarget': array([20.07376011, 11.09710761]), 'currentState': array([28.      ,  5.      ,  4.656985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5667570432334043
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.566954  , 14.644008  ,  0.37988448], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.476601620340967}
episode index:709
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89949494, 15.58578644]), 'previousTarget': array([10.89949494, 15.58578644]), 'currentState': array([ 1.     , 17.     ,  4.59468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5662038499731828
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.991277 , 15.5233555,  0.9630173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1364081906751224}
episode index:710
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 12.07106781]), 'previousTarget': array([17.92893219, 12.07106781]), 'currentState': array([25.        ,  5.        ,  0.08102863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5657066936486481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.611677 , 13.237386 ,  2.1030788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3883702172407144}
episode index:711
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42507074, 14.14495755]), 'previousTarget': array([16.42507074, 14.14495755]), 'currentState': array([25.       ,  9.       ,  1.2243143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5661080294315782
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.048439 , 15.769541 ,  2.9393406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.300545044759003}
episode index:712
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.33345606, 13.58979079]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([ 2.       , 10.       ,  1.2089034], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5661291491856968
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6500025, 13.899387 ,  0.5472677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2782219711738116}
episode index:713
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10647783, 15.86059386]), 'previousTarget': array([16.10647783, 15.86059386]), 'currentState': array([24.        , 22.        ,  0.48122144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5663618861891543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.122826 , 14.184398 ,  2.3882551], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8247989145748428}
episode index:714
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      ,  9.      ,  5.377187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5666686222095991
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.84765  , 16.73101  ,  3.1248105], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5318391746895323}
episode index:715
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.86393924, 15.35601013]), 'previousTarget': array([12.86393924, 15.35601013]), 'currentState': array([ 3.       , 17.       ,  4.5111814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5668304741269187
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.857101  , 13.663866  ,  0.12330025], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7582579640865206}
episode index:716
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.60206577, 15.58256937]), 'previousTarget': array([16.60206577, 15.58256937]), 'currentState': array([26.       , 19.       ,  2.1197977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.56675119277702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.384238  , 16.709219  ,  0.59262437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3520449767852356}
episode index:717
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3177872, 18.598156 ]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([27.       , 25.       ,  4.3949456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5671009918645133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.920037 , 16.061531 ,  3.9622467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4047479430207646}
episode index:718
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 24.       ,  1.8890672], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5670965470781265
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.51039  , 16.331875 ,  3.4985626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.013744405656828}
episode index:719
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 21.       ,  3.3904045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5670842826218345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.272871 , 14.846546 ,  4.0564938], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2820876350980013}
episode index:720
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.07106781, 22.92893219]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([ 0.       , 30.       ,  2.0772285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.5665439750547567
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.370246 , 16.57893  ,  4.3812165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6998852363887735}
episode index:721
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      ,  9.      ,  3.612629], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5670373278516725
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.596243 , 13.61916  ,  1.4997556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5040696591732188}
episode index:722
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.92893219, 13.07106781]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([24.       ,  6.       ,  1.9258552], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5674667658809218
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.793837 , 15.260147 ,  2.0203476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8126020621011456}
episode index:723
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  9.       ,  2.7514102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5674937962898396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.428664, 13.883382,  1.540113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.254296892731181}
episode index:724
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.86318339, 10.17821552]), 'previousTarget': array([ 8.86318339, 10.17821552]), 'currentState': array([1.       , 4.       , 1.7025162], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.5669608765061653
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.753077 , 14.268477 ,  1.8327742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8995796002934444}
episode index:725
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.14428833, 14.63274212]), 'previousTarget': array([19.10050506, 14.41421356]), 'currentState': array([27.000767 , 12.944597 ,  2.8946066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5674637752271031
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.42664  , 14.09549  ,  4.0891814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6892122250118289}
episode index:726
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.86266529, 10.81238194]), 'previousTarget': array([20.86266529, 10.81238194]), 'currentState': array([29.       ,  5.       ,  5.4020085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5670133332798638
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.111015 , 15.022821 ,  3.2254221], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1112496831885523}
episode index:727
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.       , 15.       ,  2.0143828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5674040536614778
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.274805 , 15.781017 ,  3.9722462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0657840852192109}
episode index:728
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 12.       ,  2.2588723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5678788317051296
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.502204 , 15.680609 ,  0.6705842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6451812371491323}
episode index:729
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42535625, 17.298575  ]), 'previousTarget': array([14.42535625, 17.298575  ]), 'currentState': array([12.       , 27.       ,  0.7758451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5673392577281038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.483046, 15.455431,  5.039494], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5513998736863475}
episode index:730
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.997858  , 21.092533  ,  0.48951635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.289451277269243}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5668541502664087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.601191 , 15.66921  ,  3.7169862], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.550648391713678}
episode index:731
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 24.       ,  0.8244273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5669936551957848
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.616648 , 15.385094 ,  6.1081557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6618805615780856}
episode index:732
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        , 10.        ,  0.26800743], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5674789908564017
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.469921  , 13.54119   ,  0.96557987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0709404293036613}
episode index:733
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15711465, 21.17127813]), 'previousTarget': array([16.15711465, 21.17127813]), 'currentState': array([18.      , 31.      ,  6.029151], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5677444727685144
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.568002, 16.84714 ,  3.879987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8969834739072244}
episode index:734
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.6725883, 14.096562 ,  6.1659126], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.3828965206994575}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5678640846702907
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.51487  , 16.327255 ,  1.4937758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9917877733119564}
episode index:735
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.68929139, 12.77860636]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([13.304066 ,  2.8750136,  1.8424836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5682968574787769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.094236 , 16.203712 ,  0.6836272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.254076017585548}
episode index:736
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84288535, 21.17127813]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([12.       , 31.       ,  6.1873384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5681391190700764
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.012293 , 16.127134 ,  4.6110916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5149813928517524}
episode index:737
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.        , 23.        ,  0.70628995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5678457181046129
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.010169 , 14.062218 ,  0.6105019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9378374209251263}
episode index:738
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.45836872, 17.91285107]), 'previousTarget': array([20.07376011, 18.90289239]), 'currentState': array([26.106892 , 24.354908 ,  3.5673208], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5682888744458228
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.504051, 15.471447,  4.395817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.690166564010471}
episode index:739
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([24.       , 27.       ,  1.4645798], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5686948973354486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.347113, 16.46969 ,  5.294915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6081826791696288}
episode index:740
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 3.        , 31.        ,  0.79617333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5682087941834909
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.52409  , 16.26982  ,  1.4048824], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9469853187854327}
episode index:741
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 5.9903975 , 17.804249  ,  0.26901406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.435928647287604}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5685564489404344
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.187658, 13.702398,  5.601098], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2289801643221305}
episode index:742
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57492926, 11.14495755]), 'previousTarget': array([ 8.57492926, 11.14495755]), 'currentState': array([0.       , 6.       , 3.0161219], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5681861478268204
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.8399315 , 13.814972  ,  0.29693145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.19578989368435}
episode index:743
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.57464375, 17.298575  ]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.        , 27.        ,  0.63232803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5678051285061838
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.21282 , 15.875839,  3.987019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.496003624314751}
episode index:744
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 13.       ,  6.0865474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5682940683979969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.787604, 16.519817,  1.235672], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7117725807112916}
episode index:745
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.39793423, 15.58256937]), 'previousTarget': array([13.39793423, 15.58256937]), 'currentState': array([ 4.       , 19.       ,  3.9641874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5685541872263469
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.568608, 13.599226,  5.673695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0027606071452895}
episode index:746
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99668849, 14.95073881]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([14.325967 ,  4.9732575,  0.9950883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5689444203821902
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.544047 , 16.71596  ,  3.4311442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.250403358559908}
episode index:747
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.12706333, 22.22436634]), 'previousTarget': array([ 7.80868809, 20.75304952]), 'currentState': array([-0.24097721, 28.98543   ,  0.92844284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5687585174318448
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.055832  , 16.491488  ,  0.04468547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.450372326223806}
episode index:748
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.45653   ,  7.947198  ,  0.92569876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.067562278043558}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5692311291501306
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.942817 , 16.761873 ,  2.3159668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9982744373064385}
episode index:749
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6822128, 18.598156 ]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 3.        , 25.        ,  0.27709436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5691944010792196
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.964418 , 13.920061 ,  4.6995816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2416975035627233}
episode index:750
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.72586235, 15.00120195]), 'previousTarget': array([11.97054486, 14.76696499]), 'currentState': array([ 3.7258668 , 15.010635  ,  0.74481916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5689541740712236
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.466091 , 16.057446 ,  4.2991724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8076543691857259}
episode index:751
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.13733471, 10.81238194]), 'previousTarget': array([ 9.13733471, 10.81238194]), 'currentState': array([1.      , 5.      , 4.805112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5684891249958184
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.499537, 13.296591,  3.223306], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.269407993969629}
episode index:752
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.       , 9.       , 5.1009727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5685961814412003
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.957006  , 16.15657   ,  0.12203506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1573692647895886}
episode index:753
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.07106781, 15.92893219]), 'previousTarget': array([14.07106781, 15.92893219]), 'currentState': array([ 7.       , 23.       ,  3.9647083], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5682121574453699
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.115233, 16.807673,  6.005084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8113416604421444}
episode index:754
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.11828302, 12.08736084]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([23.       ,  4.       ,  5.2014318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 246
reward sum = 0.08438356532646984
running average episode reward sum: 0.567571324873027
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.107978 , 13.203596 ,  2.4520774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7996461144982878}
episode index:755
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.52576695, 14.58504608]), 'currentState': array([8.99281   , 7.830568  , 0.13437957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.353453172163231}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5678809270749662
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.355707 , 13.548369 ,  0.8291123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.986246032990908}
episode index:756
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.2440726 , 19.48828759]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 9.600725, 28.800968,  5.871103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.5672467977250524
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.882647, 15.504285,  4.351481], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9490154671311184}
episode index:757
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.41741912, 16.63663603]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 1.       , 20.       ,  2.5721447], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5669862209967208
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.965584, 13.401155,  6.052027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5992149011576007}
episode index:758
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 13.       ,  3.5046992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5668846453076661
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.537138 , 15.899198 ,  2.2319632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.780828310278696}
episode index:759
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2875295, 19.025413 ]), 'previousTarget': array([15.2875295, 19.025413 ]), 'currentState': array([16.      , 29.      ,  2.954979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5665613774514255
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.76866  , 15.401314 ,  5.5432086], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8136177939787554}
episode index:760
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.8231825 , 22.68944732]), 'previousTarget': array([ 7.8231825 , 22.68944732]), 'currentState': array([ 1.       , 30.       ,  3.0185773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 412
reward sum = 0.015911098861934425
running average episode reward sum: 0.5658377897003224
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.837565, 13.708204,  5.91273 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7378119453978966}
episode index:761
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.025413 , 15.2875295]), 'previousTarget': array([19.025413 , 15.2875295]), 'currentState': array([29.       , 16.       ,  0.2097928], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5662126190722313
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.383438 , 14.505365 ,  3.5816536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.469205370491996}
episode index:762
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.90535746, 16.04106794]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([14.       , 26.       ,  3.9350867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5663212560437859
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.941113 , 13.594421 ,  3.9692497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.691551301399117}
episode index:763
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 18.       ,  3.8722022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5665879077319552
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.313207 , 13.565498 ,  6.2660985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.214286952293953}
episode index:764
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.1914503 , 12.93919299]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.      , 9.      , 4.221916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5666302189721306
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.45662 , 15.393511,  2.773789], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.508837446750634}
episode index:765
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 13.       ,  3.7955396], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5664083489818563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.180126 , 16.634148 ,  0.8813059], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.82828651653696}
episode index:766
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.02784641, 15.02876244]), 'previousTarget': array([18., 15.]), 'currentState': array([26.023933 , 15.308485 ,  3.0530024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.5658649761743227
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.27609 , 13.509349,  4.099544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9622558622184552}
episode index:767
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10647783, 14.13940614]), 'previousTarget': array([16.10647783, 14.13940614]), 'currentState': array([24.      ,  8.      ,  3.359905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5659482556730583
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.035048 , 15.594793 ,  1.6940856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.053002020965578}
episode index:768
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 12.       ,  3.1301816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5661270585850563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.42047  , 13.400456 ,  6.0314183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6538847776844332}
episode index:769
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.81857519, 19.96617283]), 'previousTarget': array([11.70588235, 21.17647059]), 'currentState': array([ 8.796891, 29.12183 ,  5.379676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5665901983718652
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.407144, 16.636631,  4.979206], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2838022250551706}
episode index:770
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.94573335, 15.00228863]), 'previousTarget': array([13., 15.]), 'currentState': array([ 4.9546146, 15.423652 ,  6.0144887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5670887714607473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.40693  , 15.391297 ,  0.7954947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7105249252030692}
episode index:771
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.92893219, 15.92893219]), 'currentState': array([21.315079 , 21.922485 ,  3.2323155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.37022000148806}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5669166763251657
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.339446, 16.510668,  3.668374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2448956693792144}
episode index:772
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.63778901, 16.96128974]), 'previousTarget': array([11.63778901, 16.96128974]), 'currentState': array([ 3.      , 22.      ,  4.253515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5667338476667744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.826452 , 15.274684 ,  5.3944993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.32491551729122986}
episode index:773
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99586307, 15.34618627]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([14.876371 , 25.345472 ,  4.5583296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5672303027084193
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.919735 , 15.408596 ,  4.7678084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1549559691804656}
episode index:774
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      , 23.      ,  3.656077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5668925333190257
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.337716  , 16.698326  ,  0.46033764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3764467876044266}
episode index:775
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.88289922, 15.89001339]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 1.1086739, 18.002954 ,  4.8446913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5666384574225447
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.545655 , 13.5105095,  1.6690042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.081754218623025}
episode index:776
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       ,  6.       ,  1.7138453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.566851668377798
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.564596 , 16.802757 ,  0.7220112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8545916638944042}
episode index:777
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45724082, 13.20923345]), 'previousTarget': array([15.1695452 , 12.96545758]), 'currentState': array([17.931194 ,  3.5200872,  0.6116009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5670834427042167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.87043  , 15.409545 ,  3.2335804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9147416064007703}
episode index:778
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.62951104, 18.18600367]), 'previousTarget': array([ 9.63386285, 19.54057759]), 'currentState': array([ 2.5487218, 24.076746 ,  4.4649334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5674267422148019
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.103088, 16.766777,  5.906625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9814014131634468}
episode index:779
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.27327206, 11.39940073]), 'previousTarget': array([18.27327206, 11.39940073]), 'currentState': array([2.5000000e+01, 4.0000000e+00, 1.3878044e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5676381225063912
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.805012 , 13.897337 ,  2.4230158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.625995910058179}
episode index:780
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.00045  , 18.957535 ,  3.1973948], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.1880096986800535}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5681289700446673
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.244839, 15.982694,  4.60219 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5859729754031013}
episode index:781
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07072178,  9.4185848 ]), 'previousTarget': array([9.91363664, 8.06404996]), 'currentState': array([3.4511082, 1.9232051, 1.6723067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.568558961227486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.974134 , 13.958088 ,  0.9780623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4621836124829681}
episode index:782
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.41743063, 13.39793423]), 'previousTarget': array([14.41743063, 13.39793423]), 'currentState': array([11.       ,  4.       ,  2.0622263], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5689535487904125
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.037562  , 13.716628  ,  0.51567507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2839215047850157}
episode index:783
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.07376011, 18.90289239]), 'previousTarget': array([20.07376011, 18.90289239]), 'currentState': array([28.      , 25.      ,  4.379848], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5685465124481524
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.184932, 15.336392,  0.729007], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8817573886618937}
episode index:784
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.79958315, 11.44684139]), 'previousTarget': array([12.47213595,  9.94427191]), 'currentState': array([7.5345798, 2.9450922, 2.4050117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5686917456743463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.933662 , 14.425431 ,  1.2691934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2112824166414649}
episode index:785
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.57662651,  9.6623494 ]), 'previousTarget': array([13.57662651,  9.6623494 ]), 'currentState': array([11.      ,  0.      ,  6.067079], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5683893809703345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.435747 , 13.924384 ,  1.1428643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8983773378440052}
episode index:786
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.54700196, 14.32050294]), 'previousTarget': array([14.54700196, 14.32050294]), 'currentState': array([9.        , 6.        , 0.22634238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.5677918261881152
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.086498, 13.321881,  1.009195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5451073253722356}
episode index:787
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.63661512, 17.66512697]), 'previousTarget': array([14.41743063, 16.60206577]), 'currentState': array([ 9.082303 , 26.567837 ,  3.8804724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5677518216183425
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.248287 , 15.680672 ,  4.2032022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7245415697467558}
episode index:788
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 17.       ,  5.0072803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5680899229362534
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.685838 , 14.596411 ,  4.2591887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.795774902225885}
episode index:789
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.8386991 , 15.21114562]), 'currentState': array([ 5.08478  , 15.319746 ,  5.3657937], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.920374029057573}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5685625687925379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.671543 , 15.240155 ,  6.1730943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.40688874505071804}
episode index:790
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.974587 , 14.7124705]), 'previousTarget': array([10.974587 , 14.7124705]), 'currentState': array([ 1.       , 14.       ,  5.5618753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5683711128015653
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.159625 , 16.077816 ,  0.6026385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1327604868884276}
episode index:791
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.41741912, 16.63663603]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 1.      , 20.      ,  1.516168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5686355701836329
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.336536, 15.850833,  5.242498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9149718268298893}
episode index:792
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.41178475, 18.59242409]), 'previousTarget': array([ 9.41178475, 18.59242409]), 'currentState': array([ 1.       , 24.       ,  2.7734385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5685551796648477
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.263878, 16.62777 ,  5.524151], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0608301033886085}
episode index:793
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.92893219, 15.92893219]), 'previousTarget': array([15.92893219, 15.92893219]), 'currentState': array([23.       , 23.       ,  2.2710829], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5681323953868668
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.124592 , 13.114103 ,  3.7169213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1957488645574004}
episode index:794
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.99248815, 19.51767535]), 'previousTarget': array([18.03861062, 20.31756858]), 'currentState': array([21.027866 , 28.667305 ,  3.0069885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5678922749485481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.621315, 15.519785,  4.318493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.702597635338633}
episode index:795
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.195289  , 17.25900177]), 'previousTarget': array([19.195289  , 17.25900177]), 'currentState': array([28.       , 22.       ,  3.2329457], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5680192592244541
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.671146 , 15.744261 ,  3.9232712], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5230810975814364}
episode index:796
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 1.       , 15.       ,  2.8384767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.568035755027332
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.268465 , 13.118026 ,  1.7027535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2695441235739238}
episode index:797
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 16.       ,  4.3432264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5679566198566047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.028662 , 16.75642  ,  5.9761677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7566539747569792}
episode index:798
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.57826285, 14.87347886]), 'previousTarget': array([14.57826285, 14.87347886]), 'currentState': array([ 5.       , 12.       ,  3.3727689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5679375045340838
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.726206  , 14.035198  ,  0.14991015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9775310474959253}
episode index:799
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.80580676, 13.96116135]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 0.       , 12.       ,  3.8401148], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.5675216034399667
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.988811 , 16.041328 ,  0.5814658], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4515057785049017}
episode index:800
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13940614, 13.89352217]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([8.      , 6.      , 4.480651], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5673338372433291
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.58415  , 13.1672   ,  1.1549222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.422537457705455}
episode index:801
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 15.71523309]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.       , 25.       ,  3.2390504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5669442810381582
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.447935, 14.666721,  6.0486  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5874445195960933}
episode index:802
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.07106781, 22.92893219]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([ 0.       , 30.       ,  1.5976291], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5669693028325533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.139764 , 16.46819  ,  5.8687534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4748266243021395}
episode index:803
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.493092 , 10.934695 ,  1.2792165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.335605579035092}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5674831469832591
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.499503 , 14.803179 ,  1.4930863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5378063787169911}
episode index:804
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.54700196, 21.67949706]), 'previousTarget': array([10.54700196, 21.67949706]), 'currentState': array([ 5.       , 30.       ,  3.3506665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5678044954017092
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.519401, 16.955301,  5.578763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.452626695523552}
episode index:805
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.      , 15.      ,  5.444949], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5681564225427678
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.571529, 14.334766,  0.61961 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5757741103155505}
episode index:806
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.        , 13.        ,  0.21954459], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5681036840854974
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.725338 , 14.863836 ,  1.7152352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7307026633964664}
episode index:807
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.059563, 16.2349  ,  4.686218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3000595225888696}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5686382092289559
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.059563, 16.2349  ,  4.686218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3000595225888696}
episode index:808
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  8.       ,  5.8891797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5689565410146254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.0625   , 13.084749 ,  0.6542409], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.190226425141696}
episode index:809
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 20.       ,  2.9887705], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.569233896252439
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.504864 , 16.212143 ,  4.6993337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9247656887059195}
episode index:810
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47213595, 18.05572809]), 'previousTarget': array([13.47213595, 18.05572809]), 'currentState': array([ 9.        , 27.        ,  0.10668682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5693652096344463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.335373 , 16.953691 ,  3.1213913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.366459646236585}
episode index:811
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.71390676,  9.28476691]), 'previousTarget': array([12.71390676,  9.28476691]), 'currentState': array([9.       , 0.       , 3.3247137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5688916127801533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.183993  , 15.147604  ,  0.05905264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8292489417430983}
episode index:812
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.67949706, 12.54700196]), 'previousTarget': array([18.67949706, 12.54700196]), 'currentState': array([27.       ,  7.       ,  6.2830167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5686512347576123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.560214 , 13.605806 ,  2.0154076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4619123161107983}
episode index:813
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.64007431, 16.81619935]), 'previousTarget': array([15.75140493, 18.25608804]), 'currentState': array([18.963947 , 26.24763  ,  5.9546375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.5681986865906588
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.447393, 16.575264,  4.848935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1392532112880573}
episode index:814
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.89633523, 14.13802944]), 'previousTarget': array([16.89633523, 14.13802944]), 'currentState': array([26.      , 10.      ,  4.505393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5685254535536765
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.739799, 16.773241,  2.454883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9213759874956686}
episode index:815
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.47942816, 11.88371698]), 'previousTarget': array([15.47942816, 11.88371698]), 'currentState': array([17.       ,  2.       ,  2.0233521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5686083702601841
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.636047 , 13.469739 ,  1.3749046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.049894057003792}
episode index:816
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.90535746, 16.04106794]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([14.        , 26.        ,  0.07205742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5690193539869266
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.326622 , 16.137291 ,  5.4342914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.023270771603957}
episode index:817
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.82178448, 21.13681661]), 'previousTarget': array([19.82178448, 21.13681661]), 'currentState': array([26.       , 29.       ,  1.1832359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5689534832820817
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.823088 , 16.481752 ,  5.1428065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6950113601701937}
episode index:818
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 15.1695452 ]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.       , 16.       ,  3.5670156], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5690049652799459
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.1995945, 16.541428 ,  5.669638 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.370117941528875}
episode index:819
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.1914503 , 12.93919299]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.       , 9.       , 3.6066585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5689456183986903
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.262728 , 14.574777 ,  1.1663936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3324024304610538}
episode index:820
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.59173362, 14.72188212]), 'previousTarget': array([17.22104427, 13.78852131]), 'currentState': array([24.641953 , 10.468232 ,  2.5190778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5693002015122623
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.048788 , 16.976461 ,  2.9528801], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9770634747334204}
episode index:821
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.70492199, 19.18123287]), 'previousTarget': array([19.77807808, 18.30790021]), 'currentState': array([27.17974  , 25.824055 ,  2.6447632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5690144066891208
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.239474, 15.397047,  3.84523 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8579310577262352}
episode index:822
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.242422, 21.04558 ,  3.347444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.17192426523325}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5693680445359643
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.121377 , 16.15764  ,  4.2060957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2066616951767877}
episode index:823
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.195289  , 17.25900177]), 'previousTarget': array([19.195289  , 17.25900177]), 'currentState': array([28.       , 22.       ,  5.6499357], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5691883918388998
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.900753 , 16.347946 ,  3.9025643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3515949215285885}
episode index:824
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.78280103, 20.65196555]), 'previousTarget': array([ 9.78280103, 20.65196555]), 'currentState': array([ 3.       , 28.       ,  1.6289932], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5690982709513377
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.034688 , 16.897583 ,  4.9157176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.129001770026916}
episode index:825
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85642931, 14.74157276]), 'previousTarget': array([14.85642931, 14.74157276]), 'currentState': array([1.0000000e+01, 6.0000000e+00, 5.2294433e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5695041835470489
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.394594 , 13.345346 ,  1.4491986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.163971194269868}
episode index:826
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.96138938,  9.68243142]), 'previousTarget': array([11.96138938,  9.68243142]), 'currentState': array([7.      , 1.      , 4.139812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 353
reward sum = 0.02878880863894463
running average episode reward sum: 0.5688503560078613
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.450618 , 15.778944 ,  0.8521662], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9531917916705966}
episode index:827
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.25900177, 19.195289  ]), 'previousTarget': array([17.25900177, 19.195289  ]), 'currentState': array([22.       , 28.       ,  2.9068084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5691218100267451
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.286657, 15.012859,  3.802537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2867215923678559}
episode index:828
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.54700196, 21.67949706]), 'previousTarget': array([10.54700196, 21.67949706]), 'currentState': array([ 5.       , 30.       ,  1.5362443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.568980581848506
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.965824 , 16.278057 ,  5.8623667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.278513956522767}
episode index:829
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       ,  9.       ,  0.7867944], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5688288558241417
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.861109 , 14.299522 ,  3.6427755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9885660056649712}
episode index:830
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.        , 12.        ,  0.77033925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.569335680305701
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4353485, 13.392757 ,  0.5290617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7035433745222404}
episode index:831
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 20.       ,  5.8809915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.403124237432848}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.569378553366797
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.881029 , 16.32013  ,  3.2380157], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7305598598108545}
episode index:832
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 14.28476691]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([11.       ,  5.       ,  2.7553267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5695653502246494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.88029  , 14.824184 ,  1.4536283], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.21270071825248152}
episode index:833
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.993965 , 18.99999  ,  0.5032222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0036156159339855}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5700226939892481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.042515 , 16.57947  ,  5.9650164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5800417655330747}
episode index:834
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.59256602, 20.49208627]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 1.       , 27.       ,  2.7593527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5699953094963157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.543563, 13.179436,  5.753104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3314509043256835}
episode index:835
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.04106794, 15.09464254]), 'currentState': array([24.25173  , 15.028635 ,  3.3272843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.251774279230013}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5704625352146215
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.762049 , 13.384426 ,  4.4681487], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3905845870617437}
episode index:836
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.3715414 , 10.75724629]), 'previousTarget': array([10.3715414 , 10.75724629]), 'currentState': array([3.      , 4.      , 0.774357], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5708506782481271
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.622443 , 13.9120655,  0.8474364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1515861240644008}
episode index:837
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.        , 16.        ,  0.90668166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.571034603853986
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.815771, 15.937801,  6.045929], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.043647741416511}
episode index:838
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 24.       ,  2.4096103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.570543433538546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.539474, 14.881285,  5.734592], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4755816916340196}
episode index:839
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.3323779 , 18.26042701]), 'previousTarget': array([12.3323779 , 18.26042701]), 'currentState': array([ 6.      , 26.      ,  1.806648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5706292289772915
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.958025, 13.620055,  5.016239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7291499645381345}
episode index:840
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28609324, 15.71523309]), 'previousTarget': array([15.28609324, 15.71523309]), 'currentState': array([19.       , 25.       ,  1.0249398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5703859508706279
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.030323, 13.760181,  4.818327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.327397143968863}
episode index:841
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 18.30790021]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 2.        , 24.        ,  0.92945236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5701989719160713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.654693 , 15.046092 ,  4.6935005], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3460967060106335}
episode index:842
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.418324 , 12.224051 ,  1.9483769], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.08803087572153}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5704269004361555
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.630352 , 15.384901 ,  2.1018784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7385746307868918}
episode index:843
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.31756858, 18.03861062]), 'previousTarget': array([20.31756858, 18.03861062]), 'currentState': array([29.      , 23.      ,  5.649129], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5703314778918049
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.725492 , 14.551545 ,  2.7486162], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7828168857948414}
episode index:844
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      , 16.      ,  5.005073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5705957178986119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.347507  , 13.560074  ,  0.10087776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.972096273053498}
episode index:845
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 14.28476691]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([11.       ,  5.       ,  5.7556853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5708783800148325
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.68206  , 13.45368  ,  1.2827009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6900626014405935}
episode index:846
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.92893219, 15.92893219]), 'previousTarget': array([15.92893219, 15.92893219]), 'currentState': array([23.        , 23.        ,  0.07298434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5712404138318149
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.861311 , 16.212431 ,  3.3220212], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2213660895378173}
episode index:847
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([3.       , 6.       , 2.2320771], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5710537502201428
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.7918005 , 13.494775  ,  0.08460331], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7007794911211795}
episode index:848
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42535625, 12.701425  ]), 'previousTarget': array([14.42535625, 12.701425  ]), 'currentState': array([12.      ,  3.      ,  4.542628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5709581513070499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.86087 , 14.300978,  5.900965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3365060792748624}
episode index:849
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.07106781, 17.92893219]), 'previousTarget': array([12.07106781, 17.92893219]), 'currentState': array([ 5.       , 25.       ,  4.4594855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5712881508597415
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.751479  , 16.461382  ,  0.17097872], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6432766062278494}
episode index:850
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 14.       ,  1.6107521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5710058299871931
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.137125, 13.073676,  6.273112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.679743819957861}
episode index:851
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.83772234, 12.48683298]), 'previousTarget': array([15.83772234, 12.48683298]), 'currentState': array([19.      ,  3.      ,  5.778698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.5705221878266444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.102074, 14.555913,  1.353125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1881833451718726}
episode index:852
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.06404996, 20.08636336]), 'previousTarget': array([ 8.06404996, 20.08636336]), 'currentState': array([ 0.       , 26.       ,  0.9190328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5705078196563294
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.916862 , 16.667831 ,  5.0465674], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.669902288472354}
episode index:853
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36352179, 20.13028453]), 'previousTarget': array([21.19548901, 21.67206508]), 'currentState': array([27.589968, 27.042482,  4.873392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5707597755358972
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.96823  , 15.598074 ,  4.8203907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0570899382178762}
episode index:854
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.9503719 , 15.00496281]), 'previousTarget': array([14.9503719 , 15.00496281]), 'currentState': array([ 5.       , 16.       ,  1.0940202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5709487154120592
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1765995, 14.145103 ,  5.848402 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1869441783558878}
episode index:855
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.19548901, 21.67206508]), 'previousTarget': array([21.19548901, 21.67206508]), 'currentState': array([28.       , 29.       ,  0.8474088], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5710476762825756
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.341873, 16.963789,  4.129513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0711344978292185}
episode index:856
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.      , 13.      ,  2.413918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5711619400892116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.140191 , 13.595104 ,  5.7456074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6471197714904833}
episode index:857
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.75724629, 19.6284586 ]), 'previousTarget': array([10.75724629, 19.6284586 ]), 'currentState': array([ 4.        , 27.        ,  0.23191318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5712013854587318
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.60713 , 14.225143,  5.598661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8687631996760302}
episode index:858
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.85504245,  8.57492926]), 'previousTarget': array([18.85504245,  8.57492926]), 'currentState': array([24.       ,  0.       ,  4.6662064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5708548054419256
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.667686, 15.95042 ,  4.312305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0068421849044595}
episode index:859
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.45299804, 11.32050294]), 'previousTarget': array([17.45299804, 11.32050294]), 'currentState': array([23.       ,  3.       ,  3.9427333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5708736216936645
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.705047 , 14.070023 ,  1.6418494], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.594290489555105}
episode index:860
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.24275371, 10.3715414 ]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([26.       ,  3.       ,  3.8209639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5706854063077514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.921269 , 15.227028 ,  3.1758661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9488303339258909}
episode index:861
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 14.       ,  4.9700317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5710211057837764
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.16606  , 14.294354 ,  0.9076233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7249218787945307}
episode index:862
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.94427191, 17.52786405]), 'previousTarget': array([ 9.94427191, 17.52786405]), 'currentState': array([ 1.       , 22.       ,  2.3026156], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5713167576007542
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.962673, 15.228411,  5.809047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0621762442390636}
episode index:863
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.77895573, 13.78852131]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([4.       , 9.       , 2.8714788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5714143761921585
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.253121  , 16.893604  ,  0.17054628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.035574818750118}
episode index:864
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 17.92893219]), 'previousTarget': array([17.92893219, 17.92893219]), 'currentState': array([25.       , 25.       ,  1.2670388], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5711562013231415
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.328023 , 16.08892  ,  5.0892005], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9953083198484076}
episode index:865
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24164379, 20.2632127 ]), 'previousTarget': array([13.42535625, 21.298575  ]), 'currentState': array([12.81551 , 30.160997,  6.254587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5709411203503589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.583598, 13.044388,  6.159362], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.999452305105667}
episode index:866
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 10.       ,  5.8629155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5712846090382696
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.396202 , 13.735057 ,  0.4965549], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8840014259408102}
episode index:867
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 23.       ,  5.1930404], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5717002550507913
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.905022 , 16.418503 ,  4.1806808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.375133210745854}
episode index:868
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1695452 , 17.03454242]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([16.      , 27.      ,  5.639901], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5714469857199596
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.552051 , 16.215202 ,  5.6195188], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.295135274805418}
episode index:869
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.05914151, 11.55779009]), 'previousTarget': array([16.05914151, 11.55779009]), 'currentState': array([19.      ,  2.      ,  3.422936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5716752571683333
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.022708 , 13.421977 ,  2.1902995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5781863333713506}
episode index:870
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.50791373, 21.40743398]), 'previousTarget': array([ 9.50791373, 21.40743398]), 'currentState': array([ 3.       , 29.       ,  3.7135456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5713767334304238
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.720698 , 15.980812 ,  5.5685267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0198047509449446}
episode index:871
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.75724629, 10.3715414 ]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.       , 3.       , 4.0595374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.5708798344392049
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.339554 , 14.516941 ,  1.1955978], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.423991011502842}
episode index:872
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 20.       ,  2.2198803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5711258806091568
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.371968, 16.831425,  4.247446], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.936114752432387}
episode index:873
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2875295, 10.974587 ]), 'previousTarget': array([15.2875295, 10.974587 ]), 'currentState': array([16.      ,  1.      ,  2.582871], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5713804439993565
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.746673 , 13.923369 ,  0.9464811], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6522599456470577}
episode index:874
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 15.71523309]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.       , 25.       ,  1.5770705], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5711499859349569
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.680228, 15.975234,  5.640599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0263212943790918}
episode index:875
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 23.       ,  3.4888675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5714316719528363
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.291111, 14.39633 ,  5.619933], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9310967936369094}
episode index:876
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.74391196, 14.24859507]), 'previousTarget': array([11.74391196, 14.24859507]), 'currentState': array([ 2.       , 12.       ,  3.5193179], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5714562269978176
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.326086 , 13.716813 ,  5.3952093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4493890685999242}
episode index:877
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 20.       ,  6.1692877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5715827627244835
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.852806 , 13.968809 ,  3.4659994], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1204351032000757}
episode index:878
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.21588904, 10.68581381]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([2.7895014, 3.9888916, 1.6915992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5712436346110565
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.105272 , 13.28536  ,  0.0675289], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5553829966660264}
episode index:879
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 10.22192192]), 'previousTarget': array([11.69209979, 10.22192192]), 'currentState': array([6.       , 2.       , 6.1326838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5708960466737119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.4617    , 13.445801  ,  0.73411053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1335658851464068}
episode index:880
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 18.       ,  3.2748582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5710792558938942
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.770981 , 15.16006  ,  0.6062364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.2794082296439306}
episode index:881
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 19.       ,  2.4012961], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.570732642508241
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.346303 , 16.117867 ,  3.9649053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.996081000731214}
episode index:882
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.      , 13.      ,  4.790744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5707924304497846
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.06806   , 16.351067  ,  0.39230323], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.64130839248018}
episode index:883
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05572809, 17.52786405]), 'previousTarget': array([20.05572809, 17.52786405]), 'currentState': array([29.       , 22.       ,  0.6561683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5708175141782367
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.615246 , 15.929612 ,  3.0681708], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1147673234473456}
episode index:884
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.85504245,  8.57492926]), 'previousTarget': array([18.85504245,  8.57492926]), 'currentState': array([24.       ,  0.       ,  2.6882715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5709437707667476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.468388 , 13.191622 ,  1.5299178], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.369824533052324}
episode index:885
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.15606821, 16.09071096]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.036556 , 23.999666 ,  1.9272897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5705621914125504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.306885 , 15.766338 ,  5.8073235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.51499909400428}
episode index:886
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([27.      , 15.      ,  5.681241], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5701977906171516
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.935704 , 14.259667 ,  2.7400784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1931616705794101}
episode index:887
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 16.80941823]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 2.       , 21.       ,  2.4425037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5699322265025939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.079874 , 14.108528 ,  0.8544625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.116980346962169}
episode index:888
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([22.171024,  9.809228,  2.649995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.85255355122996}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5703716683288003
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.370813, 13.5728  ,  2.35799 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.474585806633061}
episode index:889
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.99486814, 16.20610246]), 'previousTarget': array([17.76923077, 16.15384615]), 'currentState': array([25.552372 , 21.379992 ,  2.8178463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5706779284693116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.728102, 16.827532,  4.697867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.515195444436017}
episode index:890
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.47213595, 16.05572809]), 'previousTarget': array([14.47213595, 16.05572809]), 'currentState': array([10.       , 25.       ,  1.8409775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5707958309615574
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.919028, 15.655262,  4.195552], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2640680894490228}
episode index:891
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.25900177, 10.804711  ]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([22.       ,  2.       ,  4.7130322], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5703968159579077
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.781462, 13.856154,  4.528039], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3853032426293101}
episode index:892
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.       , 8.       , 5.0291348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5707020190681272
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.327049 , 15.040917 ,  0.2547902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6741935464216887}
episode index:893
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.59148532, 15.61158342]), 'previousTarget': array([19.38476052, 16.25278872]), 'currentState': array([27.32413  , 17.90846  ,  3.0215905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5709785346369517
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.486038 , 14.938824 ,  2.8538365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.48987312574901454}
episode index:894
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07106781, 19.92893219]), 'previousTarget': array([10.07106781, 19.92893219]), 'currentState': array([ 3.      , 27.      ,  3.923208], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 269
reward sum = 0.06696800274786396
running average episode reward sum: 0.5704153943778577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.676207, 15.894347,  5.927476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8998751079863725}
episode index:895
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.88371698, 14.52057184]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([ 2.       , 13.       ,  3.3260813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5701372533959235
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.117413, 16.968111,  5.325964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1569472954527544}
episode index:896
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.03871026, 11.63778901]), 'previousTarget': array([13.03871026, 11.63778901]), 'currentState': array([8.       , 3.       , 4.9782295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5701240191541753
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.259499 , 16.034582 ,  1.4670236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.024772909996486}
episode index:897
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.20374882, 10.58480589]), 'previousTarget': array([9.07106781, 9.07106781]), 'currentState': array([2.8464592 , 3.812045  , 0.45133364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.570496244160695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.99276   , 13.374785  ,  0.46305844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9120291402290344}
episode index:898
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 21.       ,  3.2090783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5706132995610281
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.514473 , 14.88619  ,  6.2175684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.489880265022326}
episode index:899
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.86678647, 14.49011575]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([ 1.996627 , 12.883893 ,  5.8998256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5707453393235927
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.361922 , 14.250657 ,  1.3334689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8013365810431925}
episode index:900
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.10366477, 15.86197056]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.    , 20.    ,  2.3364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5705477429740191
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.018463, 14.23023 ,  5.854127], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1258018936267917}
episode index:901
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       ,  9.       ,  2.2932754], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.570656860507939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.50857 , 16.29657 ,  6.008997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9891897600160622}
episode index:902
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.76696499, 11.97054486]), 'previousTarget': array([14.76696499, 11.97054486]), 'currentState': array([14.       ,  2.       ,  3.5855875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5705619928554099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.279789  , 13.264539  ,  0.62990415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1563129430829355}
episode index:903
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.3683562 , 18.23712963]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([12.453227 , 28.05203  ,  4.8787603], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5708539749003155
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.820834, 15.564369,  5.663746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.307266106940987}
episode index:904
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 17.       ,  3.0773587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5707645122462867
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.340046 , 13.9777775,  6.2811236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2167490513979258}
episode index:905
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 11.       ,  4.1277723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5706334737672802
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.334232 , 15.911643 ,  1.7811849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.615942115996402}
episode index:906
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.36221099, 13.03871026]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([27.       ,  8.       ,  3.6224852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5710319653594961
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.961288 , 13.072026 ,  2.2103307], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7502245658544324}
episode index:907
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.48341683, 12.24097426]), 'previousTarget': array([19.48341683, 12.24097426]), 'currentState': array([28.       ,  7.       ,  3.8962154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5706445262669927
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.104176 , 15.601466 ,  2.8984742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2573644056110804}
episode index:908
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.78852131, 17.22104427]), 'previousTarget': array([13.78852131, 17.22104427]), 'currentState': array([ 9.      , 26.      ,  3.309121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5709724704765659
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.010851 , 15.390482 ,  4.3152976], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.39063268494683384}
episode index:909
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       ,  7.       ,  3.5284016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5709523726817152
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.517328 , 15.588242 ,  2.7790534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6273638833244184}
episode index:910
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 15.      ,  5.177758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5708268593722334
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.696798 , 14.094569 ,  2.1058087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9232601167307533}
episode index:911
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.35636161, 13.47409319]), 'previousTarget': array([16.35636161, 13.47409319]), 'currentState': array([23.       ,  6.       ,  1.4389985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5711439991696776
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.168509, 14.526376,  2.768957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2608457854153456}
episode index:912
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.22104427, 16.21147869]), 'previousTarget': array([17.22104427, 16.21147869]), 'currentState': array([26.        , 21.        ,  0.50305825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5708702391208222
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.451965, 13.350285,  2.819761], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1976724742853526}
episode index:913
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.41020921, 11.33345606]), 'previousTarget': array([16.41020921, 11.33345606]), 'currentState': array([20.      ,  2.      ,  6.021631], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.5703299936101102
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.700756 , 15.843087 ,  3.1588182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0962915189369624}
episode index:914
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.47213595, 16.05572809]), 'previousTarget': array([14.47213595, 16.05572809]), 'currentState': array([10.       , 25.       ,  5.9003553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5707151462885995
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.560897 , 16.859875 ,  5.3495016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9426114631182092}
episode index:915
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 18.92893219]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([26.       , 26.       ,  0.5886198], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.570899627977573
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.804655 , 16.42598  ,  3.3090124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4392975717250065}
episode index:916
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.03213268, 16.45376762]), 'previousTarget': array([10.77802414, 15.90470911]), 'currentState': array([ 1.6425154, 19.893984 ,  1.0210164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.570718420065578
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.394184, 16.044102,  4.857533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.207129355511548}
episode index:917
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.43819303, 17.12612324]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.883476 , 26.794289 ,  1.0702381], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5708553425034691
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.799807 , 16.997217 ,  5.2084875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6885275219599114}
episode index:918
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45299804, 15.67949706]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([21.       , 24.       ,  1.3824306], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5711152690820521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.978091 , 15.877485 ,  4.3157487], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3140178392774466}
episode index:919
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.        , 19.        ,  0.08906161], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5715281764525064
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.36088 , 15.803383,  6.170607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5803221628719022}
episode index:920
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([16.260756 ,  6.9829288,  1.8014042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.115598308816166}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5719506170969663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.715285 , 14.488153 ,  2.5915406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5857041425772781}
episode index:921
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.99181166, 21.83842665]), 'previousTarget': array([17.99181166, 21.83842665]), 'currentState': array([22.       , 31.       ,  0.8361839], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5719543092150573
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.211498 , 15.239761 ,  2.8261144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3197139664842002}
episode index:922
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.21665117, 13.83553059]), 'previousTarget': array([15., 15.]), 'currentState': array([8.63499  , 5.5382385, 3.8691998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5719156460687866
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.525183 , 14.194596 ,  1.8645617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.961505085124896}
episode index:923
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.349537, 10.89127 ,  2.428564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.562954687254507}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5721300697698
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.134266 , 16.837091 ,  2.3741503], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1590424062093696}
episode index:924
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 15.71523309]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.      , 25.      ,  2.381295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5718878676559866
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.92085 , 16.8573  ,  4.461633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1480520753499315}
episode index:925
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.38155366, 16.85584669]), 'previousTarget': array([18.36221099, 16.96128974]), 'currentState': array([25.269411 , 23.00253  ,  2.7838898], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5721018582371413
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.533871, 16.755898,  4.149422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.287512019798362}
episode index:926
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.13681661, 10.17821552]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([29.      ,  4.      ,  3.265516], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5723237778716203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8915415, 13.0990715,  2.5271416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.099613146338647}
episode index:927
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.52419672, 21.84589243]), 'previousTarget': array([22.41495392, 23.47423305]), 'currentState': array([28.423132, 29.085001,  4.446213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.5718666886622195
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.363796 , 13.231585 ,  2.6635692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8793744808360469}
episode index:928
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.92893219, 13.07106781]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([24.      ,  6.      ,  5.075405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.571978488834876
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.767361 , 14.122274 ,  2.6997304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1658664826390595}
episode index:929
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       ,  8.       ,  4.9147024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5719286157151886
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.560098 , 14.191309 ,  1.9681803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.651453851380983}
episode index:930
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.19231921, 13.73462344]), 'previousTarget': array([13.19231921, 13.73462344]), 'currentState': array([5.       , 8.       , 3.8999305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5719976349099777
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.211564 , 15.392243 ,  0.7780263], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8806168851769275}
episode index:931
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.16732519, 12.55272358]), 'previousTarget': array([19.48341683, 12.24097426]), 'currentState': array([26.08042  ,  6.438565 ,  3.1927989], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.572193688184971
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.911568  , 13.487914  ,  0.90493846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.43731303671242}
episode index:932
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.      , 12.      ,  4.346828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5725304418650686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.778282 , 13.948551 ,  2.4569218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0745712742548124}
episode index:933
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.6623494 , 13.57662651]), 'previousTarget': array([ 9.6623494 , 13.57662651]), 'currentState': array([ 0.    , 11.    ,  3.7436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5726556224261008
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.70604  , 14.072488 ,  0.5491086], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9729805137908077}
episode index:934
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.86393924, 15.35601013]), 'previousTarget': array([12.86393924, 15.35601013]), 'currentState': array([ 3.       , 17.       ,  2.4659326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5724080400080132
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.228516 , 15.470831 ,  4.7857018], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3156490389194573}
episode index:935
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.27623315, 17.76737699]), 'previousTarget': array([14.41421356, 19.10050506]), 'currentState': array([11.745984 , 27.441975 ,  2.9688833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5725257179513917
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.655085 , 13.980826 ,  3.6500034], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2115488922400082}
episode index:936
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84615385, 12.23076923]), 'previousTarget': array([13.84615385, 12.23076923]), 'currentState': array([10.      ,  3.      ,  4.038842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5721786666898575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.972824 , 15.757173 ,  1.4786274], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1131363513056733}
episode index:937
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 7.999771  , 19.96975   ,  0.47257632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.584964414641444}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5720410007143099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.782677 , 16.55029  ,  5.0924172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7366575001570796}
episode index:938
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.45069462,  7.55689083]), 'previousTarget': array([21.45069462,  7.55689083]), 'currentState': array([28.       ,  0.       ,  0.4594146], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.57225186561856
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.088302 , 13.864114 ,  2.9922814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1393132250720783}
episode index:939
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 23.       ,  1.1919339], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5720650847045211
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.426003 , 16.034975 ,  4.0596337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8837830231142374}
episode index:940
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.      , 21.      ,  4.538678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5722353698107377
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.166309, 16.330338,  6.056305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.265440200982755}
episode index:941
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99503719, 15.0496281 ]), 'previousTarget': array([14.99503719, 15.0496281 ]), 'currentState': array([14.      , 25.      ,  5.564809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5726173549254897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.656466 , 16.080597 ,  4.3138633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2643720593180876}
episode index:942
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.        , 19.        ,  0.92993003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5729691732924921
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.957258 , 16.347366 ,  3.6139848], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3761851343374145}
episode index:943
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.      , 12.      ,  5.074391], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5731861777269271
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.114816  , 15.537904  ,  0.17960191], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9604234956999311}
episode index:944
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.33266  , 18.027859 ,  5.6843224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.456580986839372}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5736272505547293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.984613 , 16.900452 ,  5.0595446], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9005139461415348}
episode index:945
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 23.       ,  1.8907092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5739768856757167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.229618 , 16.064241 ,  3.8719196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0887305619370162}
episode index:946
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.71390676, 20.71523309]), 'previousTarget': array([12.71390676, 20.71523309]), 'currentState': array([ 9.       , 30.       ,  1.3529264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.574260905008038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.477526 , 16.243881 ,  5.5244904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3323930756680336}
episode index:947
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 23.       ,  0.5537115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5746091340903172
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.152786 , 16.021435 ,  4.4374623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1108120802154136}
episode index:948
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.        , 19.        ,  0.02770245], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 391
reward sum = 0.01964993362138638
running average episode reward sum: 0.5740243509496756
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.58388  , 15.5288925,  4.2436604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5116619816444794}
episode index:949
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.45942241, 20.36613715]), 'previousTarget': array([10.45942241, 20.36613715]), 'currentState': array([ 4.       , 28.       ,  1.6682262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5736831011660006
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.735098 , 13.462034 ,  2.4450166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9913101425550168}
episode index:950
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 13.19058177]), 'previousTarget': array([11.07959385, 13.19058177]), 'currentState': array([2.       , 9.       , 2.2863438], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5736909700544887
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.870932  , 15.478448  ,  0.31381845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.226257640598968}
episode index:951
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 16.52786405]), 'previousTarget': array([18.05572809, 16.52786405]), 'currentState': array([27.       , 21.       ,  5.5227914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5733965706337646
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.960255 , 14.250566 ,  1.7638707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7504876533559355}
episode index:952
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.23076923, 16.15384615]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 3.       , 20.       ,  4.4092555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.573128566954001
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.17832  , 15.048922 ,  4.8085747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1793350589846054}
episode index:953
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.06404996, 20.08636336]), 'previousTarget': array([ 8.06404996, 20.08636336]), 'currentState': array([ 0.       , 26.       ,  1.5048866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5730787429713717
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.2085905, 15.271266 ,  4.048697 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8118314501192785}
episode index:954
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 13.       ,  0.6674673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5731721495661494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.952774 , 14.167219 ,  5.8598886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3379858441008743}
episode index:955
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.96128974, 11.63778901]), 'previousTarget': array([16.96128974, 11.63778901]), 'currentState': array([22.        ,  3.        ,  0.38057202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5734722397388221
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.454746 , 14.156682 ,  0.2440989], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9581124011746494}
episode index:956
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 2.      , 15.      ,  4.183356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5737807805674848
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.779081 , 14.694687 ,  1.1037052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3768569326074611}
episode index:957
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.02945514, 14.76696499]), 'previousTarget': array([18.02945514, 14.76696499]), 'currentState': array([28.       , 14.       ,  1.4438207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5740186196165157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.757984 , 16.621767 ,  3.2286599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0427265329978868}
episode index:958
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  7.       ,  3.4961295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5742393281891859
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.059298 , 16.363314 ,  1.2538522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6563650726118284}
episode index:959
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       ,  6.       ,  4.8949175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5742841727858037
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.506836 , 13.605252 ,  1.7950226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.053259794627658}
episode index:960
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 11.69209979]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([2.       , 6.       , 1.8520873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5746467747854312
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.328329 , 13.8536415,  0.8322855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.026973465852693}
episode index:961
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 11.      ,  4.128585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5749801339117029
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.904492 , 13.254527 ,  3.0723693], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7480839163058208}
episode index:962
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.766391 , 18.425768 ,  3.5145354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.510447343837479}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5753805657664156
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.860194 , 16.642847 ,  4.4381113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6487850455175828}
episode index:963
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.30393187, 16.15103807]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([27.747263 , 19.440947 ,  3.9128997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5752817700099893
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.366067 , 13.093169 ,  3.0649803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0094463880897804}
episode index:964
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.52786405, 18.05572809]), 'previousTarget': array([16.52786405, 18.05572809]), 'currentState': array([21.       , 27.       ,  2.3814728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5753515418566988
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.359778 , 15.770738 ,  4.4803596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8122812710149343}
episode index:965
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.58821525, 18.59242409]), 'previousTarget': array([20.58821525, 18.59242409]), 'currentState': array([29.       , 24.       ,  3.2173398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5756373661105685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.890135, 14.124887,  4.768547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0828901919403777}
episode index:966
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([22.033112 ,  8.637578 ,  3.2725856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.483937576748277}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5758225594105614
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.142468 , 13.163304 ,  2.3390343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.027020311493444}
episode index:967
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42173715, 14.87347886]), 'previousTarget': array([15.42173715, 14.87347886]), 'currentState': array([25.      , 12.      ,  4.003448], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5758590208569687
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.348442 , 14.748853 ,  2.1135538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6982855272197698}
episode index:968
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 21.       ,  1.5680182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5756540442068826
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.665245 , 15.608853 ,  4.1581426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4670627627801964}
episode index:969
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72672794, 11.39940073]), 'previousTarget': array([11.72672794, 11.39940073]), 'currentState': array([5.     , 4.     , 3.88202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5755313181280547
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.654405 , 14.796453 ,  1.9817257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.401082709885139}
episode index:970
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       ,  9.       ,  2.8975413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5758067165577724
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.285457 , 13.860411 ,  1.7413405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1747975561877297}
episode index:971
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      ,  6.      ,  4.700191], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5757829272168307
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.325096 , 15.859853 ,  1.8249629], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5796286198429426}
episode index:972
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.54700196, 14.32050294]), 'previousTarget': array([14.54700196, 14.32050294]), 'currentState': array([9.       , 6.       , 3.5736146], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5758129612763586
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.341138  , 13.002447  ,  0.55265284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0264729291431864}
episode index:973
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.162241 , 13.789076 ,  3.4793248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.3861638061613455}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5762382046426047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.275225 , 13.126379 ,  3.5626361], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2664186730689835}
episode index:974
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      , 17.      ,  2.437322], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5763833249789694
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.928427 , 16.257988 ,  5.3089957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3024689898766555}
episode index:975
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 21.       ,  1.8989432], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5762288237478319
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.457829 , 16.270735 ,  5.2913995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.350694165015642}
episode index:976
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.1768175 , 22.68944732]), 'previousTarget': array([22.1768175 , 22.68944732]), 'currentState': array([29.      , 30.      ,  4.437458], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.576130474344376
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6977215, 16.533983 ,  3.3466825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6852061646082799}
episode index:977
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.15972193, 18.50453709]), 'previousTarget': array([10.75724629, 19.6284586 ]), 'currentState': array([ 5.863359, 26.273438,  5.050316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5764848856123551
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.190598 , 15.091194 ,  0.2090652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8145236185567327}
episode index:978
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24859507, 18.25608804]), 'previousTarget': array([14.24859507, 18.25608804]), 'currentState': array([12.       , 28.       ,  2.2163298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5762624784745304
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.738794 , 15.909727 ,  5.6382947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9623988129483791}
episode index:979
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.64763821, 14.63117406]), 'previousTarget': array([13.64763821, 14.63117406]), 'currentState': array([ 4.       , 12.       ,  2.2451453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5763368051580955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.4235325, 13.836098 ,  5.231189 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9595709598088675}
episode index:980
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.58504608, 14.52576695]), 'previousTarget': array([14.58504608, 14.52576695]), 'currentState': array([8.       , 7.       , 3.5323474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5763785516777532
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.452659 , 13.359666 ,  5.5112367], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.254985875607577}
episode index:981
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.7214375 , 13.03436918]), 'previousTarget': array([16.63124508, 12.43661488]), 'currentState': array([23.309757,  5.511468,  1.045654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.576426562719722
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.72728  , 16.55423  ,  3.6839159], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3236017532694504}
episode index:982
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.        , 17.        ,  0.05327815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5768372172846054
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.895353  , 16.476532  ,  0.08581751], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4802356647922665}
episode index:983
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.      , 20.      ,  2.857081], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5769106577430239
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5384445, 16.76098  ,  4.412228 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.288492324414102}
episode index:984
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0038779 , 15.08589388]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([15.454894 , 25.075718 ,  4.2933483], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5772904337756706
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.877215 , 15.815052 ,  4.6317606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3874275139026118}
episode index:985
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.90289239, 20.07376011]), 'previousTarget': array([18.90289239, 20.07376011]), 'currentState': array([25.       , 28.       ,  2.6195695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5772326752451173
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.969328, 15.398446,  5.63679 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1050086895250577}
episode index:986
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86197056, 13.10366477]), 'previousTarget': array([15.86197056, 13.10366477]), 'currentState': array([20.       ,  4.       ,  3.8246484], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5773605526714137
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.356074 , 16.29944  ,  3.3282318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3473434016970094}
episode index:987
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 13.       ,  1.6806321], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5774200920776813
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.474138 , 13.159776 ,  2.1574469], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9003242983388018}
episode index:988
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.76923077, 13.84615385]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([27.       , 10.       ,  4.8142676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.577262270166738
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.138664 , 14.28522  ,  2.4963794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.344420508489943}
episode index:989
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.026495,  8.324465,  2.967668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.329559474934445}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5773826246595489
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.482214  , 14.005852  ,  0.44089562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7847378004720107}
episode index:990
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 20.       ,  1.9384423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5773521241729003
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.793937 , 15.588797 ,  4.8414764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9884416952367697}
episode index:991
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 0.      , 15.      ,  4.556138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.5768960008746
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.103012  , 14.167091  ,  0.59468013], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0717866531935427}
episode index:992
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.74721128, 10.61523948]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([11.      ,  1.      ,  4.749651], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5768829184874679
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.79828  , 16.69802  ,  3.0322847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.876306391489413}
episode index:993
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.03237976, 14.2490939 ]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([26.412611, 10.783367,  3.810026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.577185371308908
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.238494 , 13.199027 ,  3.0671089], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8166955922560153}
episode index:994
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 0.      , 15.      ,  4.725671], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5774524645974256
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.695534  , 15.3696575 ,  0.29717267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3558314314506006}
episode index:995
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 19.       ,  1.3816869], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5773357673111009
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.23759 , 15.894116,  5.138851], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1750376154515823}
episode index:996
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.58256937, 13.39793423]), 'currentState': array([18.549274  ,  5.9485497 ,  0.96097636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.722453433008983}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.577351445023328
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.511483, 13.409186,  1.220878], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.178616605403917}
episode index:997
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.79113424, 16.96461017]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([23.52841  , 24.354408 ,  3.2890935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5774233399966197
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.893503, 15.725148,  5.472961], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.027608009571762}
episode index:998
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.      , 14.      ,  2.581565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5777415731440292
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.07619  , 15.685768 ,  0.1930117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0423816638121046}
episode index:999
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.15810839, 20.78186395]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([10.122768 , 30.31007  ,  3.7119102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5775119246853776
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.119827 , 14.161905 ,  6.1850386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0585072838931984}
episode index:1000
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 21.       ,  6.0074697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5778946260693083
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.88905  , 16.797142 ,  4.9608116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.112801556805181}
episode index:1001
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.7671853 , 10.59064624]), 'previousTarget': array([17.77114535,  9.0618314 ]), 'currentState': array([20.48734  ,  1.3083813,  3.081025 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5780487266118084
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.686846  , 14.607538  ,  0.89312005], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7910647071525531}
episode index:1002
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.75304952, 12.80868809]), 'previousTarget': array([16.75304952, 12.80868809]), 'currentState': array([23.       ,  5.       ,  3.7902107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5779368941577286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.903617, 13.710324,  1.45808 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6927254160580258}
episode index:1003
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.89633523, 15.86197056]), 'previousTarget': array([16.89633523, 15.86197056]), 'currentState': array([26.      , 20.      ,  5.089677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5780275663334377
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.034761, 13.557037,  3.069085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7756329614393058}
episode index:1004
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.13681661, 10.17821552]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([29.        ,  4.        ,  0.44741648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5781958693463712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.47771  , 14.36509  ,  2.2882235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6083343002937465}
episode index:1005
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897,  9.34803445]), 'previousTarget': array([20.21719897,  9.34803445]), 'currentState': array([27.      ,  2.      ,  5.460233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5783133816214241
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.349715, 14.363449,  4.037231], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7262904354351781}
episode index:1006
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.733805, 21.451857,  3.954451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.002209739854804}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5786646745372986
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.76946  , 16.730728 ,  2.4862504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.123593486753832}
episode index:1007
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 13.       ,  1.1944201], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5788622506135505
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.14811 , 13.139895,  3.077899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1858966705714993}
episode index:1008
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.0759934, 16.5800979]), 'previousTarget': array([18.67949706, 17.45299804]), 'currentState': array([25.033287 , 22.636621 ,  3.5064855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5791073510825514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.840368 , 14.392346 ,  3.5558932], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9380914043813406}
episode index:1009
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897, 20.65196555]), 'previousTarget': array([20.21719897, 20.65196555]), 'currentState': array([27.       , 28.       ,  5.2708235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.5787424907806843
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.493661 , 15.355099 ,  3.3017945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6081087194848142}
episode index:1010
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.        , 22.        ,  0.53451777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.5784122462784629
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.974342, 14.521541,  5.853363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1317671934987232}
episode index:1011
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.52590681, 16.35636161]), 'previousTarget': array([16.52590681, 16.35636161]), 'currentState': array([24.       , 23.       ,  6.2491736], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5781916420970663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.755856 , 14.558046 ,  3.0764606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8106219333147509}
episode index:1012
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 15.       ,  5.1149745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5783014717552816
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.896942 , 14.626192 ,  3.6728573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9334223100199472}
episode index:1013
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.71523309, 14.71390676]), 'currentState': array([23.342508 , 12.119249 ,  2.6107953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.825880655548131}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5786230502594764
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.386713 , 15.3319025,  3.3916433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4258794808949973}
episode index:1014
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 11.69209979]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([2.      , 6.      , 6.027503], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.578917531020796
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.195856  , 15.370414  ,  0.14508843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.885355177060952}
episode index:1015
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([3.       , 6.       , 4.6670833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5786483789488546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.081167, 16.439718,  5.854541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8004752675625775}
episode index:1016
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.54700196, 15.67949706]), 'previousTarget': array([14.54700196, 15.67949706]), 'currentState': array([ 9.      , 24.      ,  0.990231], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5789336271630336
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.525126, 16.528496,  5.546351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6005637311044234}
episode index:1017
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07106781, 19.92893219]), 'previousTarget': array([10.07106781, 19.92893219]), 'currentState': array([ 3.       , 27.       ,  3.2062058], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5787506973017319
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.01153 , 14.06142 ,  4.468721], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3799001316284205}
episode index:1018
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.81238194,  9.13733471]), 'previousTarget': array([10.81238194,  9.13733471]), 'currentState': array([5.      , 1.      , 3.915748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5788525656998758
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.390232 , 14.041082 ,  0.8363559], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6888659660174525}
episode index:1019
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 21.       ,  6.2002654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5790869327311478
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.606705, 14.186771,  2.976948], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8007889378878048}
episode index:1020
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897,  9.34803445]), 'previousTarget': array([20.21719897,  9.34803445]), 'currentState': array([27.       ,  2.       ,  4.7690735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5791366258736277
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.077118, 13.167415,  3.089335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0518480311592535}
episode index:1021
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.41495392, 23.47423305]), 'previousTarget': array([22.41495392, 23.47423305]), 'currentState': array([29.       , 31.       ,  4.1046124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5794285871232612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.863504, 16.136166,  4.491569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1443353353406496}
episode index:1022
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.8386991 , 15.21114562]), 'previousTarget': array([13.8386991 , 15.21114562]), 'currentState': array([ 4.       , 17.       ,  3.3941073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5794716925071985
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.146085 , 13.583022 ,  5.7673836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.333415509905299}
episode index:1023
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.23250512, 12.40565406]), 'previousTarget': array([ 9.92623989, 11.09710761]), 'currentState': array([2.9963677 , 6.734143  , 0.57331455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5792491705482637
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.850086 , 14.139574 ,  0.9423574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.040380309230809}
episode index:1024
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.66654394, 13.58979079]), 'previousTarget': array([18.66654394, 13.58979079]), 'currentState': array([28.      , 10.      ,  5.905725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 233
reward sum = 0.09616130339314856
running average episode reward sum: 0.5787778653120149
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.808155 , 15.586042 ,  2.6187644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9007552235423673}
episode index:1025
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.04936172, 10.32221908]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([12.057829  ,  0.52253556,  3.179828  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5787916943384179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.538412  , 13.464234  ,  0.69366133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.173772733419121}
episode index:1026
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.2781037 ,  8.94029042]), 'previousTarget': array([12.22885465,  9.0618314 ]), 'currentState': array([6.04443   , 0.41921905, 3.8779836 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5786956376317315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.585341 , 15.971453 ,  1.1269226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0562490633188226}
episode index:1027
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.      , 18.      ,  6.018373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5790124532322929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.717716 , 16.133217 ,  5.5855675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0578458273975397}
episode index:1028
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.99503719, 14.9503719 ]), 'currentState': array([15.152671 ,  6.6344266,  0.7624039], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.366966404045643}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5793464981702867
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.483346 , 14.220578 ,  2.3673897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.705209005417019}
episode index:1029
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.22885465, 20.9381686 ]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 8.       , 30.       ,  3.0247433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5791086635671018
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.73088 , 15.474553,  5.748066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7947559107826476}
episode index:1030
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.86825693,  7.87496075]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([27.225702  ,  0.15596545,  4.1275735 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5790776722759511
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.949389 , 13.799453 ,  2.5493565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5953357436630264}
episode index:1031
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64398987, 12.86393924]), 'previousTarget': array([14.64398987, 12.86393924]), 'currentState': array([13.        ,  3.        ,  0.78883886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5793928897204597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.611048, 14.025371,  2.162986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6967883895516742}
episode index:1032
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.14357069, 14.74157276]), 'previousTarget': array([15.14357069, 14.74157276]), 'currentState': array([20.       ,  6.       ,  1.3584592], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.579623784248898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.938673 , 15.349095 ,  1.6137553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9698529477519433}
episode index:1033
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.91512103, 18.16196238]), 'previousTarget': array([18.26042701, 17.6676221 ]), 'currentState': array([24.693384 , 25.514183 ,  2.2760553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.5792108789761969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.246339 , 16.883875 ,  3.2074804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.899912482036111}
episode index:1034
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58578644, 19.10050506]), 'previousTarget': array([15.58578644, 19.10050506]), 'currentState': array([17.       , 29.       ,  6.1835575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5792358018633094
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.632097 , 14.407023 ,  4.6206627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7364799530810329}
episode index:1035
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 17.       ,  2.2730014], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5789433461353517
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.946897 , 15.7143135,  4.4059906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1861099732129494}
episode index:1036
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.95211064, 12.89384623]), 'previousTarget': array([11.73957299, 12.3323779 ]), 'currentState': array([5.980907 , 5.724305 , 1.0618702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5792748807045828
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.142777  , 15.065188  ,  0.67766243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.858366259650124}
episode index:1037
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.      , 17.      ,  5.626653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5795047767131499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.216429, 13.424021,  4.229087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9908313495471606}
episode index:1038
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.07106781, 10.07106781]), 'previousTarget': array([10.07106781, 10.07106781]), 'currentState': array([3.       , 3.       , 4.9847765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5793028757130888
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.639799, 15.925468,  5.835349], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6451863972342768}
episode index:1039
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 21.       ,  4.6564417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5792029114499085
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.145458 , 15.58322  ,  1.7814264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0345949300977069}
episode index:1040
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.27525542, 14.46125043]), 'previousTarget': array([19.22197586, 14.09529089]), 'currentState': array([27.00618  , 12.1571   ,  3.2054935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5795329227688116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.968252 , 15.857502 ,  2.6308475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1469341638545014}
episode index:1041
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.9850674, 11.243941 ,  0.4523918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.957214132000649}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5799079381980161
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.579248 , 13.190109 ,  0.8878838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.300921455567674}
episode index:1042
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.65380251, 15.01469151]), 'previousTarget': array([11.97054486, 14.76696499]), 'currentState': array([ 3.654398 , 15.123818 ,  1.1278315], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5796361056469147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.425792 , 14.13249  ,  5.5146265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7974161928624264}
episode index:1043
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.66528823, 22.75958076]), 'previousTarget': array([ 9.66528823, 22.75958076]), 'currentState': array([ 4.       , 31.       ,  1.3110611], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5797346865754237
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.621884, 16.820015,  5.054749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9233290335213387}
episode index:1044
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 13.       ,  4.8885617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5800718451030138
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.195263 , 14.603542 ,  1.4694037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2592982598491882}
episode index:1045
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.62537819, 22.38379986]), 'previousTarget': array([19.7000106 , 22.52001696]), 'currentState': array([23.032701, 31.360186,  2.925859], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.5796296827093612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.337755 , 15.813546 ,  3.2685916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5657095426633267}
episode index:1046
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.40757591, 20.58821525]), 'previousTarget': array([11.40757591, 20.58821525]), 'currentState': array([ 6.      , 29.      ,  4.753901], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5797897041149221
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.165098 , 13.264207 ,  6.1646028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9261460914025843}
episode index:1047
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.75140493, 18.25608804]), 'previousTarget': array([15.75140493, 18.25608804]), 'currentState': array([18.      , 28.      ,  5.939984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5800654255926455
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.733139 , 14.908313 ,  4.1982427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.735562579808409}
episode index:1048
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.00999094, 15.34254574]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 4.5596886, 18.612375 ,  3.8575706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5799433838621153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.098727, 15.49789 ,  5.083978], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0296540852191913}
episode index:1049
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.58258088, 16.63663603]), 'previousTarget': array([19.58258088, 16.63663603]), 'currentState': array([29.       , 20.       ,  1.0872427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.5795455043441619
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.136192, 16.008183,  4.361453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.017339841698141}
episode index:1050
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91227901, 14.3216372 ]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([ 0.       , 13.       ,  1.9270152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.5791128994991712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.173963, 15.914142,  0.60007 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.232068489566031}
episode index:1051
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.65835043, 12.2042076 ]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([3.9886642, 5.7873616, 5.873657 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5792655491891798
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.710909 , 14.812162 ,  1.6972799], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.721189179996055}
episode index:1052
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.94427191, 17.52786405]), 'previousTarget': array([ 9.94427191, 17.52786405]), 'currentState': array([ 1.        , 22.        ,  0.75525534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5790561310965804
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.641541 , 15.414447 ,  4.3410716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4202740279142654}
episode index:1053
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.09793891, 13.02756145]), 'previousTarget': array([18.60059927, 11.72672794]), 'currentState': array([24.383564 ,  6.177768 ,  2.2555423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5790417563900868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.608416 , 15.660147 ,  2.0173664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7386185427082659}
episode index:1054
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 23.       ,  3.1896791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.579139872824798
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.758164 , 13.7725315,  4.859082 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7460912170854175}
episode index:1055
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.7012257 , 19.22005446]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 7.9176345, 28.001698 ,  3.9633818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5790841916219812
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.525895 , 13.534993 ,  5.6566715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0782757776634018}
episode index:1056
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.11828302, 12.08736084]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([23.       ,  4.       ,  5.6781697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5790698311667593
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.936777 , 13.028078 ,  2.2443926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7639793151026666}
episode index:1057
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.22885465, 20.9381686 ]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 8.       , 30.       ,  1.5483017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.5786466210043409
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.997388, 16.037792,  5.64165 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0377954931393862}
episode index:1058
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.804711  , 12.74099823]), 'previousTarget': array([10.804711  , 12.74099823]), 'currentState': array([2.       , 8.       , 5.1513753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5787711787061993
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.704969  , 13.895762  ,  0.17781231], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.310084899793765}
episode index:1059
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1194175 , 11.93103794]), 'previousTarget': array([15.33480989,  9.97785158]), 'currentState': array([15.508237 ,  1.9385998,  1.7741055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5790869768842912
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504473 , 13.212431 ,  0.7273219], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8549798832017375}
episode index:1060
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.      ,  9.      ,  5.831659], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5790029083603704
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.475356 , 14.6808405,  2.1546104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5576911783559855}
episode index:1061
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 11.       ,  0.1457572], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5788876605631796
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.523818 , 16.837276 ,  3.3140671], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3568406894766953}
episode index:1062
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.823702 , 17.007786 ,  5.1677513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.464750495046579}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5788085928294422
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.90549 , 13.420056,  2.293989], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9220234834234866}
episode index:1063
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.80868809, 16.75304952]), 'previousTarget': array([12.80868809, 16.75304952]), 'currentState': array([ 5.       , 23.       ,  2.0328295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.578768611280925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.065043  , 13.958391  ,  0.15716046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.197499890024399}
episode index:1064
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.25900177, 10.804711  ]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([22.      ,  2.      ,  6.077611], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5790009117621969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.255716 , 16.438486 ,  1.6223222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9094673979163794}
episode index:1065
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.63386285, 10.45942241]), 'previousTarget': array([ 9.63386285, 10.45942241]), 'currentState': array([2.       , 4.       , 4.1332827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5787560601224755
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.601291  , 15.740786  ,  0.40543145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5827668815529632}
episode index:1066
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 14.8304548 ]), 'previousTarget': array([12.96545758, 14.8304548 ]), 'currentState': array([ 3.       , 14.       ,  1.3630639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5790278405841873
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.43550444e+01, 1.39358559e+01, 1.05611645e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.244335368880134}
episode index:1067
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.90289239,  9.92623989]), 'previousTarget': array([18.90289239,  9.92623989]), 'currentState': array([25.       ,  2.       ,  4.3559446], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5787243592359778
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.082823 , 13.973724 ,  2.1501296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.376392275223753}
episode index:1068
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.      , 27.      ,  1.473522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5786948290986109
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.393076 , 13.375642 ,  4.2533765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.139906556542659}
episode index:1069
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.29411765,  8.82352941]), 'previousTarget': array([18.29411765,  8.82352941]), 'currentState': array([23.      ,  0.      ,  3.994392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.578373821435192
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.71436  , 16.104961 ,  1.4065166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.03959982294686}
episode index:1070
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.46979601, 17.71386605]), 'previousTarget': array([20.07376011, 18.90289239]), 'currentState': array([26.346638 , 23.874659 ,  2.9902961], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5784972382473649
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.01947 , 15.035537,  4.185811], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0200893983837955}
episode index:1071
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57492926, 18.85504245]), 'previousTarget': array([ 8.57492926, 18.85504245]), 'currentState': array([ 0.      , 24.      ,  3.705671], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5782823090274444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.582645 , 14.382297 ,  1.1731673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5461085191944923}
episode index:1072
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      , 13.      ,  5.213267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5783668285517148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.522045 , 14.082863 ,  0.6906247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7770092681410323}
episode index:1073
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 24.       ,  1.8145665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5782577551242161
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.40185  , 15.719099 ,  5.8432927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5755272604961}
episode index:1074
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.004292, 14.130955,  2.353774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8690558537687707}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5786500734915424
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.004292, 14.130955,  2.353774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8690558537687707}
episode index:1075
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.71523309, 15.28609324]), 'previousTarget': array([15.71523309, 15.28609324]), 'currentState': array([25.        , 19.        ,  0.06387435], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5786976325600476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.873464 , 16.68383  ,  3.3194728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8968982742228535}
episode index:1076
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.92623989, 11.09710761]), 'previousTarget': array([ 9.92623989, 11.09710761]), 'currentState': array([2.       , 5.       , 3.9046545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.5782340743605402
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.071505 , 14.4821205,  4.8959618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9968208976595105}
episode index:1077
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.        , 17.        ,  0.34626198], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5783245149678683
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.130816, 15.785183,  4.837905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0274021251316983}
episode index:1078
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.99051548, 20.27890585]), 'previousTarget': array([20.08636336, 21.93595004]), 'currentState': array([25.020779 , 28.256119 ,  3.5372338], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.578215986193494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.615187 , 15.240353 ,  3.5841744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6329719638576776}
episode index:1079
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.3508159 , 18.26025539]), 'previousTarget': array([12.47213595, 20.05572809]), 'currentState': array([ 8.837003, 27.183567,  4.940951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5785523419001676
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.063807 , 16.681627 ,  5.1964383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9246632155547971}
episode index:1080
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 17.       ,  4.9647512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5788047983564069
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.135329 , 14.117852 ,  4.9165936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2352490573006405}
episode index:1081
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.53288514, 12.28033378]), 'previousTarget': array([ 9.92623989, 11.09710761]), 'currentState': array([3.6647482 , 6.108428  , 0.35955983], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5789744267445683
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.226053, 13.332151,  2.351538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.070006147691445}
episode index:1082
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.35423809, 11.21863461]), 'previousTarget': array([18.30790021, 10.22192192]), 'currentState': array([24.990164 ,  3.7376928,  1.3074491], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5788448266292086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.422274, 16.810194,  2.384741], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8587946086936664}
episode index:1083
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 3.       , 24.       ,  2.3570433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5785459935425545
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.927547, 16.407944,  5.249378], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.386994915171329}
episode index:1084
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.2544106, 22.0506009]), 'previousTarget': array([21.69407375, 23.23886   ]), 'currentState': array([26.22997  , 30.068872 ,  2.7558906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.578257348617398
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.742111, 16.813505,  5.593214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5147072326173365}
episode index:1085
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.24275371, 10.3715414 ]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([26.      ,  3.      ,  6.039422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5780266537820576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.417802 , 16.921642 ,  2.6026235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0079003511609037}
episode index:1086
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.     , 12.     ,  5.54941], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5780627747454065
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.326128 , 13.973231 ,  1.0007873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9636958480370343}
episode index:1087
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.20119301, 14.3047183 ]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([ 3.873714  , 10.699426  ,  0.24544847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5783967980677002
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.324364  , 13.761304  ,  0.22660017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0837766208058563}
episode index:1088
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32050294, 10.54700196]), 'previousTarget': array([ 8.32050294, 10.54700196]), 'currentState': array([0.       , 5.       , 5.7606354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5784557648298829
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.498773 , 14.390997 ,  1.5967437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.78718413554215}
episode index:1089
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07959385, 16.80941823]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 2.        , 21.        ,  0.82189804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.5781612958775288
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.584256, 13.963512,  5.183207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7546045389625105}
episode index:1090
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.58821525, 11.40757591]), 'previousTarget': array([20.58821525, 11.40757591]), 'currentState': array([29.        ,  6.        ,  0.19790334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5777566567474455
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.157028, 13.737785,  2.772133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7122792123580637}
episode index:1091
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.39136981, 13.74263919]), 'previousTarget': array([ 9.48683298, 13.16227766]), 'currentState': array([ 1.9481775, 10.4523325,  5.5871515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5776057345994478
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.133286 , 14.586645 ,  1.8835126], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9119313878263833}
episode index:1092
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45221754, 13.38450655]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([18.14785  ,  3.7546797,  2.3270447], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.5771735837918073
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.279058, 13.653149,  1.706951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.185325685656051}
episode index:1093
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       ,  5.       ,  2.2179468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5774979821136677
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.358685  , 13.601396  ,  0.47619405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.156388343513786}
episode index:1094
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.09529089, 10.77802414]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([12.     ,  1.     ,  4.52644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.577315095049567
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.43367   , 16.079008  ,  0.96659905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9020115710257386}
episode index:1095
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.75304952,  7.80868809]), 'previousTarget': array([20.75304952,  7.80868809]), 'currentState': array([27.      ,  0.      ,  5.603028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5770008152757518
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.736034 , 16.869816 ,  4.3046894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2569493303150705}
episode index:1096
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.20719652, 22.79698364]), 'previousTarget': array([20.75304952, 22.19131191]), 'currentState': array([28.995071  , 30.140331  ,  0.14968896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.5766980481688777
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.872786, 15.56071 ,  4.05377 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0373766432025027}
episode index:1097
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.352796  , 16.473074  ,  0.56937885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.836168641757329}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5764400560681093
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.609133 , 16.530048 ,  5.3244233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0677426523257707}
episode index:1098
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.92893219,  9.07106781]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([28.        ,  2.        ,  0.44513753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 235
reward sum = 0.0942476934556249
running average episode reward sum: 0.5760013005061325
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.684136 , 15.267219 ,  5.2431173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7344714941959137}
episode index:1099
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.        , 16.        ,  0.65469456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5761044348564625
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.545414, 15.821597,  4.365565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6705814285752358}
episode index:1100
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.41548708, 11.96076093]), 'previousTarget': array([16.63663603, 10.41741912]), 'currentState': array([20.637423 ,  2.8957038,  1.3814356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5764192761458098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.24986  , 13.419103 ,  2.3049252], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7498419005026753}
episode index:1101
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 14.       ,  2.3118615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5765281635704048
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.591988  , 16.095583  ,  0.42409065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7840406154239823}
episode index:1102
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.60059927, 11.72672794]), 'previousTarget': array([18.60059927, 11.72672794]), 'currentState': array([26.        ,  5.        ,  0.32724872], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.576361511589251
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.849361 , 13.43026  ,  2.0526567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4257415887985285}
episode index:1103
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.37208221, 15.5067145 ]), 'previousTarget': array([12.80868809, 16.75304952]), 'currentState': array([ 6.589946 , 21.78671  ,  5.8519597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.576692234993066
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.561235 , 16.7671   ,  5.8220706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.820756592822786}
episode index:1104
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.40743398,  9.50791373]), 'previousTarget': array([21.40743398,  9.50791373]), 'currentState': array([29.        ,  3.        ,  0.22287633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5767289751794454
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.96813 , 15.703654,  2.846588], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1968313464908185}
episode index:1105
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.87347886, 15.42173715]), 'currentState': array([11.985695, 23.000051,  4.555984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.549085290371474}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5770673667479088
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.608184 , 15.417343 ,  5.1387196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.453040763918273}
episode index:1106
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.75724629, 10.3715414 ]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.      , 3.      , 4.087239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.576774045425724
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.79884  , 13.44886  ,  1.0690062], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9618412672228938}
episode index:1107
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.07106781, 22.92893219]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([ 0.       , 30.       ,  1.3898895], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5766224625096561
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.484006, 13.045577,  6.113343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.013462393186118}
episode index:1108
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.74391196, 15.75140493]), 'previousTarget': array([11.74391196, 15.75140493]), 'currentState': array([ 2.        , 18.        ,  0.49372876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5769262450028698
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.883786, 16.145027,  5.833469], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1509095731554282}
episode index:1109
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.3143151 , 15.71057421]), 'previousTarget': array([16.21147869, 17.22104427]), 'currentState': array([19.35962  , 24.85582  ,  4.1847377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5772461901406214
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.70455 , 16.939777,  4.354095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.332579582734449}
episode index:1110
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.10366477, 15.86197056]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.       , 20.       ,  1.6122638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5769828792343501
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.335756  , 16.448792  ,  0.07647579], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5938056695479605}
episode index:1111
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 19.77807808]), 'previousTarget': array([11.69209979, 19.77807808]), 'currentState': array([ 6.       , 28.       ,  1.7890182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5768746300153839
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.536893 , 16.427916 ,  2.8706596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5255152769770333}
episode index:1112
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.48683298, 16.83772234]), 'previousTarget': array([ 9.48683298, 16.83772234]), 'currentState': array([ 0.       , 20.       ,  1.2727945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5771447525607419
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.012   , 15.446736,  5.135465], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0375762611505053}
episode index:1113
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 10.       ,  0.7296498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5771863868895843
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.814921 , 13.488201 ,  1.5582684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.717449458671171}
episode index:1114
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.16227766, 12.48683298]), 'previousTarget': array([14.16227766, 12.48683298]), 'currentState': array([11.      ,  3.      ,  3.937899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5771215433935283
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.406046  , 16.65111   ,  0.89781857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7546921903840897}
episode index:1115
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.68243142, 18.03861062]), 'previousTarget': array([ 9.68243142, 18.03861062]), 'currentState': array([ 1.       , 23.       ,  4.7956905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.5768173059485415
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.742625, 16.756908,  5.69277 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7756601458953312}
episode index:1116
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.32015946, 15.28988195]), 'previousTarget': array([15., 15.]), 'currentState': array([22.733053, 22.001738,  5.928045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5765223385178748
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.109331 , 14.870515 ,  2.5600502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.16946886195727762}
episode index:1117
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      ,  7.      ,  5.599151], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5763835298270312
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.782217 , 16.95276  ,  1.9781697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1036011534585075}
episode index:1118
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.95893206, 14.90535746]), 'currentState': array([ 5.6100993, 15.186415 ,  6.179894 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.391750918194294}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5765635457605184
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.505236, 14.231009,  6.085527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.680972420273747}
episode index:1119
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.53698458, 20.36455994]), 'previousTarget': array([20.08636336, 21.93595004]), 'currentState': array([25.994534 , 28.000008 ,  4.6434426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.576696060751766
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.53986  , 16.919065 ,  2.5971196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4604837767612198}
episode index:1120
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36875492, 12.43661488]), 'previousTarget': array([13.36875492, 12.43661488]), 'currentState': array([8.    , 4.    , 4.7719], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5766320017223595
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.635242, 16.820335,  6.135644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2751228675765005}
episode index:1121
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.76251972, 14.55193989]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([ 2.9571886, 12.588402 ,  0.5675346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.576948787235893
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.264107 , 14.69521  ,  6.0865903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7965146202623236}
episode index:1122
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.73224847, 12.2300817 ]), 'previousTarget': array([17.75902574, 10.51658317]), 'currentState': array([22.034548 ,  3.7515428,  2.0596585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5772323041254949
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.37833  , 16.261627 ,  2.4797423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8685548987860554}
episode index:1123
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8042965, 16.396786 ,  3.9252906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6118012414788527}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5776084319687996
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8042965, 16.396786 ,  3.9252906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6118012414788527}
episode index:1124
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 23.       ,  1.2218958], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5776013144314738
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.676353, 16.856606,  4.999068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8846037195083607}
episode index:1125
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 16.       ,  5.4345613], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5778915282508141
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.168099 , 13.479393 ,  6.0037656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3807783239953544}
episode index:1126
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.45942241,  9.63386285]), 'previousTarget': array([10.45942241,  9.63386285]), 'currentState': array([4.      , 2.      , 2.273706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.5775792448677469
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.126915 , 14.6627035,  1.6505995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1763104552385184}
episode index:1127
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.55779009, 16.05914151]), 'previousTarget': array([11.55779009, 16.05914151]), 'currentState': array([ 2.       , 19.       ,  2.0220947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5772635290365157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.83473   , 14.626618  ,  0.13910979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2236288887688955}
episode index:1128
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.24695048, 7.80868809]), 'previousTarget': array([9.24695048, 7.80868809]), 'currentState': array([3.       , 0.       , 2.7007923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5771143320882305
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.855462  , 14.392432  ,  0.19366819], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.295803102102739}
episode index:1129
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47213595, 18.05572809]), 'previousTarget': array([13.47213595, 18.05572809]), 'currentState': array([ 9.      , 27.      ,  2.804121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5770927118626324
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.761724 , 15.616213 ,  5.2232547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3831285382373015}
episode index:1130
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.40757591,  9.41178475]), 'previousTarget': array([11.40757591,  9.41178475]), 'currentState': array([6.     , 1.     , 4.12612], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.5766339014825388
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.575497 , 14.76537  ,  1.6560804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6214881214225751}
episode index:1131
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 24.       ,  1.6411741], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5765982427579139
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.563763 , 16.878407 ,  1.8239906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4441286266653446}
episode index:1132
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.54722036, 13.06552629]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([2.8231483, 8.177729 , 5.9906106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5763120666063971
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.336475  , 14.868512  ,  0.77558047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.342927948382661}
episode index:1133
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.61523948, 16.25278872]), 'previousTarget': array([10.61523948, 16.25278872]), 'currentState': array([ 1.       , 19.       ,  1.8522601], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 245
reward sum = 0.08523592457219176
running average episode reward sum: 0.5758790188620988
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.550345, 13.561543,  6.152275], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0422183044296687}
episode index:1134
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.      , 11.      ,  5.871076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5759378140896078
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.915213 , 14.9452715,  2.0920172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.915994423894145}
episode index:1135
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.315584 , 21.5064   ,  0.8148101], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.302487824887576}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5756317336723945
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.020288 , 13.794968 ,  3.4463437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5789526648939478}
episode index:1136
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 16.       ,  1.8146431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5758895296962262
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.798834  , 16.874184  ,  0.92240363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.884948856854795}
episode index:1137
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58578644, 10.89949494]), 'previousTarget': array([15.58578644, 10.89949494]), 'currentState': array([17.      ,  1.      ,  4.669503], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5759713242734437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.610061, 13.808772,  3.108259], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3383564503521477}
episode index:1138
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       ,  7.       ,  2.1419463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5758507326821675
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.60544   , 14.099089  ,  0.91960734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6602525200721663}
episode index:1139
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.45069462,  7.55689083]), 'previousTarget': array([21.45069462,  7.55689083]), 'currentState': array([28.     ,  0.     ,  4.07633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.5755603945824769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.649824, 16.252567,  2.910219], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0714353767773406}
episode index:1140
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.524244 ,  7.94259  ,  1.9411521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.8884301678742075}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5758978491095738
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.560627 , 13.767781 ,  2.7901788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8947711495764492}
episode index:1141
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.02945514, 15.23303501]), 'previousTarget': array([18.02945514, 15.23303501]), 'currentState': array([28.       , 16.       ,  5.9711113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5757974324005619
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.24926  , 16.206701 ,  1.7870378], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7368875598281381}
episode index:1142
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.99015829, 16.21446438]), 'previousTarget': array([19.38476052, 16.25278872]), 'currentState': array([27.255135 , 19.97747  ,  3.6552968], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5760849080283906
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.96151 , 13.736965,  3.466277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5873745210122892}
episode index:1143
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.83941129, 23.70462796]), 'previousTarget': array([ 6.83941129, 23.70462796]), 'currentState': array([ 0.       , 31.       ,  1.7988003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.575749506284392
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.871703, 16.720892,  5.622334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9290763308557406}
episode index:1144
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.69407375, 23.23886   ]), 'previousTarget': array([21.69407375, 23.23886   ]), 'currentState': array([28.      , 31.      ,  5.366774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5759682129372752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.314188 , 14.040281 ,  3.7294471], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0098386469160132}
episode index:1145
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 13.       ,  0.8310682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5762789434215421
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.621656 , 16.171417 ,  2.2196152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0004969080480848}
episode index:1146
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.06917862, 10.44156455]), 'previousTarget': array([19.82178448,  8.86318339]), 'currentState': array([25.72856  ,  2.9814944,  1.557223 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5764411611817213
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.345247 , 15.05001  ,  4.5758343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.34885046849787654}
episode index:1147
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.017561 ,  9.257929 ,  3.8254404], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0080080743206565}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5766957819583652
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.959179 , 14.072769 ,  2.8280325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.167519107123298}
episode index:1148
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.      , 15.      ,  4.018109], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5770298987799856
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.114252 , 14.0673485,  3.8866363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9396235403141399}
episode index:1149
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45299804, 15.67949706]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([21.      , 24.      ,  3.346843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.576843239753099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.667616 , 16.146313 ,  5.5361767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.326553359147627}
episode index:1150
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.804711  , 12.74099823]), 'previousTarget': array([10.804711  , 12.74099823]), 'currentState': array([2.       , 8.       , 5.4111695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5767193371354089
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.313578, 15.947233,  6.21989 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1697975787227735}
episode index:1151
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.86318339, 10.17821552]), 'previousTarget': array([ 8.86318339, 10.17821552]), 'currentState': array([1.      , 4.      , 2.267708], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.5762980950395555
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5837555, 16.474371 ,  5.6867223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.044387003549094}
episode index:1152
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 19.       ,  0.7987091], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.65685424949238}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5765367417664031
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.111644 , 15.695892 ,  4.8819337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.012499766910924}
episode index:1153
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.81288673, 17.07266162]), 'previousTarget': array([18.60059927, 18.27327206]), 'currentState': array([25.863434 , 23.004667 ,  3.9657738], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5764368156187876
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.041002 , 16.3219   ,  2.9290376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6825884209915911}
episode index:1154
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([ 6.       , 27.       ,  4.6930637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5767366492800942
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.681576, 16.994446,  5.390087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0197049386550314}
episode index:1155
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.      , 18.      ,  3.051618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5770122562048161
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.446737 , 16.74009  ,  4.3025537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.332495707494479}
episode index:1156
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.66717653, 11.82179517]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([9.799841 , 2.5998802, 1.7388688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5773191300956564
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.175859 , 14.56442  ,  0.7059723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9321683331612874}
episode index:1157
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25608804, 14.24859507]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([28.       , 12.       ,  1.6811571], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5775632917748841
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.806963 , 16.832994 ,  3.4990838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.573904399563353}
episode index:1158
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.80580676, 15.03883865]), 'previousTarget': array([14.80580676, 15.03883865]), 'currentState': array([ 5.       , 17.       ,  1.7650135], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5776719064454834
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.694054 , 13.06948  ,  5.7273607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9546127141293248}
episode index:1159
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 18.       ,  5.0344553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5780020134313063
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.43378  , 14.954442 ,  3.4504983], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4345042834714303}
episode index:1160
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.01288586, 21.08753588]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 9.909789 , 30.593891 ,  0.6994885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.577867071319957
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.642979 , 15.363427 ,  3.9963698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5094541525537689}
episode index:1161
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.673579, 16.904951,  5.162643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0068375980033797}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5782217468179605
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.543967, 15.104278,  4.689493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4597620346794469}
episode index:1162
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89949494, 15.58578644]), 'previousTarget': array([10.89949494, 15.58578644]), 'currentState': array([ 1.       , 17.       ,  1.5298557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5782661164519977
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1438465, 13.821316 ,  5.4207735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4568099808234567}
episode index:1163
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.       , 15.       ,  5.1069202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5783921595959031
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.303194 , 15.338837 ,  2.6886854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3465233048362166}
episode index:1164
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85642931, 15.25842724]), 'previousTarget': array([14.85642931, 15.25842724]), 'currentState': array([10.      , 24.      ,  3.707735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5786413901994851
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.613627, 13.273093,  5.332385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7696018688969486}
episode index:1165
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57237055, 11.82039627]), 'previousTarget': array([17.45299804, 11.32050294]), 'currentState': array([21.00515  ,  2.856554 ,  3.2204843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.5782555059378129
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8215065, 14.62383  ,  1.4769827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9035357867863885}
episode index:1166
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1695452 , 17.03454242]), 'previousTarget': array([15.1695452 , 17.03454242]), 'currentState': array([16.       , 27.       ,  6.0021453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5785506980444882
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.901108 , 16.318977 ,  3.4545121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5974030522308802}
episode index:1167
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24859507, 18.25608804]), 'previousTarget': array([14.24859507, 18.25608804]), 'currentState': array([12.       , 28.       ,  0.5683485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5787080542228296
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.10039 , 14.764624,  4.391818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9141364977848134}
episode index:1168
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.81318091, 15.62854918]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 4.97602   , 20.308783  ,  0.16414626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5784796136986434
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.596785 , 15.540035 ,  5.5721717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6856331454617974}
episode index:1169
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10647783, 15.86059386]), 'previousTarget': array([16.10647783, 15.86059386]), 'currentState': array([24.       , 22.       ,  5.2045565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5784254748129384
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.473906 , 16.823095 ,  4.2644825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3443707389251958}
episode index:1170
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.36273687, 12.99348782]), 'previousTarget': array([17.45299804, 11.32050294]), 'currentState': array([21.981068 ,  4.7209816,  2.2817192], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5785444372875629
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.711496 , 13.332464 ,  3.3822045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3895387317698695}
episode index:1171
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 13.47213595]), 'previousTarget': array([18.05572809, 13.47213595]), 'currentState': array([27.        ,  9.        ,  0.27976137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.5780934920534151
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.821463 , 14.372799 ,  3.2847724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0335289302469863}
episode index:1172
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.53572186, 8.67738347]), 'previousTarget': array([8.07106781, 8.07106781]), 'currentState': array([2.996894 , 1.1114249, 6.256155 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5782909638148539
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.092543 , 13.851957 ,  1.2372732], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2262963731088417}
episode index:1173
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57492926, 18.85504245]), 'previousTarget': array([ 8.57492926, 18.85504245]), 'currentState': array([ 0.       , 24.       ,  3.1926146], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.578026701253612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.372847 , 14.718143 ,  1.3001761], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4673945838750957}
episode index:1174
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.77114535,  9.0618314 ]), 'previousTarget': array([17.77114535,  9.0618314 ]), 'currentState': array([22.      ,  0.      ,  4.284273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5779118257475461
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.053883 , 16.53597  ,  2.9634268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.86275991104593}
episode index:1175
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.58504608, 14.52576695]), 'currentState': array([8.62466  , 8.899948 , 1.5779902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.823581979006356}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5777076102502259
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.90558  , 13.553986 ,  2.1641812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.706174671457022}
episode index:1176
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.32854535, 20.94922467]), 'previousTarget': array([15., 20.]), 'currentState': array([13.207024 , 30.886135 ,  2.7671087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5775932010500355
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.447366, 15.631142,  5.702355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8388947494168351}
episode index:1177
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.       , 13.       ,  4.7551146], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5777833855903488
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.276428 , 13.499851 ,  3.0613356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9696993042865538}
episode index:1178
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.       , 22.       ,  2.1974792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.5774423880635471
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.149633 , 13.002365 ,  3.0530014], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1710984514802045}
episode index:1179
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.77114535, 20.9381686 ]), 'previousTarget': array([17.77114535, 20.9381686 ]), 'currentState': array([22.      , 30.      ,  2.448857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5774261370046355
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.961789 , 13.749643 ,  5.7703943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.625199563398409}
episode index:1180
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 18.92893219]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([26.      , 26.      ,  5.842413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.577214703152335
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.185247, 16.910923,  5.478721], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0773657579605613}
episode index:1181
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       , 11.       ,  3.4048467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5773214992537286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7472925, 13.322248 ,  2.1688237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.093830525067884}
episode index:1182
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 11.07106781]), 'previousTarget': array([18.92893219, 11.07106781]), 'currentState': array([26.       ,  4.       ,  1.2812479], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.5769587126876202
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.467742 , 13.479622 ,  2.0827246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6108532584794695}
episode index:1183
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157816, 15.006236,  4.669802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.842194621780942}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5773160110721746
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157816, 15.006236,  4.669802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.842194621780942}
episode index:1184
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.7661494 ,  8.95962697]), 'previousTarget': array([20.08636336,  8.06404996]), 'currentState': array([24.05696   ,  0.47391355,  2.2718935 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5774468020920751
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.600049 , 13.268719 ,  1.9318907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7768780654804492}
episode index:1185
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.        , 18.        ,  0.79763764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5776924167722413
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.800744 , 16.72055  ,  5.3439837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8977579390139276}
episode index:1186
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.30592625, 23.23886   ]), 'previousTarget': array([ 8.30592625, 23.23886   ]), 'currentState': array([ 2.      , 31.      ,  3.000393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5776760509102157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.555001, 15.891986,  4.132707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6981343279260719}
episode index:1187
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02257073, 17.31833545]), 'previousTarget': array([15.47942816, 18.11628302]), 'currentState': array([15.119924 , 27.317862 ,  3.3622203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 355
reward sum = 0.02821591134702963
running average episode reward sum: 0.5772135423752299
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.641804, 16.61054 ,  5.250079], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2998608378170253}
episode index:1188
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.8427793 ,  9.66577312]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([9.093646  , 0.39517444, 2.8224118 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5773955446807543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.591932  , 14.599361  ,  0.46458074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4639555761600553}
episode index:1189
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 19.       ,  1.2053697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5772337804237901
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.191185 , 13.484723 ,  5.0624356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3596346801200814}
episode index:1190
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.16726058, 15.87170577]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 2.6095593, 18.812853 ,  5.1463223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.577415460107434
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.290095  , 14.110372  ,  0.23530337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1381578756861337}
episode index:1191
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.   , 20.   ,  2.016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5776672265192558
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.495317, 16.778091,  4.226886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8457921121016414}
episode index:1192
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80768079, 13.73462344]), 'previousTarget': array([16.80768079, 13.73462344]), 'currentState': array([25.       ,  8.       ,  1.2037334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5776280298021023
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.218163 , 16.260595 ,  1.7783053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.753003288817879}
episode index:1193
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 20.       ,  3.5673907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5778224182764901
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.22383  , 15.752836 ,  4.8737326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0812964015061108}
episode index:1194
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.39118875, 19.48150862]), 'previousTarget': array([16.57464375, 21.298575  ]), 'currentState': array([19.355911, 29.031923,  5.555715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5781033344515589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.253373 , 16.800268 ,  2.9117515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.818010848879551}
episode index:1195
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.88171698, 17.91263916]), 'previousTarget': array([12.88171698, 17.91263916]), 'currentState': array([ 7.      , 26.      ,  1.496128], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5778831888317675
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.502815  , 16.431643  ,  0.65041035], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0715121352038537}
episode index:1196
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.71222436, 18.00952168]), 'previousTarget': array([15.58256937, 16.60206577]), 'currentState': array([18.015182 , 27.740728 ,  1.0313212], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5775532584470368
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.044174 , 13.707065 ,  2.8351507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6619210639292055}
episode index:1197
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.63117406, 16.35236179]), 'currentState': array([12.006888 , 24.000011 ,  4.4892073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.48466779102832}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5778570371540099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.53251  , 16.404903 ,  5.0625505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.031571084824735}
episode index:1198
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.        ,  8.        ,  0.10389597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5779164579973914
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.468784, 14.020713,  3.144285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7653132107474263}
episode index:1199
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.97034378, 15.81645439]), 'previousTarget': array([14.64398987, 17.13606076]), 'currentState': array([14.607351 , 25.809864 ,  5.4122224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5777524002141474
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.554285, 16.775015,  4.441877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8595455710296511}
episode index:1200
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.        ,  6.        ,  0.15670943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.577862975424021
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.643393 , 15.053726 ,  2.2933486], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6442705433874514}
episode index:1201
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.68964732, 12.43294146]), 'previousTarget': array([12.68964732, 12.43294146]), 'currentState': array([6.       , 5.       , 2.6600568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5780038315961571
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.42269   , 13.210058  ,  0.64945614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8807386813284086}
episode index:1202
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 10.22192192]), 'previousTarget': array([11.69209979, 10.22192192]), 'currentState': array([6.      , 2.      , 4.079971], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.578096471042768
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.593536 , 13.564606 ,  1.1008521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0096011252037984}
episode index:1203
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.252071 , 19.145113 ,  3.8212287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.498579350126217}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5782494994840507
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.184565 , 16.006615 ,  5.7569175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2954566883214966}
episode index:1204
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.36412067, 19.4410542 ]), 'previousTarget': array([ 9.07106781, 20.92893219]), 'currentState': array([ 3.1429443, 26.358757 ,  5.199291 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5785353876126348
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.977653 , 15.543898 ,  5.4701376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1580237215371316}
episode index:1205
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.612337 ,  8.9620695,  1.3911157], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.249498823834609}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5788521874653606
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.486696 , 14.642275 ,  1.3597939], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6040202906134207}
episode index:1206
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.89809005, 16.37783817]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 0.24395281, 18.985065  ,  1.3184892 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5785167588332238
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.270119 , 13.045271 ,  5.9049745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0865503755097703}
episode index:1207
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.07106781, 15.92893219]), 'previousTarget': array([14.07106781, 15.92893219]), 'currentState': array([ 7.       , 23.       ,  6.0927267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.578650188977723
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.355567 , 16.388645 ,  3.8060703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5308916791556302}
episode index:1208
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86197056, 13.10366477]), 'previousTarget': array([15.86197056, 13.10366477]), 'currentState': array([20.      ,  4.      ,  6.236003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5786063066771009
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.846564 , 14.07178  ,  1.3547142], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4805424406773444}
episode index:1209
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89773853, 11.83425693]), 'previousTarget': array([ 9.41178475, 11.40757591]), 'currentState': array([2.9809813, 5.7248416, 0.0377075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5786382767880638
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.877886, 13.970761,  5.831641], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0364575132101255}
episode index:1210
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.47213595, 16.05572809]), 'currentState': array([10.443061 , 23.049692 ,  5.0440936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.250039889782364}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5786701940995042
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.221657, 13.758613,  4.715985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1687662702517803}
episode index:1211
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.008192 , 12.819166 ,  3.3022182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.964603092272421}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5785300538192426
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.209952 , 13.967594 ,  2.1092238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0664298728182855}
episode index:1212
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.09464254, 13.95893206]), 'currentState': array([15.126103 ,  5.798973 ,  1.8548537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.20189102228556}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5783901446029222
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.284376 , 14.15839  ,  3.2549798], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8883564079718747}
episode index:1213
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.51123442, 21.63670822]), 'previousTarget': array([12.51123442, 21.63670822]), 'currentState': array([ 9.       , 31.       ,  4.1886473], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5786221612504002
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.483576, 15.345261,  5.375365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5552321970347414}
episode index:1214
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 18.       ,  1.8623182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5784927061564944
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.562295 , 15.8873415,  4.9352756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9894243975427924}
episode index:1215
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.35601013, 17.13606076]), 'previousTarget': array([15.35601013, 17.13606076]), 'currentState': array([17.      , 27.      ,  6.094254], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5784000976606172
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.90987  , 16.921839 ,  3.7727888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2094902841488167}
episode index:1216
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.4884921 ,  9.47823407]), 'previousTarget': array([21.92893219,  8.07106781]), 'currentState': array([29.104092 ,  2.9972894,  1.7439177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5780927380583868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.135204, 16.136478,  4.012718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6063225222842599}
episode index:1217
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.607155 , 17.094387 ,  5.0687914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5152484482269375}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5784309213604735
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.304965, 15.220071,  5.231432], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7290437563315271}
episode index:1218
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.0305336 , 19.78264534]), 'previousTarget': array([14.66519011, 20.02214842]), 'currentState': array([12.043887 , 29.58332  ,  3.3312645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5785632178756727
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.867726 , 14.936493 ,  5.7644114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1340532712703313}
episode index:1219
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.10303179, 15.88853232]), 'previousTarget': array([12.80868809, 16.75304952]), 'currentState': array([ 6.998635 , 22.926113 ,  5.4414897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5788010724616508
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.696724 , 15.591649 ,  4.0354443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9140420398523292}
episode index:1220
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.93871557, 13.90427173]), 'previousTarget': array([13.73462344, 13.19231921]), 'currentState': array([6.9814477, 6.7212067, 1.7094791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5789640702396504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.036031, 14.060497,  5.791254], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3460691034029475}
episode index:1221
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.39134169, 18.94936252]), 'previousTarget': array([15.35601013, 17.13606076]), 'currentState': array([16.37741  , 28.900627 ,  1.8327684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5790717536742106
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.633788, 15.84624 ,  4.766985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6070647600660537}
episode index:1222
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.73085102, 15.4143551 ]), 'previousTarget': array([11.55779009, 16.05914151]), 'currentState': array([ 2.8935149, 17.21069  ,  5.3130593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5791397915215613
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.155514, 14.402812,  5.840986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0343067756810649}
episode index:1223
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.52057184, 18.11628302]), 'previousTarget': array([14.52057184, 18.11628302]), 'currentState': array([13.      , 28.      ,  3.393299], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5789510278965375
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.150475 , 16.99794  ,  6.2591896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0035985343732934}
episode index:1224
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.24275371, 19.6284586 ]), 'previousTarget': array([19.24275371, 19.6284586 ]), 'currentState': array([26.      , 27.      ,  6.037819], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5789295849979791
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.73132 , 16.526306,  5.563801], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9847313283996009}
episode index:1225
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.752272, 21.035872,  6.185749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.380714121972959}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5792100214657033
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.028853 , 16.204645 ,  5.2300887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.547351169160312}
episode index:1226
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.29998387, 13.83926499]), 'previousTarget': array([15., 15.]), 'currentState': array([9.135646, 5.275994, 3.358948], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5789113382397566
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.333453 , 14.850101 ,  5.5855584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6732745971830298}
episode index:1227
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.        , 18.        ,  0.26820886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5791690148814659
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.37593  , 16.548512 ,  5.4423766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5934902323022926}
episode index:1228
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02107993, 21.46043569]), 'previousTarget': array([15.75965265, 21.07722123]), 'currentState': array([15.053709 , 31.460382 ,  2.9109678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5793243233687919
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.534222, 16.276016,  5.521488], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9955083316444582}
episode index:1229
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.5813861, 19.3314396]), 'previousTarget': array([12.88171698, 17.91263916]), 'currentState': array([ 5.385997 , 27.181099 ,  3.3973117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5792099275789065
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.270917 , 16.948677 ,  5.6021967], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.080601933569232}
episode index:1230
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.09464254, 13.95893206]), 'currentState': array([16.336842 ,  5.9714303,  1.7077007], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.127004777931354}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5794038325423658
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.487503 , 13.628028 ,  1.9285064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4645683737393287}
episode index:1231
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84288535, 21.17127813]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([12.       , 31.       ,  3.3102317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5792824731113443
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.48416 , 15.909441,  4.398995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0455493249758319}
episode index:1232
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05246157, 11.89699659]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([25.065233 ,  4.7681093,  2.5955772], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5795315423721754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.461307 , 13.75126  ,  2.0546253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3312234339169038}
episode index:1233
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.7580894 , 14.85236174]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([-0.23794663, 14.570824  ,  2.3760767 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5791726837519034
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.830892 , 13.61444  ,  3.5622838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8128957614828076}
episode index:1234
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 14.       ,  3.3220701], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5793659908400374
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.4229555 , 16.65035   ,  0.34507102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7483241536157064}
episode index:1235
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.89352217, 15.86059386]), 'previousTarget': array([13.89352217, 15.86059386]), 'currentState': array([ 6.       , 22.       ,  4.1866493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5791845950664654
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.548696, 16.837439,  5.294033], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8920508030655765}
episode index:1236
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.83276518,  9.51160505]), 'previousTarget': array([17.77114535,  9.0618314 ]), 'currentState': array([20.000175 ,  0.0264845,  3.3176978], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5788657753161679
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.810137  , 13.716955  ,  0.91276765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5174075339688236}
episode index:1237
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 20.       ,  1.0876173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5788771651958814
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.13701  , 16.04836  ,  2.9816425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.546561820519083}
episode index:1238
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24034735, 21.07722123]), 'previousTarget': array([14.24034735, 21.07722123]), 'currentState': array([13.        , 31.        ,  0.04539707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5790314557371319
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.6354685, 14.112436 ,  5.6757836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9595064138031617}
episode index:1239
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.06404996, 20.08636336]), 'previousTarget': array([ 8.06404996, 20.08636336]), 'currentState': array([ 0.       , 26.       ,  1.8379333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5790985933058793
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.064542, 16.207031,  5.83195 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5270908457449812}
episode index:1240
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.31756858, 11.96138938]), 'previousTarget': array([20.31756858, 11.96138938]), 'currentState': array([29.      ,  7.      ,  6.208171], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.5787410062080559
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.8849    , 14.05024   ,  0.25359386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1106609517967945}
episode index:1241
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.10758383, 16.05527919]), 'previousTarget': array([15.23303501, 18.02945514]), 'currentState': array([16.121809, 26.003714,  4.480887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.579040723634539
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.04145 , 16.15213 ,  4.355709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.553068889135416}
episode index:1242
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.        , 22.        ,  0.27256364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5791465261314318
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.460201 , 16.418669 ,  4.1957927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.093705127929251}
episode index:1243
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.22041896, 14.26238355]), 'previousTarget': array([15.35601013, 12.86393924]), 'currentState': array([18.083576 ,  4.6810303,  1.0514749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5789872822343041
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.056286, 16.912693,  3.074659], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.184979271521169}
episode index:1244
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52786405, 13.94427191]), 'previousTarget': array([15.52786405, 13.94427191]), 'currentState': array([20.       ,  5.       ,  3.5045762], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5791596734000948
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6519165, 13.03308  ,  0.6710677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0721411672701393}
episode index:1245
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 17.       ,  5.8479533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5794354238182552
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.487415 , 13.91238  ,  3.8620417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8426397110092412}
episode index:1246
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.22164977, 12.70236714]), 'previousTarget': array([19.3177872, 11.401844 ]), 'currentState': array([26.36322  ,  6.89592  ,  2.1244524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5792920791744166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.232357 , 15.532994 ,  2.3585386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.342678937656676}
episode index:1247
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.614237 , 14.8192215,  5.5564504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.388796319466109}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5795976111702704
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.028482 , 13.120369 ,  0.6342391], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.879846830218168}
episode index:1248
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.      , 19.      ,  4.878387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5794122592914251
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.489655 , 13.173529 ,  2.7323174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.356919210804807}
episode index:1249
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.58504608, 14.52576695]), 'previousTarget': array([14.58504608, 14.52576695]), 'currentState': array([8.     , 7.     , 5.55542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.579672235143999
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.337406 , 14.309645 ,  1.0653269], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.505073311881513}
episode index:1250
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.55689083, 21.45069462]), 'previousTarget': array([ 7.55689083, 21.45069462]), 'currentState': array([ 0.      , 28.      ,  4.909025], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5797599864235554
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.065458, 13.782089,  6.228159], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.285991739536186}
episode index:1251
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.      ,  8.      ,  2.895541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5799567984342678
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.651602 , 13.244248 ,  2.0173411], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7899846443731087}
episode index:1252
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.68964732, 12.43294146]), 'previousTarget': array([12.68964732, 12.43294146]), 'currentState': array([6.       , 5.       , 5.1837964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5799865935998768
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.889146 , 13.125464 ,  1.8456582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1789631727116525}
episode index:1253
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94427191, 14.47213595]), 'previousTarget': array([13.94427191, 14.47213595]), 'currentState': array([ 5.       , 10.       ,  4.6191783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5801895658230429
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.359027  , 13.3608265 ,  0.06123305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.76003871885283}
episode index:1254
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.86393924, 15.35601013]), 'previousTarget': array([12.86393924, 15.35601013]), 'currentState': array([ 3.      , 17.      ,  2.533799], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5799091203205029
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.360707 , 14.6851015,  4.96135  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6692638112408513}
episode index:1255
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.118507, 13.67822 ,  2.662899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.240183029706088}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5802199402884006
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.700516, 16.243076,  2.701707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1064169944420486}
episode index:1256
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.37524536, 11.72836177]), 'previousTarget': array([12.47213595,  9.94427191]), 'currentState': array([8.927358 , 2.7720068, 1.3557051], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5804090309783838
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.18554  , 13.715642 ,  1.6923561], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.223024944479207}
episode index:1257
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05572809, 12.47213595]), 'previousTarget': array([20.05572809, 12.47213595]), 'currentState': array([29.       ,  8.       ,  4.7668695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5800960438923245
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.404041 , 14.521799 ,  1.4648699], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4832424136035907}
episode index:1258
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.51123442, 21.63670822]), 'previousTarget': array([12.51123442, 21.63670822]), 'currentState': array([ 9.       , 31.       ,  4.6519084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5798853315248015
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.152473 , 15.691749 ,  0.8183505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9727824274450434}
episode index:1259
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.28609324,  9.28476691]), 'previousTarget': array([17.28609324,  9.28476691]), 'currentState': array([21.       ,  0.       ,  3.9949317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5797697331083468
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.037695  , 14.039395  ,  0.47608906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.961343973488378}
episode index:1260
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 13.       ,  1.8621643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5795944583776359
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.43676 , 15.079923,  5.474236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.565281793175268}
episode index:1261
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.41495392, 15.47423305]), 'previousTarget': array([15.41495392, 15.47423305]), 'currentState': array([22.       , 23.       ,  1.9485843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5797640461948039
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.114046 , 16.86644  ,  5.1371865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.173636654319668}
episode index:1262
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.        , 19.        ,  0.31346565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5798189461015131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.266975  , 16.133352  ,  0.13746423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6999158982722944}
episode index:1263
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.73957299, 17.6676221 ]), 'previousTarget': array([11.73957299, 17.6676221 ]), 'currentState': array([ 4.       , 24.       ,  2.2594173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5798200121363363
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.349608  , 15.666079  ,  0.93965775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9309510533319281}
episode index:1264
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.       ,  9.       ,  4.0798345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5797607756751906
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.706    , 13.505523 ,  1.9449956], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9768402736680148}
episode index:1265
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.58979079, 11.33345606]), 'previousTarget': array([13.58979079, 11.33345606]), 'currentState': array([10.       ,  2.       ,  3.8977745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5796598932696544
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3608465 , 13.806397  ,  0.13642791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.027686171138952}
episode index:1266
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.      , 19.      ,  4.330861], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5798544542251128
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.515987 , 13.319519 ,  6.0965586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2419432765042258}
episode index:1267
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       ,  9.       ,  3.6346915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5795981886939388
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.478596  , 14.373227  ,  0.23319453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8152954389507706}
episode index:1268
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.      , 23.      ,  6.265586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5798124200433484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.250719 , 15.13347  ,  4.8225417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7543653837655488}
episode index:1269
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.060165 , 12.486871 ,  3.5134046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.513849324115352}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5799441993144416
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5701685, 13.821004 ,  2.3155603], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8532268893118875}
episode index:1270
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 20.       ,  5.7747283], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5801994612150666
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.397384 , 13.916109 ,  3.7133017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1544406924665005}
episode index:1271
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.86776655, 20.61976041]), 'previousTarget': array([ 9.24695048, 22.19131191]), 'currentState': array([ 3.1242504, 28.003864 ,  3.6267653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5801323536666272
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.513198 , 15.2611685,  4.7879877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.509565984925347}
episode index:1272
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.14370892, 12.31552075]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([11.104781 ,  2.7884583,  1.4236035], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5801471405109317
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.737231 , 13.34589  ,  6.2055945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6748513833327805}
episode index:1273
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.116945, 17.205505,  4.088417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.670491651970004}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.580461075251504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.630327 , 14.078171 ,  3.4473422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8728950790805154}
episode index:1274
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.86318339, 10.17821552]), 'previousTarget': array([ 8.86318339, 10.17821552]), 'currentState': array([1.       , 4.       , 3.5816565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.580653787054315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0680485 , 14.106371  ,  0.74647796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.128616829921493}
episode index:1275
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00496281, 14.9503719 ]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([16.       ,  5.       ,  4.2294645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.580802211316659
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157464, 14.45108 ,  2.005588], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9225638150426647}
episode index:1276
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.79749309, 15.71998407]), 'previousTarget': array([18.66654394, 16.41020921]), 'currentState': array([26.080502 , 19.438284 ,  3.1164837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5806806670034813
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.678375 , 16.629618 ,  3.9191139], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3393583049833926}
episode index:1277
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69209979, 10.22192192]), 'previousTarget': array([11.69209979, 10.22192192]), 'currentState': array([6.        , 2.        , 0.05729884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5804801700260634
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.354654  , 16.645885  ,  0.04519814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.767882922897228}
episode index:1278
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.      , 16.      ,  5.134329], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5804331491915241
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.633819, 13.150128,  6.135458], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9554414127686914}
episode index:1279
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89948976,  9.80392119]), 'previousTarget': array([10.07106781, 10.07106781]), 'currentState': array([4.7045894, 1.953876 , 5.793788 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5801748709940764
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.579385 , 13.339132 ,  3.1023717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7590247974325313}
episode index:1280
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.      , 13.      ,  5.798172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5802337971061607
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.322799 , 13.028244 ,  0.2979753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9980041716103847}
episode index:1281
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.      , 20.      ,  3.870204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5805304914999937
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.640699 , 16.104633 ,  5.4313507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.276992756675506}
episode index:1282
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.80451099, 21.67206508]), 'previousTarget': array([ 8.80451099, 21.67206508]), 'currentState': array([ 2.      , 29.      ,  4.090065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5804755072870159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.807414, 13.863081,  4.962606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.647679003389534}
episode index:1283
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.47213595, 18.05572809]), 'previousTarget': array([13.47213595, 18.05572809]), 'currentState': array([ 9.        , 27.        ,  0.36713475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5803973655029695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.310078, 16.594135,  5.160976], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.323166975449343}
episode index:1284
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.      , 21.      ,  3.138044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298221}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5802939614938339
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.483905, 16.907648,  5.615642], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9762275774235574}
episode index:1285
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.      , 14.      ,  5.050472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.580235323801216
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.616061, 13.259722,  6.003629], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8461039466536193}
episode index:1286
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.81141608, 13.88772053]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([27.11013 , 10.208874,  3.977234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.5799636070137393
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.43251  , 13.259153 ,  1.9674008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2544695562549153}
episode index:1287
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15384615, 12.23076923]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([20.        ,  3.        ,  0.75777984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5801419954152979
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.944956 , 14.347724 ,  3.0723002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0514183375289594}
episode index:1288
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.      , 19.      ,  3.819939], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 216
reward sum = 0.1140780353265762
running average episode reward sum: 0.5797804252367961
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.806017 , 13.210765 ,  5.6494064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.799719903619455}
episode index:1289
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.45069462,  7.55689083]), 'previousTarget': array([21.45069462,  7.55689083]), 'currentState': array([28.       ,  0.       ,  6.2515087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.5794529623351458
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.461369 , 16.436409 ,  1.8892554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.049114168593395}
episode index:1290
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.07682239,  9.89423902]), 'previousTarget': array([20.49208627,  8.59256602]), 'currentState': array([25.316504 ,  2.079741 ,  2.1975043], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.579399153492322
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.212628 , 13.626114 ,  3.0763855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.390242374648009}
episode index:1291
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       ,  7.       ,  3.0688558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5795966106192243
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.045679 , 13.034631 ,  6.0508046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9658999893903277}
episode index:1292
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17821552,  8.86318339]), 'previousTarget': array([10.17821552,  8.86318339]), 'currentState': array([4.       , 1.       , 3.7057483], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5796503662400666
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.930862, 13.720346,  1.50264 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.281519882795222}
episode index:1293
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.003918 ,  9.874879 ,  2.8278885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.893915006993626}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5797040387764872
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.778355, 13.220341,  3.109397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1586117056231267}
episode index:1294
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.92893219, 11.07106781]), 'previousTarget': array([18.92893219, 11.07106781]), 'currentState': array([26.       ,  4.       ,  4.8701134], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5795390413266778
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0960245, 14.098844 ,  1.5725646], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2764225725955436}
episode index:1295
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94085849, 11.55779009]), 'previousTarget': array([13.94085849, 11.55779009]), 'currentState': array([11.       ,  2.       ,  4.5509534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5795977760328873
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.240196, 14.575177,  1.150631], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8705034115477864}
episode index:1296
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.2722845,  9.7621517]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([10.139776  ,  0.26544744,  3.3082473 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5796878419095385
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.494326, 16.020176,  2.48179 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8093556996401619}
episode index:1297
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17821552, 21.13681661]), 'previousTarget': array([10.17821552, 21.13681661]), 'currentState': array([ 4.       , 29.       ,  1.9490879], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.579406783824636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.130994  , 15.772141  ,  0.24835747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.022223143873248}
episode index:1298
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.26042701, 12.3323779 ]), 'previousTarget': array([18.26042701, 12.3323779 ]), 'currentState': array([26.       ,  6.       ,  4.5639167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5795188496846307
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.832148 , 13.755531 ,  2.0879128], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4970544222287812}
episode index:1299
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.068216 , 15.482107 ,  2.9696095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.0873374039214125}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5797617876881493
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.210603 , 15.158723 ,  3.7286215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.2637166557025318}
episode index:1300
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.82352941, 18.29411765]), 'previousTarget': array([ 8.82352941, 18.29411765]), 'currentState': array([ 0.       , 23.       ,  4.3798985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.5794121458930462
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.709364 , 14.8044   ,  5.1458974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.35032628675539484}
episode index:1301
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.       , 11.       ,  4.0690327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5794364969634301
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.34995  , 16.411392 ,  2.4344766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9530476472343146}
episode index:1302
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.07106781, 7.07106781]), 'previousTarget': array([7.07106781, 7.07106781]), 'currentState': array([0.       , 0.       , 5.1970477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.579371571531839
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.411363 , 15.34869  ,  2.3541875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.539262428936919}
episode index:1303
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.264727 ,  9.994396 ,  2.6607623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.976161755788441}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5795868221323831
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.559422 , 13.640041 ,  2.8090632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9810991463169234}
episode index:1304
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.80580676, 13.96116135]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 0.       , 12.       ,  0.4994723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.579842707515794
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.894268  , 14.070529  ,  0.11861628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9354654551019573}
episode index:1305
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.76923077, 13.84615385]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([27.       , 10.       ,  4.9805913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5798219117804546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.014179, 16.231203,  4.072304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2312847245238083}
episode index:1306
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.41495392, 14.52576695]), 'previousTarget': array([15.41495392, 14.52576695]), 'currentState': array([22.       ,  7.       ,  2.5801654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5800564664552332
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.247241, 13.205595,  2.653689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9459021883303387}
episode index:1307
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.58821525, 11.40757591]), 'previousTarget': array([20.58821525, 11.40757591]), 'currentState': array([29.       ,  6.       ,  5.0923486], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.5797262589056097
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.637111 , 15.5721445,  2.5046728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8563056670540207}
episode index:1308
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.54700196,  8.32050294]), 'previousTarget': array([10.54700196,  8.32050294]), 'currentState': array([5.      , 0.      , 3.353056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5794923115351869
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.475635 , 13.652844 ,  1.9949291], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.03433480078951}
episode index:1309
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.90289239, 20.07376011]), 'previousTarget': array([18.90289239, 20.07376011]), 'currentState': array([25.       , 28.       ,  2.0714598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.579388155558157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.327896, 15.979413,  4.176629], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6500175723089139}
episode index:1310
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.61943459, 15.92561101]), 'previousTarget': array([13.84615385, 17.76923077]), 'currentState': array([10.816794, 25.17439 ,  5.47051 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5796500598593545
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.148033, 16.685297,  4.092995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8884050271969466}
episode index:1311
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.36336397, 10.41741912]), 'previousTarget': array([13.36336397, 10.41741912]), 'currentState': array([10.       ,  1.       ,  3.9316614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5795359127203792
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.183824, 14.742186,  2.293204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.31663761498971094}
episode index:1312
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.157275 , 12.993807 ,  2.1535676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.4793527111297493}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.579769613374603
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.633359 , 16.441576 ,  1.3296211], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4874700101137786}
episode index:1313
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.33471177, 22.75958076]), 'previousTarget': array([20.33471177, 22.75958076]), 'currentState': array([26.      , 31.      ,  5.975812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5795959753176649
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.199475 , 16.527626 ,  5.3950214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7246684100553378}
episode index:1314
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22192192, 11.69209979]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([2.       , 6.       , 2.9305344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5793137673225448
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.820557 , 13.129942 ,  0.7160301], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0421631538871288}
episode index:1315
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.74370623, 16.33801591]), 'previousTarget': array([12.80868809, 16.75304952]), 'currentState': array([ 6.898776 , 23.62821  ,  1.5841974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5793020586775067
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.32029  , 15.816254 ,  5.6734567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0622034893143864}
episode index:1316
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.85575877, 11.08493925]), 'previousTarget': array([9.34803445, 9.78280103]), 'currentState': array([3.5865512, 4.2177258, 1.5221593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5795284967673483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.54851 , 13.510817,  6.283105], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5561204514058171}
episode index:1317
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([27.      ,  6.      ,  5.413158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5796333541541698
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.826236 , 16.3577   ,  2.7598248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5893444511070998}
episode index:1318
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21719897,  9.34803445]), 'previousTarget': array([20.21719897,  9.34803445]), 'currentState': array([27.       ,  2.       ,  3.1873648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5794119366069467
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.778946, 16.179354,  2.680657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.134367301832197}
episode index:1319
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.80580676, 14.96116135]), 'previousTarget': array([14.80580676, 14.96116135]), 'currentState': array([ 5.       , 13.       ,  1.5955614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.5791680522661564
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.858215 , 14.348676 ,  1.8224548], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9690575226984206}
episode index:1320
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.02945514, 14.76696499]), 'previousTarget': array([18.02945514, 14.76696499]), 'currentState': array([28.       , 14.       ,  5.9984627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5792729443784441
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.993889 , 14.100121 ,  2.2054036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1875501824108667}
episode index:1321
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.05572809, 16.52786405]), 'previousTarget': array([18.05572809, 16.52786405]), 'currentState': array([27.     , 21.     ,  5.16793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5795120255508195
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.17278 , 15.050847,  4.649226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1738808291158664}
episode index:1322
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.     ,  8.     ,  0.39874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5797856220163147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.671612 , 13.766846 ,  1.3482406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0772470923371613}
episode index:1323
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.47213595, 22.05572809]), 'previousTarget': array([11.47213595, 22.05572809]), 'currentState': array([ 7.       , 31.       ,  1.4921002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5799351958360902
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.210686 , 16.628748 ,  4.8442354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0294284390042825}
episode index:1324
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.105927 , 20.357752 ,  3.7796755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.123290185450514}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5801025131143566
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.338451, 16.592028,  3.692572], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7240065698478162}
episode index:1325
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.45299804, 11.32050294]), 'previousTarget': array([17.45299804, 11.32050294]), 'currentState': array([23.        ,  3.        ,  0.16897124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5802695780287039
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.598564, 13.593071,  3.025361], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4630789451335913}
episode index:1326
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.02945514, 14.76696499]), 'previousTarget': array([18.02945514, 14.76696499]), 'currentState': array([28.       , 14.       ,  1.4031341], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5800767942697248
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.186438, 13.162787,  2.321561], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.009286880373892}
episode index:1327
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.8085497 , 12.93919299]), 'previousTarget': array([19.8085497 , 12.93919299]), 'currentState': array([29.       ,  9.       ,  6.0862627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5800689060228931
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.693471 , 16.738586 ,  3.9519749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1747875541334496}
episode index:1328
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.97785158, 15.33480989]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([ 0.       , 16.       ,  2.8380415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5800047749119656
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.237015, 15.166948,  5.842736], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7810366197989875}
episode index:1329
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.66528823, 22.75958076]), 'previousTarget': array([ 9.66528823, 22.75958076]), 'currentState': array([ 4.       , 31.       ,  1.1795894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5801836487184959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.503235, 16.67463 ,  4.905635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.250355094189522}
episode index:1330
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 20.       ,  3.2714834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.5799491356217253
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.539488, 15.798008,  5.863119], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7340240822975408}
episode index:1331
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.71390676, 14.28476691]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([11.       ,  5.       ,  5.3404903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5800109471122376
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.923183 , 13.819502 ,  2.2376223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2565926453761063}
episode index:1332
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.81238194, 20.86266529]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 5.       , 29.       ,  1.0351985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5798476770977951
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.807657, 15.324529,  4.918914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3772460867289898}
episode index:1333
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.50791373, 8.59256602]), 'previousTarget': array([9.50791373, 8.59256602]), 'currentState': array([3.        , 1.        , 0.63947934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5797622446375792
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.866772 , 13.184737 ,  1.5257049], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.820145267150612}
episode index:1334
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.       , 12.       ,  4.3976364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5796876223244211
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.39398308e+01, 1.41701565e+01, 3.51351104e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3463279854905645}
episode index:1335
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.51316702, 15.83772234]), 'previousTarget': array([17.51316702, 15.83772234]), 'currentState': array([27.      , 19.      ,  5.108049], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5798910430944588
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.245851, 13.080245,  3.614849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9354332048006366}
episode index:1336
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.29411765,  8.82352941]), 'previousTarget': array([18.29411765,  8.82352941]), 'currentState': array([23.       ,  0.       ,  3.3833818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5799941391973038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.745758 , 15.493309 ,  2.2321217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8941525984084183}
episode index:1337
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.015065, 19.000057,  5.677134], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.119533267330045}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5802503055315569
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.221735, 15.766211,  6.244589], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4421217104976252}
episode index:1338
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.      , 17.      ,  2.399918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5805271836080084
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.01478  , 16.757301 ,  3.6613615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7573634843502153}
episode index:1339
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.991128, 21.00002 ,  3.847429], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.327384587949163}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5806515454816826
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.18154 , 16.366974,  6.113247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.80683514861619}
episode index:1340
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.57464375, 12.701425  ]), 'previousTarget': array([15.57464375, 12.701425  ]), 'currentState': array([18.       ,  3.       ,  4.2155356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5807025902862215
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.546709, 13.203325,  1.304904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3108644776338125}
episode index:1341
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.59256602, 20.49208627]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 1.       , 27.       ,  1.3157563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5805870072259403
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.185674, 16.386   ,  5.40213 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6075205791343847}
episode index:1342
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 18.       ,  1.8340586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5807054832990322
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.196413, 16.397316,  4.879214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6119069288276908}
episode index:1343
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.      ,  9.      ,  6.076595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.211102550927977}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5809599767596936
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.293556 , 14.798472 ,  1.4089085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.309160436664374}
episode index:1344
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.19231921, 16.26537656]), 'previousTarget': array([13.19231921, 16.26537656]), 'currentState': array([ 5.       , 22.       ,  4.3872733], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5811547598203808
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.748236  , 15.895648  ,  0.69878507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5391878381355126}
episode index:1345
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.      , 8.      , 3.454955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5809545416343695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.648003 , 16.912125 ,  5.8566833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9442537816585557}
episode index:1346
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 20.       ,  2.3241174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5812082833959089
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.599927, 16.903637,  4.370657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.363056943812096}
episode index:1347
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.1768175 ,  7.31055268]), 'previousTarget': array([22.1768175 ,  7.31055268]), 'currentState': array([29.       ,  0.       ,  3.7487607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5810060105377775
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.899796 , 13.521083 ,  2.1276937], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4075753192120795}
episode index:1348
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 22.       ,  2.4814298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5810018213157161
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.23517  , 16.827143 ,  3.6608438], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.205469639709266}
episode index:1349
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.27327206, 18.60059927]), 'previousTarget': array([18.27327206, 18.60059927]), 'currentState': array([25.       , 26.       ,  1.5133106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5808319008603401
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.664352 , 13.234602 ,  3.2871625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.426252080149929}
episode index:1350
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.83772234, 20.51316702]), 'previousTarget': array([16.83772234, 20.51316702]), 'currentState': array([20.       , 30.       ,  5.9031115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.580425066814575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.664616, 15.626624,  4.869999], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.475096330623472}
episode index:1351
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 17.       ,  1.2947117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.5801768717200634
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.395878 , 14.779254 ,  5.8625083], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6192395513063695}
episode index:1352
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.80580676, 16.03883865]), 'previousTarget': array([ 9.80580676, 16.03883865]), 'currentState': array([ 0.       , 18.       ,  1.2677314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5801648453480991
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.46895  , 13.748537 ,  0.2547986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9774409604335372}
episode index:1353
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([24.338196 , 13.887163 ,  3.7933216], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.404270659717493}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.580178723605941
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.446426 , 13.870446 ,  3.1133094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2579091840827743}
episode index:1354
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.16275038, 19.5716586 ]), 'previousTarget': array([16.83772234, 20.51316702]), 'currentState': array([21.439137 , 28.611156 ,  5.5433955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5799875961896744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.654764, 16.262568,  5.008801], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3089173073549647}
episode index:1355
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6822128, 11.401844 ]), 'previousTarget': array([10.6822128, 11.401844 ]), 'currentState': array([3.      , 5.      , 5.473832], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.5796770911107734
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.240669, 15.559125,  0.898698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8460404632839107}
episode index:1356
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.68965565, 10.2208831 ]), 'previousTarget': array([16.25278872, 10.61523948]), 'currentState': array([17.117922  ,  0.32340586,  3.2310762 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5795142843359548
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.254211  , 14.030628  ,  0.32307053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9968623948792197}
episode index:1357
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.94427191, 13.47213595]), 'previousTarget': array([11.94427191, 13.47213595]), 'currentState': array([3.       , 9.       , 3.5629673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5794707101373644
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.861587 , 15.166356 ,  1.6286569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1505040128189596}
episode index:1358
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42535625, 12.701425  ]), 'previousTarget': array([14.42535625, 12.701425  ]), 'currentState': array([12.      ,  3.      ,  4.330897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5794762775191158
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.188108, 14.12192 ,  1.316277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.013448817991124}
episode index:1359
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.83772234, 20.51316702]), 'previousTarget': array([16.83772234, 20.51316702]), 'currentState': array([20.       , 30.       ,  1.0158088], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.579347789059674
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.527276, 16.17948 ,  5.780142], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2706848813303913}
episode index:1360
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.        , 22.        ,  0.81230664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5794879032086421
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.735187, 15.665803,  3.362127], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9918633366007384}
episode index:1361
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.45299804, 18.67949706]), 'previousTarget': array([17.45299804, 18.67949706]), 'currentState': array([23.       , 27.       ,  1.5539088], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5792959069976366
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.97744 , 15.480014,  4.854779], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0348665248931748}
episode index:1362
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74157276, 15.14357069]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 6.      , 20.      ,  6.086321], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5795547253695437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.739106, 15.832943,  5.867924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8728456709132741}
episode index:1363
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.1768175 , 22.68944732]), 'previousTarget': array([22.1768175 , 22.68944732]), 'currentState': array([29.       , 30.       ,  2.2267537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5793303371185559
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0016775, 16.813126 ,  4.541485 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.069800054768065}
episode index:1364
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([11.660064  ,  6.1154323 ,  0.08524019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.491613038915412}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5794873216947648
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.537037, 13.002285,  2.555866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5205847784782205}
episode index:1365
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.12652114, 14.57826285]), 'previousTarget': array([15.12652114, 14.57826285]), 'currentState': array([18.      ,  5.      ,  5.868529], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.5792025297021369
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.884129  , 13.0816    ,  0.13582349], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.688902806802584}
episode index:1366
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.       , 16.       ,  1.4837624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.579077890085985
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.712818 , 15.899352 ,  5.6727047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1475817449431491}
episode index:1367
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.48184699, 11.09856601]), 'previousTarget': array([11.40757591,  9.41178475]), 'currentState': array([7.0589075, 2.6966777, 0.9780832], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5793291085102115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.832836 , 13.794992 ,  2.5021303], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6775919209255643}
episode index:1368
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.8085497 , 17.06080701]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([29.       , 21.       ,  4.3042684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5791429262029457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.658536 , 14.776162 ,  3.5713725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6955379150750719}
episode index:1369
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.85504245, 13.57492926]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([21.       ,  5.       ,  4.3957176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5792234416479575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.430037 , 14.151552 ,  2.0823379], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.662789279568893}
episode index:1370
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.42337349, 20.3376506 ]), 'previousTarget': array([16.42337349, 20.3376506 ]), 'currentState': array([19.        , 30.        ,  0.89419293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.5788816957412846
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.706429 , 16.921911 ,  3.1739364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.57014418537018}
episode index:1371
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.25278872, 19.38476052]), 'previousTarget': array([16.25278872, 19.38476052]), 'currentState': array([19.       , 29.       ,  1.8375604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5789572590789442
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.079532 , 16.164219 ,  3.5324953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.166932277218765}
episode index:1372
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.73462344, 13.19231921]), 'previousTarget': array([13.73462344, 13.19231921]), 'currentState': array([8.      , 5.      , 3.155346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5789261672844274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.785933 , 13.103672 ,  1.3715918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.052742004682522}
episode index:1373
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.03976  ,  9.754406 ,  1.6503327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.274300204968946}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5790824177330148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.509907 , 13.464161 ,  2.5322883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1399017547113077}
episode index:1374
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28850076, 20.84955419]), 'previousTarget': array([14.7124705, 19.025413 ]), 'currentState': array([13.081069 , 30.776392 ,  1.6546899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5789802614305253
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.59727  , 16.1865   ,  4.1107473], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.989736665465866}
episode index:1375
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.95893206, 15.09464254]), 'currentState': array([ 5.8072424, 15.143335 ,  5.840292 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.193874995471893}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5790456622278648
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.359591 , 13.131416 ,  6.0219536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4864730431150424}
episode index:1376
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.77895573, 16.21147869]), 'previousTarget': array([12.77895573, 16.21147869]), 'currentState': array([ 4.      , 21.      ,  1.032501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5789018724354732
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.377033 , 16.580254 ,  4.8761234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6246093385731277}
episode index:1377
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       , 22.       ,  1.1390443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5788337225790426
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.732691 , 14.021855 ,  2.7423882], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6008871065815788}
episode index:1378
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.      , 10.      ,  6.202279], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5790070896675258
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.436367 , 15.277429 ,  1.5643246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4629138395917811}
episode index:1379
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.24451 ,  9.041742,  3.535907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.937614659855592}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5790172051434199
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.272964, 14.571007,  1.140123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3433070193578187}
episode index:1380
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32050294, 19.45299804]), 'previousTarget': array([ 8.32050294, 19.45299804]), 'currentState': array([ 0.        , 25.        ,  0.53385127], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5790449190723111
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.952507, 15.785979,  3.916902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.309581978913063}
episode index:1381
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.94085849, 18.44220991]), 'previousTarget': array([13.94085849, 18.44220991]), 'currentState': array([11.       , 28.       ,  1.8803428], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5790062443750994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.148046 , 16.079496 ,  4.8917575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5758563366174185}
episode index:1382
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.59242409,  9.41178475]), 'previousTarget': array([18.59242409,  9.41178475]), 'currentState': array([24.      ,  1.      ,  5.999244], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5788984228054311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.298321 , 14.316823 ,  3.3144913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7454703800580658}
episode index:1383
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.       , 20.       ,  1.0420504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5789306677274584
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.521526 , 14.770445 ,  4.7830343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.49618846769791}
episode index:1384
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.55689083, 21.45069462]), 'previousTarget': array([ 7.55689083, 21.45069462]), 'currentState': array([ 0.       , 28.       ,  1.1504543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5790361187514514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.861746  , 14.041624  ,  0.43754226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4879875571819914}
episode index:1385
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.10366477, 15.86197056]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.      , 20.      ,  1.389587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5792451445047109
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.704596 , 16.524273 ,  6.0336647], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0003701104649205}
episode index:1386
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.6284586 , 10.75724629]), 'previousTarget': array([19.6284586 , 10.75724629]), 'currentState': array([27.       ,  4.       ,  3.6738355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5790638017598897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.185923, 14.21429 ,  2.381725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1313985621520921}
episode index:1387
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.       , 21.       ,  2.0311747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499368}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5791049557111173
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.847396 , 16.70467  ,  4.3332767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0577647739832368}
episode index:1388
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.      ,  8.      ,  3.175144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5789216155917165
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.830935 , 14.204868 ,  3.7053921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9961351725283378}
episode index:1389
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 15.1695452 ]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.      , 16.      ,  3.646128], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.578857564266114
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.641042, 16.705555,  1.483639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7429196051770273}
episode index:1390
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.38086138, 17.43192731]), 'previousTarget': array([15.90470911, 19.22197586]), 'currentState': array([16.928091 , 27.311506 ,  4.1640215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5788763626146916
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.472315 , 16.149061 ,  5.6812344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8676328923035805}
episode index:1391
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.435359, 12.91864 ,  2.205674], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1565895154269743}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.579171710055342
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.249201 , 14.528929 ,  3.5955837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8130653274795034}
episode index:1392
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.035252, 17.62615 ,  3.392891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.67894562067503}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.579459526487463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.429375 , 16.02635  ,  3.5554442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.759689286366838}
episode index:1393
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.45299804,  8.32050294]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([25.       ,  0.       ,  3.7897992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5793848353221246
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.703673 , 13.82833  ,  1.6540794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3667357815746093}
episode index:1394
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       ,  6.       ,  3.5703886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5795617412637113
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.555488 , 13.994744 ,  1.4530447], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7598735749544716}
episode index:1395
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.85504245, 13.57492926]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([21.      ,  5.      ,  1.328069], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5793005039474093
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.165462 , 15.407632 ,  3.1661077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.43993325182612814}
episode index:1396
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.62286563, 14.76714487]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([26.52149  , 13.346851 ,  2.2890122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5791276005092929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.877453 , 15.298819 ,  3.6345608], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9269390774267487}
episode index:1397
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.       , 24.       ,  1.7133863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.578940806133978
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.137056, 13.491177,  4.949866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.738165636087342}
episode index:1398
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 18.       ,  2.6188672], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.5786350249537201
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.40543  , 16.212215 ,  5.8977203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0030277651737705}
episode index:1399
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3177872, 11.401844 ]), 'previousTarget': array([19.3177872, 11.401844 ]), 'currentState': array([27.       ,  5.       ,  0.5165165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5783676738371434
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.534399 , 16.505774 ,  2.7872272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5977910666869368}
episode index:1400
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([ 7.986394  , 10.232896  ,  0.39121348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.480327357591493}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5785270335200141
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.687218, 16.987288,  6.234137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.102755305974008}
episode index:1401
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.82178448,  8.86318339]), 'previousTarget': array([19.82178448,  8.86318339]), 'currentState': array([26.        ,  1.        ,  0.12376039], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5786527055982478
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.133644, 15.909427,  3.530187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9191940216676822}
episode index:1402
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.       , 11.       ,  4.6638055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5786586814188744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.262562 , 13.639732 ,  4.1200304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3853760312512966}
episode index:1403
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.127579 , 14.702882 ,  2.9499354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.134777940149873}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5789376275147299
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.436543 , 16.240269 ,  2.0984528], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3148523349732044}
episode index:1404
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.        , 21.        ,  0.05053367], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.579107712432938
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.382193 , 16.627123 ,  5.3376565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1349438585779765}
episode index:1405
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.95893206, 14.90535746]), 'currentState': array([ 5.9989734 , 14.064071  ,  0.14618067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.049554909099351}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5793790412363286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.268639 , 13.586922 ,  5.2706513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2348160024290142}
episode index:1406
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38476052, 13.74721128]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([29.       , 11.       ,  6.1955686], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5795724163108549
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.477657 , 16.6613   ,  3.0585194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7286053319771704}
episode index:1407
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([15.       ,  3.       ,  0.7109505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5794030936156868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.928736 , 13.911888 ,  1.3513825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4305724447367107}
episode index:1408
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.63956931, 10.59203403]), 'previousTarget': array([11.70588235,  8.82352941]), 'currentState': array([7.9188776, 1.7764189, 0.8241415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5795782288394057
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.579868 , 13.996769 ,  3.3100638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0876502968312458}
episode index:1409
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74157276, 14.85642931]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([ 6.      , 10.      ,  3.518341], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5797024423559761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.487265 , 14.500619 ,  5.8650846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7157366868046461}
episode index:1410
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.35636161, 16.52590681]), 'previousTarget': array([16.35636161, 16.52590681]), 'currentState': array([23.       , 24.       ,  1.6927555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5796284789255365
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.456782 , 16.971106 ,  4.2288427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.02334062803629}
episode index:1411
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89949494, 14.41421356]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([ 1.       , 13.       ,  4.7979293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5794824702863925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.681794 , 16.08791  ,  5.8733945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.709155970461102}
episode index:1412
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 19.       ,  1.2051032], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5793705465439073
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.053633, 16.882877,  4.64418 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7080569861360244}
episode index:1413
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([15.673604, 24.90499 ,  5.688853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.92786755851737}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5792196708683269
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.155766 , 15.321906 ,  4.9215074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1997572666768224}
episode index:1414
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.38709722, 13.7379615 ]), 'previousTarget': array([14.16227766, 12.48683298]), 'currentState': array([10.018561 ,  4.7426353,  2.4954987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.579037325570586
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.701775 , 13.033967 ,  5.7984633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3559870293295533}
episode index:1415
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.005379 , 11.146577 ,  3.2120166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.995261145552412}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5792932880167939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.178655 , 14.455824 ,  1.6249344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.900901480390736}
episode index:1416
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.50791373, 8.59256602]), 'previousTarget': array([9.50791373, 8.59256602]), 'currentState': array([3.       , 1.       , 3.7574677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5793809058057725
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.079986  , 13.077224  ,  0.53183883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9244391650428117}
episode index:1417
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.05572809, 14.47213595]), 'previousTarget': array([16.05572809, 14.47213595]), 'currentState': array([25.      , 10.      ,  4.984849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5791502850422207
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.256855, 13.804371,  3.168539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7347084800997081}
episode index:1418
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52786405, 16.05572809]), 'previousTarget': array([15.52786405, 16.05572809]), 'currentState': array([20.       , 25.       ,  1.3349363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5789489266465074
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.212531 , 16.476973 ,  5.6225977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.492185466376842}
episode index:1419
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.41421356, 19.10050506]), 'previousTarget': array([14.41421356, 19.10050506]), 'currentState': array([13.       , 29.       ,  1.2896184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5786880418120625
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.892327 , 13.475669 ,  1.1269194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8842834372073354}
episode index:1420
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       ,  6.       ,  4.3110094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.999999999999998}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5785743421907543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.020004, 16.115654,  5.329275], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5116522566493151}
episode index:1421
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92893219, 17.92893219]), 'previousTarget': array([17.92893219, 17.92893219]), 'currentState': array([25.      , 25.      ,  3.203936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5788163747872643
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.454216, 15.0738  ,  4.352746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.456087441548227}
episode index:1422
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.54143274, 12.747793  ]), 'previousTarget': array([11.72672794, 11.39940073]), 'currentState': array([5.167681 , 5.9929585, 1.48387  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.578699813505709
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.635978, 16.982687,  5.845237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4065752508312555}
episode index:1423
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.476477, 22.704285,  4.054602], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.844487593215991}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.578967999036955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.207474 , 15.575361 ,  3.7533379], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8826021644369055}
episode index:1424
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.976622 , 17.254662 ,  5.1012506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2547827608646065}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5792564425464027
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.734892, 15.403979,  5.164185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8386091867875441}
episode index:1425
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.57464375, 17.298575  ]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.       , 27.       ,  1.9711787], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5792919034080134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.523352, 14.809161,  4.871702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5352588580561344}
episode index:1426
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.      ,  9.      ,  4.271209], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.579193322888323
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.125286 , 14.239486 ,  3.0993319], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7707649097391868}
episode index:1427
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.78280103, 20.65196555]), 'previousTarget': array([ 9.78280103, 20.65196555]), 'currentState': array([ 3.       , 28.       ,  2.8777215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5789773291464629
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.911435, 16.024548,  6.126688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0283683552230785}
episode index:1428
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 12.      ,  5.535562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5789871185917079
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.60746  , 15.99658  ,  2.8196206], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1671245099175513}
episode index:1429
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.13733471, 10.81238194]), 'previousTarget': array([ 9.13733471, 10.81238194]), 'currentState': array([1.       , 5.       , 3.1757534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5788331054302325
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.096365 , 16.769842 ,  4.2505217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5992628833431057}
episode index:1430
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36613715, 19.54057759]), 'previousTarget': array([20.36613715, 19.54057759]), 'currentState': array([28.       , 26.       ,  2.8583407], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5786508244787223
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.04221 , 16.850615,  5.217434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1239056257799134}
episode index:1431
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.57492926, 11.14495755]), 'previousTarget': array([ 8.57492926, 11.14495755]), 'currentState': array([0.       , 6.       , 5.5700192], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5788294997140375
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.85     , 13.507827 ,  1.3932499], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8839002006884125}
episode index:1432
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42173715, 14.87347886]), 'previousTarget': array([15.42173715, 14.87347886]), 'currentState': array([25.       , 12.       ,  5.2863703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5789469753557804
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.691801, 13.005199,  2.740981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.38550073192289}
episode index:1433
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.60866103, 19.30005806]), 'previousTarget': array([16.25278872, 19.38476052]), 'currentState': array([17.010162 , 29.20136  ,  3.0998309], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5787106054145171
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.885844 , 16.7641   ,  5.7708354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9740227227726936}
episode index:1434
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        ,  8.        ,  0.44129816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5789439201476664
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.081829 , 13.903891 ,  1.4711559], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4298579125884239}
episode index:1435
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.195289  , 12.74099823]), 'previousTarget': array([19.195289  , 12.74099823]), 'currentState': array([28.      ,  8.      ,  5.858077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.57902572327991
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.366875, 15.507467,  5.337696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6261949151179257}
episode index:1436
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.740457 , 18.016912 ,  3.9963636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.4414394167734645}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5791172525102457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.961799, 15.9963  ,  5.350376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4389145857763488}
episode index:1437
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.41495392, 15.47423305]), 'previousTarget': array([15.41495392, 15.47423305]), 'currentState': array([22.       , 23.       ,  4.6511145], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5793758566808923
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.357035 , 16.086403 ,  3.9140537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7383366697925706}
episode index:1438
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28601658, 11.75397274]), 'previousTarget': array([15.90470911, 10.77802414]), 'currentState': array([16.163744 ,  1.7925675,  3.1627262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5792811187552115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.313814 , 16.201027 ,  1.9132911], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.780048681809809}
episode index:1439
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.      , 13.      ,  5.967317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5794525684115173
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.517426 , 16.136158 ,  2.5226114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8956358399705349}
episode index:1440
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.       , 14.       ,  2.0835993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.579128824266175
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.577244 , 13.488656 ,  0.3769673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6178290897528393}
episode index:1441
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51658317, 17.75902574]), 'previousTarget': array([10.51658317, 17.75902574]), 'currentState': array([ 2.       , 23.       ,  1.6193807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5792053293019607
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.480848 , 13.96524  ,  5.9564652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8380835375345772}
episode index:1442
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33480989, 20.02214842]), 'previousTarget': array([15.33480989, 20.02214842]), 'currentState': array([16.       , 30.       ,  1.3568629], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5791267953074131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.997711 , 15.103714 ,  4.4175763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.000401598884522}
episode index:1443
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 14.       ,  2.3296518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5789741786193068
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.84811 , 13.089707,  5.486306], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2307102071914304}
episode index:1444
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96116135, 20.19419324]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([12.       , 30.       ,  0.4865963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.5787429614016013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.8095875, 15.130229 ,  0.6103961], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.23068706024141747}
episode index:1445
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.       , 10.       ,  2.9765966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5790070368155699
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.174244 , 13.834579 ,  2.1498742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6544043692629118}
episode index:1446
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.236435 , 10.5220302]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([4.802439 , 2.866725 , 6.1271157], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5789775006638018
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.174451 , 14.151279 ,  2.0402203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8664638944918777}
episode index:1447
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 12.       ,  3.6552556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5791597974129179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.738144 , 14.241659 ,  1.8650885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4721961782522497}
episode index:1448
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6822128, 18.598156 ]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 3.       , 25.       ,  1.6269633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5792809564810965
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.077226 , 15.77236  ,  3.9303684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0721005769028107}
episode index:1449
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.69646944,  9.58300913]), 'previousTarget': array([15., 10.]), 'currentState': array([16.971685 , -0.3353488,  6.2558203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5793381986083398
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.746994  , 13.944233  ,  0.75282884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6384956738105727}
episode index:1450
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.       , 19.       ,  5.8574815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5792504697673048
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.632631 , 14.565948 ,  2.5137153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6893449950461028}
episode index:1451
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  6.       ,  3.4724448], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.578984033708852
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.443115, 16.92345 ,  2.766604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.973832014474821}
episode index:1452
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86197056, 13.10366477]), 'previousTarget': array([15.86197056, 13.10366477]), 'currentState': array([20.      ,  4.      ,  4.756196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5789472907314375
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.078819, 13.828572,  1.514826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1740764017237157}
episode index:1453
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92980427, 13.32622456]), 'previousTarget': array([15.23303501, 11.97054486]), 'currentState': array([14.510787 ,  3.3350072,  2.3030782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5791966255723381
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.715121 , 14.121563 ,  0.9133686], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5564591192730128}
episode index:1454
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.32050294, 12.54700196]), 'previousTarget': array([11.32050294, 12.54700196]), 'currentState': array([3.       , 7.       , 3.5037458], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.579035399151565
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.442519 , 13.69299  ,  0.6029793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4209360308535857}
episode index:1455
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.49192996, 14.75828524]), 'previousTarget': array([11.74391196, 14.24859507]), 'currentState': array([ 3.6179566 , 13.175676  ,  0.35169333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5792843309855275
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.4561    , 13.840826  ,  0.58856976], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9306242668778666}
episode index:1456
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.       , 10.       ,  2.8158858], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5793893543476886
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.700546 , 14.35992  ,  1.1373471], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4485451305731807}
episode index:1457
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.043041 , 14.587314 ,  2.5563924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.084305036745128}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5796709803049261
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.375838, 15.692046,  2.647606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7875165443612013}
episode index:1458
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.       , 24.       ,  2.3315668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5794630164169167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.240145, 16.920107,  2.55963 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9350658778980268}
episode index:1459
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.95893206, 15.09464254]), 'previousTarget': array([13.95893206, 15.09464254]), 'currentState': array([ 4.       , 16.       ,  2.3495018], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5794805116571363
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.726368 , 15.5298395,  5.3149076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8058450134734516}
episode index:1460
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.93691626, 12.96028011]), 'previousTarget': array([15., 11.]), 'currentState': array([14.627788 ,  2.9650593,  1.3943092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5795605477326958
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.186797 , 15.393407 ,  2.1388369], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8553904084934478}
episode index:1461
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6822128, 18.598156 ]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 3.       , 25.       ,  1.2722243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5796501460087147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.9604435, 15.3290825,  5.5806017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0904003889031904}
episode index:1462
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28476691, 14.71390676]), 'previousTarget': array([14.28476691, 14.71390676]), 'currentState': array([ 5.      , 11.      ,  4.149966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.5793099048214583
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.157885 , 13.327465 ,  1.7577832], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0342246375659716}
episode index:1463
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.       , 11.       ,  1.3890308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5795196554819054
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.904373 , 15.792805 ,  2.3332002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2026762447222106}
episode index:1464
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.      , 23.      ,  5.913737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.579735231317248
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.509844 , 14.935946 ,  3.8543918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.511201917416415}
episode index:1465
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.40743398, 20.49208627]), 'previousTarget': array([21.40743398, 20.49208627]), 'currentState': array([29.       , 27.       ,  2.6173427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5796330169804176
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.525915 , 16.108896 ,  3.6178603], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8862841614811126}
episode index:1466
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.      ,  9.      ,  4.810564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5794262130613438
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.832084  , 13.48561   ,  0.31208304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7279294934756737}
episode index:1467
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([20.710224 ,  6.978896 ,  0.6765064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.84605337064917}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5796537955098604
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.250263 , 13.62486  ,  3.9488516], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3977275195233907}
episode index:1468
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 1.       , 15.       ,  3.2033365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.579752724400567
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.126309 , 14.418203 ,  2.5242696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5953498136879597}
episode index:1469
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32050294, 10.54700196]), 'previousTarget': array([ 8.32050294, 10.54700196]), 'currentState': array([0.      , 5.      , 5.577526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5796123921257736
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.225986 , 13.337107 ,  6.1240363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8342062359145501}
episode index:1470
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.618567 , 11.44625  ,  1.7937098], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.6481146948082905}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5795829263426883
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.269476 , 14.469438 ,  2.0312233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3758872103132125}
episode index:1471
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.478982 , 21.058203 ,  4.9986377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.19655174753882}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5797853299409602
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.220438 , 14.054061 ,  4.5684423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.01535148599313}
episode index:1472
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.78885438, 16.1613009 ]), 'previousTarget': array([14.78885438, 16.1613009 ]), 'currentState': array([13.       , 26.       ,  1.9692965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.5795318487509918
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.884752, 16.570549,  5.289358], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8026122107224922}
episode index:1473
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96116135, 20.19419324]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([12.      , 30.      ,  2.003331], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5795790473803116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.566729, 15.806347,  6.255818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9855843998613213}
episode index:1474
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.92893219, 13.07106781]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([24.       ,  6.       ,  2.4411938], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5798244040596476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.87231 , 13.138384,  2.934073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8659902061299738}
episode index:1475
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38476052, 13.74721128]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([29.       , 11.       ,  5.4722977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5797228218167372
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.485373 , 14.24983  ,  1.4387599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6640571661911983}
episode index:1476
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.3216372 , 20.08772099]), 'previousTarget': array([14.3216372 , 20.08772099]), 'currentState': array([13.       , 30.       ,  0.5871284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.579727773719324
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.807405, 15.797329,  5.743511], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9754614631452294}
episode index:1477
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.     ,  7.     ,  5.52849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5795045729633965
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.854422 , 13.029376 ,  3.4270923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9759939011211531}
episode index:1478
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.       ,  9.       ,  2.5496979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.5792468107481331
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.979087  , 13.797762  ,  0.50118816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.577225416631593}
episode index:1479
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.99503719, 15.0496281 ]), 'currentState': array([13.017566 , 23.257925 ,  3.8384607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.492547998444802}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5791733917149152
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.150604, 13.420424,  5.45394 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7934692709078865}
episode index:1480
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.       , 21.       ,  2.0537975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5792919237172709
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.62056  , 14.77512  ,  5.7166915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4410737576867036}
episode index:1481
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.       ,  7.       ,  3.4121182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.579375699541348
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.32066  , 14.466365 ,  1.2404203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8638690816669836}
episode index:1482
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.10647783, 14.13940614]), 'previousTarget': array([16.10647783, 14.13940614]), 'currentState': array([24.       ,  8.       ,  1.8953773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.579588755883032
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.828537 , 16.509396 ,  3.8475208], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9106545123324397}
episode index:1483
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 15.1695452 ]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.       , 16.       ,  3.1563632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5794375241167397
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.651324 , 14.768888 ,  0.7206605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3683342999984984}
episode index:1484
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.      , 17.      ,  5.321582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 353
reward sum = 0.02878880863894463
running average episode reward sum: 0.5790667169009297
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.78191  , 14.23891  ,  2.8126352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.091165235240341}
episode index:1485
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.0496281 , 15.00496281]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([25.     , 16.     ,  0.51784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5791798429961051
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.794564 , 16.475552 ,  3.5762613], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9053419362347892}
episode index:1486
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.96545758, 15.1695452 ]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.       , 16.       ,  1.4173353], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5792634124998062
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.689568  , 13.303578  ,  0.27806425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.143613651770393}
episode index:1487
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.55779009, 16.05914151]), 'previousTarget': array([11.55779009, 16.05914151]), 'currentState': array([ 2.      , 19.      ,  4.210057], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5794819062246106
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.626617 , 14.090054 ,  6.1333222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6474775281458625}
episode index:1488
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.       , 23.       ,  5.2641463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.579731408000081
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.826554 , 15.0912075,  5.3067327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8315712940214929}
episode index:1489
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.09955992,  8.25890849]), 'previousTarget': array([21.19548901,  8.32793492]), 'currentState': array([26.132627  ,  0.28381628,  2.960044  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5795473325758718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.485787 , 13.423796 ,  1.7008778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6493663586377496}
episode index:1490
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.     , 10.     ,  1.48029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5794142003059379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.734535 , 13.835915 ,  1.6956182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3764580547240441}
episode index:1491
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84615385, 17.76923077]), 'previousTarget': array([13.84615385, 17.76923077]), 'currentState': array([10.       , 27.       ,  3.5182934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5793852820920647
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.328006, 14.456634,  5.982896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7580704523791464}
episode index:1492
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.09464254, 16.04106794]), 'currentState': array([15.665681 , 24.028141 ,  5.0232205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.052649410492538}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5796341801281049
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.963453, 16.405342,  4.51244 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4145673022276637}
episode index:1493
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.56835598, 12.9251069 ]), 'previousTarget': array([20.31756858, 11.96138938]), 'currentState': array([27.213146 ,  7.898418 ,  3.0078223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5794791994951494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.73064  , 16.050423 ,  2.7754617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.024476232485672}
episode index:1494
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.52835287, 20.16112546]), 'previousTarget': array([ 9.07106781, 20.92893219]), 'currentState': array([ 3.9801583, 27.718979 ,  5.2183313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5796904764548575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.0078   , 13.85753  ,  0.1559326], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5131751914942828}
episode index:1495
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.45299804,  8.32050294]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([25.       ,  0.       ,  0.6502529], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5798074743230051
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.293893 , 13.362568 ,  1.3119208], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6635975746060188}
episode index:1496
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.2365781 , 10.50535523]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([27.82474  ,  3.9923062,  1.4672949], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5799832496864392
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.382235 , 15.479071 ,  4.2742944], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4629016085793172}
episode index:1497
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.92893219, 13.07106781]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([24.      ,  6.      ,  3.503016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5802182844649576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.042129 , 13.615173 ,  2.5044394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3854673198938432}
episode index:1498
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.35601013, 12.86393924]), 'previousTarget': array([15.35601013, 12.86393924]), 'currentState': array([17.      ,  3.      ,  3.273371], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5803449187954047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.548799 , 14.610157 ,  2.1030529], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6731696778833252}
episode index:1499
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.78380153, 14.09704457]), 'previousTarget': array([14.16227766, 12.48683298]), 'currentState': array([12.455275 ,  4.3719244,  1.4450362], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5802593846163855
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.843226  , 16.504559  ,  0.78592384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8978465575403995}
episode index:0
done in step count: 25
reward sum = 1.0
episode index:1
done in step count: 106
reward sum = 1.0
episode index:2
done in step count: 94
reward sum = 1.0
episode index:3
done in step count: 24
reward sum = 1.0
episode index:4
done in step count: 88
reward sum = 1.0
episode index:5
done in step count: 98
reward sum = 1.0
episode index:6
done in step count: 0
reward sum = 1.0
episode index:7
done in step count: 105
reward sum = 1.0
episode index:8
done in step count: 122
reward sum = 1.0
episode index:9
done in step count: 118
reward sum = 1.0
episode index:10
done in step count: 11
reward sum = 1.0
episode index:11
done in step count: 102
reward sum = 1.0
episode index:12
done in step count: 43
reward sum = 1.0
episode index:13
done in step count: 94
reward sum = 1.0
episode index:14
done in step count: 196
reward sum = 1.0
episode index:15
done in step count: 58
reward sum = 1.0
episode index:16
done in step count: 61
reward sum = 1.0
episode index:17
done in step count: 129
reward sum = 1.0
episode index:18
done in step count: 8
reward sum = 1.0
episode index:19
done in step count: 177
reward sum = 1.0
episode index:20
done in step count: 12
reward sum = 1.0
episode index:21
done in step count: 98
reward sum = 1.0
episode index:22
done in step count: 6
reward sum = 1.0
episode index:23
done in step count: 55
reward sum = 1.0
episode index:24
done in step count: 19
reward sum = 1.0
episode index:25
done in step count: 8
reward sum = 1.0
episode index:26
done in step count: 113
reward sum = 1.0
episode index:27
done in step count: 71
reward sum = 1.0
episode index:28
done in step count: 19
reward sum = 1.0
episode index:29
done in step count: 180
reward sum = 1.0
episode index:30
done in step count: 58
reward sum = 1.0
episode index:31
done in step count: 82
reward sum = 1.0
episode index:32
done in step count: 26
reward sum = 1.0
episode index:33
done in step count: 52
reward sum = 1.0
episode index:34
done in step count: 26
reward sum = 1.0
episode index:35
done in step count: 18
reward sum = 1.0
episode index:36
done in step count: 9
reward sum = 1.0
episode index:37
done in step count: 15
reward sum = 1.0
episode index:38
done in step count: 175
reward sum = 1.0
episode index:39
done in step count: 2
reward sum = 1.0
episode index:40
done in step count: 35
reward sum = 1.0
episode index:41
done in step count: 49
reward sum = 1.0
episode index:42
done in step count: 43
reward sum = 1.0
episode index:43
done in step count: 12
reward sum = 1.0
episode index:44
done in step count: 7
reward sum = 1.0
episode index:45
done in step count: 24
reward sum = 1.0
episode index:46
done in step count: 39
reward sum = 1.0
episode index:47
done in step count: 0
reward sum = 1.0
episode index:48
done in step count: 18
reward sum = 1.0
episode index:49
done in step count: 131
reward sum = 1.0
episode index:50
done in step count: 50
reward sum = 1.0
episode index:51
done in step count: 31
reward sum = 1.0
episode index:52
done in step count: 2
reward sum = 1.0
episode index:53
done in step count: 124
reward sum = 1.0
episode index:54
done in step count: 36
reward sum = 1.0
episode index:55
done in step count: 6
reward sum = 1.0
episode index:56
done in step count: 69
reward sum = 1.0
episode index:57
done in step count: 8
reward sum = 1.0
episode index:58
done in step count: 9
reward sum = 1.0
episode index:59
done in step count: 8
reward sum = 1.0
episode index:60
done in step count: 8
reward sum = 1.0
episode index:61
done in step count: 17
reward sum = 1.0
episode index:62
done in step count: 95
reward sum = 1.0
episode index:63
done in step count: 76
reward sum = 1.0
episode index:64
done in step count: 40
reward sum = 1.0
episode index:65
done in step count: 72
reward sum = 1.0
episode index:66
done in step count: 34
reward sum = 1.0
episode index:67
done in step count: 38
reward sum = 1.0
episode index:68
done in step count: 45
reward sum = 1.0
episode index:69
done in step count: 65
reward sum = 1.0
episode index:70
done in step count: 2
reward sum = 1.0
episode index:71
done in step count: 47
reward sum = 1.0
episode index:72
done in step count: 170
reward sum = 1.0
episode index:73
done in step count: 27
reward sum = 1.0
episode index:74
done in step count: 0
reward sum = 1.0
episode index:75
done in step count: 137
reward sum = 1.0
episode index:76
done in step count: 61
reward sum = 1.0
episode index:77
done in step count: 82
reward sum = 1.0
episode index:78
done in step count: 14
reward sum = 1.0
episode index:79
done in step count: 12
reward sum = 1.0
episode index:80
done in step count: 9
reward sum = 1.0
episode index:81
done in step count: 8
reward sum = 1.0
episode index:82
done in step count: 7
reward sum = 1.0
episode index:83
done in step count: 18
reward sum = 1.0
episode index:84
done in step count: 171
reward sum = 1.0
episode index:85
done in step count: 16
reward sum = 1.0
episode index:86
done in step count: 49
reward sum = 1.0
episode index:87
done in step count: 30
reward sum = 1.0
episode index:88
done in step count: 96
reward sum = 1.0
episode index:89
done in step count: 80
reward sum = 1.0
episode index:90
done in step count: 78
reward sum = 1.0
episode index:91
done in step count: 17
reward sum = 1.0
episode index:92
done in step count: 170
reward sum = 1.0
episode index:93
done in step count: 218
reward sum = 1.0
episode index:94
done in step count: 11
reward sum = 1.0
episode index:95
done in step count: 141
reward sum = 1.0
episode index:96
done in step count: 114
reward sum = 1.0
episode index:97
done in step count: 83
reward sum = 1.0
episode index:98
done in step count: 13
reward sum = 1.0
episode index:99
done in step count: 20
reward sum = 1.0

Process finished with exit code 0
