/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/TimedMaze/DQNExample/DQNHER_CNNDynMazeStochAgentCPU.py
episode index:0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.     , 10.     ,  6.07042], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.394064,  8.728472,  2.460682], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.425364335167377}
episode index:1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      , 12.      ,  4.504295], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.150296, 10.584608,  4.921687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.48537672055772}
episode index:2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  0.       ,  3.0373895], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.36394763,  4.0463457 ,  4.463694  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5229225088435148}
episode index:3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 3.5207996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.16557051024599587
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7376935, 4.2361274, 1.3844372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7667564827450775}
episode index:4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 3.      , 1.618727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.2649128163935934
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6620717, 1.5134107, 4.7558923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.229894646908055}
episode index:5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  9.      ,  4.234384], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22076068032799448
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.953959 ,  9.722843 ,  5.5447817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.39565477344229}
episode index:6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  1.       ,  5.0393147], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.18922344028113813
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.592616  , -0.30657256,  0.06026992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.206870977205917}
episode index:7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  0.4816844], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16557051024599587
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.662701  ,  7.3357053 ,  0.37244815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.590850966216749}
episode index:8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.     , 6.     , 3.03928], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.24566099485378848
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.66599  , 4.077696 , 5.2242765], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9841750436727694}
episode index:9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 4.       , 2.4675674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.22109489536840962
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.499862 , 2.5552604, 5.7156367], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.528006330999898}
episode index:10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  2.8887682], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276355}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.20099535942582694
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.92545  , 7.791616 , 3.9767256], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.871654803873171}
episode index:11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 5.5666213], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.18424574614034137
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.392656  , 11.779741  ,  0.19432259], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.720545303407384}
episode index:12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 5.       , 1.4974774], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.1830593161742144
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2669034, 3.5177562, 6.2697716], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.897497705552497}
episode index:13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 3.6960156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.22073745268121994
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.015592 , 2.7507505, 2.957296 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.24973667331155383}
episode index:14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  6.       ,  1.7675766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505136}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.20602162250247194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.819385 ,  4.4091234,  4.5211105], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.903087017406845}
episode index:15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 13.       ,  1.8663678], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.19314527109606744
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.44222  , 13.103699 ,  1.2593584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.548759405139261}
episode index:16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.        , 9.        , 0.10070818], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.18178378456100464
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.220998 , 11.622159 ,  1.5005629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.624120821373566}
episode index:17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  7.       ,  3.1733587], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1716846854187266
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.365065  , 12.454653  ,  0.95600563], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.19725387593407}
episode index:18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 2.      , 4.200751], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.19144636736462317
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4531789, 3.9479809, 1.0745606], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8142004628710506}
episode index:19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 6.      , 5.099857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639887}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.181874048996392
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.4669333, 2.3954573, 6.0741854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5399274747847143}
episode index:20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  9.      ,  0.494775], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.17321337999656383
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.2967399e+01, -5.1685572e-03,  5.0944748e+00], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.410575093399242}
episode index:21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  3.       ,  5.5541325], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16534004454217455
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.8991694, 13.370973 ,  2.053521 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.543430084318153}
episode index:22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.        , 13.        ,  0.07763817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.19803902718557}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.15815134695338437
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.591541  ,  6.176501  ,  0.89247215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.159953005212914}
episode index:23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.        , 11.        ,  0.29877168], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.15156170749699335
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.805646  ,  0.30276608,  4.8935776 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.209477252373139}
episode index:24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  0.       ,  5.2367296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1454992391971136
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.844296 ,  5.144248 ,  3.7686267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0220486466879}
episode index:25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 11.        ,  0.93807894], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.13990311461260924
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.8718   ,  4.9512887,  6.0913715], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.999264883224345}
episode index:26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 4.        , 0.16203865], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.14292343582098893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.084159 , 2.7185547, 3.155789 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1200946173867994}
episode index:27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  1.       ,  3.2509856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.13781902739881075
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.537659 ,  4.9717436,  2.075455 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.762384874606044}
episode index:28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 12.      ,  5.623641], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.1402560423759125
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4896922, 4.277422 , 5.7186575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3680662642246926}
episode index:29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  3.6961832], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.13558084096338208
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.717901 , 13.46625  ,  1.4061847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.621460997253315}
episode index:30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 1.926609], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.13120726544843428
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 4.341002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
episode index:31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.724936 , 11.987777 ,  4.7399864], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.57896387134378}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1271070384031707
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.480974 ,  7.1716027,  5.455783 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.03317907975862}
episode index:32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  2.       ,  0.6079523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.12325530996671098
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0428448, 8.833502 , 2.270169 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.833659152659745}
episode index:33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 0.9809603], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11963015379121948
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.620275 ,  2.4891915,  2.6596901], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.629850693126183}
episode index:34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  9.      ,  5.406824], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11621214939718463
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.267374  , 13.077251  ,  0.52333736], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.46736838881844}
episode index:35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 11.       ,  3.3268871], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11298403413615173
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.488322 ,  5.9244967,  3.2630875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.928793478137504}
episode index:36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  1.       ,  2.5473597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.10993041159193141
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.27618575, 7.992055  , 4.104306  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.6868072395639055}
episode index:37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 13.       ,  5.1345897], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1070375060237227
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.835149  , -0.14879638,  4.3328533 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.379486828099799}
episode index:38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        , 13.        ,  0.21388724], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.10429295458721698
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.419712, 7.458759, 4.443907], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.816216049466177}
episode index:39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.      , 8.      , 4.529929], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.10168563072253656
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.861503 ,  6.350298 ,  3.5834184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.473685944513607}
episode index:40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 13.      ,  4.892755], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09920549338784054
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.190381,  5.915522,  3.104348], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.534179640246109}
episode index:41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  1.       ,  1.6156514], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.16552506059644}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0968434578309872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.2223635, 6.621685 , 1.337738 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.847703455057193}
episode index:42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      , 12.      ,  2.141067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09459128439305726
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.674818 , 13.369467 ,  5.5878167], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.614967741798601}
episode index:43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  1.       ,  4.6350245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09244148247503324
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.520211 , 6.5161757, 5.901192 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.407877309584473}
episode index:44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.3536606, 2.892682 , 2.193361 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.35610595670527}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09038722730892139
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.142912  , -0.31389666,  5.362493  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.501280340395116}
episode index:45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 8.      , 5.002831], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0884222875848144
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.746346 , 7.2450724, 3.1850667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.144308026546931}
episode index:46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 9.      , 4.590746], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08654096231705238
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9292858, 9.5814705, 4.1064153], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.581850371078026}
episode index:47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  3.3025503], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0847380256021138
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.402363 , 12.605459 ,  4.9096346], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.15206950305817}
episode index:48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  2.       ,  0.7134597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08300867814084617
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.592199 , 11.999002 ,  3.4519138], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.67518757694812}
episode index:49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1897397, 2.8502693, 3.142374 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.24170319105884297}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.10134850457802924
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1897397, 2.8502693, 3.142374 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.24170319105884297}
episode index:50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  2.      ,  4.375407], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09936127899806789
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.797455 ,  6.8566194,  4.9214873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.326313997983679}
episode index:51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 11.       ,  3.0517502], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09745048517118196
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.7402644, 7.455242 , 5.564213 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.50532777102687}
episode index:52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 0.      , 0.824327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09561179677172571
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.640612 , 10.322239 ,  1.6819918], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.481649426964024}
episode index:53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  2.       ,  2.8649552], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09384120794261967
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.587996 , 13.391288 ,  5.8569837], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.910568828976576}
episode index:54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.268675 , 11.981872 ,  2.1716883], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.07102824400065}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09213500416184477
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.585019 , 13.496166 ,  1.1532834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.504366456370263}
episode index:55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.       ,  3.9866617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09048973623038326
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.16945  ,  8.150634 ,  4.7743497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.020026998150378}
episode index:56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  1.       ,  2.5557027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08890219699827127
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.668243,  3.991311,  5.935438], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.701702701603919}
episode index:57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 7.       , 3.6375425], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08736940049830108
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.49262032, 12.038303  ,  2.5940561 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.689650385671076}
episode index:58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.230532 ,  4.9321923,  3.1868365], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.430592903387032}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0858885632017197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.834995 , -0.4828354,  5.877353 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.238347258200829}
episode index:59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.0715437, 9.311275 , 5.294918 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.019014662719877}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0844570871483577
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.035349, 12.679365,  1.899547], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.444731555476773}
episode index:60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  3.      ,  5.469579], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08307254473608955
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.329649 , 11.433728 ,  4.6745224], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.496454190709414}
episode index:61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.879605 ,  2.6567183,  1.8000569], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.884179246952643}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.08173266498228166
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.35550034, 11.410871  ,  3.291984  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.816809053895767}
episode index:62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  8.       ,  2.8084826], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.09160115752223859
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.705781 , 2.536778 , 6.0164857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.374619041834803}
episode index:63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  8.      ,  5.446681], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.09016988943595361
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.9624  , 13.030947,  5.318519], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.1921062781139}
episode index:64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      , 12.      ,  4.113613], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.09703294075551101
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2638878, 1.1299617, 4.3580174], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5516913673481025}
episode index:65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8912845, 5.650418 , 0.6641196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.256020803082202}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.10559732106200302
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1537639, 4.3568726, 4.8384223], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.29122039820462}
episode index:66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 6.      , 6.078918], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.11243773702305354
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.83092  , 2.142045 , 1.3822782], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0219682479650665}
episode index:67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 13.      ,  4.196961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11078424089036157
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.509501 ,  6.780194 ,  3.4555008], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.311362964622575}
episode index:68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.997117, 12.107351,  5.956406], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.945893243437899}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1091786721818056
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.382126,  5.979484,  6.280366], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.668028631387175}
episode index:69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 9.       , 3.4873796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.11717571627306096
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.828811 , 1.54697  , 4.4004173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.335775340880234}
episode index:70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 13.      ,  2.944076], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11552535407203195
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.013458  , -0.45842135,  0.69145155], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.46516891487739}
episode index:71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.951557  , 11.437522  ,  0.21034917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.932333846716437}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.11591729272594294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7203226, 4.6928988, 5.30431  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8397746709165566}
episode index:72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.970382  ,  8.657077  ,  0.18433827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.605199989974828}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11432938460640947
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.639062 , 11.020885 ,  1.0721468], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.135146488179005}
episode index:73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  0.       ,  6.0393357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.1149107886266984
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8226719, 4.559212 , 3.629375 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9537769030575736}
episode index:74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.896703, 11.365557,  5.149872], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.841911650272367}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11337864477834242
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.996695 , 13.077102 ,  1.1976225], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.194431339252425}
episode index:75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  5.       ,  6.0086203], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.11188682050494318
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.294703 ,  1.0915048,  1.2525032], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.41331500229757}
episode index:76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 12.        ,  0.33424652], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1104337449139699
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.283877  , -0.26855707,  1.5592158 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.842451137951993}
episode index:77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.251678  ,  1.5599045 ,  0.50469804], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.352332421345258}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.1114353231223469
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2296715, 1.4974279, 0.6617137], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9416010761377243}
episode index:78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 10.       ,  5.1023035], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.12194221965751972
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5920868, 3.1730905, 4.102063 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6168687671403541}
episode index:79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.568361 ,  0.7589351,  5.2354226], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.76660006568249}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.12451447637977328
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3120794, 4.8948016, 4.0808916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9203298510741071}
episode index:80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8127694, 9.390502 , 3.439104 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.393244152864716}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.1347178785219983
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2773553, 2.4442272, 5.3689556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8100795841967958}
episode index:81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 8.       , 4.9106903], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.1443279616428022
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7939332, 4.9171333, 2.7142363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2649497674433525}
episode index:82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  3.1770322], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.15274501262763504
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.410676 , 4.9926043, 2.8514004], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.441409122571522}
episode index:83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  1.8710952], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.544003745317532}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.1564728193245655
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8673916, 2.4243836, 3.8901658], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0410102581420886}
episode index:84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.6283793, 9.101279 , 5.159361 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.643341521696218}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.16593309215604118
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9510942, 4.3627586, 3.7703815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6618336813543202}
episode index:85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 11.       ,  1.2885054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.16728108754423215
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7761788, 4.072241 , 4.5301156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6270951526628197}
episode index:86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 3.9966266], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.1746655332991405
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.058128 , 3.4955926, 2.4042583], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.168437687688334}
episode index:87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 12.       ,  2.3852549], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.212670403551897}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.1760487270957322
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4055314, 4.31447  , 5.2894936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4426447246224459}
episode index:88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 12.       ,  1.6828587], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.18073319585197678
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1724393, 4.38898  , 4.7287607], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6168246184141724}
episode index:89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  3.5167055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.17872504923139926
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.15510985,  9.107697  ,  4.5712395 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.874494535845875}
episode index:90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  2.5170722], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18659991961631486
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7480175, 4.7105393, 3.3332891], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.119765292040372}
episode index:91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        ,  1.        ,  0.70412385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.18457165962048536
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.751165 , 13.088265 ,  5.7688823], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.138835746794292}
episode index:92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  5.      ,  4.912967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.18258701811918981
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.920689  , -0.24258327,  3.5852387 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7687389052758533}
episode index:93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 0.      , 4.592901], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.16227766016838}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.18535787943309387
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.630855, 4.29625 , 1.659252], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8854235188056765}
episode index:94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5752363, 9.08451  , 4.585825 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.1116410998739195}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.19362041754432446
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.092727 , 3.1814122, 4.516585 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.107683661008178}
episode index:95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.        , 13.        ,  0.69250643], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.19302749657986898
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2931526, 4.773821 , 3.639604 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7978817907944311}
episode index:96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  6.       ,  4.9369583], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.19400230186633283
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4073336, 3.0329216, 4.313123 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5930066072325049}
episode index:97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 9.      , 5.315862], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.20192369674524782
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6374881, 3.3937035, 4.815228 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.749261988035708}
episode index:98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.9363456, 7.0010133, 3.9860032], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.962885512219738}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.20978406344479078
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7035267, 3.7220583, 4.103854 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0081260091905588}
episode index:99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1577616, 7.006232 , 4.949632 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0093368314313125}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.21748722281034286
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.094373 , 3.117434 , 4.9607177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1006559445765693}
episode index:100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 0.7646633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2153338839706365
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.386234  , -0.05355406,  6.158576  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.730093250845927}
episode index:101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.788521  , 11.895094  ,  0.13044232], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.504430970165783}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.21322276746112045
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.453117 ,  2.3667278,  1.1729481], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.46608774423246}
episode index:102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.392029 , 11.436055 ,  1.1357436], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.827617087475756}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.21508208013313213
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1258563, 4.9945354, 4.9224234], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.73689355438002}
episode index:103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.294462 , 12.044578 ,  1.5125321], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.07940015142436}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.21854624042009171
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6474679, 3.8363752, 5.932458 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9076356382807939}
episode index:104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 10.       ,  2.0251942], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.22199976588388234
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2595329, 3.3216972, 5.8999457], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7699476653752009}
episode index:105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5142946, 5.66109  , 3.5719957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0617785109336366}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.22924505111139287
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6966994, 4.8266153, 3.5861816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8516249194553014}
episode index:106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  0.      ,  2.006734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.23055799117249878
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4498835, 2.8485172, 5.28536  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5705920183396161}
episode index:107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4284744 , 0.60019255, 5.0457096 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.46692466314817}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2347431449117385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1621395, 2.272152 , 2.3346732], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9767381913691746}
episode index:108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.279215, 8.865602, 1.706674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.260644684258976}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23258953807768584
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.335306  ,  0.09857923,  0.12086146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.625978443014862}
episode index:109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  5.7280545], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.2334543855264142
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.358949 , 3.5481358, 3.8416514], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.465331257573118}
episode index:110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.266163 , 7.139494 , 3.9310112], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.719205893142391}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2401809225937438
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.989484 , 3.7239566, 4.7915635], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2260472523824675}
episode index:111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 1.      , 6.277847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23803645007058538
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.82315  , 12.672301 ,  0.7230874], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.09203508192215}
episode index:112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.105112 , 11.360157 ,  4.1606746], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.432881949908479}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.23981150362580067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3352842, 3.3052084, 4.863443 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4533957560600248}
episode index:113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  1.       ,  0.9495496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421556}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23770789394487257
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.598275,  4.361417,  2.79679 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.705388670212944}
episode index:114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 13.       ,  1.8758864], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.2384339216024371
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8033226, 2.4326448, 5.2795553], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3243597636657065}
episode index:115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 4.7676744], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.23758083235392663
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4700824, 4.1189632, 3.1647947], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.89544891568443}
episode index:116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.916712 , 10.571152 ,  6.2468276], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.608840574190273}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.23907971131785793
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.981576 , 1.8032794, 4.5585275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3149046423837274}
episode index:117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.415638 , 12.2205715,  2.6115193], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.223345724339627}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.2404835440412517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.368523, 3.515368, 3.60586 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4623473226669357}
episode index:118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0017145, 6.73304  , 1.4504629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.86421538860861}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23846267392325798
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.247076 ,  3.3864896,  5.0221133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.251973703003971}
episode index:119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 8.       , 4.1326056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2364754849738975
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.291103  ,  0.25074512,  5.338906  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.553145286880671}
episode index:120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.00036  ,  1.037979 ,  3.1749825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.14758149695296}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.23452114212287356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.483815  , -0.20372513,  3.7081475 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.888347313830016}
episode index:121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 10.       ,  1.3743148], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.23726769999463
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.695716 , 1.3423139, 4.463253 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.371365783985802}
episode index:122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9666822, 7.3635397, 0.6096693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.36366689292186}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.24185601616978783
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.283044, 3.787571, 4.329255], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.505479842720979}
episode index:123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.216013 ,  7.9040966,  3.1093462], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.163629286982578}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.24712603421889212
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.362228 , 3.3631015, 4.270017 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5128857086762338}
episode index:124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  0.7034557], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.25182513603674167
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2120934, 3.6320753, 5.5141406], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6667104226737587}
episode index:125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 1.4614298], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.25095594835218094
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8613997, 4.9500613, 5.2247586], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.695838981805966}
episode index:126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 4.7554655], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.25307708673248114
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0055809, 4.5962048, 0.6877156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.55452092701222}
episode index:127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 9.        , 0.41733402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.32455532033676}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.25659560710956775
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7048545, 3.8427434, 4.2597685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8929318595585182}
episode index:128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.     ,  6.     ,  2.94071], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.2592964633888543
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5264132, 3.958323 , 5.305492 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0689562343081205}
episode index:129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.723238 ,  8.539441 ,  2.6408803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.069517008379544}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.26334555321514946
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4693443, 3.8477778, 3.4626336], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7497526315057028}
episode index:130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.206999 ,  6.989259 ,  1.7717986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.721545686390941}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.2670964977494127
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9150956, 4.2338195, 4.044128 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5361348960562966}
episode index:131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.840803  , 8.78195   , 0.32285982], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.956982385518584}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.2679597600248304
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7701316, 4.671765 , 3.940662 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8406249967238326}
episode index:132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 7.      , 1.534093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.999999999999999}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.26769588561071894
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.89393  , 3.513163 , 4.1223726], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9622199274788377}
episode index:133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  4.       ,  0.3302846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.26569815512108674
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.372781 ,  0.8822039,  4.920713 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.60906240696995}
episode index:134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.205645 ,  5.0106006,  4.558165 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.370136414977019}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2637300206387083
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.507676 ,  3.4398782,  3.892234 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.519040197917938}
episode index:135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  8.       ,  2.1751404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.26837419882709074
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9081326, 4.59772  , 3.2990685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.488710410174121}
episode index:136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  9.      ,  4.912312], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141666}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.2677642014922108
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.135716 , 4.8383923, 4.4380713], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8433949428514016}
episode index:137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 3.      , 4.360914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.2682226716866224
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0386734, 1.7192655, 2.9068933], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.648976468347204}
episode index:138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 13.      ,  1.501739], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.26860381127567395
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8233852 , 1.9537833 , 0.29884422], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1022138727812245}
episode index:139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  7.      ,  2.033245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269006}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.2731450845880535
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2567036, 4.749542 , 3.902917 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7682745213192752}
episode index:140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 12.        ,  0.60214406], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991377}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2712078854065779
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.333399 ,  5.0153503,  1.5672362], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.465275824949353}
episode index:141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  2.        ,  0.87707597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.2711667471272901
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9811032, 4.1587443, 4.0556526], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.158898409713183}
episode index:142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 6.       , 0.7664596], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2748202264036283
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4238946, 2.8093224, 4.9027343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5875975741246195}
episode index:143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.8587685 , 6.738228  , 0.59062064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.3725637704472}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.2784229629122396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9408882, 4.5541215, 3.7662249], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8806944183033123}
episode index:144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 12.       ,  2.9010797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2812100776163648
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.435002 , 3.2299032, 5.349839 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4920186054674736}
episode index:145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.948291 , 4.0006685, 4.6261497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4516988105600568}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.286133296262828
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.948291 , 4.0006685, 4.6261497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4516988105600568}
episode index:146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        ,  6.        ,  0.10385369], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.28418681125423734
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.752258  , -0.47049844,  0.14076632], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.154982164733953}
episode index:147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.813516 ,  4.610048 ,  2.3318348], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.959370084905794}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2822666300971141
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.684218, 12.198984,  5.805501], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.668781251546758}
episode index:148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  9.       ,  6.1575193], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.2834676726294697
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1340187, 4.5390625, 4.519345 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4188013026411035}
episode index:149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      ,  7.      ,  4.216119], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.28157788814527324
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2919219, 5.9408207, 4.406139 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9552740520862986}
episode index:150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.27111   , 1.5441111 , 0.52210677], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6281563014499816}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.28633565047543696
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.27111   , 1.5441111 , 0.52210677], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6281563014499816}
episode index:151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 8.       , 3.4640465], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2905838721690656
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.725413 , 2.3201492, 5.5263615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4445654167432798}
episode index:152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 5.7794256], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2944779963491118
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6961043, 1.2377194, 3.386234 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1922082205101017}
episode index:153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  4.       ,  4.6418076], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2925658015676241
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.733554 ,  5.865966 ,  5.6297646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.10958771723492}
episode index:154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0683775, 6.5184927, 2.6127844], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.01384558411741}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.2948241615709602
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4326997, 2.9241242, 3.3667305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.434707463156085}
episode index:155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      , 12.      ,  4.772927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29293426309935144
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.502781  , -0.13167846,  0.8774135 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.061164012765163}
episode index:156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 12.       ,  6.1780043], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.999999999999998}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.295875568985366
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.509712 , 4.727957 , 4.6389875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2945730736460916}
episode index:157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  0.4585326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.2984106173971648
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5306993, 3.1306567, 5.3821745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5362654007888}
episode index:158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.547188 ,  4.0519342,  3.461863 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.60496541572702}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2965338210613336
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.762071  ,  0.40903232,  2.8889048 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.10005628865048}
episode index:159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 12.       ,  5.0411367], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.492855684535897}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29468048467970026
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6254725, 5.208907 , 3.3295877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.742522870272635}
episode index:160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  8.       ,  4.1263504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.2928501711102611
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.313934  , -0.20834416,  5.2185802 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.669089547502931}
episode index:161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  6.      ,  0.357135], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505136}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.29104245400464224
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.506889  ,  0.44760138,  4.9460254 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.81246791427923}
episode index:162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.833202 ,  5.200394 ,  0.5935437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.03604638461693}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.2909179865537671
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9736335, 3.4853148, 4.135232 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.48603054497046727}
episode index:163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 3.6788323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.29248041738204145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.484764, 4.192275, 5.95724 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.298840953301855}
episode index:164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8559427, 7.254697 , 5.695086 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.641871244888999}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.29566482053486076
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.397601 , 1.3649719, 4.8538456], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1509546433470046}
episode index:165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  0.      ,  5.631504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.29541930812619477
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.719736, 4.32158 , 4.508592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3509705303284074}
episode index:166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 1.5724387], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.29686151721051207
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.514333 , 4.3005543, 4.124551 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9961576004392374}
episode index:167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 5.5644803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.300012154749947
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6254346, 1.9983152, 3.2496026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0694258363185691}
episode index:168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  8.      ,  5.514886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.30257009093281356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2781584, 4.8877983, 4.7703667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9081809575635074}
episode index:169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  5.479592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.492855684535904}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.3053656866296744
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.302833 , 4.383825 , 3.8909528], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5495202034874265}
episode index:170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.893693  ,  1.6433706 ,  0.39327627], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.964866434473835}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.30553534259610854
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.060453 , 4.824966 , 4.6733766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0526201339239947}
episode index:171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 0.9941797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.30652257922058296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2787297, 3.1786852, 3.7316566], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7430742056262124}
episode index:172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  8.       ,  4.0288706], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.30612413977299746
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5883733, 4.090869 , 4.736178 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7840081267613193}
episode index:173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  0.      ,  2.747402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.304364805636371
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.22187082, 1.1843927 , 2.931994  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.3187997252646784}
episode index:174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 13.       ,  4.3243575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.30763998402130016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0946892, 4.162259 , 4.3576207], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23182778770426}
episode index:175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 10.       ,  0.7770704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.30793037216709895
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7286077, 3.994342 , 3.661148 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9941917191690133}
episode index:176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  6.       ,  0.8651273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.30950724453868306
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3269513, 3.6327872, 3.94735  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7887179050974806}
episode index:177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 13.       ,  2.7106876], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.31295239875154396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4491916, 4.6916966, 4.162185 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.227553308479918}
episode index:178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 4.       , 3.4455166], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.31673478758533424
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.091661 , 3.4014668, 3.1644273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1631420017468412}
episode index:179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 9.      , 5.507048], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.319002818409626
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1850941, 2.1080828, 5.4870954], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0222264222728534}
episode index:180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.951504 , 11.759159 ,  0.3645203], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.1824090592516}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.3191635382774869
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7515035, 4.283166 , 3.8264854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3070062489959968}
episode index:181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.9265614, 11.312478 ,  4.129775 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.812607613941628}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.3226878925177205
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.929479 , 3.9022965, 4.895368 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1300302191723097}
episode index:182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 4.       , 4.4409313], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3260178239679351
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7322167, 2.1388555, 4.6879306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5325940033409353}
episode index:183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.0967026, 11.385623 ,  4.362389 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.457034307143498}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.3273106900901332
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.394381, 3.923183, 5.850074], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.104101943982684}
episode index:184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  6.       ,  2.2378116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.3301439175550237
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.621864 , 3.5238504, 5.6639714], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.646069700716296}
episode index:185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  9.      ,  6.198923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.3294786681978322
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3908029, 3.9121218, 5.4406147], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.992316987631171}
episode index:186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  1.8419995], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.32996987436872494
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7093625, 4.9521294, 5.026068 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0770181077670666}
episode index:187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.7032003, 8.4774   , 4.217286 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.219556932051692}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.33146533907704484
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.900359 , 4.9370904, 5.2400002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1361098712616275}
episode index:188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.98797  ,  5.7809677,  6.2250934], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.334428696571075}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32971155421420334
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.848492 ,  1.7462143,  2.9373128], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.909520394497669}
episode index:189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  1.       ,  5.6906867], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.3297010868627486
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4944881, 4.4944677, 3.2872705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.12132030405772}
episode index:190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  4.       ,  0.6695963], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.33114247419403026
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.779538, 4.83126 , 5.923099], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5534819324978524}
episode index:191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.18980314, 10.009027  ,  4.7282276 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.700733531518462}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.3316122228813258
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4913955, 4.10661  , 4.4385176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2108075024535514}
episode index:192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  5.       ,  5.6427474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.3322837759618272
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.704879 , 3.201044 , 5.0458922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.35709256637207093}
episode index:193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.692256,  8.876374,  1.09213 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.492239426261383}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3305709729929518
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.7579   , 8.855946 , 5.9827995], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.942109054748753}
episode index:194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 13.       ,  1.2118596], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.3331985228411107
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7790942, 4.8642344, 5.5217624], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8772771343506223}
episode index:195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 13.       ,  1.6741334], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3314985303776356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.038582  ,  0.21979004,  4.067828  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.311231523551427}
episode index:196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.9496975,  2.7020175,  1.5554917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.953751355781266}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32981579672089634
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.8839326, 6.7263627, 0.7688592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.711989621953095}
episode index:197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  6.       ,  6.0628753], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3281500603738211
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.350491 ,  1.014795 ,  2.0391178], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.894461318787967}
episode index:198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  4.1088233], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.3277192391962872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.017354 , 1.4074613, 5.0917892], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5430424047712754}
episode index:199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 7.      , 5.885567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927977}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3285548362983068
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5561063, 4.1352034, 5.488089 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8367132615744735}
episode index:200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  0.       ,  2.4978814], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.327858328879745
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1211972, 3.801371 , 5.5123158], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8104840547695845}
episode index:201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.5187386, 3.9412336, 2.3854747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.653785740135781}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3308033108874092
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3257287, 2.1267614, 4.94472  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.888314096224793}
episode index:202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.572622  ,  5.235661  ,  0.44900462], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.78659268491308}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.33149189872336116
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.797411 , 3.4724112, 4.37841  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9268422449792499}
episode index:203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.3208346, 10.118848 ,  4.2377157], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.314204740494141}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.33356654278453896
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5943794, 4.6539373, 5.3063827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1705478397523486}
episode index:204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.656042 , 11.878606 ,  5.7894926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.135612382278925}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.3328778538582434
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0675416, 4.4642725, 3.6260977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.735964464112146}
episode index:205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 12.       ,  4.8208065], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.33416949537575763
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6383464, 4.803772 , 4.684891 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.260020744179667}
episode index:206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.742153 , 10.017706 ,  0.4118659], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.327255628646835}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.33255514998746893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.777148 ,  6.1404796,  3.2699385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.225396929678803}
episode index:207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  9.       ,  4.7991343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.3321818548466461
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4137235, 3.380935 , 3.9128737], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4641466747301302}
episode index:208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  5.      ,  2.991735], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3305924679813511
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.649573 ,  0.4068427,  4.266693 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.029927101686546}
episode index:209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 10.       ,  4.2664194], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.32996238126043476
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2689757, 3.6408951, 4.446524 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6950500063145801}
episode index:210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  1.1044067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.33011598143389476
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.545115 , 2.3058476, 3.1247325], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8826085479750493}
episode index:211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  4.6923027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.3313278720023079
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.808239 , 3.0469296, 4.2332306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8088478673878803}
episode index:212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.365816 ,  7.1530147,  2.7304227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.339936298363522}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.33075117993532416
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5729144, 2.2126238, 3.7077885], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9737515555608316}
episode index:213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  5.9904985], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.330272111151212
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3517706, 3.0390716, 5.400688 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6486923945578533}
episode index:214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.742872,  9.016598,  4.943355], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.312950596192456}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.32873596179702036
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.42285  , 7.9426455, 5.388583 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.012125086585963}
episode index:215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.287244 ,  9.9673   ,  4.2188206], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.264431712158633}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.3314009900989267
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.282443 , 4.4816527, 3.2832298], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9595803121481585}
episode index:216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  5.6990995], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.33260636086529805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.310599 , 4.545642 , 5.6099133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.026494072667914}
episode index:217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        , 13.        ,  0.68856704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.40121946685673}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.3325102816019212
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4080381, 4.2176275, 4.546326 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2841775241760667}
episode index:218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  1.      ,  3.177487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421555}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3309919698137846
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.177904  ,  0.26397085,  4.5916333 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.42253640777799}
episode index:219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 10.       ,  0.7360748], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611664}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.33187653580338355
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2833781, 3.7587748, 4.197637 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8099643780692865}
episode index:220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.857263 ,  5.2579927,  5.5458617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.08957540425416}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.3314607708431155
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0621696, 4.054147 , 4.339963 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4964723727957998}
episode index:221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  4.9053936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.332665704336913
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9657326, 1.7016872, 4.2872424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.355784505728867}
episode index:222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.9030323, 11.002352 ,  4.4883895], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.384953887338023}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3353117984628816
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6966333, 4.9304256, 4.352631 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0522770708687705}
episode index:223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  1.9751827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.33789307278886715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8731622, 4.7853565, 3.4032154], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1112227377925192}
episode index:224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.        , 11.        ,  0.07891036], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.34013773999151187
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.702274 , 3.526525 , 2.5298016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8777342915161039}
episode index:225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9344158, 3.6924925, 2.0771868], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1630471879475832}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.34305748450482376
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9344158, 3.6924925, 2.0771868], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1630471879475832}
episode index:226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.480647 , 6.6554985, 6.156485 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.943980871396931}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.34383846705172016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9044094, 3.9293578, 4.2280173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1190755188822292}
episode index:227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 5.       , 1.1398916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.34427359650160816
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5903485, 4.3368516, 4.635319 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4613977974215342}
episode index:228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.      , 11.      ,  4.984414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.34486690593422736
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2378383, 1.098084 , 4.1733685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9167294060145061}
episode index:229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 8.       , 2.3089943], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.34584401157137057
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9582777, 4.3413453, 3.9884062], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.37361723971546}
episode index:230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 10.       ,  1.7794335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.3475490174666818
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.484756 , 3.6158047, 5.3068514], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6073940875762567}
episode index:231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.        , 13.        ,  0.69947875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.3496843371904631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6721654, 4.846249 , 4.424673 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.490938147900382}
episode index:232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 0.9459603], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.35169387624800286
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6738076, 3.3133903, 5.119222 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7028932450502974}
episode index:233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 11.       ,  1.7550409], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.544003745317532}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.35417409621235746
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1095343, 4.905719 , 4.6808906], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.205182631842709}
episode index:234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.999772 , 1.9698178, 5.382782 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0301822676080838}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.35692229154762406
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.999772 , 1.9698178, 5.382782 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0301822676080838}
episode index:235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.     , 12.     ,  5.07296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.3587390536207579
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3539499, 4.8114114, 2.9710555], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.447589039741402}
episode index:236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 4.       , 3.6856935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.3591327439019635
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7738984, 3.856865 , 3.9125311], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.886193803349121}
episode index:237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        ,  6.        ,  0.41530246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35762378279313173
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0315146 , 6.4893093 , 0.19063729], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.621221251291455}
episode index:238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 6.      , 6.189762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.35916083950093386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.544342 , 4.346389 , 4.5922127], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0488424013922124}
episode index:239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.821023 , 11.991976 ,  2.5813408], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.286362704593577}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.36053704052746777
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.169693 , 2.5020819, 2.402556 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2712608549827158}
episode index:240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  0.       ,  2.2737505], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35904103620992645
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.3981767, 12.908563 ,  1.4094061], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.926822518404109}
episode index:241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      ,  7.      ,  5.699611], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.35946327146285273
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.774693 , 3.5765817, 4.0984592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9657099663226991}
episode index:242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      , 10.      ,  4.570508], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.36052428738663644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1037028, 4.8032355, 4.0654106], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6167922181014855}
episode index:243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 4.      , 3.088615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.36060839734859507
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7342622, 4.742033 , 4.214716 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7621848805210087}
episode index:244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.      , 9.      , 5.135662], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.3597534770939198
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9557695, 2.465896 , 4.9997263], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0273878978273245}
episode index:245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.890974 ,  7.3356504,  4.4825225], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.55187357992523}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.35829106458540794
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.928808 ,  2.9660053,  6.146539 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.928861083329386}
episode index:246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.3561993, 11.53006  ,  5.200882 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.537493681063385}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.3600213766348889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8895893, 4.827873 , 2.9903288], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0328525979023175}
episode index:247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.0463446, 10.295547 ,  5.488769 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.552600113195727}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.36244304854361925
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1952987, 3.1834288, 4.887029 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2092911254986778}
episode index:248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.999997 ,  2.996421 ,  6.0575223], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.999997631613091}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3609874539711549
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.484517 ,  0.2816224,  5.422689 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.866389476148314}
episode index:249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  9.      ,  5.953124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.361315696081775
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6287467, 3.718162 , 3.9057271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9545046973151501}
episode index:250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 8.       , 1.4974914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.36228657405410875
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.526704 , 3.722936 , 5.0943356], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6892190312588409}
episode index:251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4875546, 4.0603375, 4.9599304], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1776739978363113}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3648171828872273
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4875546, 4.0603375, 4.9599304], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1776739978363113}
episode index:252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.57652  , 4.5951138, 3.7833807], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6503706335728174}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.36732778690743595
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.57652  , 4.5951138, 3.7833807], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6503706335728174}
episode index:253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  0.      ,  4.216197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.3673372430127205
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.798739, 4.992738, 4.395558], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3268072363333197}
episode index:254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.7052517, 10.021838 ,  3.5630734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.939464821681519}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.3687113343444286
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1673355, 2.9449708, 3.9521806], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1686318501864978}
episode index:255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  7.       ,  4.0878086], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.36905468752177034
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2144616, 4.6555367, 4.0999126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6693697516423045}
episode index:256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  8.       ,  4.1406074], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3676186770644872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.7098665, 13.001867 ,  1.1776863], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.146969641328113}
episode index:257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  1.4019666], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36619379847121397
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.6023035, 6.913095 , 1.8286121], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.699393151308682}
episode index:258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  9.      ,  2.194344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.3660709531369233
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8714895, 3.7384949, 5.204076 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0119263702002783}
episode index:259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  5.      ,  5.053748], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3646629879325505
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.593687 ,  3.9281945,  1.6194661], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.64366833667408}
episode index:260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  9.      ,  6.161548], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.3653204792631046
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5067377, 3.9914017, 4.574447 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1134003690197438}
episode index:261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 1.       , 5.9668007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.36707944164696904
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3243815, 2.5014453, 5.7892957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.748214589587417}
episode index:262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  8.      ,  6.251188], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.36568370232511743
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.1286077, 5.3228626, 5.2181845], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1506605225395545}
episode index:263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  4.      ,  4.056047], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.36578607856008977
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0037775, 3.470769 , 4.065472 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1086896146072787}
episode index:264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 11.       ,  2.5439088], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.3664294074153618
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5498533, 3.893267 , 3.5757484], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0489349181928447}
episode index:265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 10.       ,  5.6748195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042628}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.36702777989697905
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4776647, 4.7558765, 5.94005  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8319215550471781}
episode index:266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.551069 ,  7.3786225,  2.3717318], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.423545762965963}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.3664108220214427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3756742, 3.989399 , 4.565988 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1699115093279935}
episode index:267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.47422  ,  5.2930574,  2.3159077], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.667945144530481}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3663038950769556
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.250297 , 3.3544397, 4.580783 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2995654228100912}
episode index:268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 11.       ,  1.4220912], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.3665564134104681
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9144611, 4.2073555, 4.970437 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2103818401187685}
episode index:269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  2.9385247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498527}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.36796610111758343
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3640437, 3.536097 , 4.7236376], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6480184190332432}
episode index:270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 1.      , 4.270926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.36902769934436036
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4485292, 2.729166 , 2.3727906], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.574932553221685}
episode index:271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  9.       ,  2.0778782], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.3687827608749657
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9405606, 4.4783254, 5.872674 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.818751708390058}
episode index:272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.      , 11.      ,  2.063817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.999999999999999}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.3704279043794429
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.129882 , 2.889267 , 3.6711571], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1352950362480543}
episode index:273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  5.501727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.371468894584533
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9431434, 3.3701477, 3.556288 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0131775440356499}
episode index:274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.     , 12.     ,  4.24497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3727019285215794
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9382086, 4.3573914, 4.378761 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.36625522648244}
episode index:275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  5.5923066], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.3727321647157206
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6853045, 2.1556644, 5.102519 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5624745532962097}
episode index:276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9356496, 5.0010357, 4.263452 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2089780601854434}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.37496056845320896
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0676336, 3.1992178, 4.071657 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.21038545131171463}
episode index:277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 0.       , 5.4425282], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.3767683398724383
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8581157, 1.2002156, 3.1431313], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.993887277091867}
episode index:278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.731762 ,  7.9819303,  1.9096771], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.932832175741853}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3754179157151894
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.566781 , 7.9329185, 3.3788314], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.175759678232901}
episode index:279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 9.        , 0.13038152], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.374077137444778
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.05724511, 5.457078  , 3.5513508 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.833671682483931}
episode index:280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 9.       , 1.6489916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.37397227995688737
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.526827 , 4.330269 , 3.4551182], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.41191636994301}
episode index:281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  1.       ,  2.5047727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.37376248170590887
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9980674, 2.645    , 4.412189 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0593221920462723}
episode index:282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  6.       ,  1.4724014], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.37244176622284914
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.650705 ,  8.436585 ,  1.6270097], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.626887981449032}
episode index:283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 13.       ,  1.2536185], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.37198393129264395
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.118387 , 2.6752648, 3.9680693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1645784242106296}
episode index:284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.323164 , 11.881992 ,  2.1381948], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.511668805965767}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.3736068429774069
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2951431, 3.7663102, 5.376465 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8211825687894427}
episode index:285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  2.9596415], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.3745701149542985
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1905077, 4.302278 , 6.2378836], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5333642627759367}
episode index:286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7000000e+01, 1.2000000e+01, 6.2277764e-03], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.3739769907270927
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2949567, 4.228213 , 5.1879163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.101351820422293}
episode index:287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 4.9409184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.3740149042276705
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8718395, 3.2964637, 2.7760248], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9208663835788066}
episode index:288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  4.       ,  1.1735282], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3727207350088896
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.078079 , 6.579566 , 1.6116667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.053817392778082}
episode index:289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  1.5984243], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.37383691598489194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.481018 , 3.8441978, 4.9976926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9909652308009333}
episode index:290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.275118 ,  6.8640137,  2.4234953], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.788833354162373}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.37509417872510975
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7375517, 4.9777384, 5.505094 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9950759928642805}
episode index:291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 6.       , 3.8473372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.37713255140070867
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4714713, 4.645462 , 6.2467036], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7282615221270434}
episode index:292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        , 11.        ,  0.32342237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.37584540958705437
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.349422 , 11.578222 ,  3.4265285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.10951126394439}
episode index:293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  5.      ,  4.225139], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.37559561035604067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.38657  , 4.2749825, 4.211309 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4148769018059477}
episode index:294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  5.      ,  4.771451], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.37432240489720664
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.059283 , 11.175881 ,  2.7614768], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.22982289449389}
episode index:295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.327575  ,  6.4958425 ,  0.59483147], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.854740138223107}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.37541054953623487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6333225, 4.6311793, 5.2324634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7498123830401755}
episode index:296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  8.       ,  1.2973723], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.3758810093607722
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.241596 , 4.6872554, 4.7839956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8498668541649934}
episode index:297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 13.       ,  1.0887927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.3763139116407262
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.111916 , 3.8448458, 4.785247 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3964675843700265}
episode index:298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 12.      ,  4.994888], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.37826803237102474
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.529453 , 4.8686852, 5.2507234], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9422422782668038}
episode index:299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.798693, 10.599018,  1.626169], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.625922299910192}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.37939957403844893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.865285 , 4.483472 , 4.4558935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8676902401548103}
episode index:300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 3.3980274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.3802106232771624
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5761876, 4.742127 , 5.881106 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8349382669642123}
episode index:301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7533193, 3.9847288, 1.3569633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.010924979979043}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3822629059815426
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7533193, 3.9847288, 1.3569633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.010924979979043}
episode index:302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.004661 , 11.136462 ,  2.9265866], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.136141477302752}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.38327672175674893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0637856, 1.9162816, 3.1400023], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.518580032322383}
episode index:303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  6.      ,  4.553095], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3820159430667596
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.599215  , 4.764599  , 0.20619124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.141612303340973}
episode index:304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 5.8243017], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.3834721813643623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.743349 , 2.570057 , 0.5501289], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7955826504245826}
episode index:305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 5.5685997], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.3834642563210296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0655435, 3.7791305, 5.0062814], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0854654389438068}
episode index:306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  5.       ,  5.4175925], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.3829813650276074
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4865942, 4.386358 , 3.7718816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4783684998488116}
episode index:307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.    , 7.    , 5.9412], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3847338433698162
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7402997, 4.521782 , 5.0673847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5437825174555535}
episode index:308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  1.       ,  1.3035506], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3834887500255773
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.334887 , 12.623904 ,  2.5219626], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.121947904372446}
episode index:309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4673241, 1.6409711, 5.157242 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0484274325094893}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3854774959932367
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4673241, 1.6409711, 5.157242 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0484274325094893}
episode index:310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  4.       ,  5.8819685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.3854881275754907
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.14025  , 4.7824306, 4.4220915], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.115946490359193}
episode index:311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.025635 ,  4.3191867,  3.1968527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.097774383439518}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.3854257692860604
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5023582, 4.573866 , 5.4988637], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6520949126951099}
episode index:312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 13.       ,  6.2241273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730953}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.3847615416741521
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1787945, 4.2556753, 4.7520027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2121279258630877}
episode index:313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.162521,  5.993386,  0.678967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.52546543594048}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.3846336774756597
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.981042 , 4.871609 , 3.3973258], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8717052246073873}
episode index:314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 7.      , 6.274402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.3844426039911764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5606475, 1.3051938, 4.460672 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3039072676105934}
episode index:315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.852006 ,  4.005483 ,  4.8551273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.908928434791386}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.383226013472217
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.615596, 12.898603,  2.004132], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.91772698521593}
episode index:316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 12.       ,  2.6452196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.38459724667134954
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3367116, 3.5556617, 4.78346  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.753649922933674}
episode index:317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  6.       ,  4.1519084], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.38338782136735156
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.55246735, 10.840755  ,  2.8462625 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.21388229258735}
episode index:318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 9.        , 0.53343046], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.38369112429902574
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1031623, 2.427412 , 4.1603847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2429094958985043}
episode index:319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 6.       , 2.5431254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.38396267279054647
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6612344, 2.5968308, 4.8396974], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.398155483524107}
episode index:320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.        , 5.        , 0.38820732], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.3857291132176787
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5942364, 3.7923532, 6.0837736], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7802845658213569}
episode index:321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.999866  , -0.02320489,  0.1424362 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.407752167314166}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.38660874876225015
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4747812, 4.6377993, 4.0948825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.238007808127527}
episode index:322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        , 11.        ,  0.86410236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.3865450447142965
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7109728, 4.3281775, 4.4148436], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3592615917089166}
episode index:323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  1.5413798], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.3876580911637327
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.833496 , 4.958601 , 4.2660747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6828764032039474}
episode index:324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 2.3077886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.3872575573655795
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2470958, 3.1589372, 3.0121257], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.293798203626603}
episode index:325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.07079394,  1.9987466 ,  2.0744133 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.229904601646917}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.38881608711065047
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5458531 , 3.1762238 , 0.54368055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4647859598374018}
episode index:326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8181643, 2.9917169, 1.033426 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.18202421404558494}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3906851510644405
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8181643, 2.9917169, 1.033426 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.18202421404558494}
episode index:327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  6.       ,  5.0436316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.3914730092269526
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.417142 , 3.8441067, 4.705253 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6494869756615553}
episode index:328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 11.       ,  1.8038175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.39071134176092615
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.081766 , 3.5672822, 5.6677475], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2214855918651164}
episode index:329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.110106 , 11.345552 ,  3.6455092], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.637098973917409}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.390778730637814
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.288528 , 3.7235084, 5.10012  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7789176663808848}
episode index:330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.087367 , 7.5846663, 2.451668 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.620708781988002}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.3923030796517744
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3541667, 3.1371033, 3.7328215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3797780986286164}
episode index:331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      , 12.      ,  5.338705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3911214438696907
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0372311, 6.7139053, 3.0940833], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.200661214210937}
episode index:332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  3.      ,  5.174645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.39113812964312117
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4781146, 4.398679 , 4.0422873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.066987538466482}
episode index:333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  0.      ,  4.568913], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.3904725129573055
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8975277, 4.0288982, 4.1762667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1585279564226822}
episode index:334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  5.600307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.3901151450365732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2698603, 4.7469735, 4.536414 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1597364561195502}
episode index:335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 6.       , 1.1909429], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.3902860023839757
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4611199, 4.080024 , 5.348839 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8800540754140496}
episode index:336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.583508, 11.087013,  6.211976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.108037051099231}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.3916544646056698
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9627185, 3.2117376, 5.2082763], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9857280101135939}
episode index:337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.        , 11.        ,  0.18094271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.3915786595070532
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.351682 , 3.5736485, 3.8480425], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4683723232357766}
episode index:338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  6.       ,  0.6104894], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.39186866426663175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4129148, 4.7220373, 3.4874918], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3418480089223217}
episode index:339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  0.       ,  5.1702576], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.39071610937172985
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.934103 , 12.153205 ,  1.8645847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.908470966512482}
episode index:340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  1.      ,  4.175002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.390531378134387
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3181067, 4.338509 , 5.2436833], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8785663972668256}
episode index:341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7464546 , 3.9746263 , 0.73715734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5878515385699943}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3923134501281461
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7464546 , 3.9746263 , 0.73715734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5878515385699943}
episode index:342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 9.       , 1.5307453], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.3931397346731378
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3473357, 4.1459675, 4.764053 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.011104366827749}
episode index:343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  6.       ,  5.3342443], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.39363614588179835
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3120763, 3.944671 , 4.1637917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1686069117882367}
episode index:344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 11.       ,  1.3426362], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.3947724994902779
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.90404  , 4.2189627, 4.7320766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5176160543628336}
episode index:345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 2.4729335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3963253689943724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9173571, 4.6139336, 3.7138493], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.943424097644215}
episode index:346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 7.       , 4.9808674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.39792382628804857
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.43063  , 3.407509 , 3.8274045], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4875370404063655}
episode index:347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.129512 , 6.0041976, 4.3451567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.682393766634269}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3996251946033128
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4114447, 4.1375484, 4.4123864], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.812785841743595}
episode index:348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 2.3218884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.40131681295688504
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6351252, 2.4618878, 2.0996988], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4671223632763475}
episode index:349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 13.       ,  2.5486856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.40070892733462926
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8783703, 3.2843215, 4.8573093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8997667408222951}
episode index:350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  3.       ,  6.0471663], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.000000000000002}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4003703283836488
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0818624, 3.6091237, 5.7195153], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6145999925718267}
episode index:351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  5.       ,  2.1982203], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.4002938907474858
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.075541 , 4.5790815, 3.7779212], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5808873903183884}
episode index:352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.017118 ,  6.7388797,  3.2604334], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.543433616006977}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4001361521996105
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0494165, 4.780903 , 3.1223521], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.018718384608577}
episode index:353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 6.       , 3.9127345], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119284}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.401509736153047
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.294    , 4.9561777, 3.2215517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.345435487056262}
episode index:354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  6.       ,  6.0427327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4010967784757041
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6903765, 4.414534 , 3.176839 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9276982015460167}
episode index:355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.4602046, 11.723665 ,  3.182039 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.384846885136783}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4023618374437355
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.256541 , 4.457059 , 4.3789454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9240362203349912}
episode index:356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 8.       , 0.7497537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.4018187859711613
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.473524 , 4.8103924, 2.766076 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3342651570029003}
episode index:357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 3.      , 5.985368], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4019464519705821
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.726281 , 1.4580399, 3.16039  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3146679409024564}
episode index:358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  9.      ,  5.845585], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727816}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.40082682397066405
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.3192625 , 12.381689  ,  0.48820752], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.899062713009462}
episode index:359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  2.      ,  5.684334], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.3997134161263011
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.44283628, 12.865147  ,  1.9343698 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.448647752297731}
episode index:360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  8.      ,  1.867597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.39959993934390664
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9396386, 4.1978397, 5.5050063], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5224128091745495}
episode index:361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 0.7752601], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.40064474989654547
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.561661 , 3.5600443, 4.463185 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6590460979658004}
episode index:362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 8.        , 0.15805167], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.40140255788322243
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2726328, 4.9459834, 3.7526221], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9649885738869322}
episode index:363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.9314322, 12.4808   ,  5.7498517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.526443644314579}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.40275952408205623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.581804 , 3.2235727, 5.7090607], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.597525594023218}
episode index:364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 6.        , 0.96511567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.40385232152166445
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.519282 , 2.947836 , 4.5058327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5218955767903729}
episode index:365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.018925 , 3.2571597, 4.1148424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0352370965356017}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4053999900420971
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.388568 , 1.359095 , 4.3409305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.149579069076598}
episode index:366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.056183 ,  9.763295 ,  1.5697026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.823664738108778}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4042953579166418
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.192778 , 5.991172 , 5.2520204], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7088252586889103}
episode index:367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.06615  ,  4.692136 ,  0.8842312], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.160877925482463}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4042218288106822
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6826692, 4.8497324, 3.7814145], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8767548609584455}
episode index:368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 0.9232881], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4046700113951442
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3288871, 4.7691846, 4.6720705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4336459108087025}
episode index:369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.071419 ,  2.6888046,  0.808247 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.075791437574994}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.40413980180146747
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6653059, 4.5506616, 4.072871 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.045961815871355}
episode index:370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  2.      ,  5.200753], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4040775573710176
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8950677, 4.3006024, 4.1281548], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.298444746609201}
episode index:371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.043426 ,  6.4145083,  3.3304613], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.518105545243799}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.40539815064222107
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5630711, 4.6057243, 4.422515 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.154788901873613}
episode index:372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  5.0052724], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4048931821329197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.426565 , 4.8565845, 4.2446446], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4336400156734914}
episode index:373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.940236 ,  4.485268 ,  5.5010843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.025196024747643}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4044921569419127
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5543768, 3.5064573, 4.818504 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7508879458852086}
episode index:374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.340736 , 5.515942 , 5.4809394], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6008832675264637}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4059751005500676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6164474, 3.9756305, 5.4334574], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.154063417695874}
episode index:375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  2.052692], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.40647241795924693
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6778362, 4.6578393, 4.4342513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7910591909179854}
episode index:376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 12.       ,  2.1748793], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.40692038170465195
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6740797, 4.8864036, 3.3376973], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.305771703926646}
episode index:377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      , 11.      ,  4.814429], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.4065529910570652
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9340549, 3.7689452, 4.3560915], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2098492520233612}
episode index:378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.470294 ,  5.9439197,  1.5177226], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.813075172304218}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.4061665331951144
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.557551 , 3.0031579, 5.9724865], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44246036186244364}
episode index:379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.264887, 10.005323,  3.679542], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.001059275389622}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4058469048795304
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3519163, 3.5562532, 3.3142161], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.461880752508251}
episode index:380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5688449, 8.759546 , 5.8186884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.934692766211904}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4070848946908675
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.266043 , 2.548092 , 3.8547695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3442790969873875}
episode index:381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  3.       ,  3.4235592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.40711116690354376
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.183476 , 4.637533 , 4.5032325], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0204282506000926}
episode index:382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.352194 ,  5.133461 ,  2.8121011], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.569748009096463}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.40648459449652374
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8032483, 4.3280153, 3.8237164], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.787691073514063}
episode index:383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  0.       ,  4.6325984], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.40599697125399753
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6426091, 4.8529544, 3.70386  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2969436139053934}
episode index:384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  9.      ,  4.861165], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407864998735}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.4055848718115037
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1341684, 3.0135946, 4.1236577], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.13485536629558642}
episode index:385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  1.       ,  2.5330427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.40610150703255565
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.050637 , 2.3328805, 5.49341  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0603554331052467}
episode index:386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.018744, 8.000088, 5.237316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.840692825034582}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.40696351960711824
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.257002 , 4.9829197, 3.4872725], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3477700541613733}
episode index:387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 13.       ,  5.6354012], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4073107401132
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5122585, 4.639567 , 4.2568107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2304944462361944}
episode index:388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  7.      ,  0.676029], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.60147050873544}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.408243214163822
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4018543, 4.9117546, 4.7512927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9535333479280035}
episode index:389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.        , 8.        , 0.78088903], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4082876934182446
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.319137, 4.796462, 4.037805], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.82458883472518}
episode index:390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 1.3166103], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.40872983848397315
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5759877, 3.0855677, 5.917517 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4265808331706744}
episode index:391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  9.        ,  0.60857034], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.4089495038439636
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7948866, 3.754724 , 4.729298 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4219376527328398}
episode index:392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.858144 , 8.358    , 4.0474586], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.232546102068775}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4103287418746406
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1594393, 4.0825553, 4.5669365], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3705722208057802}
episode index:393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 11.       ,  2.6278067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.41040054075772503
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.006293 , 3.6914022, 5.20854  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2209267944447406}
episode index:394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 13.       ,  2.7749069], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4105409970473758
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.165106, 1.486904, 4.287482], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9096939546507234}
episode index:395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  5.7182827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276355}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.4098564883901229
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6411889, 3.556295 , 4.7249274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6619739194947039}
episode index:396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 10.      ,  2.733621], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280517}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.41013478570563977
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4893537, 1.4647628, 2.7200403], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1389547807355607}
episode index:397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  1.      ,  5.074892], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.4097643781718322
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6317382, 3.5036733, 4.341242 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6239419620675651}
episode index:398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.        , 10.        ,  0.19960648], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4087373997804241
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.819879 ,  6.7170286,  1.8291707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.658337180056849}
episode index:399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  7.      ,  4.805935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4089159099224016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6802707, 3.3720384, 3.3747392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7753584535973729}
episode index:400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.      , 9.      , 5.595331], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.40936010162318737
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4721575, 1.8757966, 4.399656 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8523177436522131}
episode index:401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 6.      , 4.270593], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.41077985261417443
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3348311, 4.0282273, 5.0993276], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9570485032983735}
episode index:402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 7.       , 3.2453065], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.41191624457485637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4021142, 3.7620382, 4.8297057], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7702941692506866}
episode index:403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.1003447, 11.786231 ,  1.7349848], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.832170494256026}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.41167588053675264
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7121036, 3.6686902, 3.642737 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9768511321559196}
episode index:404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 11.       ,  1.7850997], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4120658196032722
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.002733 , 4.707309 , 2.3927941], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6275423372485367}
episode index:405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  8.       ,  5.4286566], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4130253387410451
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3999166, 4.3859715, 3.6550405], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.969945113593306}
episode index:406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.098997 , 12.37856  ,  2.4788394], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.187276466496026}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.41313242574105213
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3477385, 4.448588 , 4.669198 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.197356352011943}
episode index:407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  3.0639343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.41389675885432864
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.763807 , 4.795692 , 2.5164022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9513869074961545}
episode index:408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.4250035, 1.9155623, 2.013473 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6564350734493876}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4152571555319464
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8496075, 3.1476855, 2.194453 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.862347878881244}
episode index:409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 10.       ,  2.6925392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4160667041631651
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.56348  , 4.262906 , 4.1790977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3829104639054794}
episode index:410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 13.      ,  2.347229], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.4159539619088745
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4445691, 3.0660164, 0.2984463], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5568312113491964}
episode index:411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  0.      ,  5.630817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.41494436491395004
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.406105 , 2.5973902, 5.6939545], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.429817229103175}
episode index:412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 13.       ,  1.5028332], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.41434433965995715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3203225, 4.5499163, 5.070801 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6923953206864764}
episode index:413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  3.       ,  3.9292397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4133435079216481
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.322152  , 12.827803  ,  0.26573366], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.685688372497456}
episode index:414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.0592763, 11.235053 ,  4.772189 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.460644612343959}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.41459344006619103
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7289581, 1.8403463, 5.993955 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7205650395069825}
episode index:415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.304369 ,  1.5161198,  0.6712262], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.386864236994155}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.41359682121987806
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.719705 , 13.103346 ,  2.2670727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.715008349659149}
episode index:416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.48363  ,  5.3040786,  1.9868686], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.679072893316778}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.41385280131923163
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00492  , 4.4054623, 3.0422523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7277697752981562}
episode index:417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 8.      , 2.883609], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.41425307312018594
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8556907, 3.4643445, 5.029563 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2349330142292958}
episode index:418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.      , 7.      , 4.992285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.414343265428411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.745791 , 3.142294 , 4.2133765], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7515803244433799}
episode index:419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 3.       , 1.4165311], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.41569030527262907
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1771541, 4.6168823, 0.8190111], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.43661153163817}
episode index:420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 5.        , 0.99495345], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4160025768572327
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6436899, 2.4170856, 2.5384967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8684041640091694}
episode index:421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.560438 ,  4.251013 ,  1.2295538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.627930314236353}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4155686002839881
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.968407 , 4.6215525, 4.876587 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9218783650517666}
episode index:422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  4.7606564], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4158162880437194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.215023 , 3.9574492, 4.2188926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2381024968993741}
episode index:423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 7.       , 5.9597483], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.41710114587852193
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2211261, 3.619063 , 4.6206083], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.88351567544657}
episode index:424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  7.      ,  4.787877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4172494760213288
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1091557, 1.3225186, 4.2712245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0110122220901516}
episode index:425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.965227  , 5.3713255 , 0.19317782], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0798217893369526}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4177484763856055
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.434089 , 4.4329286, 2.858267 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.027287856072631}
episode index:426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.     , 13.     ,  5.71176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.4176360200888002
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1754332, 4.0940557, 4.086422 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3699884110860054}
episode index:427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  8.      ,  5.179323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.08304597359457}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.4174336767902774
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.126337, 4.37475 , 4.929117], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7772374893142198}
episode index:428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9891155, 4.7916265, 6.1667056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.046527491774165}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.41879164024764265
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9891155, 4.7916265, 6.1667056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.046527491774165}
episode index:429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.010694,  2.206544,  3.097943], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.045560833968961}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4191030166125607
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.848766 , 4.9457355, 4.3217907], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6839935967702413}
episode index:430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 8.       , 3.1730542], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42027155878846645
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0880315, 4.139907 , 4.212094 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4598199176740747}
episode index:431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.443925 , 1.2564355, 2.6579695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.263832203722591}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4216135227727524
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.443925 , 1.2564355, 2.6579695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.263832203722591}
episode index:432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 13.      ,  5.558459], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.4210336472718675
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0143194, 2.2220428, 4.6668935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.278304068717945}
episode index:433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  8.       ,  3.5254397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.083045973594572}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.42091543526812986
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9108664, 2.3148975, 5.61242  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2866924369393284}
episode index:434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.817442 ,  9.991651 ,  1.5740778], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.487877589956426}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4212838283229574
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3270621, 3.421213 , 3.7573512], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5332822458762654}
episode index:435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.550417 ,  5.3779364,  1.9201573], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.75748453639367}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4214755761680586
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6966639, 4.757449 , 4.8064   ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.187992828615527}
episode index:436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  9.        ,  0.96957177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42195234517271574
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.441105 , 4.4558206, 3.6773932], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5211794855539102}
episode index:437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 1.       , 4.2305603], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4216521993944902
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8302236, 3.3705826, 3.0906541], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9091768855133076}
episode index:438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  1.       ,  2.2314606], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4206917160245711
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.136374,  5.363169,  5.920907], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.332537281656316}
episode index:439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 1.0038772], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4219855984881517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0740714, 2.687119 , 1.0506418], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9511779672344307}
episode index:440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.245663 , 2.9848552, 1.1650466], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.245714234060093}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4231001827262367
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7339191, 4.294294 , 2.19076  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4878958157271231}
episode index:441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 2.4037063], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4240500537910731
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8551382, 4.262029 , 4.433536 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7039443093242075}
episode index:442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  4.       ,  0.5167877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.4237227220411724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.942771 , 4.3417573, 2.852135 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6398566170811926}
episode index:443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  1.       ,  4.5466733], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965904}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.4236528758842279
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.631885 , 3.0369015, 4.59141  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3686125202081585}
episode index:444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.325831 , 11.094148 ,  2.4813435], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.27284623510944}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4244139766448186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7972794, 4.8953876, 5.164931 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0562462684619605}
episode index:445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.359482 ,  6.1439843,  3.1518779], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.724445040250961}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4249177628594455
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5540236, 4.9106574, 5.3875732], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3961342625226423}
episode index:446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.        , 10.        ,  0.14461693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42587199106578877
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3571763, 4.583321 , 4.5849667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6231083144298355}
episode index:447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        , 12.        ,  0.49478552], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841895}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.42619281519840346
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1258228, 4.8793693, 4.629546 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.654160706091427}
episode index:448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  5.       ,  1.7455454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.42594474472620425
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8519843, 3.8582172, 5.320502 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2093031106630359}
episode index:449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 3.3273704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.000000000000001}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4270487446144303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0641146, 3.439691 , 5.2906237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44434098656117355}
episode index:450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  3.2467933], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.42755564145691305
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.03653, 4.19382, 4.24585], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5810125456748412}
episode index:451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.636619 ,  7.463281 ,  2.2493753], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.620042230649652}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4277842916814666
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9208965, 4.541315 , 4.8588705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4628226999693923}
episode index:452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.449444,  8.73676 ,  3.706966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.920624689390062}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4283316311017289
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6582683, 4.7672477, 3.9296243], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2188754401614714}
episode index:453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 0.8491271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.42893761362132765
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6480112, 3.3864431, 4.756117 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5227182883646811}
episode index:454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.     , 1.     , 4.78771], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.43008498161314895
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.940499 , 1.4335213, 0.8430431], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.827127282044414}
episode index:455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 7.        , 0.05445397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4302490186902847
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7479692, 1.4244723, 2.773752 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5955585171685556}
episode index:456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.     , 7.     , 4.30236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.43115206940077405
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9562188, 4.61845  , 3.7268484], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9258400890590035}
episode index:457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 9.        , 0.48822215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4317777865693275
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.144249 , 2.2567523, 3.753886 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3644496700155797}
episode index:458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 2.1151524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.43083709422386063
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.3600136,  8.531004 ,  4.530116 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.471606920811807}
episode index:459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 13.       ,  3.6551754], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4310662923346939
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0091538, 2.0215921, 1.2941587], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2182764393237884}
episode index:460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 0.7606797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.43217348074481604
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.014017 , 3.7181187, 5.2549014], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7182554559449594}
episode index:461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.601622 ,  4.4298735,  2.4374294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.6824842117231}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4317078344589459
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4351037, 3.087132 , 5.481882 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.567320181857583}
episode index:462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.140366 ,  5.7360435,  2.1614685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.444854787974807}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.431366109440724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.735085 , 4.8010736, 4.8698344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8204521109876814}
episode index:463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        , 11.        ,  0.34762272], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4313536611949221
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.817701 , 3.3407946, 4.600932 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8858758521613972}
episode index:464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  0.       ,  6.0655246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4312454750807493
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6586437, 4.0585165, 3.005484 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2467031479246595}
episode index:465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 3.6264927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4318296000161974
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8289547, 3.1241589, 4.909027 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1776088165501866}
episode index:466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 12.       ,  1.1267922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4326918787344713
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3946285, 4.8090477, 4.098521 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.284215904702559}
episode index:467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 8.       , 1.1420743], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4327728504499652
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.436445 , 4.396306 , 3.2740822], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0032587122244028}
episode index:468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.427139 ,  5.916202 ,  1.3319197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.827255797951658}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4330285234280301
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.575899 , 3.5764477, 3.9760497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6780196749387462}
episode index:469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  6.       ,  3.6702845], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.43339443309549713
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.111302 , 3.6375685, 3.5937765], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0937448850635323}
episode index:470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  4.      ,  3.429759], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4324742750634472
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.458858 ,  9.732025 ,  0.7457769], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.949255512608124}
episode index:471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.279506, 11.019627,  4.691173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.185607654531116}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.43249667698413097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.041521  , 4.9661055 , 0.39806914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.18729342771497}
episode index:472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  2.       ,  5.4689198], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4315823076881814
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.35854185,  5.3967395 ,  3.3136315 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.126034837707585}
episode index:473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.     , 10.     ,  4.66859], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.43261851525514294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0652227, 4.8771276, 3.953517 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1583113053532132}
episode index:474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  2.4370487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.4322777484009468
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5299273, 3.3054037, 4.5711975], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5605709700962429}
episode index:475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.1459634, 11.005333 ,  4.670764 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.050759843080963}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.43336748012678517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.65549  , 3.2700374, 4.5925593], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7089338766815801}
episode index:476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.235958 , 9.05758  , 3.4906971], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.693759187094559}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.43372730944965887
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1414862, 4.222324 , 3.3901293], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.230485285883845}
episode index:477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  2.7443552], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4345310325210973
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9790567, 4.896474 , 3.2872968], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.153819487357083}
episode index:478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 12.      ,  5.181277], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.212670403551897}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4347893521578964
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0400836, 3.829177 , 4.230322 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.128099257020325}
episode index:479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 4.       , 2.5585036], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4355704740663617
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4531856, 1.8876987, 6.2632823], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.905216315645902}
episode index:480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 9.      , 4.099557], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.43668217578347945
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.765633 , 4.116238 , 4.8137126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.140576773051701}
episode index:481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.221578,  8.914993,  3.184671], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.95557574140847}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.43757857337058637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.885827 , 3.7406614, 4.9312296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0260609568915715}
episode index:482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  3.       ,  1.1309836], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4375105679861303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9803858, 4.7226286, 3.5602205], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9820710252752616}
episode index:483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  7.        ,  0.76161647], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.4368862341781157
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4442947, 4.9802856, 3.6424246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0567789439538604}
episode index:484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  5.       ,  0.8235687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.43686294322803426
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2172017, 3.6433256, 3.7995694], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.01323291927091}
episode index:485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 11.        ,  0.06225842], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4368052420576526
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.654386 , 4.357456 , 3.7614589], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1400188166833547}
episode index:486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  4.7709055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.43629161995222765
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7636595, 1.2857684, 3.6862497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4594887203202047}
episode index:487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 8.       , 0.9897292], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4361706056632344
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3451772, 3.5599334, 3.3366218], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6577786257490803}
episode index:488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 2.2366135], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4367171845780326
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9081594, 3.6917195, 4.2777543], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2925137759235241}
episode index:489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 8.      , 4.789317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4370119789240328
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0095448, 1.6371005, 2.285767 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3629329643596455}
episode index:490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 13.       ,  5.4207764], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4372820180758722
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2563076, 4.319085 , 5.3229275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5142866108504556}
episode index:491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.71849  , 9.464509 , 3.4847312], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.323517570107414}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4381948287540028
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7244544, 3.7712166, 3.8214748], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8189630442821962}
episode index:492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.407254 , 8.790388 , 3.5398493], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.718480166408792}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.43844981934568306
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0744488, 3.9110632, 3.894929 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.298722881714613}
episode index:493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 6.       , 1.5768183], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.4378561899668776
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.318342 , 3.271963 , 5.0880585], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7339083580041639}
episode index:494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 4.       , 3.5530376], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4380441684577629
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0844617, 4.397887 , 2.6722245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7692218239866075}
episode index:495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 13.       ,  2.6824994], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.43787706492197115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5158868, 2.4552512, 5.392807 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5809311420157195}
episode index:496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.475912  ,  5.942552  ,  0.78048354], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.753173265896564}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.43809694334746174
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3098412, 4.92635  , 3.822601 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.562705931268018}
episode index:497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  0.3929796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.4375701368377102
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.287868 , 4.020624 , 4.9933896], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.993256025824291}
episode index:498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 9.       , 3.5523236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.43795541438152885
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.931071, 2.838428, 4.042884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9378183498278996}
episode index:499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1250262, 6.003912 , 5.1430893], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.541046990485244}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4390595035527658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9600408, 4.186565 , 4.927727 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5777995552382154}
episode index:500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 1.2449998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.4389211205868915
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9901032, 2.3823144, 1.6282877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1669789552200707}
episode index:501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 7.      , 5.926143], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.43937938879004446
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9389753, 3.683176 , 4.732927 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2619440732546032}
episode index:502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  5.1093965], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.439961742628741
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.399641 , 3.4837053, 3.4364057], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6274422351765979}
episode index:503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3472033, 2.873828 , 3.2235198], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.36941777196809616}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.44107292964733474
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3472033, 2.873828 , 3.2235198], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.36941777196809616}
episode index:504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 2.      , 6.254385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4421599139450628
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9991705 , 1.9424074 , 0.00886029], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4560775310133316}
episode index:505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  2.      ,  5.814297], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.441286080123037
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.393481 , 13.359875 ,  2.9202719], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.25298527810069}
episode index:506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  2.6003013], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.44070789257160614
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.491168 , 4.573537 , 3.5473695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6484126656424032}
episode index:507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  1.6138825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865699}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4415333856071764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9362998, 4.9678397, 2.9866989], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1792316242936214}
episode index:508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.85875  ,  3.8062522,  0.6655288], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.886126466666596}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4412032397631981
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.657802 , 4.784476 , 3.1447687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.816990173943824}
episode index:509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.874389 , 10.201266 ,  5.1033483], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.441207625661664}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4421474386939132
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4615066, 4.0537243, 3.865407 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1503578676649007}
episode index:510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.     , 9.     , 3.29025], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4431620151543948
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3836837, 3.3842142, 4.4982524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4360365779926307}
episode index:511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.025546 , 11.681358 ,  3.2936392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.822663505610516}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.44391007493697526
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.747326 , 3.9316592, 4.469182 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.965315030185086}
episode index:512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  2.3407009], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.44348519790110197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4934728, 1.7584724, 6.2002745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9521820704500499}
episode index:513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  2.       ,  5.9254303], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4431133602847367
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0973825, 4.995514 , 5.3201427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9978886728680514}
episode index:514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 2.2614207], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.4426829494632884
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.013747 , 1.0031416, 5.373205 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.816495051550362}
episode index:515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  8.      ,  5.817379], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.928388277184121}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4426498238313608
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3902512, 4.3628345, 4.104931 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4176084492885985}
episode index:516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.775209, 10.418897,  4.06198 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.422301455215698}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4435081121251418
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3768144, 4.653059 , 6.111901 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1513302566517094}
episode index:517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.371054, 11.101467,  4.151936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.77483765856224}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.443588195635082
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5637382, 2.5900917, 4.803073 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.493610622264407}
episode index:518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 3.2121637], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.44398417720104216
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8587987, 1.7231871, 1.2746322], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7124811007340253}
episode index:519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 6.      , 4.204427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.44501516916796324
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5694404, 4.0827785, 5.1892695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9067124163998896}
episode index:520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.016314,  8.745073,  3.628079], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.862410983752264}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.44547109896804465
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.598162 , 3.9974432, 4.539676 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0753449518089242}
episode index:521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.525569 , 12.3513155,  2.1578605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.654344221538855}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.44537002603584114
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6477021, 4.549438 , 4.910462 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0565669770363684}
episode index:522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.118382 , 5.3419223, 5.341895 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5023689413744483}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4464113835386407
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2958732, 3.7252855, 5.8295627], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.783313494076811}
episode index:523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 11.       ,  2.2004378], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.44697109535133084
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5484686, 4.251218 , 4.365393 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9908041244527757}
episode index:524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  2.      ,  4.424183], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4461197218363759
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.4393816, 11.158992 ,  1.808307 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.85429236030963}
episode index:525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 6.       , 0.5830109], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4460257257994665
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1444817, 4.359829 , 4.9962955], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6065635382290564}
episode index:526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  5.       ,  5.0805516], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4461278345358986
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8406869, 4.675411 , 3.673266 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.037402658886263}
episode index:527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.765038 , 11.426828 ,  4.4428554], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.458937322686351}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4469616546820733
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5666418, 4.678695 , 4.290971 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.771750693202786}
episode index:528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.100495 , 8.626004 , 2.3578653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.298672595491047}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.44742004302080113
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0297644, 4.023194 , 5.3449283], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4100647925276963}
episode index:529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.904488 ,  3.673276 ,  1.9099464], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.920778577163665}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.44804344173094895
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8721433, 3.2420049, 4.5567822], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8877199940337908}
episode index:530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 2.8510313], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.44864984418683257
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3162293, 4.3171234, 5.675541 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4840338257960224}
episode index:531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 3.      , 4.343817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.44922516268874385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.050866 , 2.067577 , 1.7302748], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9338095114060125}
episode index:532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 10.      ,  3.682628], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.44988633609746864
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3751497, 4.1106195, 4.6014385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7676290744956333}
episode index:533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.918604 , 8.435235 , 5.7054987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.03564881334233}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4506707171399247
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0067492, 4.394853 , 4.3859873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3948694432531796}
episode index:534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 3.7902608], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4510538732211101
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4351335, 1.6986811, 5.7547755], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.418627875370101}
episode index:535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 0.        , 0.36467904], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.45184952088860614
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4491725, 2.2537212, 2.3378851], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8710269362711383}
episode index:536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3981583, 6.8024597, 3.3099957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.8232486106913495}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4510080878888135
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.361409 , 7.513815 , 4.2039995], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.276656377939119}
episode index:537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 10.      ,  4.312326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.45082993310594416
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.52323  , 2.621139 , 1.9462156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5696386474991335}
episode index:538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  1.       ,  6.0358586], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.45107174475902795
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2604637, 3.686627 , 3.9810169], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8701451824279005}
episode index:539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  0.      ,  4.848389], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.451259914633849
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.301978 , 4.688096 , 3.7361965], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.714893303304021}
episode index:540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3647604, 4.538005 , 5.957634 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.664027748247449}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4522742216308289
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3647604, 4.538005 , 5.957634 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.664027748247449}
episode index:541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  1.9558986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.45294882073777804
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3236372, 3.1378174, 5.2050405], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6820183442412655}
episode index:542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  1.       ,  4.6636677], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45211466084691654
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.756147, 6.459592, 2.637091], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.423247862546049}
episode index:543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  6.2348003], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.45262971362045973
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0207376, 4.417187 , 2.4390728], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7465179487484934}
episode index:544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.018954 , 8.7253065, 3.0473912], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.30704190630603}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4533151061162673
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0616226, 4.944328 , 3.6947734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1589263126463227}
episode index:545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1397285, 9.265568 , 3.1443424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.368384470092459}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.453122391845894
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4314194, 3.0095637, 6.2827244], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5686610536709724}
episode index:546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.134316 , 8.279427 , 3.1950438], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.3643431688731065}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.45403257037981376
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8574417, 4.552861 , 4.162763 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7738613687898954}
episode index:547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  0.6946487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45320404379152945
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.868703 , 6.633367 , 0.5038005], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.770484899590419}
episode index:548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  2.      ,  3.096914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4532619442040661
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0113745, 2.5114098, 2.830751 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1232091296296711}
episode index:549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  4.9044623], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4524378315782405
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8151581, 5.927812 , 1.3527741], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0391720031241825}
episode index:550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  9.      ,  4.005307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141666}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45161671028681
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.02608024,  2.9732537 ,  4.1683507 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.026198433159386}
episode index:551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 1.       , 4.1381454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.45212520061175127
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.599772 , 1.5611644, 1.948252 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4934625588649595}
episode index:552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  4.2663054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.45272837048552245
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1733327, 4.7568097, 2.7884364], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1126026522960952}
episode index:553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0214753, 9.000115 , 4.330403 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.000153826159937}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.45366261349908654
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8339896, 3.199647 , 4.3595595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1829789137312003}
episode index:554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 8.       , 0.9726831], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.45417799685023824
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5317729, 4.956521 , 4.8298326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0117681823716844}
episode index:555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 11.      ,  1.503904], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.45484704474049964
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1804967, 3.5360568, 5.11733  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.979256104112729}
episode index:556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 9.       , 3.0732474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4548924566109322
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.888318 , 1.6902307, 3.8954163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.298095004696362}
episode index:557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 11.      ,  5.447276], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.4547953100093921
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1518443, 4.8226576, 5.003742 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.010335501044261}
episode index:558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 13.       ,  1.1761276], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.4542929746220341
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8315706, 1.5478072, 4.7706676], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3374162734761654}
episode index:559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.456913 ,  3.2723527,  1.8922116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.460149708953615}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4534817371673519
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.0929735, 13.048725 ,  3.440883 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0491552265772}
episode index:560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 9.       , 2.8571053], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4539787454249046
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1523898, 2.5713925, 5.4874125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8966728603860228}
episode index:561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 7.       , 1.5505977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.45396725871376387
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7946339, 2.2968645, 3.3707924], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0610572547492223}
episode index:562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.950356 , 11.297577 ,  4.0737176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.823919090289616}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.45433726720802714
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.007483 , 4.2000055, 5.353019 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5668552203685289}
episode index:563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  8.       ,  1.0572567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.928388277184121}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.45483011490740016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0157824, 4.133896 , 4.3577843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5014673567454435}
episode index:564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.     ,  0.     ,  5.80206], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.4543622057832543
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.56392  , 4.5997224, 4.471563 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2371762430872484}
episode index:565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 3.1539445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.454607089600601
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8782558, 4.052561 , 3.0646043], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3708455706713734}
episode index:566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 10.      ,  1.592937], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.45416570754089314
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3700099, 2.3797705, 5.0061336], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.744004684641862}
episode index:567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.743267  ,  4.980316  ,  0.23597738], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.896220665265428}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.45415401300959546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4108236, 4.694257 , 3.2411864], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3229265437347584}
episode index:568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  4.       ,  4.3957353], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.453355851299561
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.52361  , 6.7245398, 0.3098976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7548826509330904}
episode index:569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  1.5883901], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45256049015693023
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.7341375, 7.1972594, 2.766605 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.61789727755274}
episode index:570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 11.       ,  1.0060941], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4528381727302681
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2298985, 1.866852 , 2.2605395], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1562342264296595}
episode index:571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 5.6410246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4531042005176934
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.364902 , 2.4450254, 3.811712 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6641914194032967}
episode index:572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0020691, 5.0909505, 3.0391524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8920238184217517}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.45376983674968707
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0914474, 3.4136744, 4.447314 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.99829573941095}
episode index:573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 12.       ,  0.5404993], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.45321506874996115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0132442, 3.3817482, 5.3892956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0580258934539384}
episode index:574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 6.        , 0.61972564], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.45363802205309095
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0766978, 4.848092 , 3.5288615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.065897199006959}
episode index:575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.000222, 12.970211,  2.862629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.783139806443168}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4544364234861301
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9435444, 3.9261072, 4.7942605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.322100789943307}
episode index:576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9999845, 4.9921365, 5.800109 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9921364784843498}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.455381940949759
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9999845, 4.9921365, 5.800109 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9921364784843498}
episode index:577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 12.       ,  2.9359608], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.45532303486187836
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2345324, 4.0161405, 4.8421106], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0428551493616924}
episode index:578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  7.       ,  3.9840176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4557638469385803
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3481779, 2.6512203, 5.2815886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6882427199695595}
episode index:579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 10.      ,  2.123975], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.455875186034635
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1759456, 4.728268 , 4.85348  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.512784331357296}
episode index:580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 1.6849608], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4565268875413741
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.999976 , 4.9902034, 5.6868773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.227320681626103}
episode index:581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 8.      , 2.985933], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4565510451943712
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4459405, 2.95139  , 1.0037203], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5561877964962599}
episode index:582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  1.       ,  5.0965314], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421558}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4567449562703279
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.023241 , 3.064086 , 4.3010783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9788590655560097}
episode index:583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.682211, 8.918258, 5.040287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.957447952241941}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4569988280355115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.238305 , 2.8017807, 2.666696 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2540695320207387}
episode index:584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 10.       ,  3.6250196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.456217633457673
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.44116563, 11.277937  ,  0.8133732 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.964700818921493}
episode index:585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00298 , 8.890859, 2.876656], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.97563260107133}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.45690678144604097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9474583, 3.7713337, 4.4682198], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.094647791572194}
episode index:586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 7.        , 0.03644293], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4572911899870365
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4303002, 3.4455113, 4.21658  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6193857020704258}
episode index:587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  7.      ,  2.260178], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.45714227578238126
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9112291, 1.7229017, 3.911081 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5688590111519722}
episode index:588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 6.        , 0.87892157], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.45781174181856543
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.710735 , 4.1305466, 4.6612206], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0505485250849023}
episode index:589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  4.1096683], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.45827698186574484
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.136365 , 4.0900087, 4.9303536], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3906777362664027}
episode index:590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.005881 ,  2.8467355,  3.4566321], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.007185363632846}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.45897151457454893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7146807, 4.1645145, 3.664277 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1989583965945385}
episode index:591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 1.       , 1.3692992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.45986852215128116
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4002728, 2.959536 , 0.8852756], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6010906655050011}
episode index:592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 8.       , 3.6805556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.46027927960970993
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4146054, 4.3437824, 4.6491275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.078275080326486}
episode index:593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.169097,  9.992839,  1.116811], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.53135439287286}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45950439866760606
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.4718847, 7.952338 , 5.8748245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.5349677450032635}
episode index:594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 10.       ,  2.9989824], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.45950728533777496
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4207716, 4.1958556, 4.112729 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3287498631044317}
episode index:595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        ,  0.        ,  0.49769247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45873629995969145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.304092  , 11.757095  ,  0.43284395], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.92751058765385}
episode index:596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.0501833, 7.554787 , 3.7981257], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.095085790727552}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.458981307944914
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9566417, 4.6349177, 5.59301  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8942331683759506}
episode index:597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  6.       ,  4.6976476], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.4584493733378226
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3862147, 4.6980233, 3.4104557], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8055513137355113}
episode index:598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  8.       ,  1.8352201], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.45919383527717317
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3555923, 3.8629856, 4.210322 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6069768247306238}
episode index:599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.3645608, 11.033506 ,  4.8643966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.041774031966908}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.4588205399337786
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8908787, 3.9630125, 4.404415 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1219837804890127}
episode index:600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 1.5286006], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4595319614508873
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5489593, 4.3460727, 4.4498   ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.052117544750229}
episode index:601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  9.       ,  6.2359047], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727819}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.45876861932223134
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.9601254, 12.135707 ,  0.5375707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.957094663686869}
episode index:602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 8.        , 0.34625396], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4593372129710155
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.459863, 3.298232, 4.529787], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4900143939481911}
episode index:603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  4.      ,  2.856976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.45955845342371493
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7309295, 3.5179229, 4.02224  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.370687439671483}
episode index:604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  4.      ,  5.223805], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.45935712441127724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5534666, 3.656197 , 3.9954646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5884122870312658}
episode index:605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  6.       ,  1.4754107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.4591851833061515
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.313005 , 4.7677755, 3.844045 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7952722511632768}
episode index:606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.49862045, 11.063152  ,  5.7788553 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.442234590264524}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.45906279598421956
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7882247, 3.4270194, 3.5919843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.896461769945304}
episode index:607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 10.       ,  2.5473735], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.45919868789208573
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3926315, 3.9309707, 5.5730896], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.857509026102912}
episode index:608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 13.       ,  5.2147784], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.45888040802649566
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2784033, 2.2023606, 3.578233 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5068322787786028}
episode index:609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.      , 12.      ,  2.183605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.45863906486817213
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3063025, 2.1517751, 3.7424712], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.557533880746408}
episode index:610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 13.       ,  2.0744002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.45836385935171026
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9820693, 4.4696784, 5.1259027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4697877795125671}
episode index:611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  1.9264958], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.4581723596820409
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9656618, 2.7677598, 5.4876575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0600901361052892}
episode index:612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.048129 ,  8.436117 ,  2.9378307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.424371665582536}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.45884213693014353
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1739827, 4.185985 , 3.2746246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.177360710956402}
episode index:613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.854212 , 11.360742 ,  3.6963925], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.700945745254195}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4595530426586917
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.738469 , 2.2455454, 4.2388043], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8951191670231884}
episode index:614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.440953 ,  5.9202776,  1.4724835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.841656981371463}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.45987191449270054
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6441133, 4.5229263, 5.4520063], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.03905207033068}
episode index:615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.      , 8.      , 5.445991], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4601581378231732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2962928, 4.571823 , 4.1370115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5995053402534773}
episode index:616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 1.       , 5.4989614], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4609849463518229
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9551864 , 1.4210064 , 0.24282922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.845427228249725}
episode index:617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.      , 12.      ,  2.138958], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.46097047823517995
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.336504, 4.212522, 4.849258], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8045643707809822}
episode index:618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8443063, 3.0060692, 4.9699874], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.155709586446649}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.46184128521702944
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8443063, 3.0060692, 4.9699874], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.155709586446649}
episode index:619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  1.0700685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4622083945729198
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9006865, 4.584813 , 5.812987 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.587921846225063}
episode index:620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 13.      ,  6.166431], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.461955980130658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4582224, 4.6457915, 4.831208 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7326721391440276}
episode index:621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.2370915, 12.848776 ,  1.5239935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.721535838751763}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.4618838979759997
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.971101  , 4.6708293 , 0.38590497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5839715418036557}
episode index:622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.8274424, 10.007458 ,  5.0695176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.104882489487072}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4626999735811747
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4809196, 4.607095 , 3.780972 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6775092381181256}
episode index:623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      , 13.      ,  5.594524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.62049935181331}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.46235484331244503
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.676777 , 4.7467327, 4.0613956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7763863171835403}
episode index:624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  1.4948654], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4624905261909703
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.93975  , 4.9922686, 4.4013352], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.780605121489077}
episode index:625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 8.        , 0.97459835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.46235433788543123
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6326929, 3.888258 , 4.9805026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.630500185492746}
episode index:626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.2856135, 10.131939 ,  4.55799  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.8523760274447545}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.46316445696376385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2267284, 4.6094217, 3.7368062], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0236355820634784}
episode index:627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 13.       ,  2.5723186], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.463409880027424
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7774704, 3.8164177, 5.02201  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8462016876827443}
episode index:628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 11.       ,  1.2664361], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855955}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4631257749292455
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8228414, 1.3885856, 3.5438857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8093436524230582}
episode index:629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.967648 , 4.000262 , 3.8240323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1316862010267115}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.46396208322300864
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.415575 , 2.7388854, 4.5266514], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4394559706079009}
episode index:630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 6.       , 0.5857782], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683786}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4647039267486568
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1750169, 4.381564 , 4.792265 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3926056089629184}
episode index:631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.129299 , 11.707445 ,  2.9263728], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.912392951964126}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4653571183566478
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5406784, 3.6380806, 3.3039067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5927229618382177}
episode index:632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  1.3866627], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4654195650713877
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7475696, 4.3083105, 4.660557 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5068299956494289}
episode index:633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 8.       , 0.6158423], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.46557490517451233
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4943132, 3.8927255, 1.3033515], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7406696470152982}
episode index:634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  7.       ,  1.1730118], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.46582364610320004
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6817992, 4.297565 , 4.157689 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3360114715600635}
episode index:635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.309083 , 9.931918 , 3.3868096], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.681244649102532}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.46660159007159124
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2300941, 4.4559097, 3.817231 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2917766299619946}
episode index:636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.       , 4.9902196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4672746460593261
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2883487, 2.4350717, 3.8632758], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4067644736351674}
episode index:637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.749905 , 4.0315824, 6.0109696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6207713255464846}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4681096387771015
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.749905 , 4.0315824, 6.0109696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6207713255464846}
episode index:638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.674804 ,  4.8827214,  0.9385977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.813870986231175}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4682079109276148
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4692297, 4.134654 , 4.320709 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2526599360424862}
episode index:639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  1.4666992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.46802023155818484
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8673012, 3.078223 , 5.169851 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8708215960834819}
episode index:640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 0.      , 4.058159], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.46833372848019966
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8726766, 3.5344243, 0.725653 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2475846108545123}
episode index:641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  0.5956408], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.46760423669128964
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.424713 , 6.5315056, 5.2002764], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.472947110040728}
episode index:642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 0.5498138], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4681618795950911
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0397663, 1.7680938, 4.0501866], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6120504980760824}
episode index:643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.243887 ,  3.5661244,  0.9599454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.258129976680399}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.46845302453449955
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.929979, 3.310204, 4.751005], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9547493167849033}
episode index:644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.7226981, 12.016027 ,  0.6335312], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.106055740675137}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.4682999650199495
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.619062, 4.929056, 5.643484], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0259554269101043}
episode index:645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 10.      ,  6.184133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4684392315424386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1853967, 3.8253567, 4.0148025], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4444303329226176}
episode index:646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  0.      ,  3.747352], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.468893487311843
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9162161, 3.692749 , 4.560016 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6977972094410775}
episode index:647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.        , 13.        ,  0.16633482], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.46871249305142043
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1011057, 4.4957466, 6.0052385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4172417192933766}
episode index:648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1677313, 7.1982574, 3.4140046], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.35763260722054}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4693423983363935
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9068463, 4.328077 , 5.388535 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3313398161697088}
episode index:649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.826061  , 0.8157831 , 0.61957234], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5717534944760665}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4700399403303805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.968512 , 1.5570745, 3.355457 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.737828953160633}
episode index:650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  4.7876687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.4695695379177503
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8364699, 2.8765044, 5.1979594], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.170065587010595}
episode index:651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.8819985, 11.204985 ,  5.46078  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.252253920855193}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4688493392399623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.4051317 , 10.064048  ,  0.83382857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.841918986357461}
episode index:652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  5.0427547], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.46917660609412837
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2408488, 4.2141676, 3.9096713], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1374788804776284}
episode index:653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.6433904, 10.530438 ,  3.8234346], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.557873647645227}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4699280119104982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3943942, 3.8125362, 4.65764  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9031953795307661}
episode index:654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 7.       , 1.6632634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.47002110737774194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8679827, 4.377389 , 4.592433 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7828806455510813}
episode index:655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 12.       ,  2.3617113], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.46995993040540374
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1644764, 4.515515 , 3.2627134], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7305735920585783}
episode index:656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.354129,  7.863731,  4.056003], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.666816796389892}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4702129373394349
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.271002 , 4.545514 , 3.9060783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.001014610121883}
episode index:657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.251246 , 8.029505 , 3.5565317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.023340981137741}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4709729465532048
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2798996, 4.818669 , 4.354083 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.223892833937949}
episode index:658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.095876 , 5.3269625, 5.7074156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.872880567713967}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4711856086062847
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0639682, 4.8513527, 5.294098 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8524574805213883}
episode index:659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.697181 ,  7.8745503,  1.3681602], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.672224686028912}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4707488978634101
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6553357, 3.0992627, 5.6740437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3586733757350653}
episode index:660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.448316 , 9.050895 , 5.2908397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.067480073660311}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.47148996762458506
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3098907, 3.674584 , 4.902814 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4733897052202438}
episode index:661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.636664  ,  8.149491  ,  0.37051094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.926232714406238}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4715960025314465
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.311392 , 4.610711 , 4.8933516], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0770504984871616}
episode index:662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  0.       ,  5.4298215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.47109296303002274
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0502665, 4.9875937, 4.866427 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.7842394338623144}
episode index:663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 1.       , 4.4353294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4717731614206822
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4545608, 2.2694948, 3.0968344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8603855936230651}
episode index:664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4569461, 4.727607 , 3.0950115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.316385200057011}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47256748749373384
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4569461, 4.727607 , 3.0950115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.316385200057011}
episode index:665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  7.        ,  0.24604118], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.47210388461417585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8587985, 4.4313397, 3.3555422], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4382876332525056}
episode index:666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.970759 , 12.659251 ,  6.2573705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.659295473186509}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.47247214045823
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7273483, 4.0153027, 4.4939322], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.003639621412949}
episode index:667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.61253  ,  5.440461 ,  2.2255607], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.829563152475297}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47176484683478953
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.6043262, 12.060895 ,  0.9545392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.751460674709087}
episode index:668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 11.       ,  4.6629653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4719025753005856
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5570588, 3.716134 , 4.9888825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.610877798754267}
episode index:669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.0208   , 13.28769  ,  2.5541816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.682521513627957}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.47144273260567143
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7663757, 4.1542444, 4.788542 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6894109026089161}
episode index:670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 6.       , 1.2867607], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.47159759403245427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3064547, 3.9020362, 5.1702437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9187926182413284}
episode index:671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4286003, 4.0464644, 5.356909 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7708717289186686}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47238390713657263
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4286003, 4.0464644, 5.356909 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7708717289186686}
episode index:672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.195618 , 10.137327 ,  3.0967302], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.867823543140679}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4725809682955637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9766111, 3.4719763, 3.072846 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0846801015772312}
episode index:673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 12.       ,  5.3614993], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4728624238930243
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3552545, 4.5787416, 4.782517 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2798273051277933}
episode index:674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2910962, 1.0390608, 2.0000992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9824276602185897}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4736433684502198
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2910962, 1.0390608, 2.0000992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9824276602185897}
episode index:675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 11.       ,  1.6133057], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4734576432224716
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6536859, 4.764674 , 3.1051803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.219602794946269}
episode index:676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  2.      ,  4.053186], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187263}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.47348191594002215
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1648517, 4.548293 , 3.658447 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5570445201002532}
episode index:677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 13.       ,  1.0617414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4731036902478876
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.959808 , 1.4399633, 0.5549589], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8316511645186693}
episode index:678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3874288, 2.4405007, 0.9396977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6805442459065519}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4738796789220439
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3874288, 2.4405007, 0.9396977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6805442459065519}
episode index:679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  3.       ,  2.9449213], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.000000000000002}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4731827970412762
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.4978495, 2.792224 , 1.2771199], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.504015092837691}
episode index:680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 10.       ,  4.7690296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4731652334148105
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0595113, 2.552966 , 5.398127 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9913150608247663}
episode index:681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.176496 , 11.178518 ,  3.7086823], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.248768648526312}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4731342633515431
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.230642 , 3.9501436, 3.869237 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5547514776935736}
episode index:682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.99453   ,  6.1478224 ,  0.25785214], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.436278660081088}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4728887652001182
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1284337, 4.069173 , 5.394092 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0768592383651492}
episode index:683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 8.       , 1.7494094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.4729430005525297
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0236607, 2.033522 , 4.046823 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4078213957293582}
episode index:684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 4.       , 3.4168956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4735728386174294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9913893 , 3.814613  , 0.82886285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8146586116792124}
episode index:685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  1.5115123], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.47311653446654517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.96276  , 3.7404313, 4.6154404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2145555919442153}
episode index:686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.455593 , 10.729248 ,  3.2772133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.51754688372524}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.4726615587120246
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8039417, 4.0961585, 5.5731177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1135539113543609}
episode index:687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 12.      ,  2.832225], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137419}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4724275463904215
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.154675 , 1.7372438, 3.7347586], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7110896130722202}
episode index:688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  0.3013131], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.4720731239139991
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1864023 , 1.5643482 , 0.24428862], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.650162784257002}
episode index:689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 11.      ,  5.011412], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47138895996629765
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.4553485, 6.511927 , 5.8862867], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.673073446859293}
episode index:690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 4.       , 2.3763719], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.47166521623405117
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8827715 , 4.674664  , 0.27009365], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.013131670001665}
episode index:691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4251652, 9.045713 , 4.944963 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.072979984144565}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4718756568766932
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4647638, 4.8103595, 4.270863 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3736789484768592}
episode index:692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 1.4188037], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.47207658268139185
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4552007, 4.362282 , 4.0598273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.993344098988823}
episode index:693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 2.5793762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4721165833257979
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.403509 , 2.4248376, 1.3449944], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6969370186820747}
episode index:694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.085973 ,  8.580086 ,  3.5255413], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.824475198274769}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.471663686489513
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0413538, 2.651816 , 0.3697184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9893534188602229}
episode index:695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 13.       ,  5.3942294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.471501451735479
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.534642 , 4.0464773, 3.2963834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8574825756103885}
episode index:696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 12.       ,  6.2744713], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.47179446120079443
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2005525, 1.5558568, 3.7444637], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8779977983763698}
episode index:697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.        , 2.        , 0.62947506], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4722441512861904
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9548273, 1.4993197, 1.1076124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.501360055794466}
episode index:698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.        , 13.        ,  0.36613292], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4722152521431007
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9364676, 4.436033 , 4.218232 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.410829125520905}
episode index:699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.672258 ,  8.504305 ,  3.9808702], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.904998562485439}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.47184455278750914
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1214192, 3.1676202, 4.6127725], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8944276173821555}
episode index:700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 2.       , 1.5942317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4725837189033615
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9531336, 3.9994507, 1.683637 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1939991937663446}
episode index:701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  7.       ,  5.4883947], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4724267221782292
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9518738, 3.4906096, 3.7728171], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1572667795802771}
episode index:702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.     , 7.     , 5.65498], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.47310746659888603
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9026256, 1.9038823, 4.7917733], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.195781891090589}
episode index:703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.112381 ,  3.3390212,  3.8114936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.118685350918604}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47243543894746715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.20147  , 12.87452  ,  1.8829929], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.1607098902207}
episode index:704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 1.4047025], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.47200478180935823
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0740461, 2.5549233, 5.1721125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9767123115850522}
episode index:705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.475628 ,  5.2947154,  2.6315882], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.68491270235017}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.4717911788245926
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.33098 , 3.572142, 4.899688], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4487420852274842}
episode index:706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  6.        ,  0.84861267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4715515936150373
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9721847, 3.6400142, 5.4238024], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6406183209189675}
episode index:707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 11.      ,  6.226625], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4718891665439317
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2157326, 3.4353733, 3.572709 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2913386885037104}
episode index:708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  0.8982254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47122359649238876
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.0218164, 11.305401 ,  3.4509213], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.537733523310209}
episode index:709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.002796 ,  2.8942766,  2.5462728], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.003354872931236}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47055990128606145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.6072958, 12.488695 ,  1.5155886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.508109364013674}
episode index:710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  5.       ,  0.9763053], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4705467677644469
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.92483  , 4.0562406, 3.6947465], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0589120136329284}
episode index:711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.        , 12.        ,  0.32534644], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.470533671134747
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00406  , 3.5270867, 2.6720262], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1340002164945808}
episode index:712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 6.       , 1.8424606], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.470964649659662
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1053843, 3.5703359, 5.6077147], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5799903965727239}
episode index:713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4337072, 1.3944476, 0.6975693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.663099693018769}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.471705595528486
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4337072, 1.3944476, 0.6975693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.663099693018769}
episode index:714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.048794 , 11.560917 ,  3.1902301], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.089399988814858}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.472189793209701
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1401563, 4.863719 , 3.285915 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8689816051644403}
episode index:715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  0.       ,  1.5703423], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.47205717708360295
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.920613 , 2.3740852, 4.648824 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0200304269421188}
episode index:716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  1.      ,  4.159744], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.47184229826454516
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1879044, 3.8042445, 4.2721987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8259039247425238}
episode index:717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.345632 , 8.030092 , 5.0498257], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.2069716363101834}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4725365276541489
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.801441 , 4.5476465, 4.52827  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7428474866599153}
episode index:718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.        , 8.        , 0.15059537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4718793141247272
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.988224 , 7.9603214, 1.1918702], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.775835368697826}
episode index:719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.980569 ,  3.9999056,  1.6034262], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.030531714005129}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.4714402869513411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.013883, 3.496386, 5.317335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.49658015733066346}
episode index:720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.3470116, 11.890398 ,  1.6511284], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.897167756038453}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.47120584055566517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3238442, 2.982621 , 4.8282275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6762458995707048}
episode index:721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8244562, 4.3819466, 3.5897477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6091936972516965}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4719382424385521
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8244562, 4.3819466, 3.5897477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6091936972516965}
episode index:722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  0.       ,  0.7087139], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.4715569883190285
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0787706, 2.826772 , 4.53143  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9290231237810098}
episode index:723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  9.        ,  0.59375536], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407864998735}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.47112741815713877
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7251787, 4.454913 , 4.063775 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.256770669348079}
episode index:724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 12.        ,  0.10508173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.4707710019986172
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9567976, 3.2053456, 4.696096 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0632206148415442}
episode index:725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 11.       ,  2.4513621], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4708392795752725
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3506653, 3.6403003, 6.1190815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.769262358015047}
episode index:726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 3.       , 2.0882812], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4713866063059378
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3613595, 1.5348378, 4.96872  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1981453347647}
episode index:727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.8577757, 10.358256 ,  3.696005 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.893720178056025}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.47181832544673624
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0580354, 4.650358 , 4.6558967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.900257757719352}
episode index:728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.     ,  0.     ,  5.30594], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.47158178507069454
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5897486, 4.4257636, 5.234088 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.00539542612563}
episode index:729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 9.      , 2.955217], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.471417439072732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2698383, 1.4044249, 3.6998205], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.039203097680779}
episode index:730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 11.       ,  1.2336736], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4717062723913881
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.74789  , 3.8163629, 5.341207 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1071529980571149}
episode index:731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 0.       , 2.6162848], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.47190515745771455
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0118752, 2.5126672, 1.6651623], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.48747748416502823}
episode index:732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 13.        ,  0.21651858], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.47184783666107905
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8648725, 3.4200726, 5.137736 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44127135095764536}
episode index:733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 12.      ,  2.158378], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4719967720527099
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4300966, 4.872384 , 3.1719146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3560557020469566}
episode index:734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 13.      ,  2.369253], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4715797187515018
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1377506, 3.4326453, 6.051219 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9647051141801679}
episode index:735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  0.       ,  3.5174544], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.47134168460564263
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3663967, 4.9972878, 4.150527 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0953786155900707}
episode index:736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.0381155,  3.6114013,  3.2892196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.058771627003674}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4714153003490889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9293728 , 1.51959   , 0.40529507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8269801138899118}
episode index:737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.312849  , 8.508783  , 0.61670923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.37846984565168}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4719187256784044
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6629484, 4.118327 , 4.0800843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3000600505849655}
episode index:738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      ,  8.      ,  5.686011], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4717906037856373
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.067609 , 3.6891549, 4.1243887], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6924632930847378}
episode index:739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.        , 8.        , 0.93011844], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.47146773062234315
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4008198, 4.783619 , 4.70219  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.395552933621102}
episode index:740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 2.      , 5.819131], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.47215414394134136
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9901276, 1.8015237, 6.010682 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.198516975902929}
episode index:741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 1.6577568], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.472262674040022
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1564832, 4.313751 , 3.9239538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5612376201931233}
episode index:742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  5.      ,  0.612085], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421555}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.47189660991191623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0759498, 3.395772 , 5.2644854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9643331383236424}
episode index:743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.20102921, 10.010129  ,  4.7458434 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.70639320670973}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.47249018603768467
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.181561 , 4.305812 , 4.8414717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7610312467020788}
episode index:744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 7.       , 0.9597521], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.47238309991999355
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2178922, 3.635487 , 4.492967 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6718041575421946}
episode index:745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 11.       ,  2.6324048], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4728244504422711
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7314796, 3.4748955, 2.965942 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3544997622644093}
episode index:746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 1.6390141], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.47334283585619213
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.774537 , 3.6440675, 4.236719 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8878042401661352}
episode index:747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.90298   ,  4.615355  ,  0.20814124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.990270467254755}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4733391511044936
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7814047, 4.765353 , 4.966063 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.778835523923843}
episode index:748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 10.       ,  4.7769184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4739391585054595
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.248921, 1.490588, 3.288006], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5297994511065638}
episode index:749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.0665412, 11.231199 ,  3.9292572], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.231468220369937}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.47398722062245163
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2115612, 4.5800877, 5.5277047], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7658745016006}
episode index:750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.57228  , 11.599422 ,  4.5504513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.06726096586375}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4741377526614863
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.312724 , 3.9105797, 3.2738547], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9627833231387996}
episode index:751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9228764, 3.9985123, 1.4180473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.16667492664645}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47483703756486195
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9228764, 3.9985123, 1.4180473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.16667492664645}
episode index:752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 1.        , 0.31972092], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.4747603892811546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7018108, 1.0779127, 2.9048996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.945080052600566}
episode index:753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 11.      ,  1.408392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473745}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47413073359245284
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.601655 , 7.5208097, 5.7642913], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.780107151301757}
episode index:754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 13.       ,  3.0840096], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4747249242690561
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.34791  , 4.188657 , 4.0785217], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2385258666091348}
episode index:755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.200655,  8.166684,  4.543104], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.111360861088489}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.47431365845614476
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7774155, 3.2761898, 5.104091 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8250185993491908}
episode index:756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  1.       ,  3.2609391], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.4739767015352866
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.125545 , 2.0137887, 3.104046 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3180607844744667}
episode index:757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.      , 7.      , 6.030555], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4740108180634712
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1556065, 4.9191523, 4.488398 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9254502802744846}
episode index:758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.    ,  9.    ,  5.9118], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4742415055605791
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7986805, 3.171752 , 4.5143604], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8169390139221483}
episode index:759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  2.2964747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.47468293498513264
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8031845, 4.04685  , 4.458438 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.085034705968223}
episode index:760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.        , 10.        ,  0.09844282], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4750609373233218
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4948517, 1.9942319, 5.4214396], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8102598629926665}
episode index:761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  4.      ,  5.511101], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.474637864875753
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1328397, 1.6879184, 4.734868 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2820704901740143}
episode index:762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.39653  , 10.195359 ,  2.2309685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.583966720374998}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4744907274615783
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.151664 , 3.4240131, 4.5447965], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2272394239490774}
episode index:763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        , 12.        ,  0.22495621], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357855}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.4742240566920108
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.392235 , 1.3937639, 2.304053 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1256321775323137}
episode index:764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 13.      ,  1.982265], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4744612268408763
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.539867 , 4.252002 , 3.8364215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3338781280140013}
episode index:765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 0.       , 4.2540293], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.47438636999112743
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8336227 , 3.390017  , 0.61081016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2298574330595926}
episode index:766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 11.       ,  2.6574023], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.31370849898476}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4742955559137966
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.097862 , 3.7278886, 2.2161016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3172403918615134}
episode index:767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 10.       ,  0.4336064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.4739692448696102
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2034087 , 2.3642716 , 0.79830176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6674770807469123}
episode index:768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  7.       ,  5.7468224], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4735680658719445
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.465963 , 3.745112 , 5.1609654], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9167264830957155}
episode index:769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  8.       ,  2.8987217], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4740700013119047
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3450036, 1.0492743, 3.734146 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9809992794153002}
episode index:770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  6.      ,  5.987378], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.47372554276511203
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.930958 , 3.6237946, 4.608775 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.029216018716865}
episode index:771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  3.8087568], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4731119086423593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.29853  , 7.5763817, 2.713768 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.586108321154687}
episode index:772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.        , 9.        , 0.09562099], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.47280722642521306
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4084165, 4.037615 , 5.113025 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8999427692597546}
episode index:773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  9.      ,  2.904079], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.47302661192348117
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2926636, 3.085677 , 4.785495 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7094847771911916}
episode index:774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.4335132, 11.605353 ,  5.7063007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.616266014651544}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4730054288729269
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3653803, 4.529465 , 4.5770607], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0502505022722772}
episode index:775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.     , 10.     ,  4.83245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4726091107888957
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2902718, 1.7248063, 3.6157415], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3078136886825165}
episode index:776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 1.        , 0.19147176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.47240197046799515
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.521682 , 2.6034021, 4.213623 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5725156027565552}
episode index:777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 13.       ,  4.3187976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.620499351813308}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.472164414733932
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.697026 , 4.669275 , 3.9949417], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8089565074527927}
episode index:778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 11.       ,  5.4775143], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.47233494317090713
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1415155, 3.8631282, 4.5697145], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8746524447353695}
episode index:779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 10.        ,  0.94041485], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4724523409238321
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3180625, 4.696044 , 4.856114 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8280054256331726}
episode index:780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.418418,  8.913573,  1.770544], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.487005499867195}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.47202420836578296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.919548  , 1.8572766 , 0.09355259], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.233938566812402}
episode index:781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 2.1204188], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47142059684613363
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.307209  , 2.0975692 , 0.01386929], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.4281208799938083}
episode index:782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.040658 ,  7.5987763,  3.2848473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.043711189865425}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4719970005467489
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.883837, 4.767219, 4.051838], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9759127118454507}
episode index:783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.144942 , 1.8080033, 1.6184094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6528000056034635}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4726704737603373
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.144942 , 1.8080033, 1.6184094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6528000056034635}
episode index:784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 5.0136175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.4723620219699655
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4668474, 4.8053794, 3.286017 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.326163385753107}
episode index:785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 8.       , 6.1377516], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4722556885044493
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.815047 , 1.5286729, 2.6360228], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4829062475740458}
episode index:786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.142809, 11.257813,  3.642791], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.521755995713047}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.47254051382788664
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4886556, 4.0986886, 5.8959684], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2118538584066667}
episode index:787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 1.        , 0.25980988], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.47298928046495226
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.000002 , 1.0028218, 2.9057221], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2335452905418713}
episode index:788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 3.4521563], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.47317217128938494
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.03148  , 3.1442816, 3.7189136], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0415218857962871}
episode index:789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 1.8954291], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.4728621184904558
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5139008, 4.792676 , 4.4527736], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8648809957765198}
episode index:790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 2.       , 1.6027927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4735158958374969
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.936018 , 3.9989762, 1.6374369], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1785589657810283}
episode index:791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  8.       ,  5.1526804], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.47351819905235576
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.852665 , 4.2832937, 4.444509 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.253710316332447}
episode index:792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 5.       , 1.8972766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.47388241660001623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7956315, 4.132459 , 4.671487 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6531688280261614}
episode index:793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.847853 , 4.0057955, 4.3551335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.017238060368271}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47454503320379454
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.847853 , 4.0057955, 4.3551335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.017238060368271}
episode index:794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  7.       ,  3.6153195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.47438159565680554
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7941136, 3.9332826, 4.919402 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.022340272080244}
episode index:795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  0.      ,  2.913878], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.4741257824204427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.764018, 4.635351, 2.693854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8050199664918571}
episode index:796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  5.       ,  4.5859385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4738032217105962
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4848971, 4.423245 , 4.87967  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5035795389174123}
episode index:797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 4.      , 2.505451], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4740394107071794
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3049027, 3.6564703, 0.839814 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8177756192538304}
episode index:798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 5.       , 1.2483726], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.47443937925903984
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6712673, 2.0369658, 5.4433427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6410257294812052}
episode index:799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 0.7541421], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.47413449980786415
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8047376, 1.483622 , 3.735131 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.357218713781649}
episode index:800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 11.      ,  5.849654], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.4737293898275842
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4275861, 3.7049212, 1.939641 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7231945631697325}
episode index:801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  2.2042062], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4736640591945757
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6788932, 2.688924 , 4.973135 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.357236672863922}
episode index:802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 0.       , 4.2288656], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.47399536220104366
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5474286, 1.2317252, 0.4127986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8510736864948958}
episode index:803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.        , 8.        , 0.04706129], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.47432584107067954
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1454844, 4.1413093, 3.7951148], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6170100337957882}
episode index:804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  6.       ,  2.3180656], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.47443711976556363
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.035791 , 4.6696706, 4.7120214], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.670054143361854}
episode index:805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.0150309, 10.259357 ,  4.1960263], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.259373013448688}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.47505233301647487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8903642, 4.630706 , 5.081883 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6343872033280824}
episode index:806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.841267, 10.630135,  2.100112], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.805050323478692}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4753353508132321
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9983754, 1.3804749, 4.9421053], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.572229727474227}
episode index:807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  1.461199], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.47512132740340013
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7646677, 2.1878846, 4.7363086], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.478369792154735}
episode index:808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  0.5342674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.47513958320760386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1655507, 4.237531 , 4.8986187], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6999976143910107}
episode index:809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  8.       ,  6.0459576], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4748645475037542
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.41588  , 3.5502148, 4.567722 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5190302960354956}
episode index:810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  1.      ,  0.721264], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.47454934740740734
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3101556, 4.0963383, 3.532864 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.139365666390572}
episode index:811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 1.0669177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4743685264837995
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.732855 , 4.4903727, 4.581737 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.285606389218947}
episode index:812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 4.        , 0.09180471], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4747910829550337
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7959203, 1.0461807, 2.6630588], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2950420032315675}
episode index:813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  3.       ,  3.3490417], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.4748157114275708
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.059121 , 1.7256014, 4.1638207], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6570543971909752}
episode index:814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.        , 13.        ,  0.42579645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.4747053805901056
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.612748 , 2.7622833, 5.346418 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.407471937373801}
episode index:815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.5323925, 12.944568 ,  1.7854784], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.1032876849492}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.474465597144021
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1513562, 1.6313088, 3.5715418], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.377034612030341}
episode index:816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.250789, 12.030329,  4.014143], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.116540510556849}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4748759548809577
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9128426, 2.7490275, 5.6229973], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1157501264612328}
episode index:817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 7.      , 1.452028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.475305408021489
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.309257 , 2.9074416, 4.851963 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3125246764604794}
episode index:818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0226827, 6.0001287, 4.9669847], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0002144916895555}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4759338507467375
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.526391 , 4.0645986, 4.4331255], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.18762688183289}
episode index:819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4186616, 6.2244873, 2.9149814], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5227715079877804}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.47636096632367514
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2627232, 4.3311996, 4.2929707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.188657842457428}
episode index:820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 10.       ,  5.6982985], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.47671867908796445
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6952236, 4.706376 , 4.279034 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1480597860619333}
episode index:821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 13.       ,  2.3241248], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.47702070056834145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.442213 , 3.8388686, 5.333932 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7692938459963738}
episode index:822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 1.9372878], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.47729582449839153
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3946062 , 3.8886247 , 0.30077142], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8349231452739514}
episode index:823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        ,  6.        ,  0.19610947], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4767165819929323
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.6202706, 6.4056487, 1.6592367], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.154702768545373}
episode index:824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  2.        ,  0.38810855], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47613874371172876
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.765432 , 11.83282  ,  4.2329006], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.255610274690891}
episode index:825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.9432726 , 0.4729611 , 0.37715116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.8792755967115173}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4766246786745462
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9330506, 3.8950138, 2.4071708], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2929165485178868}
episode index:826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.16643  ,  7.201236 ,  3.8491185], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.000304112797995}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4764567581451924
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4267848, 3.4859362, 4.8712626], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7514716726475364}
episode index:827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 10.      ,  2.427945], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.47677468521674204
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4503334, 3.8576248, 5.019816 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7711541292708182}
episode index:828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 8.       , 5.8535304], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4767078089042427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7652096, 3.7822433, 3.6531086], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4617154122827833}
episode index:829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  6.       ,  3.6633136], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47613346214652674
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.722347  , 8.936584  , 0.91673493], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.007060779780894}
episode index:830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.     ,  1.     ,  4.74397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.4757556479802987
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0129056, 4.4420376, 4.9203825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.747520457140583}
episode index:831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.8237667, 11.179101 ,  5.960566 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.220479584505805}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.47543694942046266
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3547697, 4.815377 , 4.10495  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2651700742145033}
episode index:832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.987164  ,  4.7737627 ,  0.08450859], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.099183597804196}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.47522559700976863
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0479652, 3.4250622, 4.677442 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9977782254589298}
episode index:833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 13.       ,  1.7115474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.19803902718557}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4746557821452485
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.1157017, 13.392912 ,  1.7734362], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.43046506714311}
episode index:834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  2.       ,  4.8051214], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.47450842097688056
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5036755, 4.5667634, 4.7197556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1715863893177882}
episode index:835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.        , 10.        ,  0.11274785], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.474643024279465
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2243636, 4.8818264, 4.7437224], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.58730658220181}
episode index:836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.978528 ,  3.2922783,  0.6862595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.982093314228466}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47407594778689693
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.5355535, 6.5062876, 3.2947507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.732826327679524}
episode index:837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 13.       ,  2.4100037], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.47411879957503855
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7005658, 4.453504 , 3.838944 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.484026602809782}
episode index:838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.311938 , 11.927412 ,  4.3442206], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.914206638808878}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.47469862938484186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5640197, 4.5391326, 5.330285 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.194330582360581}
episode index:839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.118416, 12.32203 ,  3.758455], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.09326189848491}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.474804827671827
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4622297, 4.945259 , 2.9959185], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.433546531958471}
episode index:840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  7.      ,  4.782856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.4746979207172749
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.513953, 3.071216, 5.194804], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5156272802447708}
episode index:841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  5.5080996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.4743118678489689
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.597073 , 4.003459 , 4.8431306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1676584169479713}
episode index:842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  3.2260664], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.474779761022065
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2598398, 1.6235257, 5.644931 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.56285589653878}
episode index:843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  7.      ,  5.664056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.47491276697101686
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1544194, 4.695206 , 3.6616008], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7022248091408738}
episode index:844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 1.      , 3.041082], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.47507395569594213
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8550606, 3.467048 , 2.8733654], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.912951519117181}
episode index:845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.     , 13.     ,  2.20099], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.2065556157337}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.47536935330854485
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7118797, 3.1624207, 3.821619 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.719567594098992}
episode index:846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 13.       ,  3.2598124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.475783520097833
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3278098, 4.178923 , 3.7753828], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.22364987577463}
episode index:847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 13.       ,  5.0995755], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733704}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.47615832052654267
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6444287, 4.58806  , 3.9945192], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0879434190051778}
episode index:848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.173365 ,  8.814495 ,  3.0323257], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.490706421601336}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4761862106435894
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.437795  , 4.6375065 , 0.70189166], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2631641160266263}
episode index:849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 1.      , 4.639696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4766077724680676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6623843, 2.8871267, 1.3844267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3423696295086551}
episode index:850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 11.       ,  4.0315933], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.47671035462786115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.778292 , 4.6627045, 5.041831 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.063287844518788}
episode index:851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  0.5559069], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.47715019901338585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.837385 , 3.6259637, 4.3148804], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9410859922926325}
episode index:852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.6136286, 11.18161  ,  6.048791 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.2982389419006}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.47714250433879296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.015679 , 4.2695494, 3.736686 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6258410730902881}
episode index:853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.        , 8.        , 0.70262563], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.47739110689327807
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7305218, 4.5582485, 2.2656379], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.009903817294733}
episode index:854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  3.0141988], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.47778937102275637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.757515 , 3.2492704, 5.593745 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.775104099484677}
episode index:855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 7.      , 6.055679], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.47771849661727794
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9848657, 3.2998567, 4.633345 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0295020141553863}
episode index:856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 12.        ,  0.26869696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991377}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.47795751190128394
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1055188, 4.9152617, 4.258085 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6939351333913226}
episode index:857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.142399 ,  4.741161 ,  2.6529849], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.26660061399631}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.47740045186410296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3721256, 8.477926 , 5.2124195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.647159000745783}
episode index:858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  8.      ,  1.819887], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.4771444380746965
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9492035, 2.8472624, 1.3188018], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9551785161325415}
episode index:859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.256655 ,  8.019821 ,  3.9427261], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.175242482148441}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4774760639773388
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3991718, 4.0456133, 4.220823 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2059442941859544}
episode index:860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 6.      , 3.324994], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4780371788972258
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.983336 , 4.2584343, 5.323922 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.61779560595557}
episode index:861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.010935 , 7.2088532, 2.2638779], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.174956442800269}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.47784781926182407
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3922915, 2.0132675, 4.3276615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7064925260678185}
episode index:862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.355272 ,  4.1067686,  3.9396515], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.42051253540557}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4777537636270155
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8174813, 3.4574435, 4.529638 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9367658061689571}
episode index:863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7647324, 2.4270682, 4.612999 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9555450762742496}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4783582152894842
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7647324, 2.4270682, 4.612999 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9555450762742496}
episode index:864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  6.       ,  1.3867898], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.47829721171503237
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0213652, 4.035642 , 4.0865107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0358625039461558}
episode index:865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.     , 11.     ,  1.37026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4777449054659388
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.292442 , 12.910338 ,  2.7413833], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.580224067118603}
episode index:866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 4.      , 2.674445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.47748203597458067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.102928, 4.967243, 3.674066], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.255326210621984}
episode index:867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 11.       ,  3.7730067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.47786480767071743
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4503207, 3.096788 , 4.482323 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5581354794541705}
episode index:868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  4.9661474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4782010313049343
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6118602, 4.042038 , 4.8342233], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7357348139840607}
episode index:869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.018894 , 4.2742634, 2.8224168], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6315307377852477}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4788008002344688
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.018894 , 4.2742634, 2.8224168], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6315307377852477}
episode index:870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.       , 2.6155632], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4790834403443694
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0299335, 3.6839948, 4.2780104], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1869616471598163}
episode index:871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  0.6656289], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.47916150594304646
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5574074, 4.465678 , 2.6351128], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5680929871457294}
episode index:872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 12.        ,  0.08526237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4794599467992266
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.362317 , 1.149114 , 5.5795264], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9576563635274773}
episode index:873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.259566 , 12.014662 ,  3.3272455], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.423659750293636}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.47997780194923545
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8104038, 3.6066146, 3.6213257], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.012292259251741}
episode index:874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  4.      ,  3.460227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4802746277451658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0481815, 3.3290405, 4.1125827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.098613761274737}
episode index:875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  6.      ,  4.088081], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4805134113731611
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.611235, 4.837986, 3.492633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4442322292283194}
episode index:876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 11.       ,  3.4691982], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.48103902909041063
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6247666, 4.8951807, 4.0839467], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9319704985713313}
episode index:877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9248527 , 4.4569144 , 0.01459616], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.810674265610909}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4816301008112644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9248527 , 4.4569144 , 0.01459616], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.810674265610909}
episode index:878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.9457517, 11.237741 ,  5.0245295], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.291853275823494}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4821749994565303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6048434, 3.453714 , 4.4704623], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7561030415893668}
episode index:879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 12.      ,  5.056641], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.48251096122919235
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.995665 , 3.7627811, 5.0105333], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1364723629318716}
episode index:880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.222683 ,  3.9171398,  2.805998 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.25445169848812}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.48215490129202043
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9729183, 4.4689574, 5.0950646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7924097717764214}
episode index:881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  4.1577005], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.48251711862563385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2390323, 4.8740425, 5.718995 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8892251740805681}
episode index:882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.     ,  1.     ,  2.62757], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.4821717113868265
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2383304, 1.0136358, 3.307929 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3407488131855048}
episode index:883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.182279, 12.165799,  3.843287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.167611442780625}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4826700971142486
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0088058, 3.9961677, 5.592196 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4052814821289223}
episode index:884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6072507, 7.564643 , 3.1525633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.772390962635481}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.48260559996879593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6683154, 3.3175201, 4.254224 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6982624501574786}
episode index:885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        , 11.        ,  0.77941144], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473749}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4820608983886957
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4500635, 9.701665 , 3.7889707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.724190877139265}
episode index:886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.973007 ,  9.283816 ,  4.2141914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.787587557198057}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.48252682550918047
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7050328, 3.9498854, 4.8892584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9946295024029117}
episode index:887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.      , 13.      ,  2.504956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4822278593731035
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7635782, 3.247848 , 5.02903  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2610184667726234}
episode index:888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.149043 , 11.809937 ,  2.4697707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.209720026018996}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4822147646399343
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.744898 , 3.3295999, 2.8091578], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.81456075081613}
episode index:889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.        , 13.        ,  0.33584112], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.48192177253049484
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.67613  , 1.2849755, 2.864252 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3980659089697443}
episode index:890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        ,  6.        ,  0.03189629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.4817036601139251
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1836338, 4.6122046, 5.5324655], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.428660057174771}
episode index:891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.818056 ,  2.6133847,  2.080634 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.825665223464616}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.48171283793106673
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5312902, 3.132464 , 5.1967683], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4746712337530132}
episode index:892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.  , 0.  , 5.35], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4814922275563098
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.866916 , 1.9955673, 1.7507732], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.119967223083074}
episode index:893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 1.       , 4.5165477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4812359282671969
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.447942 , 1.9820921, 1.5077748], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8560766107772524}
episode index:894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 5.       , 1.8084531], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997902}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.48170871725796965
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9930768, 4.8337317, 5.74871  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.085371262457197}
episode index:895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.277395 , 11.461084 ,  5.4884834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.491884750602948}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4822218550170579
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6888657, 4.285466 , 4.9664392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.458409831313007}
episode index:896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  9.      ,  5.805764], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.48217324369798636
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3683043, 4.2195187, 4.8192015], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0370706364158373}
episode index:897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 11.       ,  5.5244455], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4820439108445067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5494428, 1.5697917, 3.9716992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5321172309642537}
episode index:898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 2.       , 5.9475017], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4824548272630277
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4554834, 3.2706175, 2.011299 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4804275927530457}
episode index:899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  0.       ,  3.3208857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4826546352782732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6520474, 2.5689502, 4.878503 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4151961609568044}
episode index:900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.337824 ,  8.887718 ,  3.6758285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.207082465674528}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.4823598408958032
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6807113, 2.361502 , 2.984302 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9332993168939662}
episode index:901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 9.      , 5.398717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.482234973708169
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8124142, 4.4116464, 5.926176 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4240554437426762}
episode index:902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 13.       ,  5.4492774], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.48208256751729345
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.081352 , 4.104453 , 4.3550687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5456840798788662}
episode index:903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 2.       , 3.0822377], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.48234324004503787
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6740494, 3.9906325, 1.5477623], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6551428551446772}
episode index:904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  8.       ,  4.0818577], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48181026408918703
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.635249 , 11.113478 ,  4.3511686], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.184764467590316}
episode index:905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 10.       ,  1.9113712], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4820786637270112
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7438004, 1.0120244, 4.2457848], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1225659031783595}
episode index:906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  7.       ,  3.7988913], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4815471547262096
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.3505065 , -0.45095968,  4.6513214 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.511547305567163}
episode index:907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 12.       ,  6.2535954], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.48173168718616793
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.318457 , 2.5572662, 3.7167797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3908063780009323}
episode index:908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  0.      ,  4.047196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.481383756392415
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.86296  , 3.456396 , 4.8668065], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2252172899673779}
episode index:909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5122795, 5.0667205, 5.4252253], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.560922272118305}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4817718113430278
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.537798 , 1.1799675, 3.4333527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3827170804846625}
episode index:910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  7.      ,  5.704709], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.48167404978102424
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8389132, 4.589448 , 3.5186133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.79725352321408}
episode index:911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  0.       ,  3.7579136], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.48155953508490856
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.560438 , 4.536978 , 3.2431865], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.635968006643634}
episode index:912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  2.       ,  3.0264022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.48136330825093715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.377383 , 4.576023 , 5.2607093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0930918244416485}
episode index:913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  8.       ,  6.1450977], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48083665255263197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.1596557, 11.771587 ,  1.9924569], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.323313209171703}
episode index:914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  8.       ,  0.0492079], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4811195637229442
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6074586, 4.428831 , 4.5237956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5525991929641043}
episode index:915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  9.      ,  6.186889], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.48079607573192407
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8725538, 4.4144483, 3.4863565], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8088114220263722}
episode index:916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  9.       ,  3.6099217], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4813088281574073
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.580478 , 4.8715606, 4.8939137], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9595136219073581}
episode index:917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.73646  ,  5.5503116,  1.6655154], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.101088821148759}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.48143045954983
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.970072 , 4.5709786, 4.3860416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5197533133401806}
episode index:918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0857193, 3.221211 , 4.304552 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9270196433243216}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4819947354371533
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0857193, 3.221211 , 4.304552 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9270196433243216}
episode index:919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  2.       ,  3.1046395], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.48232482609516425
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.854898 , 1.6240702, 3.7976172], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.309508476911793}
episode index:920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 0.       , 6.0001106], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.48246466584916825
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9426804, 3.552947 , 1.915921 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.019840936976122}
episode index:921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  3.2366571], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.48296251344521146
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.101434 , 4.6946774, 5.339402 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6977102771855275}
episode index:922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  2.4706044], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.48265624531239704
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0859395, 2.1397038, 5.868857 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0985083113266927}
episode index:923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 9.       , 1.4235718], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4825462787245097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7208965, 3.8802524, 3.8554468], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1377767610582694}
episode index:924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.9745035,  2.6816673,  0.5882856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.978734080544458}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.48240092395236694
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.042914 , 3.5935545, 4.2653446], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1261974592263353}
episode index:925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.2267966, 11.074933 ,  3.227049 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.167592898732616}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.48279027845499284
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6720006, 1.5186757, 4.432574 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9894481669737236}
episode index:926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.     , 13.     ,  5.17796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865699}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.48292871099121487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2754354, 4.864609 , 4.1363883], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.259093053956672}
episode index:927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 2.5983007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4828488526543952
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2834355 , 1.9873363 , 0.72145617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.993008183753969}
episode index:928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  1.       ,  4.8621473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.4825557951662815
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6547006, 4.772798 , 3.6253638], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2254535139730853}
episode index:929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.647605 , 12.96871  ,  1.7895988], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.585837959775528}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.48239646297458644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8221788, 2.6791131, 3.7819688], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8825794110014199}
episode index:930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  9.       ,  5.9923267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4824206191784666
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2133965, 3.4573426, 4.2949786], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2967241271780283}
episode index:931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 7.        , 0.09669893], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4826353552040374
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4921722, 3.0165217, 4.927571 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5080964464669923}
episode index:932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 2.5967171], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4830491927791338
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.129909 , 3.2354174, 4.632461 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9013765125825217}
episode index:933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.219535 , 9.088987 , 2.5566535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.4081199711326144}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.48316052852769725
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.030102  , 4.4837737 , 0.18160075], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8062929127755816}
episode index:934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 1.4849658], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4834114055373984
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2573407, 1.25303  , 5.54439  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4675424866608937}
episode index:935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.602187 , -0.4304266,  4.7846866], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.14334775310816}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.4833145034250271
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.328131 , 3.010898 , 5.5502677], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.671904558947626}
episode index:936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 9.      , 3.210389], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.48354193001480783
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3657396, 4.5249486, 3.512188 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5681943969530465}
episode index:937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8490183, 1.6356165, 2.3244405], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3727118002349983}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4840925249721481
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8490183, 1.6356165, 2.3244405], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3727118002349983}
episode index:938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 2.7813547], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.48383775689340774
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4155188, 4.304358 , 2.8362513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3689432592520834}
episode index:939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.966915 ,  6.9997263,  1.4631417], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.56977149561709}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.48360534039644426
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9445453, 2.3291156, 2.0095103], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.057022640183516}
episode index:940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      , 13.      ,  2.032676], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.48334137789787246
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0466188, 4.8143063, 4.3304515], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6659717251632675}
episode index:941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.        , 8.        , 0.36638495], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.48298397042837593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0685377, 4.4169507, 4.6710157], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7746893068824823}
episode index:942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  2.       ,  4.1677413], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.48293227091232444
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.133497, 3.393665, 4.003991], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9517350686883234}
episode index:943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.        , 12.        ,  0.02877444], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.48298877086390796
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.862644 , 4.26793  , 4.0483136], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2532398812161643}
episode index:944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.        , 13.        ,  0.20511407], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4831051493565403
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9414268, 3.1906228, 5.1649685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9605318253548315}
episode index:945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 0.      , 1.704374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.48340032648655146
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3026319 , 4.979108  , 0.10274196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.098378001766348}
episode index:946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 9.       , 1.2262208], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4837279019428948
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2506065, 4.6550803, 4.73212  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.074441508110961}
episode index:947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  6.        ,  0.28229672], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.48401375783452
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7491466, 4.963226 , 2.862897 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3278509362510436}
episode index:948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  6.       ,  3.4078968], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4843233548856945
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4765925, 4.8464375, 4.3202195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9069535186390307}
episode index:949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 5.      , 4.651175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639887}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.48479466224676954
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.779271 , 1.191125 , 3.9279761], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5372887847171772}
episode index:950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.7999336, 4.8330593, 0.722827 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.863633801205598}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.48530518205513257
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7813182, 4.560818 , 0.1455037], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.576063188434616}
episode index:951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.8250821, 12.18203  ,  5.963905 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.256894836376919}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.48500790773273095
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0259995, 4.687978 , 5.819884 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9488319339123734}
episode index:952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 2.8430684], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4852830013178294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.725156 , 1.0118481, 0.5034169], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.007059340300058}
episode index:953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.170031 , 5.007241 , 4.5668917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.014429468305734}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.48581205477556755
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.880062 , 3.0283728, 4.384603 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.1232481758160835}
episode index:954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.00296  , 12.89123  ,  3.0905313], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.11931826538667}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4862891941416675
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0028617, 4.535888 , 3.5059857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.535890861097283}
episode index:955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.34494054,  9.02997   ,  4.270776  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.895590437171907}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.48619545838045447
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1937687, 3.912047 , 5.6643705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.217307853382607}
episode index:956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 9.       , 0.7077427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.48592831142114207
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5684888, 4.9349594, 2.5618184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.016741798447597}
episode index:957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 10.       ,  2.5408213], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4863842784180177
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7694988, 2.4974403, 3.8144057], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9190727150357576}
episode index:958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  8.      ,  5.860584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4858770998169561
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5103705, 6.5727663, 3.0238402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.8708726576097976}
episode index:959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.06325352, 10.232939  ,  3.8168716 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.806400135540169}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.48617310611486053
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0775318, 4.512947 , 5.565019 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4464040546228056}
episode index:960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 10.        ,  0.76705533], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4862842334200495
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0498066, 2.161824 , 5.335777 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1226854087814795}
episode index:961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.0000305, 11.011066 ,  3.6489835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.813188354457209}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4867379345229683
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.791665 , 2.9385943, 5.0142093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7927170451064214}
episode index:962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  0.       ,  3.7177465], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.48642248133894556
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.268578 , 4.292821 , 5.3376184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3204242229622598}
episode index:963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.097223 ,  9.61599  ,  2.8639402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.673396921221292}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.486766344882782
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3438048, 3.2668872, 4.9358463], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.43523618477883935}
episode index:964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  1.5048777], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4866190348708283
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.868634 , 3.342205 , 3.5828843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.181987036024378}
episode index:965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1580386, 2.220784 , 3.4221528], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3957904896006295}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4871504851452892
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1580386, 2.220784 , 3.4221528], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3957904896006295}
episode index:966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.000006,  9.004877,  4.148227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.532305245105666}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4872980271784411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9391437, 4.6226377, 3.3122125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9386514494866935}
episode index:967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  5.       ,  0.4669549], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48679462012557084
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.273696 ,  7.0838685,  5.9509006], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.22670182443358}
episode index:968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 13.       ,  2.3639326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.48649893633478847
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5903277, 3.707328 , 4.947304 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9213033456110626}
episode index:969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.6805525, 11.025677 ,  3.8656306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.832607153683757}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.486967989131764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1375546, 4.844172 , 4.352145 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.621006117353425}
episode index:970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      ,  9.      ,  4.707796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.48733459593325945
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.494997 , 4.63769  , 4.2192836], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2174410611956166}
episode index:971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.339019 ,  9.887619 ,  2.6170447], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.064814603869385}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4875716287899107
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6513057, 1.2707243, 5.3513565], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1930277789838097}
episode index:972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 12.       ,  5.0389757], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48707052742424795
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.0124025, 2.5728242, 1.7267009], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0425397548647357}
episode index:973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  7.      ,  3.613705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48657045501416146
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.755688, 12.586393,  1.734884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.745839040548526}
episode index:974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.8660813, 3.8027487, 1.1476991], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.279915452981145}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4869624297400638
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7987878, 3.6295042, 4.697691 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3561659981446357}
episode index:975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      ,  7.      ,  5.966932], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.944271909999161}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4867020834626129
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8820167, 4.1290936, 4.2464204], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4327616166248949}
episode index:976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 11.       ,  2.2941206], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.48690960342413436
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2316718, 3.1963954, 4.3944497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7930317517942826}
episode index:977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 10.        ,  0.49887738], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.48673688303599016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6985483, 3.804164 , 1.6732136], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8792940205964104}
episode index:978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.962764 , 9.000347 , 4.8472724], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.077094728341944}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4872110946466786
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8914776, 3.151114 , 4.848624 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8975043849766302}
episode index:979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 0.      , 4.565265], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4875156528570465
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3486173, 1.9693822, 1.0610864], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0879830653565856}
episode index:980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 6.1890306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4878779643152798
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.665615 , 4.9718485, 1.1553085], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5811741629980727}
episode index:981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.632287 ,  7.459233 ,  2.0420353], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.716024728160585}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4881812231508113
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5808854, 4.324435 , 4.4608126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4462214730653313}
episode index:982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.153758 , 10.769019 ,  2.6223333], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.26239000016562}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4884601259902785
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.020279 , 4.1842194, 5.166454 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1843929788969292}
episode index:983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 0.9403636], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4887949296606108
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5402412, 3.9313188, 5.1409883], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.731545629119894}
episode index:984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  5.2168655], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260022}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.48879106817900014
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1901332 , 2.0756073 , 0.02467554], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0322695803587716}
episode index:985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.104271 ,  9.788202 ,  2.2384303], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.82600406020704}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.48865193850189975
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1139134, 4.9268084, 5.637044 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6962776275110287}
episode index:986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  3.5855892], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.48905491006544
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.984377 , 3.727265 , 6.0181956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7274326692931056}
episode index:987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.403638 , 10.795164 ,  4.0707173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.214458193749955}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4895033011968586
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.197169 , 3.3019595, 2.8213372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2346630102765075}
episode index:988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  4.5494003], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.48992279439585956
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2799885, 4.1790743, 5.0143747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3815327343442396}
episode index:989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  9.       ,  4.9832487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276355}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.48989380366153856
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.810871, 4.47488 , 4.28328 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3354926434579095}
episode index:990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  8.       ,  4.6584477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.48974720263195837
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6574225, 4.2711873, 4.0516424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.088771564859177}
episode index:991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.     , 7.     , 2.75448], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4897940988240705
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0582643, 4.8313255, 5.684496 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8322521453079599}
episode index:992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  7.      ,  4.641953], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.48989799846916354
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8144836, 3.051845 , 4.446711 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.815224173718093}
episode index:993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.153986 , 12.769567 ,  2.3411956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.38805025276741}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4899724523846396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.059594 , 1.797384 , 2.6928012], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2040916166761755}
episode index:994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  2.       ,  0.9911046], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4894800177591274
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.731029 , 9.034084 , 5.6884274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.094416905688257}
episode index:995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.9955845,  6.729494 ,  2.0116787], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.668684804536994}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4889885719581644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.34221292, 5.110777  , 4.0831122 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.3939963632548826}
episode index:996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  0.       ,  2.5775516], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48849811200635085
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.787884 ,  7.19141  ,  1.8855083], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.370407424225366}
episode index:997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  2.      ,  6.181922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.48846158448957744
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.073006 , 3.7518902, 5.041944 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3102217564738257}
episode index:998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 10.       ,  3.6701446], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4884251301009657
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7011144, 3.7705927, 3.6121998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.510270456950551}
episode index:999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 5.4073715], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.48887818512026576
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3708768, 2.4562612, 0.8000215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8315334886899196}
episode index:1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 6.       , 0.8200934], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.48905141574550426
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1417257, 4.357909 , 4.756194 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3015431622027034}
episode index:1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 8.      , 5.712526], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4894130987348749
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1281464, 4.465162 , 4.2695513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4707551134362675}
episode index:1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 10.       ,  5.2375665], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.48924538983739724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.619289 , 4.2080054, 5.6233673], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0202410068265433}
episode index:1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 5.       , 1.6322557], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.48931975218860735
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2745316, 4.194181 , 4.1478467], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2253308855768743}
episode index:1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.     , 7.     , 5.80762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4897886837884197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8356094, 3.1728883, 4.8451185], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8533073810727005}
episode index:1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 12.        ,  0.73901695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4897790940993372
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.748277 , 4.5284977, 3.4088197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.322235631937647}
episode index:1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 10.      ,  1.976777], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.552945357246852}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.4896214515911164
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8316638, 3.8938704, 3.8890269], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2209295489821579}
episode index:1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  0.2327016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.48929025942613186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.156229, 4.843198, 5.019534], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6070806744132136}
episode index:1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  8.       ,  1.3890667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4893989469851408
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7364714, 3.1760433, 5.5601964], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7572195045664498}
episode index:1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 11.       ,  0.4740556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4889143935722842
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.730998 , 6.614185 , 3.3306122], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.775446162591435}
episode index:1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 13.       ,  1.5306728], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4886658061946542
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6156931, 1.8487687, 4.70485  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8004552466371464}
episode index:1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.        , 10.        ,  0.56150275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4887345812266238
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7845778, 2.4429612, 3.1860762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9622133880649816}
episode index:1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 1.      , 5.395843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4890355483563543
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7523456, 3.4902987, 3.5233145], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8196449738621046}
episode index:1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 10.       ,  1.9298898], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.48921966423476054
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2738433, 2.969798 , 4.2212605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2742012709697148}
episode index:1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0869976, 5.5834565, 2.3650482], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.214626828405311}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4894963376156181
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1834482, 3.5857835, 4.932783 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.908665173858349}
episode index:1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  7.       ,  5.2287273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.4893338860331849
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.803783 , 4.0816126, 4.362046 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.347572930417777}
episode index:1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.      , 10.      ,  3.642876], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.48976005201980705
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.449606 , 3.7814474, 4.8047028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6468204036657046}
episode index:1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  9.       ,  6.1408987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727816}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4892789517722434
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.536265 , 13.33881  ,  1.1078622], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.065253243480964}
episode index:1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  0.7303921], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.4890540317620427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1961226, 1.5269032, 4.475843 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8975573001316441}
episode index:1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 4.       , 0.5918171], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4894523496272355
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9162083, 2.319109 , 3.623822 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.033584698895262}
episode index:1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 13.       ,  3.3838856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.48946746572827354
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0561967, 1.3934497, 5.764673 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5217801829717605}
episode index:1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0749438, 8.22679  , 3.9204936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.2273276872812735}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.48981333238938474
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6600412, 1.6449386, 5.722405 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9056969802853434}
episode index:1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 1.3519968], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4902009878530472
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1815915, 3.9316344, 4.8624463], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.240054498465914}
episode index:1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  1.8507576], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.49026745772677266
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3967211, 4.381282 , 4.971569 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1162331414109836}
episode index:1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.21181  ,  4.408926 ,  5.9742374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.276917567733756}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48978914801191725
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.83653  , 7.283936 , 5.2194033], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.067852665261505}
episode index:1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.140276 ,  7.2641835,  4.0818443], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.92849569145618}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.4896614278848899
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.206994 , 4.4250727, 3.2495039], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.867529589496603}
episode index:1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 9.      , 6.150225], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.49009220093262323
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.751077 , 3.1806664, 4.8906336], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7725004400215415}
episode index:1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 7.       , 5.8686247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4901423885542519
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8598728 , 2.4434052 , 0.02294129], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2687347306176464}
episode index:1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 13.       ,  0.8032487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.49045296725169313
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9965491, 4.3060107, 3.3418512], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6469904852256811}
episode index:1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 10.      ,  5.707101], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.49023186008664316
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0467563, 4.779773 , 5.409686 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.064773947146597}
episode index:1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  8.       ,  4.6800885], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.083045973594572}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.49009740552453973
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.370524 , 3.3086782, 4.002801 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7010865307563852}
episode index:1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.      , 13.      ,  5.119629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.48998804432434495
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2863817, 2.54505  , 5.3879247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7729825697548622}
episode index:1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 6.      , 6.067458], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4898680484840244
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9976876, 4.368873 , 4.1851344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3688750725267826}
episode index:1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.685741 ,  3.5075557,  2.015222 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.696758758469528}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.48939428828239573
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.21107  ,  7.3068433,  4.0078917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.009870563494902}
episode index:1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.672387, 10.027015,  4.38918 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.43860992563354}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.48977785406349117
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4656143, 4.4007564, 3.9193196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4992286129142214}
episode index:1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.1862073, 11.991313 ,  1.7307217], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.918066375471737}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.48972004630235255
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9167068, 4.4442334, 5.356906 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4466333057637437}
episode index:1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.000174 , 10.973656 ,  2.6831713], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.586132774823488}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.48971080947522533
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.527606 , 3.8483524, 5.527562 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7473642929937168}
episode index:1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 9.      , 0.505245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.4894611225858643
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8173478, 3.9385357, 4.9716196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2445507666948767}
episode index:1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  1.       ,  3.1275692], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4896739157568811
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2503066, 4.0391293, 3.2062712], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6257478964810994}
episode index:1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 13.       ,  4.5568643], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.49001359775459935
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0438766, 4.656639 , 4.2773685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9580938590553942}
episode index:1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  6.      ,  5.482272], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4903285769475318
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6317663, 3.2677813, 4.162831 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4553052224145176}
episode index:1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.286386 , 10.031274 ,  2.5023787], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.867520282642122}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4904328259201984
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.799132 , 3.6793523, 4.031079 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.923121164068166}
episode index:1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 7.      , 3.323328], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.49083846774336565
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5816412, 3.0850785, 4.077168 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4209081746763645}
episode index:1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.311376 ,  1.8777103,  1.6557906], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.362423050622224}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.49036831595433944
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.366382 , 13.305845 ,  2.2691042], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.84172380792996}
episode index:1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 11.       ,  0.5146767], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4898990639773496
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.185799 , 2.5733485, 5.836108 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.2142413612458363}
episode index:1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  2.9148722], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.48965110676352663
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5294847, 3.1169338, 4.7585263], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5422431356225759}
episode index:1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 11.       ,  5.8015146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4897060308663225
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1236467, 2.1315646, 4.502858 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0675786768471323}
episode index:1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 1.6788931], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.49018341060786225
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7842274, 1.9883264, 1.793516 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0344279766711708}
episode index:1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  3.8837435], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4905278094262483
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1084156, 3.819888 , 4.769266 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2112552401698375}
episode index:1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.041222 ,  7.4039626,  2.4275072], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.764750197371502}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.49029151308017055
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.25858  , 3.8190513, 5.093431 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5016221613308622}
episode index:1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  8.       ,  0.7694665], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.49060323089607644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0846386, 4.2611938, 4.034656 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.293298713720056}
episode index:1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  9.      ,  6.236896], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.4904391489882087
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.395257 , 4.8906507, 4.6520925], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9315248764860797}
episode index:1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  4.       ,  0.3950509], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4902410687855042
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.478359 , 3.6326919, 3.8933132], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.793174827846523}
episode index:1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 7.       , 0.9933611], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.49060850707223425
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8616064, 3.4831653, 5.7274537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9878330785922104}
episode index:1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.5773926, 8.594231 , 3.686194 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.640267633475559}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4910631900039193
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3872523, 3.930798 , 5.195596 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0081415498547046}
episode index:1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 10.       ,  5.8747773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.49139640970408976
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.786599 , 3.4799256, 4.3979297], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8499365309404319}
episode index:1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 7.       , 2.7703478], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.49161053848639263
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4815946, 3.3421974, 4.4852147], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5205990664049365}
episode index:1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 7.      , 0.629745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4916682633811715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.021022 , 2.1949418, 3.5239625], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3002324311522484}
episode index:1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        , 10.        ,  0.43827438], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.4913525740692986
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3062267, 4.203679 , 4.713055 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3893036704828279}
episode index:1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 10.      ,  4.860688], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4915329533343374
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1336273, 1.9585581, 4.8347025], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.137275915618526}
episode index:1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 0.6367667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4918802911301027
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.801705 , 4.3940725, 6.1147237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.608157003108998}
episode index:1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.        , 12.        ,  0.94559854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4918343096710594
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1903887, 3.7494578, 4.192872 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4066670737813334}
episode index:1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.875906 ,  2.6935248,  6.28085  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.879859852827222}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4919350826689852
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2593603, 3.3599901, 5.239329 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8234925815052012}
episode index:1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 5.       , 5.4354296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.49219646242757187
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3645407, 4.4697104, 4.568886 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.198812326592149}
episode index:1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 12.        ,  0.25227797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4923197759791809
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2135515, 4.1840334, 4.032827 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6954770339429346}
episode index:1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 5.8900423], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4927590595007764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.955812 , 4.908733 , 5.3196216], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.73284864937478}
episode index:1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.822655 , 10.992122 ,  2.6048353], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.66327584160274}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4927898349722149
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5626034, 3.2390966, 5.4626875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4571466366233479}
episode index:1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        ,  0.        ,  0.72433263], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.492562351097202
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.904276 , 3.915207 , 4.6641173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1127873455954567}
episode index:1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 10.       ,  1.3983839], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4923574182627071
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8499684, 1.8479426, 1.9034745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4316712653519967}
episode index:1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1526425, 4.0058336, 4.647088 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3151866921113644}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4928318505820878
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1526425, 4.0058336, 4.647088 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3151866921113644}
episode index:1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.629226 ,  6.8984404,  1.0823293], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.321584557242607}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4929480581361122
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9525585, 3.932283 , 3.1728168], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9334892317878336}
episode index:1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.305183 ,  4.875428 ,  2.0011911], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.427595203105222}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.49287732382808713
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.470162 , 2.734647 , 2.4403355], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.49391708321437}
episode index:1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  0.        ,  0.76605123], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.4927839722013674
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6931267, 4.0966907, 4.7519355], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7060622118672535}
episode index:1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 8.        , 0.52327645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.49265583145881964
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8621168, 4.085535 , 4.8836894], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.094256878723985}
episode index:1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.67058  ,  3.972684 ,  1.6788574], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.719373939876482}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.4924933507447362
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1861305, 4.098681 , 3.7453923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6167886379356835}
episode index:1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.1141024, 10.339044 ,  4.8089824], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.637472750825247}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.492937408039583
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.97981  , 4.366447 , 4.4034243], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.405582238259329}
episode index:1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 0.      , 5.935227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4930414643618653
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1864978, 2.9493484, 5.797125 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.814209409560932}
episode index:1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 2.        , 0.09560773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.49325662101455175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6684484, 3.972327 , 2.036461 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9310981351000496}
episode index:1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.0868435, 11.4170475,  3.673968 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.834768188696493}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.49333809440945775
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.743112  , 3.6939893 , 0.41058272], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7400084568097255}
episode index:1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  4.       ,  1.1033416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.493646270825593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.495451 , 3.7967482, 4.5989504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7024909414355394}
episode index:1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.727491 ,  6.5429587,  2.0968657], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.419217715645216}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.4934072054772256
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9890516, 2.01897  , 5.2904453], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4087003027911436}
episode index:1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  0.9793794], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.49310721929525103
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8587642, 4.8492613, 4.1880274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1730592518521585}
episode index:1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  7.        ,  0.08886688], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4926519033032886
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.6427345, 10.802483 ,  1.4464507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.237886900433145}
episode index:1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.787281  ,  9.897568  ,  0.16320226], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.52897095997008}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.49263593295153807
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6967623, 4.088836 , 4.2998486], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2926878937517907}
episode index:1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  7.       ,  3.6845007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.49273392380270364
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4002554, 1.4408977, 1.5948684], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.60965971266014}
episode index:1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 1.      , 3.079911], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.49279945903902933
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3370502, 2.6140742, 0.6960316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7071440298798597}
episode index:1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.323632 ,  7.0907755,  2.1828227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.104585842834194}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.492495292002205
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3808823, 4.8915277, 3.0840557], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9902722510055184}
episode index:1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  6.2211356], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.52417469626002}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.4922792895341551
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6319423, 4.4940524, 3.2839143], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5387199534697367}
episode index:1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.347944 , 10.872742 ,  4.4954495], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.644172537288748}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.4920148856518889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.246611 , 1.1239848, 2.776064 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0216398755241096}
episode index:1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  7.       ,  4.9002576], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.49170078154175284
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.447071 , 3.4943085, 2.3308315], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5291682584162556}
episode index:1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.150951, 11.994295,  1.680326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.326236823105377}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.49200735151635755
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.809983 , 4.843625 , 3.4230125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5836004036128526}
episode index:1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 9.       , 4.0237064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4922983043704829
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.203854 , 3.7764864, 4.531668 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1121058600416531}
episode index:1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  9.       ,  1.9132931], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.492650749675724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8088713, 4.808629 , 3.9436524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8186998601769393}
episode index:1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 11.       ,  1.6679064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735446}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.4924910954838989
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.64367  , 2.2237   , 2.5751734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0084407728388234}
episode index:1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 11.       ,  0.6679287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.49253602149347236
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2452407, 1.4302495, 5.311745 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3544207404380284}
episode index:1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.6582694, 10.483158 ,  1.6559552], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.329501207456502}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4923570530317075
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.137568 , 1.5942342, 1.1553731], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3334160304641953}
episode index:1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 2.       , 4.4727535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.49275788101245077
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0709069, 1.4721744, 2.3166485], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5294701143869998}
episode index:1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 6.       , 1.9481333], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.49265614079122316
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0305057, 2.1482682, 5.2724824], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.145776080259541}
episode index:1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.411622  ,  5.957184  ,  0.56886727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.823437989358725}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4925912436860035
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.113081 , 4.885035 , 3.9663036], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.083262452345797}
episode index:1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  2.0807881], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.49296559898720604
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.739979 , 3.743118 , 4.4913845], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8920228919000985}
episode index:1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 7.        , 0.11425163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4930618663872778
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4799528, 3.6098628, 3.1476877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7760717377536798}
episode index:1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0013766, 3.0741925, 3.6278687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.07420529440725558}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49352188284246173
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0013766, 3.0741925, 3.6278687], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.07420529440725558}
episode index:1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 8.      , 4.391086], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4937796339544805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7828887, 4.5189195, 4.4773936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9464008801289894}
episode index:1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  9.       ,  5.4234343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.493541188469303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3112696, 3.5429058, 4.500266 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7738536978797856}
episode index:1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  7.        ,  0.81061167], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.4933346953120889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5996041, 2.7726498, 3.9259346], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6412591066102911}
episode index:1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.042948 , 10.243854 ,  3.7076383], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.826365998081336}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.49323664954679175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.44425  , 3.3174329, 5.3317666], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6400168570741452}
episode index:1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.080156, 11.224081,  4.707998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.227600397711239}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4932755217921941
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.33042 , 4.302295, 4.845028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4643463258499512}
episode index:1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 3.0752301], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4936143938959638
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0334605, 3.2877035, 5.4205475], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9874735030981425}
episode index:1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.179149, 10.176212,  4.202673], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.150773311088457}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.4934708072981224
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2626805 , 4.1356897 , 0.32116905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0755891004864577}
episode index:1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  0.      ,  3.646747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.49336609183832547
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3267746, 3.7429261, 5.117767 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5206150244538688}
episode index:1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 12.        ,  0.11492508], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.49356804002982857
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7441535, 4.7428265, 5.5400696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8950484180870395}
episode index:1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.663456 ,  2.4878345,  2.6434474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.674695651318745}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4934072856531659
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6933293, 4.353011 , 3.9888444], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.387330536898655}
episode index:1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.        , 10.        ,  0.93476164], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.49377653523928955
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4694705, 3.6722665, 5.405953 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.615947331359666}
episode index:1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  5.957535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.764823060233402}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4940385654058676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7020698, 4.7431474, 4.0927973], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8792191760708723}
episode index:1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  5.7006426], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.49404376582245363
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0805341, 2.4382348, 5.120925 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9999823680072815}
episode index:1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 3.      , 4.145957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4942841770666514
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5598621 , 4.2000866 , 0.44241583], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2782523893159037}
episode index:1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1746638, 5.182597 , 3.1466522], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.333433143860413}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.49433151141340526
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.181512, 2.825855, 5.146018], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.836808753056403}
episode index:1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.        , 8.        , 0.40461773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.49450603518304365
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0046202, 2.419639 , 5.651868 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0780662478471923}
episode index:1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.        , 9.        , 0.70942295], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.49484831846080585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3175354, 3.4785457, 3.8088675], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4017508642700918}
episode index:1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 5.       , 6.1470513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.49529041817646585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.981496 , 4.7285724, 5.3891993], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6295034257093635}
episode index:1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.041151 , 10.755166 ,  1.9743674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.911549553764143}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4948485890790738
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.569097 , 8.418191 , 2.7148983], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.769918261473249}
episode index:1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 0.       , 5.6028023], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4949051109591708
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.083312 , 4.8403387, 2.1496773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.135512004343927}
episode index:1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.99999  ,  8.993702 ,  6.0140634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.229057615449516}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.4947504323871366
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.15962  , 3.1435332, 5.153597 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1684690358018563}
episode index:1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 4.0352063], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4949617784167338
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3525515, 4.4657764, 6.088714 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.602401383508095}
episode index:1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  0.      ,  4.253935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505136}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4947461329808872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9854677, 3.9930148, 5.372983 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4196317345124196}
episode index:1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 6.       , 3.1079273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.4948177214506883
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.516057  , 3.3657067 , 0.12689877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6065823857133861}
episode index:1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 6.       , 3.2769923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.49513417224895284
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6624293, 3.4720626, 5.5130873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4184281358865547}
episode index:1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.654128  ,  4.1242156 ,  0.37952366], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.71327707812633}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4949857578741202
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.759237 , 3.8422794, 5.122547 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.133964525419443}
episode index:1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 12.       ,  2.3528006], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4952025112979591
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0308986, 4.637194 , 4.2280464], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.902514726939089}
episode index:1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.752871, 8.436457, 4.536321], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.91521199421282}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.494764278987076
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.459816  ,  7.601368  ,  0.11061229], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.174283019701065}
episode index:1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 7.      , 2.908184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.49515092891538715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9200346, 4.497527 , 4.0519414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8463240902729767}
episode index:1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.476147 , 2.3494406, 1.2805054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6569113923183998}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49559690866016154
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.476147 , 2.3494406, 1.2805054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6569113923183998}
episode index:1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 1.4928744], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4960073226948834
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.177409 , 2.6166966, 1.2328955], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8624605391910447}
episode index:1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.831061 ,  8.377169 ,  3.6580734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.99568989045407}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4963515709744436
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2831485, 3.87119  , 4.2832093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1282057321816055}
episode index:1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.        , 9.        , 0.95020425], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.49591425681499474
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.322876 ,  7.7483096,  2.9040527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.582103563166935}
episode index:1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.     ,  6.     ,  5.46241], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4954777125748407
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.5023867, 4.335594 , 2.6640701], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.832293073932932}
episode index:1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  1.       ,  2.8144183], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.49541250282073335
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1889877, 4.737592 , 4.0381484], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7478395483827218}
episode index:1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.      , 8.      , 6.262716], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.49568870261458264
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6907372, 4.04739  , 4.7577634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0920939652621033}
episode index:1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.679913 , 9.497545 , 3.5300834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.318762046563744}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.49609687408726516
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8708806, 4.4018345, 4.1979795], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.650325106627827}
episode index:1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0062876, 1.8415352, 2.9517832], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1584818521902653}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49653889437315357
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0062876, 1.8415352, 2.9517832], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1584818521902653}
episode index:1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  4.       ,  3.1120648], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4965417849389082
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.171363 , 3.9287057, 4.0013046], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9443831744494211}
episode index:1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.748141 ,  3.8548002,  0.6579848], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.78554760905315}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4963665176906248
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.752729 , 1.931872 , 4.5632634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0525486928045553}
episode index:1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  1.       ,  2.6192775], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421558}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.49632378514125747
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7183099, 4.344837 , 5.1420107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8577719530821188}
episode index:1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.        , 11.        ,  0.33768895], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4958899356787214
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.353721 , 12.909873 ,  1.4850285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.26356551042915}
episode index:1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.148962 , 10.242598 ,  3.2655466], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.500787001050831}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4954568440318404
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.48509938, 11.900347  ,  1.9560654 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.248832225156193}
episode index:1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.       , 5.6947393], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.49586272463041653
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3360147, 4.316317 , 4.573345 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8755335413630743}
episode index:1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  8.       ,  4.2644367], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.083045973594572}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4957904377485538
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2726116, 3.9976134, 5.8218365], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.234636146388939}
episode index:1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.000207 ,  5.9712057,  2.9035428], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.432267364701064}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4953585645449401
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.0184107, 2.1950686, 5.0422087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1729924634493245}
episode index:1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 3.       , 4.2782907], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4957634709378514
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6704106, 1.1157098, 4.490371 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3061564369290974}
episode index:1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  4.5599055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.495798692463303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.86027  , 2.9639995, 3.623698 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8610229662218402}
episode index:1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.       , 2.7846272], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.49581101831368196
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.491965 , 1.8160666, 4.7114525], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.904640978533948}
episode index:1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 0.5848271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.4956086759256061
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5542437, 2.2329483, 4.9686027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6366366827484398}
episode index:1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 2.8265858], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.49561672207726387
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4733759, 1.8332965, 5.4895782], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9214000265495608}
episode index:1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.      , 12.      ,  5.064003], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.49569594223312197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2681317, 4.9897757, 3.2029033], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.007760392577116}
episode index:1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.970772  ,  1.7486002 ,  0.78321594], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.048994548078522}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.49540029375655115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0602465, 3.7178183, 5.2549043], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7203420656350934}
episode index:1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 13.      ,  3.455932], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4957619866230971
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0181773, 3.3540576, 4.993473 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.35452385278523263}
episode index:1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.973363, 9.283604, 4.681126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.358546251967047}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4961721309734661
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8520377, 3.329546 , 4.012632 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9135473373292489}
episode index:1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  1.1061633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.496293040606532
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1207294, 4.187222 , 3.7468045], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4773668444364478}
episode index:1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.715525 ,  2.533011 ,  2.8667338], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.724828234917775}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4958648326336187
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.467923 , 8.260165 , 4.9891825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.587326394433698}
episode index:1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 3.9100776], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.49566612695871703
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2787685, 4.709122 , 3.5008838], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8550669662021704}
episode index:1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  2.       ,  2.8703928], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.4955576545303716
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3336558, 4.671177 , 4.174343 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.799123901557022}
episode index:1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.184686 ,  2.8394248,  2.7687695], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.185951469131933}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.49552413653830063
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2291863, 4.111887 , 4.2791286], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0909504596744073}
episode index:1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 12.       ,  1.5376967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.495479015166923
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8923306, 4.550727 , 4.7888513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7891360685615472}
episode index:1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.0528984, 11.543059 ,  3.2278059], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.607697607199535}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.495583663900407
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1783806, 3.2483397, 0.7834606], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.838469415175247}
episode index:1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7983654, 3.4012272, 4.08582  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.26685007633801}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4960166392962006
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7983654, 3.4012272, 4.08582  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.26685007633801}
episode index:1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  7.       ,  4.0267124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.49590516048142963
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.82445 , 4.725478, 4.651108], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.087867976650539}
episode index:1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        , 12.        ,  0.22955966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4958675756397715
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.22873  , 4.9527617, 5.010394 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.30717488948416}
episode index:1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  5.       ,  1.3696274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.4955724432419234
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.332038 , 1.0193484, 2.7999055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.386902985812467}
episode index:1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.2589235, 2.0168312, 5.1181517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4636063983014003}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4957502663828624
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5269747, 1.4513986, 1.4586861], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.635808232371619}
episode index:1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  1.       ,  5.7113857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.495984704741343
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8764138, 4.4693737, 3.837567 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.710894580847782}
episode index:1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.932119 ,  1.7695066,  1.4225398], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.990528297932991}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4962737987265768
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3150742, 4.439431 , 4.161325 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2160634209044727}
episode index:1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      , 10.      ,  5.415953], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4958503569187896
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.8116856, 10.295544 ,  5.0174866], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.517124585003763}
episode index:1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.898211 ,  9.669149 ,  1.9560212], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.639830182703339}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.49612491495858374
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.099653 , 3.2155714, 5.03002  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9257946534375704}
episode index:1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  2.      ,  3.469441], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.49570232133425784
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.167112  , 6.140982  , 0.17844743], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.4605343282129}
episode index:1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.     ,  6.     ,  5.69572], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4954973914954171
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4172096, 3.4941335, 4.246392 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.500883410019861}
episode index:1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  0.       ,  2.0347962], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4954202100168311
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9377097, 3.4218082, 4.749236 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1429710837280913}
episode index:1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 13.       ,  0.4963662], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4956469920935773
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4346566, 4.387514 , 3.8644388], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9958544623761445}
episode index:1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  5.519137], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.49522623912915154
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.3475275, 11.1906395,  2.450974 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.781749626557518}
episode index:1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 3.9206824], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4954149620243755
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4180346, 4.6430726, 5.9500747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6954174924430687}
episode index:1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.170668, 6.378415, 5.489843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.3827231881548676}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4958341018870667
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.573602 , 4.9530115, 4.4492106], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.508082504903182}
episode index:1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  2.6993685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.49588696559296075
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9998817, 4.7925634, 4.2828655], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.052570871735741}
episode index:1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.        , 8.        , 0.99212646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.49580303229416983
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7116052, 3.3148856, 5.408031 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.326316001041113}
episode index:1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 10.       ,  3.3342087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4960683956381487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2022533, 2.7034488, 5.175773 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8510830520237381}
episode index:1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 8.      , 3.087576], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.49646073315027867
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5415845, 4.847371 , 4.0314984], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.903398214886838}
episode index:1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.286216 , 11.969008 ,  4.1792693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.952607970693457}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.49684430219394937
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4230976, 4.345202 , 3.8615074], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4101701761790075}
episode index:1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.274146 ,  9.989343 ,  3.7735538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.087820003593098}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.496940653743139
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5990858, 3.682581 , 4.6244707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7916116092569703}
episode index:1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.     , 5.     , 0.52717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.49729160285328267
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1740594, 4.183948 , 2.7258239], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4435757170995476}
episode index:1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.706356 ,  8.474718 ,  4.3430653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.284608590125595}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.49716897457357284
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2392136, 4.747346 , 3.743451 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4806423715806103}
episode index:1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 4.3393316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4973984734560216
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3422683, 2.4826717, 1.1555862], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7365778882355212}
episode index:1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 11.       ,  5.6532955], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.49745914801822433
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0681129, 4.1414332, 3.2460163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.243893446369421}
episode index:1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 2.3266833], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.49787270037085385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6281313, 2.455327 , 2.7792373], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4760394476861378}
episode index:1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  5.      ,  4.095112], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4977112795030329
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5179262, 4.6204963, 4.3486342], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.690681313565073}
episode index:1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.0025077, 7.8998766, 4.154631 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.3268363676617145}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.49811562880772436
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4833765, 4.836694 , 3.4961722], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3609002163074044}
episode index:1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.391084 ,  9.905051 ,  2.2201638], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.866923136727982}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4980695085002021
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2438116, 4.141617 , 5.041563 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6882998197167762}
episode index:1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 10.       ,  6.1083646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4981641091119449
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0114377, 4.3154593, 4.753949 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6455055615383811}
episode index:1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  7.       ,  1.3486787], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.49792016548987594
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.115915 , 3.7943907, 3.8832672], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8028031209971822}
episode index:1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.335135 , 13.1082535,  2.3440068], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.18756347443772}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.49800457304290546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9030547, 3.7414918, 3.8994179], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.042407238208983}
episode index:1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.      , 10.      ,  3.901389], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4975888764043054
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6939073, 5.229604 , 5.546321 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8000812074106185}
episode index:1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  2.       ,  4.2810326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.49744176397574863
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0613554, 4.6545753, 3.4253125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.902281092884165}
episode index:1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 5.8958507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.49766251476772483
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1242225, 4.6924047, 4.425346 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9055760355875644}
episode index:1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 3.       , 1.7621454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.49805605055892566
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9638191 , 4.752442  , 0.72517717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.035859355522945}
episode index:1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  6.      ,  4.124158], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4980923475850554
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.173024 , 3.3200517, 4.784656 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8867482600857274}
episode index:1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  1.0214891], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.498039262779741
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3326554, 4.3958044, 3.9641109], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5471324160951014}
episode index:1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.304251  ,  0.48377752,  5.1334715 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.580909283386903}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.49776030150667727
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6675382, 4.801771 , 5.9675646], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8321872716082308}
episode index:1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 1.        , 0.86987597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.49766344409306557
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.639556 , 2.9031992, 2.24624  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6424110468632653}
episode index:1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.      , 8.      , 4.768273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4975118236362562
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.823343 , 2.5291162, 4.0226693], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8831650388412586}
episode index:1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 4.       , 1.2175914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4976538782633758
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3831761, 4.074568 , 5.2365327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9413437374557536}
episode index:1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.763626, 11.427938,  4.337276], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.295106991885447}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4978604330780019
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.20213  , 2.8474145, 3.6433232], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2117749341521105}
episode index:1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.005927, 8.846143, 3.324072], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.381440534583362}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.49821957692814994
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3663063, 1.8824742, 4.3937273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7651223380381846}
episode index:1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 11.       ,  2.2604828], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.49808416972150676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4043207, 3.9028685, 3.8907363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6695173616553263}
episode index:1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  3.7454414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.49806935327794766
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3144246, 3.6618295, 5.4295344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8108513368017733}
episode index:1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  2.      ,  4.327087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.04536101718726}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.49779630425297816
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.273892 , 4.4918556, 2.8112173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.516789366373045}
episode index:1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.131876 , 12.648895 ,  1.4584758], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.50108366825492}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.49805346135435347
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3750114, 4.372328 , 5.01924  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9426631397560363}
episode index:1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.      , 10.      ,  5.598996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4976432031489545
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.571003 , 7.476184 , 1.6980469], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.146488383207169}
episode index:1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 11.       ,  6.0268393], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.31370849898476}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.49769309856903593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6611912, 3.8073084, 4.5879936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5633796230636914}
episode index:1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  2.       ,  5.7697163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.0356688476182}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.4974068718478472
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3316869, 3.9796097, 4.261433 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9346586186776955}
episode index:1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.5196441, 7.068687 , 5.0755644], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.7651210979120595}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.49780349726128365
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4544969, 3.6177063, 5.7717485], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8240963213191582}
episode index:1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 6.       , 1.0953604], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4981225295884223
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.974137 , 2.048658 , 4.5003386], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1914077720707668}
episode index:1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  2.       ,  2.7703881], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4979965982133272
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.466653 , 4.3130684, 4.9680653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.393525643932736}
episode index:1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.019672 , 6.720174 , 3.4822326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.247944006575092}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4983837313295458
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8782861, 3.7862334, 3.8793037], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7955986843850479}
episode index:1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 2.5008159], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.49856931413431915
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.775414 , 2.7089114, 6.016567 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3676566848523183}
episode index:1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  3.       ,  3.3979845], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4987978346296259
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5640767, 3.4545588, 4.421287 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.629803861583998}
episode index:1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 11.       ,  1.9045534], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4983899868498797
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.482296 , 13.159984 ,  1.7357496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.598075102019148}
episode index:1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.      , 13.      ,  2.877088], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.49834842902873094
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7558782, 2.8968213, 1.7782905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7628877510812992}
episode index:1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6064106, 5.191405 , 1.2913524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5969879544750953}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.49865079423994746
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4697378, 4.464761 , 5.8283978], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5382385102624396}
episode index:1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 10.      ,  4.636535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4982440643914646
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.610531, 8.468257, 5.400961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.579099733504448}
episode index:1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 4.6654153], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4984288535288456
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3492788, 2.476295 , 0.7357741], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7318046621500325}
episode index:1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  8.       ,  5.4051766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.49834599436996385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4597187, 4.998425 , 1.2609088], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.474768919218421}
episode index:1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.3204885, 10.118973 ,  4.3370113], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.151329202399537}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.49873000820692887
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7790575, 4.332066 , 4.2050815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8069589279889704}
episode index:1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.3094535, 10.122995 ,  4.9162755], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.12971420154695}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4991133976311508
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8803744, 4.6462355, 3.8693056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8668557387651328}
episode index:1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.      , 12.      ,  5.795507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.4990113268617139
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1610484, 1.5421193, 5.9671617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8637193067628075}
episode index:1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 0.6595703], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.49936283174892604
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9110513, 3.572733 , 1.622364 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9950288151331637}
episode index:1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.775628 ,  6.8434753,  1.3701183], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.386998006569439}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.49935117606240964
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2101812, 3.4619837, 5.5232005], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.915009617873436}
episode index:1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      , 10.      ,  5.367783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991383}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.49960269688263553
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0317342, 4.3306456, 3.735765 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6456476580838386}
episode index:1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 9.       , 5.0243793], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.49945306649907145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7738402, 4.2140036, 4.456379 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4396642972862836}
episode index:1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 7.       , 0.2432451], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4995191776217406
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.447583 , 4.067658 , 4.7101097], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7987191580851485}
episode index:1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 12.       ,  1.9683973], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4997765646548655
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5430713, 3.0206966, 5.5757365], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.45707572735233}
episode index:1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 12.       ,  1.2104328], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357855}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.49998249577162546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.55029 , 4.186145, 4.120081], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3075774384496923}
episode index:1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  9.      ,  5.883524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5001582407569577
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9824314, 3.1944082, 4.684196 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.001481911260033}
episode index:1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  8.       ,  1.9393115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.4999352814289107
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.250155 , 4.010804 , 4.8185887], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2585676973405264}
episode index:1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 10.       ,  5.9629307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5002395406888381
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.576195 , 4.7568107, 3.8112633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3602486404199152}
episode index:1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 2.      , 4.437506], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5003388046616258
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0909054, 1.403881 , 3.5107698], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5987056660016943}
episode index:1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 0.5118412], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336757}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5005877097811428
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4267046, 4.51612  , 5.7915707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5750227230493559}
episode index:1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  9.       ,  1.7553358], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5004708071323678
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9265966, 1.8236685, 4.0775948], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2573281693919243}
episode index:1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.130638 , 1.8011692, 2.603275 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4447523799682638}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5008640032712174
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4134932, 2.8265538, 2.3807113], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44839733187337577}
episode index:1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 6.      , 3.402783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5010497491511396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8609431, 2.5522966, 5.6623983], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2238827432390793}
episode index:1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 13.       ,  1.1180351], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5012531729988161
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0946562, 4.5652046, 4.082526 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.465806235132014}
episode index:1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  8.       ,  6.0196548], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5010289731704828
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3788157, 1.5010815, 5.3789454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.207939189194992}
episode index:1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.        , 12.        ,  0.09390835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5009651663241932
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7474103, 3.3801963, 4.8845086], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3090187984325}
episode index:1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  3.1520987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5009723827881335
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4370775, 3.5318537, 2.6133175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5323381276804229}
episode index:1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 10.       ,  1.3576524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449804}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.5006927509353477
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.77968  , 2.9145517, 6.22218  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2233079232448225}
episode index:1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.     , 11.     ,  5.96747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5006161049463246
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6935666, 1.5948015, 1.0204722], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.438222566303854}
episode index:1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 10.       ,  5.9133296], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.764823060233402}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5002165709439731
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.865261 ,  7.505743 ,  4.3652015], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.06443887100488}
episode index:1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 13.       ,  5.0426235], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5004195236682631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.661072, 4.560334, 4.433855], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2789915056311516}
episode index:1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 6.        , 0.36538327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5005328241291527
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1485777, 2.4297261, 5.1991944], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9372601434421934}
episode index:1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  1.       ,  1.2605494], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.5003142057624367
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3409317, 4.303402 , 3.319299 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4605573320584258}
episode index:1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  1.4376266], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.500417236331602
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9637836, 2.5815163, 1.3438299], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1175298735187578}
episode index:1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 1.6854545], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5007907512470776
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3051298, 1.9765869, 1.9188714], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.979888735306498}
episode index:1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.178957 , 10.173077 ,  3.4186432], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.45247741000926}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.500883284519274
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.922808 , 1.4923416, 4.5932083], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.443404386818581}
episode index:1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  5.4931145], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5011414474869853
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.61872  , 4.5431275, 4.6979556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5895335767115006}
episode index:1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  2.       ,  4.0600452], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5009767220897119
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2804201, 2.8905125, 4.4524274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.723062005011918}
episode index:1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 2.       , 1.3166842], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5012476939370131
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.679984 , 3.5025172, 0.9874966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7535307558005084}
episode index:1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        ,  1.        ,  0.07513478], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421558}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.501320313693517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.805654 , 4.6149454, 3.777925 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.422485340944953}
episode index:1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  8.       ,  5.1415224], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5010919951725799
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3986304, 3.248393 , 5.486644 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.620519598612791}
episode index:1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  4.5169363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.501071637897349
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7967778, 4.8099947, 3.652199 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1734360724188093}
episode index:1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 12.       ,  3.9213905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5012902395731007
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9601045, 1.7585273, 0.6985956], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5694123428463853}
episode index:1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.062135 ,  3.4946506,  2.6320736], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.074285820810692}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5010676247584152
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4093293 , 2.6044257 , 0.18032998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6391193802806918}
episode index:1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.106765, 12.644717,  3.240361], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.599214483511908}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5013856960914201
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3288622, 3.9229393, 4.64709  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9797792060713005}
episode index:1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.884069, 3.328968, 5.921231], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.34879790901414687}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5017786151646342
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.884069, 3.328968, 5.921231], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.34879790901414687}
episode index:1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  1.       ,  5.6583767], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5018320187766914
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6523232, 3.2932696, 3.190443 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6781475464483164}
episode index:1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.268754 , 3.9986076, 4.0326056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2377147951957896}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5022239684078662
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.268754 , 3.9986076, 4.0326056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2377147951957896}
episode index:1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 7.       , 0.4269386], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5025843237864764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1105046, 4.2021685, 4.176347 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2072366338308669}
episode index:1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  5.      ,  4.983312], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5026647807726123
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2814151, 3.239894 , 6.2107916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.735247282494971}
episode index:1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 9.       , 0.2282098], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5024167481948597
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1731615, 3.7296865, 5.776678 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9671759624078686}
episode index:1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.4167595,  7.7779727,  3.334747 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.559563552323995}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5022575039933832
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4013495, 4.84352  , 3.3876238], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8867029585419226}
episode index:1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.156826 ,  7.8135757,  1.6553943], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.95280021901063}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5025244990477645
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.062337 , 4.490155 , 4.8447967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4914585048179718}
episode index:1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  5.        ,  0.09480541], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.5022402007468463
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8656774, 4.3306036, 5.5352626], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.291562335723952}
episode index:1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 4.0420876], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5026064439387501
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0468127, 4.491441 , 4.073185 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4575061321923015}
episode index:1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  4.       ,  3.7893116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5024251677976815
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7819777, 2.2919006, 4.434969 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.917511165190668}
episode index:1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.616134 ,  8.556077 ,  3.6656609], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.876511700369862}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5026464591828452
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.83036  , 3.9207284, 4.473403 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.048891984455763}
episode index:1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  0.      ,  5.328374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.5023926543956289
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.767802 , 4.6964307, 3.5867243], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4501020654449177}
episode index:1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.        , 8.        , 0.37891978], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5027133444058379
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.903778 , 4.0161552, 4.03018  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1579950048659358}
episode index:1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        ,  3.        ,  0.17623012], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5028698014211097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6914177, 4.382623 , 3.7395957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4166403709707154}
episode index:1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.000734,  8.054196,  3.191177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.021617613148837}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5026911560547555
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1501011, 2.4874718, 4.685484 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9195862063891183}
episode index:1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 8.      , 5.960478], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.5024573886392488
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.613493, 4.627493, 5.97331 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6727584530629127}
episode index:1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  1.7410488], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5024753817176986
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2961832, 3.3424978, 4.3677034], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7378999831479411}
episode index:1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 1.7067139], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5025275385325856
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8159493, 4.7424564, 4.4795427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.106687077578429}
episode index:1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 13.       ,  1.5661948], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5023606566000034
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2068732, 4.9164014, 4.4577193], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.074040604297367}
episode index:1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 8.       , 2.5530093], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5026448964418723
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3213916, 4.2848268, 5.6664047], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.113884105396568}
episode index:1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  4.1291704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5025223904627294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3739694, 4.197234 , 4.1939497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.019243673993797}
episode index:1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 8.       , 2.2571754], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5025485297615244
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3334734, 2.9308639, 5.2160254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6701025713313731}
episode index:1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.094154 ,  6.606427 ,  2.4965544], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.958224874305515}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5025788212059558
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1885853, 3.0658348, 5.3032966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1904071516948556}
episode index:1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.897587 ,  4.668738 ,  2.9800925], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.99741424796615}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.5023738820981308
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5661817, 4.458614 , 4.3844757], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5646456790309695}
episode index:1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 12.       ,  2.8622305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5023349870194356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4026246, 3.898286 , 4.353703 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6656151156274241}
episode index:1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  2.      ,  3.221648], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5019470835545558
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.005405 , 12.531444 ,  1.3579271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.11280839136484}
episode index:1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 0.      , 3.465044], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5019862320064137
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8415358, 3.3237486, 1.2966661], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2028517929721554}
episode index:1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  2.8673437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.501846844837993
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4922695, 2.1764991, 4.51147  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7044125069884533}
episode index:1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 10.       ,  1.2278073], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5018264235723287
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.790804 , 3.6938243, 4.3753533], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0520280272248983}
episode index:1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      , 12.      ,  0.695874], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5019655522647367
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.723725 , 4.048981 , 4.6438537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0178178479677613}
episode index:1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  8.      ,  5.073928], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841895}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5018526255435369
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8155065, 2.7067363, 6.0890846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3464699832536688}
episode index:1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.633371 ,  5.4602485,  2.4373195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.870699059577968}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5019968195945174
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.678407 , 3.4808939, 4.6191278], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5785161904723047}
episode index:1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.07304   ,  0.31222498,  4.9946136 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.394575444770279}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5020576257347045
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.098218 , 4.792552 , 5.5876775], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.10221914870569}
episode index:1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  4.      ,  4.883714], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5018719218480145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0255946, 2.7034688, 4.159598 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9965488893545178}
episode index:1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  5.        ,  0.79804856], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.501759413330267
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5824254, 3.6609628, 5.6205525], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5640938977746444}
episode index:1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5862706, 4.912142 , 0.9068835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3780069726854296}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5021412068832706
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5862706, 4.912142 , 0.9068835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3780069726854296}
episode index:1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.590445 ,  8.581144 ,  3.5208464], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.24426229094788}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5024492014224172
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.619342 , 4.7901483, 3.6953294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4138970181928343}
episode index:1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      , 11.      ,  3.272298], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5023970071801597
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7099686, 4.391243 , 4.2003236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2044385978814525}
episode index:1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 10.      ,  4.256621], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5022137622107943
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6024975, 4.789827 , 4.5009503], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2707913990641617}
episode index:1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 6.0627656], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5025864025757975
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9516115, 4.5627217, 6.1003056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5634707059188657}
episode index:1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  0.      ,  5.990403], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5026080202402091
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2530879, 2.08255  , 6.153489 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9731741874710158}
episode index:1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.045786 , 11.425497 ,  3.4429424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36184627167275}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5025364810748257
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.546171 , 3.8521152, 4.5019236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0121279280230715}
episode index:1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 2.6490803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5028930073849821
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6453285, 2.471348 , 2.664806 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4541690758580297}
episode index:1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.046776 ,  9.569982 ,  3.3544154], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.607635576566997}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5030848019621479
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9881577, 4.8091393, 3.6696765], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.061417126476534}
episode index:1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  3.      ,  6.120854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5027019368160579
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.152552 ,  7.5005455,  1.2401004], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.312304131373272}
episode index:1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.655939 , 6.5189533, 4.6369605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.661282130303761}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5026997581796192
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.099079 , 4.7830505, 3.3564773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6062942691097497}
episode index:1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  7.        ,  0.76230675], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.5024460517954256
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0560392, 2.65271  , 4.816081 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9747389468524563}
episode index:1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 8.      , 2.643706], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5026095176122842
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8106074, 1.7988467, 5.776878 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6903916338436917}
episode index:1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 0.2367177], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5028873144978356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.966487, 3.211877, 4.22755 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9778682357341788}
episode index:1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  5.      ,  3.384507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.19803902718557}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5028926961746755
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.942667  , 4.8250823 , 0.94829994], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.109236465301488}
episode index:1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.    , 3.    , 3.7397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5029519944458447
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9617426, 2.6106858, 5.712745 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1088479936993327}
episode index:1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 9.      , 3.363725], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.502795623963599
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8752279, 4.230839 , 4.0882196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6673560823032638}
episode index:1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.099984 , 10.213951 ,  3.9245346], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.447269314024847}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5031419177503134
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.687958 , 3.611682 , 4.2556963], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.795370771017859}
episode index:1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  0.       ,  3.5698538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5027616139576071
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.486806 ,  6.5519514,  4.594996 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.200121382924618}
episode index:1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.257353  ,  0.98141813,  2.3806357 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.436899294423903}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5029693630100553
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7943766, 4.107737 , 5.398325 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.126659834010435}
episode index:1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 5.       , 4.9890556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5033369333021233
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5463011, 3.0760574, 4.6047554], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4600297828536392}
episode index:1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  2.       ,  3.8451562], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5029573428546857
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.748625 ,  6.1493826,  5.920382 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.081132061137788}
episode index:1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 12.       ,  0.6686108], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5031357475498881
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4771621, 4.681776 , 4.654242 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7481574193409228}
episode index:1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  3.      ,  5.133215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5031370654273257
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7252145, 3.6463938, 4.06907  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9714736010510517}
episode index:1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.36134  ,  3.8533566,  2.8835633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.396421266592789}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5027584822328732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.6003685, 11.652162 ,  2.2013454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.799147445314265}
episode index:1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 8.      , 2.765789], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5027600817866734
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1421726, 4.374435 , 4.9929266], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.381768601204908}
episode index:1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 12.       ,  6.1463175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5025817242870197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5100614, 4.966462 , 4.728861 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4671624209439726}
episode index:1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.00362  , 6.87972  , 2.4517598], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.575051856528095}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5023350336745492
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.876471 , 3.80255  , 3.3797834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1883972885237437}
episode index:1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 6.      , 3.697416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5027008738593396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3010675, 4.944714 , 3.1399884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5823021798335164}
episode index:1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  9.       ,  1.7925167], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5023240366225634
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.498156  , 13.281014  ,  0.17530155], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.33858712395725}
episode index:1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 9.      , 1.215409], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5023297757308982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.008687 , 2.5314307, 4.6029234], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0456990384389355}
episode index:1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 1.      , 4.710629], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5024910038423258
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8004787, 2.600359 , 1.705437 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8946949773592835}
episode index:1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 11.       ,  2.5150552], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056605}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5025411984561141
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4344316, 4.554272 , 5.3305335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1150309391081406}
episode index:1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 9.      , 3.289969], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5024364382314537
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6354713, 4.664215 , 5.182209 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7036704492982904}
episode index:1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 4.      , 3.808327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5025558150819036
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5806038, 2.7156453, 0.5038634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4475990648876407}
episode index:1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.87132  ,  4.6510854,  1.8380911], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.969238403108237}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.502488944825226
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.732687 , 1.5925817, 4.1206822], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.232270225813885}
episode index:1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.        , 10.        ,  0.01204816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5023121195492547
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2972164, 4.151417 , 3.4510212], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.734511754459061}
episode index:1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.631662 ,  2.4586477,  1.8715316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.648621712790563}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5022815009560123
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.694289 , 3.4664402, 3.9222908], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7573225029680715}
episode index:1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 10.       ,  0.5552966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449806}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5021912296359442
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9681923, 3.9214168, 3.8007498], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3833422910060107}
episode index:1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 12.       ,  0.3587252], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5019537038090641
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.294069 , 4.7523217, 3.7771013], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7768252637804915}
episode index:1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 10.       ,  6.0177426], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449806}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.501785895603778
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.491154 , 2.1947293, 5.1566916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7102857191294354}
episode index:1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.75674 ,  8.433377,  3.671551], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.790858650737341}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5017919876176307
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.626466 , 3.6426325, 5.1163445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5164339948529117}
episode index:1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  5.       ,  5.3411064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.501655534073608
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.201705 , 4.8686857, 5.5088162], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.879540163829733}
episode index:1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.033588 ,  6.751014 ,  2.7480197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.526242164391618}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5018432668281555
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4795113, 3.3165486, 4.419176 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5129958951085771}
episode index:1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  5.       ,  1.8466959], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5017653087403823
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.420171 , 3.2649322, 3.8388424], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.444670932355589}
episode index:1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.775493 ,  2.9206655,  0.5449071], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.775759913491104}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5017676202811576
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5937806, 4.7653484, 4.5007715], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.256968761964345}
episode index:1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 9.      , 0.820014], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5020457501129251
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4963391, 4.359913 , 5.267249 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.027402075180995}
episode index:1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 4.      , 4.797825], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5021792625721687
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5322697, 4.1715364, 1.4584588], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.877958953015142}
episode index:1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 10.       ,  5.9000125], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5021559125197027
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1917105, 2.1196127, 4.2961416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4816395169780607}
episode index:1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3977356, 2.9071646, 2.6987233], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4008151954763868}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5025235964838684
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3977356, 2.9071646, 2.6987233], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4008151954763868}
episode index:1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 4.2466745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5024544426668489
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.055546  , 2.3348825 , 0.48185647], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.667432884190065}
episode index:1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 3.2894843], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.5022272112186306
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8551884 , 4.751469  , 0.04216498], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5513463150096367}
episode index:1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  3.      ,  5.089966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.5020468555779123
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8019831, 4.8881483, 3.570969 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.236145899012168}
episode index:1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.523102 , 11.05769  ,  3.4300017], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.741684955505406}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5021008378271016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5593338, 4.46902  , 5.445545 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5336903649121094}
episode index:1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 9.      , 3.106813], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5023392983024573
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1064531, 2.948915 , 5.172171 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8942359110060192}
episode index:1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  0.4876357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5021690151856996
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.867652 , 4.7410336, 3.8435605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5532962231471577}
episode index:1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.267326 ,  8.99892  ,  2.5892863], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.764783180421505}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5022965390900893
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0438719, 4.7519145, 3.3363962], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9958420283536615}
episode index:1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  9.        ,  0.24038532], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5023995604285583
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6533365, 3.9231215, 5.2232985], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.130929631146837}
episode index:1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  2.       ,  0.5538966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.502208817277873
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.246993, 4.939313, 3.400875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.305629277059762}
episode index:1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 3.1252437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.502551991898637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3544917, 2.1367948, 2.9522803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.858176724115048}
episode index:1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  5.733639], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.492855684535904}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.5022849800460281
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2150602, 2.1185985, 5.5725546], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.990698028502575}
episode index:1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 12.       ,  3.2780876], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5023259618604511
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.170039, 4.234067, 4.655509], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7005625378761837}
episode index:1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 2.5024662], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5025813618672063
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2862102, 3.020585 , 4.4894247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7139134441785404}
episode index:1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.214668 , 7.0985622, 2.5755754], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.63256946041012}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5024981766012757
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0849423, 4.980508 , 5.7530923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.982328567050253}
episode index:1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.52637 ,  9.352188,  2.858663], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.943659347006273}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.502624714857272
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5504327, 4.920395 , 3.1289961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9977218774516232}
episode index:1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 10.       ,  0.5080239], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449804}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5024635732373327
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9193559, 3.330727 , 4.538926 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3404172303778589}
episode index:1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.      , 10.      ,  1.397096], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5023409716936803
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4985113, 4.3540854, 3.1233473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4429348301403015}
episode index:1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  3.      ,  4.106781], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.000000000000002}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5023776644819229
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6690035, 2.3893547, 4.5037208], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7772057600935909}
episode index:1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      ,  4.      ,  4.176142], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.502181369360631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3070297, 4.399975 , 4.0823803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4332469279654108}
episode index:1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        ,  8.        ,  0.04130232], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5023278514025807
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1484601, 3.5652504, 4.704014 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9358997463737464}
episode index:1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  6.       ,  3.2345617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5026007191637417
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2579618, 4.949818 , 4.5346656], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9668083336600435}
episode index:1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  5.8427067], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.502848061078146
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2768693, 4.2056084, 4.322606 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4058483383281117}
episode index:1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.548267 , 1.6243287, 4.118746 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.071135387098177}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5032091009756927
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.548267 , 1.6243287, 4.118746 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.071135387098177}
episode index:1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.6788363, 11.118729 ,  4.325526 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.29049131041613}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.503506857250372
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.71048  , 3.580563 , 5.5373964], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8063211331344822}
episode index:1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  7.        ,  0.35174173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.503288433153112
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2522504, 4.560712 , 3.8277612], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3431709414924624}
episode index:1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 11.       ,  3.7982655], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5036128546145229
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8932488, 3.253454 , 4.6671314], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.2750176982355328}
episode index:1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.008928 ,  8.188769 ,  2.7635002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.273951080370889}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5038900247934524
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0610936, 4.5916014, 4.0001497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8479015702006032}
episode index:1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.    ,  4.    ,  4.8013], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5037155114522488
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8068713, 3.2078354, 4.0830064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2110952378554372}
episode index:1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.398891, 9.198521, 1.484416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.22007829982304}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.5034796524611471
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.538837 , 2.117272 , 3.1288476], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0341925175684235}
episode index:1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.        , 6.        , 0.81769127], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5037693218416006
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1115541, 4.9448037, 5.2956862], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9480004662119594}
episode index:1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 12.      ,  3.910508], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5040142127235806
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9658713, 3.9013371, 5.064721 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3211041163047397}
episode index:1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 1.       , 5.3858275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.504303078425085
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7999544, 2.5999658, 1.8534799], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8438718085458057}
episode index:1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  2.        ,  0.14224929], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187263}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5042312895961399
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9598758, 3.8845425, 5.229119 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.365384077203177}
episode index:1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  5.       ,  4.9844007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.5039780103761686
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4066095, 3.3347178, 3.967507 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4458860111007996}
episode index:1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  7.      ,  4.324763], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.503615175235509
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1404848, 6.198991 , 0.2520358], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.2020740559849545}
episode index:1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.1828885, 11.387308 ,  4.928672 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.666714472606978}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5039234127698051
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.762563 , 1.8786037, 4.5605216], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0890569172755757}
episode index:1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 5.588636], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5041610765718758
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6585479, 2.8504312, 2.7443128], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6753192820821418}
episode index:1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.        , 13.        ,  0.33393487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5042843294256749
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.274395 , 3.363505 , 4.4445467], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8115654222367541}
episode index:1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2980266, 2.9496257, 4.3313584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.30225386437067947}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5046401913571712
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2980266, 2.9496257, 4.3313584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.30225386437067947}
episode index:1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  8.       ,  1.7314532], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5045156525458109
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8201339 , 3.234153  , 0.11656349], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2028763854567737}
episode index:1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  9.       ,  6.0333886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727816}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5044137574671833
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8272159, 4.424483 , 4.959536 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8451487292954105}
episode index:1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.4418244, 11.049413 ,  5.434527 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.754381800267746}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5047002677233019
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3917146, 4.814896 , 2.455208 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8566874224851702}
episode index:1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 10.       ,  3.9734676], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.504942388643603
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2537897, 4.3565316, 4.867167 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.211205231559374}
episode index:1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 1.2158989], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5052893540308393
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6949885, 2.8753643, 1.4275676], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7060758152438582}
episode index:1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 8.       , 0.3413903], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5052611849251487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.620438 , 2.1594553, 4.6369715], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.825468440191592}
episode index:1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.008969 , 12.189204 ,  2.6134653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.340114341409196}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5052609168564786
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3522038, 3.1161427, 1.2348554], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6581253645726751}
episode index:1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 1.       , 4.2566085], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5053150963691564
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0462418, 3.3728783, 3.6634963], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.37573466268799727}
episode index:1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 8.       , 1.8248992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.505270683305859
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2663354, 3.896966 , 4.8963394], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9519582541471139}
episode index:1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  1.3690697], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5049105473947357
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5883117, 9.742885 , 4.962605 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7685008559544935}
episode index:1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 11.       ,  5.3391924], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.505121886456092
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7955196, 4.0993466, 5.6095   ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3569872691200762}
episode index:1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8034387, 8.864644 , 1.0291033], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.919422566956596}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5053563290717462
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6977236, 3.650491 , 5.1500807], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7172932076131273}
episode index:1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6201115, 1.9635898, 1.6304617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9232543485209967}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5057081382260338
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6201115, 1.9635898, 1.6304617], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9232543485209967}
episode index:1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.     ,  8.     ,  5.98027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5054813885021459
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.887647 , 3.3445365, 4.06972  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3623929434025593}
episode index:1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 8.      , 6.224082], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.505348226339729
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2192445, 4.3120413, 3.422659 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5267715969294662}
episode index:1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 3.3307383], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5056123660108852
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4305466, 3.5678308, 5.6306376], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6690163760593317}
episode index:1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.20061  , 10.126963 ,  3.3473313], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.275815031730314}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5058338515226487
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0731611, 4.373586 , 4.719156 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6570362964601253}
episode index:1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.52255267, 11.069472  ,  4.875207  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.441216066819937}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5061493413868425
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7146754, 3.8461084, 5.331381 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1075470426479856}
episode index:1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 1.6261389], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5064850004935091
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7540053, 2.8524244, 1.4111875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2547036793272066}
episode index:1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  2.4498534], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5067053981843115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0468942, 4.4487686, 4.522268 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.431779759030214}
episode index:1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 5.       , 6.2179317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5070332578744216
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.928096 , 4.4685426, 6.1638265], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4236688673906657}
episode index:1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 12.       ,  3.4900928], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505136}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5072301800531727
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4479933, 4.382126 , 4.227536 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.001738347598616}
episode index:1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.7608743, 7.430106 , 3.8758924], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.963821576586113}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5073035466206018
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.558154 , 4.9340606, 3.9228492], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4836333305763496}
episode index:1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.     ,  3.     ,  4.43914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5069455342376656
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.9353285, 12.987551 ,  0.8976503], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.159356515409742}
episode index:1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.      , 13.      ,  5.129369], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5069405211880617
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9440095, 3.6251192, 4.659561 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6276216672150097}
episode index:1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 3.       , 1.6737554], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5071481956548347
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7998135, 1.1668884, 6.182144 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.191060453710017}
episode index:1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 5.0625925], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.507415263736568
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.724961 , 1.4991091, 4.602135 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2865176845947666}
episode index:1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  2.      ,  2.727053], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5070581805108562
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.593021 , 13.183866 ,  0.7608514], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.990610312855871}
episode index:1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 13.      ,  5.783122], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5071313584707872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7091875, 4.6644225, 4.7837224], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6896372996452307}
episode index:1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  1.1452612], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5070990960737016
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3720872, 4.1961145, 4.2932243], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0200965191040456}
episode index:1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.      , 12.      ,  3.553672], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5071942593503947
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.517096 , 4.0617666, 5.8053927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1809897044869473}
episode index:1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.0890875, 7.5902667, 2.8916283], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.625453090433408}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5073952558586707
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.07337  , 4.9262204, 4.9142914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.205095963584429}
episode index:1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 13.      ,  2.482158], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5073470246145972
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2269754, 1.8234425, 6.1225753], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.407783582758051}
episode index:1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 6.       , 2.5461595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5076446548341434
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5031968, 4.673508 , 3.9162319], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2452281173886353}
episode index:1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.3067255,  8.935658 ,  3.7019417], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.209492145007953}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5079484612028877
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6977444, 4.6792684, 4.168083 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8184579815158932}
episode index:1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.406539 , 3.5781531, 5.5989413], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8285270349911348}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5082927939802125
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.406539 , 3.5781531, 5.5989413], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8285270349911348}
episode index:1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 2.      , 1.368275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5086227290893173
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2064894, 3.5951123, 1.1356789], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8896663179430362}
episode index:1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.9100617, 4.2190485, 5.203978 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.419487811936753}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5089591213121759
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8541164, 2.4558806, 5.062116 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2685089709225261}
episode index:1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        , 12.        ,  0.69599295], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5091468742717338
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8731794, 4.60746  , 5.463253 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4683453811575924}
episode index:1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 12.       ,  3.0177436], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5093289372665233
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6852145, 4.7841606, 3.9374812], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.911216375592325}
episode index:1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 8.      , 6.113625], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5091977462883491
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7483819, 3.3174362, 5.396981 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.40506474113635327}
episode index:1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.46028   ,  1.3665944 ,  0.83064926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.566885129132299}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.509473136064461
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5015807, 4.6456947, 2.7671795], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.720434354157284}
episode index:1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 10.       ,  2.8641534], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5094993780605099
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2764409, 3.4000444, 3.7429209], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8267849694499597}
episode index:1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  5.853494], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5091448203861463
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.54356897, 3.6095548 , 3.3784723 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5309307430054107}
episode index:1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  1.      ,  4.870811], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596437}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5090779252893088
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4433794, 3.1872158, 3.7925906], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4812847929472109}
episode index:1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 11.       ,  0.5148923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865699}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5090715035412963
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5039754, 4.363332 , 4.0676045], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4507632456380959}
episode index:1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.        , 11.        ,  0.34590134], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5089825977180763
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8197678, 4.3121223, 4.304798 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7648266251302946}
episode index:1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.9079313, 10.00212  ,  5.417925 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.257402200390155}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5092762012921144
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.430945 , 4.5226965, 4.32443  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.089547314359212}
episode index:1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  2.2663345], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5094059773092832
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0637207, 4.210724 , 5.0441747], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6116308635916445}
episode index:1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        , 12.        ,  0.26460728], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5095453724929027
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8140638, 4.0492625, 4.964233 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3280254868009786}
episode index:1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 10.       ,  1.1811002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5091925017363287
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8385347, 7.5827646, 1.3248645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.727656208585458}
episode index:1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.630874 , 2.1576915, 1.3340476], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8355473736242562}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5095321609046772
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.630874 , 2.1576915, 1.3340476], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8355473736242562}
episode index:1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  2.1008592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865699}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5097397651282709
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9199204, 4.936387 , 3.3691053], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.217243040443834}
episode index:1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.27157 ,  6.993754,  3.810398], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.905096109959745}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5095065309645274
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4747915, 4.0939364, 3.451882 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.83622084443774}
episode index:1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.947361  , 10.4558325 ,  0.07289438], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.703683385066057}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5095213092877252
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.576192, 3.654666, 3.493654], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8721150212128692}
episode index:1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  5.5101104], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5094977197312849
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.29908  , 1.9282743, 4.0391884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6841035020709803}
episode index:1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.6605358 , 11.885271  ,  0.52870435], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.891753368401952}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5096218241217247
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.723741 , 4.6482925, 4.6934175], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0846355072906784}
episode index:1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  6.        ,  0.24480551], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5094445249066784
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3250527, 4.9539013, 4.6290317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9807547897607416}
episode index:1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 8.      , 2.680604], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5090936677958611
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.073087  , 5.069002  , 0.44995615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8273244774972133}
episode index:1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  5.      ,  5.944803], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.142135623730953}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5087432936266967
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5159874, 8.387686 , 2.5078173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.596907700136906}
episode index:1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  9.      ,  4.807976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.508726958053552
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.463595 , 3.2750373, 4.385936 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.539041514137473}
episode index:1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        , 11.        ,  0.41000682], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5085810196544768
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.165121 , 2.682694 , 4.7520976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2075554859529103}
episode index:1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2221994, 8.012382 , 4.7958674], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.017304199518622}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5089048651080107
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8613596, 4.0901923, 4.2857394], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.098972452655686}
episode index:1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.287482 , 9.131226 , 5.2007904], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.096258138979984}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5091952978347087
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9024734, 4.5855927, 4.341762 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4765923332954105}
episode index:1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.784686 , 2.839638 , 1.4113357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7918762254769067}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5095319265741911
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.784686 , 2.839638 , 1.4113357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7918762254769067}
episode index:1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4333477, 8.756779 , 3.3354404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.932536289975871}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5093941695790304
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5153737 , 3.6856804 , 0.29497832], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.663284421777533}
episode index:1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.       , 13.       ,  1.2147492], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5093981031049515
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7537963, 3.9791214, 4.108964 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2356729858475075}
episode index:1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 2.6631603], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5096938471475907
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7796855, 4.631893 , 5.2774396], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6466978347894836}
episode index:1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 11.       ,  3.6521235], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5096575379140724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5872438 , 3.797108  , 0.82204884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6221162577365746}
episode index:1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  1.1047989], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5094516834532526
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2599049, 3.745388 , 4.9170084], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0504018901695895}
episode index:1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 11.       ,  0.1575939], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5095746324986187
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9893813, 3.9291244, 6.174529 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.357257326668877}
episode index:1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 1.8366523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5099025679030564
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4745295, 2.929736 , 2.821344 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5301474184076445}
episode index:1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  5.       ,  6.1031485], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5095547489617857
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.4686337, 10.870483 ,  1.8436842], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.248555087501742}
episode index:1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.155705 , 7.2263227, 3.751763 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.666565971849008}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5097995963127109
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8538268, 4.594278 , 3.762096 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.600964867377732}
episode index:1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  8.        ,  0.38764098], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5097016622152726
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.384564 , 1.8539076, 4.0325828], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2088909107826122}
episode index:1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.036982, 11.247118,  4.327446], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.591226160414344}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5097234344506378
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.334206 , 4.0258937, 4.1340203], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0789585625173752}
episode index:1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.070825 ,  3.5275254,  2.8673499], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.083385805320898}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.509756456698323
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0648365, 3.931409 , 4.381958 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4147081967257755}
episode index:1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.      , 11.      ,  4.293846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5100629417787457
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.324536 , 3.2740417, 5.2073226], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.352587903473782}
episode index:1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.        ,  4.        ,  0.29222316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5100734944592802
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0850806, 4.858655 , 3.4010873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8606012554874578}
episode index:1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.        , 10.        ,  0.21676305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5100986697124583
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.76884  , 2.4355698, 4.474392 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8567110324656864}
episode index:1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 6.       , 5.9989834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5102750228169989
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1689067, 2.2858796, 3.4906735], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.36978493766949}
episode index:1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.602212, 10.43045 ,  2.295678], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.305717193781886}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5104356310010766
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0696896, 4.6234045, 4.698509 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5222094082012996}
episode index:1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  0.       ,  5.0738497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5102137616835346
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2931597, 3.0924282, 5.6476517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7128577349749978}
episode index:1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 5.       , 4.4688196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5105386000303974
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5176635, 3.0590334, 4.843313 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5188111721440885}
episode index:1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 1.       , 4.9088054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5107038102382279
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8365585, 1.5342817, 1.4939604], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8713434370342605}
episode index:1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  4.      ,  4.910884], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187263}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.510640535775547
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.915129 , 4.1916885, 6.2457123], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.194706937452468}
episode index:1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.024142,  9.309817,  2.660903], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.70218513579188}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5108708176913033
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.934492 , 4.245553 , 4.5977836], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5571377020750756}
episode index:1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 0.      , 3.062509], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5107609070206761
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7589546, 1.5151231, 2.0469022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9352139151781957}
episode index:1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.320694, 9.498078, 4.908279], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.630931665739511}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.5105718212658165
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.596013 , 3.509159 , 4.8139186], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6752613807811807}
episode index:1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.    , 13.    ,  1.4826], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5104646313705313
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4022093, 2.1196535, 3.1000242], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8242656613952344}
episode index:1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  1.       ,  5.2569127], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5103879556124798
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2766976, 4.5408764, 5.094565 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5655227965012009}
episode index:1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 13.       ,  1.5975643], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5102740014750399
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2248719, 4.025209 , 5.842001 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0495812264417799}
episode index:1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  7.      ,  5.588271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.510057502715748
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.580512 , 3.3444865, 4.2786536], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6176183918266032}
episode index:1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.2136436, 12.1005945,  2.645879 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.651338016040787}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5103540276297925
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9950843, 4.835238 , 4.4419813], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0876520731543464}
episode index:1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  4.       ,  1.2918073], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.5101470020918214
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.749946 , 1.4642228, 1.6810912], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5560008311729994}
episode index:1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.691057 , 11.975994 ,  2.3381722], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.209279314903814}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5102066454795813
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2919679, 4.974393 , 4.2174416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.995863854116836}
episode index:1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 6.       , 3.1812575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5104897721255058
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3340302, 3.3488119, 5.0573635], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.702094372711395}
episode index:1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 10.       ,  1.1495054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5104630094625012
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8552966, 3.5378199, 3.7680652], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2647513792615335}
episode index:1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5038614, 7.0645094, 4.547709 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.333801540039415}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5107777795633976
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.92004 , 3.235123, 4.849788], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9343828189505068}
episode index:1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  2.       ,  3.3840446], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.04536101718726}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5106735485085696
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1190877, 1.8849516, 4.12237  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5797753716349452}
episode index:1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.036357 , 10.379615 ,  2.2307143], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.533956155607493}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5106312792082051
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0499969, 3.972373 , 5.3463597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1789955350805914}
episode index:1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 3.501363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5106308474135839
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6887522, 1.9824297, 1.097052 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6597650702768993}
episode index:1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  4.       ,  5.2661357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5105546755947393
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0120583, 4.8994317, 4.236295 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1522320324406827}
episode index:1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.32415012, 10.0264435 ,  5.2513056 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.7730870306894255}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5107280145861959
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1859634 , 1.0030692 , 0.47330445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.005571029293786}
episode index:1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 11.      ,  4.164071], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5107108339157606
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.564013 , 2.1026058, 4.206689 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.803178580302373}
episode index:1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 11.       ,  5.4465537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5109496831344752
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5599523, 4.2139874, 4.8908734], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9766679887235201}
episode index:1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.2041836, 12.119635 ,  3.8815076], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.382225802084704}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5112367034453195
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2971599, 3.1653647, 4.8672967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7108506274401014}
episode index:1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 0.       , 1.3703891], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5115490707314985
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3242452 , 2.4985867 , 0.19917363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8414629362361037}
episode index:1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  7.       ,  0.6234469], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5117368638159939
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.136064 , 4.5354214, 4.713675 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9100157903809436}
episode index:1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  2.5592322], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5117604298696032
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6703196, 3.9538474, 4.1425447], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9234844167400151}
episode index:1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0114617, 1.7386178, 3.1612961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2614342954687048}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5120850572433602
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0114617, 1.7386178, 3.1612961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2614342954687048}
episode index:1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        , 13.        ,  0.55421907], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.62049935181331}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5119193612500093
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7743583, 3.163218 , 5.1827545], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7818494330001187}
episode index:1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 9.      , 4.021034], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5122237302000424
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5898407, 3.631455 , 4.386893 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7529714729041654}
episode index:1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.08601  ,  1.6794587,  1.4495828], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.164382942477694}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5121366853346837
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7021823, 4.9569783, 3.9853241], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.079140236173414}
episode index:1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.114085 , 10.334172 ,  3.2463198], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.941138069755095}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5124089718128623
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8956661, 4.4287434, 3.262824 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4325477840846876}
episode index:1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 1.2195703], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5124247831139851
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3275127, 4.6069775, 5.251606 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6400125491658915}
episode index:1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.4804273, 9.300345 , 2.1628914], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.197758426759425}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5125240263311175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2769775, 4.028685 , 4.113769 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.006738503468587}
episode index:1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7385142, 2.0112796, 5.0128922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6027833679588201}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5128466444473775
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7385142, 2.0112796, 5.0128922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6027833679588201}
episode index:1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.33754  ,  3.1118574,  2.63475  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.338091458204278}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5126539229809697
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3598936, 2.791276 , 4.058482 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.41603977231416506}
episode index:1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.509549 , 11.934001 ,  2.0039287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.047907808406048}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.512470553983124
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0054264, 1.9922237, 5.1406517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.234711795607522}
episode index:1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  7.       ,  5.8514543], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561038}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5123596831967068
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3915098, 2.7003326, 1.3425878], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.636166660631346}
episode index:1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.        , 12.        ,  0.78343767], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5123228845594442
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.452549 , 3.0544436, 4.8920517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.453568934282364}
episode index:1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.        , 13.        ,  0.71187645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5123880523397697
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1826642, 2.6738048, 5.0121374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8800234181910386}
episode index:1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 4.4512577], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5124073648141448
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1378869, 2.67268  , 1.2048316], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8906622416486554}
episode index:1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 13.      ,  5.556848], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5123829462879205
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.352869 , 1.3489916, 2.9923677], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1344983608176875}
episode index:1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 13.       ,  1.2637503], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5123780107661952
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.293388 , 1.3659642, 4.1316595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.083968645015778}
episode index:1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      , 13.      ,  1.289632], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5123209134718679
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5380573, 3.5045705, 4.8596272], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6840923960806121}
episode index:1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 12.        ,  0.03650492], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5124328356819524
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.775079 , 1.6682873, 3.0929277], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2190906759604734}
episode index:1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.137337 ,  1.7283447,  3.296064 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.203771965108764}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5123757773952945
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3827261, 4.497169 , 3.932172 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2038806732516414}
episode index:1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 4.      , 4.342609], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5126828845670638
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8191614, 2.3858068, 3.5952275], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9200472760907579}
episode index:1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 2.       , 3.9969857], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5125039538551327
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4974568, 1.5360672, 0.9159871], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.546144353254551}
episode index:1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.      , 0.      , 6.223133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5128041473280146
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6257596, 1.1648628, 6.177938 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8729080583255744}
episode index:1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.014319 , 10.238899 ,  3.0539713], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.026600913920507}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5129153861534946
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4533672, 1.6727293, 4.5118985], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0380678469825297}
episode index:1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.      , 10.      ,  6.068663], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5130542630033993
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5706685, 4.0461826, 4.099232 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1308508622194198}
episode index:1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1799784, 4.8291693, 2.4512274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8380022972162886}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5133729447684494
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1799784, 4.8291693, 2.4512274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8380022972162886}
episode index:1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.      , 12.      ,  1.423488], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5133183444210037
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7509428, 4.672575 , 3.9118028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0874987290200075}
episode index:1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.31445518, 11.024876  ,  3.7342706 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.46231528745755}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5133862998435665
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5588038, 4.1420527, 5.466858 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2243113763049935}
episode index:1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3802342, 7.826817 , 3.7326016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.020279798701681}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5136847405360266
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9093494, 4.1698112, 5.688757 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2392127294286017}
episode index:1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.788515 , 1.8379999, 1.4037527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.181088483725278}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5140021786949457
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.788515 , 1.8379999, 1.4037527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.181088483725278}
episode index:1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 3.7684438], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5142872327531356
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.761672 , 1.5705235, 2.1448877], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.619736808489208}
episode index:1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 10.       ,  1.7006477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.764823060233399}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5142003747905224
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3067201, 3.715432 , 4.657553 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8382164010073332}
episode index:1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  1.       ,  4.7337575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5140453593461632
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4700677, 3.4560313, 5.6832876], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.596451467737596}
episode index:1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 9.       , 2.4053056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.513888747231369
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.630291 , 1.5372509, 4.251228 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.003930583626205}
episode index:1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  2.5836473], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5139852945923011
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5604954, 4.0159636, 4.357925 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.862076197200936}
episode index:1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 13.       ,  3.4158382], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5142632496344394
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2313346, 3.8108463, 4.919231 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9456744800878274}
episode index:1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.        , 0.        , 0.35979596], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5145532644235009
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4110518, 2.6503382, 1.3723092], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.45372983563553}
episode index:1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  4.      ,  4.189064], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5147720335836771
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.203971 , 3.7203026, 4.1704035], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4029902919597017}
episode index:1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.4431615, 6.0790806, 4.808843 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.93062021604449}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5148634594026196
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1139426, 1.697177 , 2.3750048], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.307796067399585}
episode index:1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.283161 ,  6.025896 ,  2.5049822], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.763868507167185}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5145295661085841
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.038481 , 13.2825165,  4.7856255], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.085695297548927}
episode index:1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 8.       , 1.7517895], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5144637333833899
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.64896  , 2.0844712, 3.46804  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8860706223517685}
episode index:1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.69157  , 11.976074 ,  1.6123817], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.195848392114765}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.514300682122941
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.244923 , 4.7180824, 4.7258773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.456033900583812}
episode index:1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 7.       , 2.3994358], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5144918971301243
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.118885 , 3.3294275, 5.721325 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.35022295447604146}
episode index:1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.        , 6.        , 0.50503415], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5145918194208355
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0183573, 4.448652 , 5.9323597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4487680974121167}
episode index:1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 12.       ,  2.3997614], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.514259180882102
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.847366  , -0.31112802,  0.72338235], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7916130305282905}
episode index:1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0044055, 1.9999951, 1.5292873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0000145916948944}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5145729669409637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0044055, 1.9999951, 1.5292873], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0000145916948944}
episode index:1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  0.       ,  3.2832897], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.514419112002783
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6006773, 3.9754684, 5.9471087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7057674951193247}
episode index:1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 3.5110881], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5146590899122755
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5113268, 3.458981 , 4.557673 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.55782268537038}
episode index:1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 9.       , 3.4158118], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5147585822840727
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8198087, 4.735838 , 4.0351906], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.099043781153133}
episode index:1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  7.      ,  5.460274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5145739636486676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.254106 , 3.5557342, 4.6767855], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3717224301615454}
episode index:1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 12.      ,  5.945073], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5146282985120402
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.983155 , 3.8856564, 4.384605 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.171932328332994}
episode index:1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        , 13.        ,  0.36019385], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5147877028980344
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8517003, 4.8714337, 4.3048043], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6326903059631044}
episode index:1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  3.       ,  4.6383395], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5146360980013701
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1041065, 4.1461577, 4.4718885], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2154208538151936}
episode index:1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.076147 ,  7.5466156,  3.493537 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.903683261163438}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5144434491258589
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4572794, 4.082677 , 4.4730535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1752845325371455}
episode index:1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  9.      ,  4.384757], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407864998737}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5142442840729498
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6317942, 4.4392085, 4.097848 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9857764729024139}
episode index:1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  4.        ,  0.53096694], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5142725394352572
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4811081, 4.066506 , 4.1309123], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.855927679259349}
episode index:1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  7.       ,  5.8640585], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5139426660937336
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.20758295, 4.1348743 , 3.2875566 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0142217493414964}
episode index:1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  4.      ,  5.704606], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.5137270121582624
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3742439, 4.14651  , 4.2548733], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9893638446408966}
episode index:1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.       , 11.       ,  0.7034409], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5133979109333052
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.1476336, 4.443077 , 0.6877347], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.587431420584828}
episode index:1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 12.       ,  1.6278783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5134375760031155
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.128072, 4.293739, 3.862101], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.716480743666867}
episode index:1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  1.4352107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5136017510317796
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1735597, 1.2190667, 6.220779 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9633458447339047}
episode index:1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  6.        ,  0.82374537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.513548226263552
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5680566, 1.7881302, 5.247023 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8759238916818697}
episode index:1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.       ,  4.8282976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5134470202497766
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.579751 , 3.778531 , 4.7221775], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9706811387144811}
episode index:1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.451554,  7.734174,  3.816291], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.570916705038384}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5135638569022669
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5325222, 4.0984654, 4.5062165], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2207400312284522}
episode index:1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 2.0630515], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5137849765562166
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9156194, 3.3425245, 3.8707821], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9775898370115178}
episode index:1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 5.       , 2.6547678], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5140340818485971
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0356236, 3.0003173, 4.0352874], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9643764752169073}
episode index:1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.866217 ,  5.280808 ,  5.6650934], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.066813541767058}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5140806100194631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6907408, 3.5298383, 4.6294994], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8705466872815696}
episode index:1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.25682896, 5.856797  , 0.85977376], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.960590572212877}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5143409056146278
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9674237, 4.646328 , 5.6043377], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9095298804749627}
episode index:1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.297836 ,  8.050067 ,  2.7651892], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.580781450783817}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5142955250137439
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.917287 , 4.7856445, 4.2070923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6200220122813405}
episode index:1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 4.       , 0.8657215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5145794311746766
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00061 , 4.049388, 2.488258], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4499776387431464}
episode index:1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 1.       , 3.8607936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5147990617681074
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9373181, 1.4354393, 0.7671715], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8913336252792219}
episode index:1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 10.       ,  0.9989415], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611664}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5148843880493019
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5293875, 3.3246412, 5.819948 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5717237762316069}
episode index:1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  9.       ,  1.2790831], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5150415044469513
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6404586, 3.660223 , 4.4929404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7683322042270337}
episode index:1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 2.1767254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5149205679148552
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.331721 , 3.2600951, 4.3270197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6884325470049768}
episode index:1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1136549 , 0.33874357, 5.8669233 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.261990775187652}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5150140120956219
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9966397 , 1.0128119 , 0.05201352], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.987190941868509}
episode index:1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.4890594, 11.060717 ,  5.542087 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.436265139881355}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5150108256153646
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9991217, 4.736044 , 3.871292 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6477039038316557}
episode index:1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.     ,  7.     ,  3.16292], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5152516916246385
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8458858, 4.536199 , 3.71196  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.543910247991418}
episode index:1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      , 11.      ,  3.626002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.60147050873544}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.515150368284816
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.490136 , 3.5990067, 4.2095556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.606024503355487}
episode index:1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 9.       , 0.9803201], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5150318182463296
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5071073, 1.0177735, 3.8536816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4900992245619222}
episode index:1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 7.      , 0.976179], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5152284913219233
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8866706, 4.677605 , 3.5287015], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6814287382312199}
episode index:1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 5.       , 1.1705402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5154629552387864
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9442673, 4.5311446, 0.8399808], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7989009219573424}
episode index:1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4932556, 7.0617795, 5.0253057], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.327570338118607}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5157439735814386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.28925  , 3.3345838, 3.9586163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.331957802978486}
episode index:1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 2.       , 0.0695935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5159294523793186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0109096, 1.7913884, 3.3597898], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5756522938246464}
episode index:1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  9.       ,  1.9905146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5157800908636854
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6851733, 4.9069114, 4.945197 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.932725239909626}
episode index:1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  6.        ,  0.02639504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5154550876558318
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.083614 ,  6.7802963,  0.4751361], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.029211204128956}
episode index:1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.08055794, 0.43809068, 4.218975  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.88413713471119}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5156560062161557
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9693818 , 1.0956194 , 0.04502055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1653727559343072}
episode index:1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  8.       ,  2.4808068], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5158259383335825
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.184355 , 4.0797815, 5.2408915], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6026928612067666}
episode index:1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 2.6811926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5160936453845683
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4961146, 2.3272681, 6.2294717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6474948979752666}
episode index:1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.64067  , 4.032544 , 5.3750324], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8353632051937536}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5163669932189588
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5218153, 2.3747854, 4.693368 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.645240126614226}
episode index:1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 12.       ,  3.4704242], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5165361585126701
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.797209 , 2.427175 , 4.3963323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8862894395000929}
episode index:1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 13.       ,  0.3877834], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5165767299223408
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.150814 , 2.5056477, 3.8946035], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2525003908796044}
episode index:1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.       , 1.       , 4.8890905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.516814346938863
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7159395 , 2.0274    , 0.67422307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2076920659715848}
episode index:1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 8.       , 2.1616542], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5166818357658156
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.17632  , 4.3172417, 4.532829 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7660278975533972}
episode index:1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      , 12.      ,  1.411982], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5166776402210059
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2557473, 3.2464795, 5.749521 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.355188457403443}
episode index:1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  4.      ,  5.635488], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618197}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5163541100768475
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.02690113,  7.2876616 ,  1.1862781 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.248444723363492}
episode index:1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 3.9163146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5165912716188887
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3162475, 2.845831 , 1.4279686], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3518246211390479}
episode index:1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 10.        ,  0.51285905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5167745965698596
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4811766, 4.1229076, 4.60186  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.221659731392786}
episode index:1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.      , 12.      ,  1.390539], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5164516124470034
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.721662 , 12.188874 ,  2.2131808], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.217169087699018}
episode index:1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 10.       ,  1.5943935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733704}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5162850824307708
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0414705, 3.712799 , 4.344146 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.084207334093831}
episode index:1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 10.       ,  3.8633974], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5164292066579248
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6901972, 3.4446464, 4.873719 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8210253343044039}
episode index:1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.        , 13.        ,  0.35603333], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5163177438970021
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.361063 , 3.5912304, 3.7798185], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.483929202007014}
episode index:1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  4.       ,  2.6969092], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5161787819129798
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8313494, 4.450765 , 1.9055524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.336356136452368}
episode index:1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.       , 11.       ,  2.6216278], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5161654858866167
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.926315 , 3.5327673, 4.0570216], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0685974915459613}
episode index:1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  5.      ,  6.158821], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5160126769038181
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9744267, 4.065375 , 4.1260676], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2435205705322785}
episode index:1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 8.       , 1.0528133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5161036970432582
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4061165, 4.2448034, 5.5128055], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.877950783977728}
episode index:1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1507597 , 0.76178133, 2.9753077 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.516718234736146}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5163922519580323
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5768342, 1.9957824, 2.3173392], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0897349817464244}
episode index:1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.220065 , 7.0879517, 2.8326502], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.443155166596392}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.516569528737138
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2088492, 3.6003113, 4.8030615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9931229700188929}
episode index:1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.296586, 6.022113, 4.090657], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7957177819360046}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.516863584930469
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1316986, 4.396371 , 4.2695   ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7973851552809739}
episode index:1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 5.8944564], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5171450470130696
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9235306, 1.54775  , 6.134944 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4542618904403044}
episode index:1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  8.      ,  4.464022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.517013728141429
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0464816, 3.7071614, 5.146221 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.26301268476933}
episode index:1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.809921 ,  6.828668 ,  1.0792922], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.415028788985309}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5169133853556657
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5582457, 2.5268426, 4.77124  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6284985305842492}
episode index:1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  7.      ,  5.227998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.5167730973190823
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3912816, 2.2153544, 4.6098523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.789872488730604}
episode index:1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.955561 ,  7.2943783,  4.5304236], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.040610079927326}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.5165535287696655
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8429543, 4.3740315, 3.6155946], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.796306623599136}
episode index:1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.    , 12.    ,  6.2126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5164558770177549
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4585978, 3.5215409, 5.8635173], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6272448174083283}
episode index:1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.     , 10.     ,  5.26016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449806}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5164714794908216
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9263091, 1.6142718, 5.014804 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.753013087560148}
episode index:1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 12.       ,  1.3761823], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5165699081493935
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5957532, 4.3005896, 3.5307019], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.361965009988952}
episode index:1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.      , 10.      ,  0.997414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5168038601852857
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6373315, 3.4025176, 3.9298148], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5418014719792188}
episode index:1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 2.        , 0.07747161], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5167475554094852
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7810035, 3.9778857, 2.4046373], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0318055679878073}
episode index:1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 11.       ,  4.3279486], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5165640203779966
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.740217 , 4.3608475, 4.8177414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.549137504709231}
episode index:1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.486172  , -0.33839154,  6.2301993 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.865867928305152}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5165299007399202
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.912206, 3.441189, 5.108096], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44983950067521}
episode index:1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 7.      , 4.543034], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.12310562561766}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5168155261861679
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8329854, 3.4066029, 5.7823195], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8775413087847865}
episode index:1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 11.      ,  5.168306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5170322320276598
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3521013, 4.0275393, 5.3018823], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.698238767864828}
episode index:1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 9.      , 4.539298], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5170440695619242
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3776064, 1.7886198, 5.3930063], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.024747666610042}
episode index:1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 3.       , 0.4912066], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5173349403678517
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7635287, 3.9433804, 5.757982 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5552581007718616}
episode index:1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 1.       , 2.8122573], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5176193688003238
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.595386 , 2.4237485, 2.6245537], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6962671895541115}
episode index:1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.443392 ,  1.9209859,  1.110803 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.490086902595213}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5175875883374058
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.007351 , 3.7948275, 4.841149 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7948614528295482}
episode index:1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.     , 1.     , 2.71953], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5175797911001128
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3684905, 2.9657607, 1.3193815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6318687803501892}
episode index:1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  0.       ,  4.7361565], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5175628036656982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5842443, 4.2013354, 3.917373 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2712433513454313}
episode index:1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7971706e+01, 6.6647725e+00, 5.9701758e-03], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.413713043705494}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5172454751533342
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.657058 ,  5.278708 ,  5.2038608], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.877689410536922}
episode index:1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  8.      ,  5.664252], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.08304597359457}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.51708473023026
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3983617, 1.5670595, 4.697788 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0021822654962422}
episode index:1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 6.       , 1.9030719], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5172302504733546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6487498, 1.8060797, 5.758164 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8031424239408633}
episode index:1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.02989  , 11.655521 ,  2.8803391], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.642764639550165}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5173442146376913
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8094752, 4.365922 , 5.344156 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3791455852305585}
episode index:1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.05963  ,  4.9991107,  1.4074212], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.277572228833245}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5173274232360806
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.625491 , 4.3174524, 3.8343384], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0923437482477123}
episode index:1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  7.       ,  3.4703329], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5175640092090468
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0857477, 4.9065204, 3.4817247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9084476885164696}
episode index:1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 4.       , 2.1261694], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5174806146512554
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7668953, 4.8449907, 5.736882 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.998028825390135}
episode index:1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.179539 , 10.171797 ,  4.1606283], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.134207898559904}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5174406653445491
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8030307, 4.5828013, 4.0253196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7748573014359221}
episode index:1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.       ,  5.5770197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5173331152506928
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.437471 , 1.9376664, 5.5223875], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8894574403066817}
episode index:1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  7.        ,  0.48021778], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5170176682291985
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.277076, 6.537387, 5.513247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.205191616507688}
episode index:1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  8.       ,  1.5460107], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.08304597359457}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5167026056647688
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.680824 , 9.245388 , 0.9065541], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.442549150323483}
episode index:1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  5.      ,  5.880372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5165528198266733
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4988666, 4.525218 , 4.7893176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1400213990373342}
episode index:1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  6.      ,  1.320416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991377}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5165426458827126
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2164187, 4.578642 , 5.0673437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5934074475517719}
episode index:1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 10.       ,  3.8891504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.516578419668658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4416676, 2.8099134, 1.3400574], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5698830912996773}
episode index:1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9664254, 6.248994 , 4.327966 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.389681294881902}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5168662139424157
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2163773, 4.3949637, 4.0030575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4116454786731703}
episode index:1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  8.       ,  1.1782489], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5167768235558466
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6509855, 3.952641 , 4.110351 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1538227718356935}
episode index:1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.        , 8.        , 0.08720182], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5166830744327772
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8185296, 3.2772188, 4.7871428], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2135578130596623}
episode index:1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 0.6186718], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5167634261038546
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4899244, 4.865503 , 4.227853 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9339798114517108}
episode index:1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  3.8414404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5170096245685751
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2220712, 3.9822621, 5.937282 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2530012626747133}
episode index:1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 1.5995016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5171141333329997
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.961101 , 4.7398853, 4.6739626], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6216632718062227}
episode index:1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.0081925, 10.1808405,  4.0654273], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.249010430009381}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5173711690786497
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.816991 , 4.3269906, 4.303305 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7777554459277298}
episode index:1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 3.1514544], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5174924519863492
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1712856, 3.9283333, 3.0813196], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4945610421401379}
episode index:1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 13.       ,  0.7933009], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5173920991457996
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2993765, 4.392413 , 6.1647854], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9045192588908975}
episode index:1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 12.       ,  1.0361496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.727922061357855}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5174837434380752
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.254477 , 3.2752705, 3.9675305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.767094872025804}
episode index:1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 7.        , 0.26315504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5173363146813285
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.864996 , 4.1131434, 2.9720588], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1213003249974178}
episode index:1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 1.2625455], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.12310562561766}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5175178186806739
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8666573, 3.8173904, 4.566019 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1913110155413245}
episode index:1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.438963 , 10.250265 ,  2.0831494], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.063589915425709}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.517447309829902
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0240552, 4.806678 , 3.248332 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8068379545008546}
episode index:1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 13.       ,  6.0098176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5174822359096046
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7296767, 3.1697555, 4.1730657], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7491628905598746}
episode index:1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.000425 ,  6.041269 ,  4.1157174], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.500366958569122}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5175537869946886
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0301595, 3.473274 , 3.8263466], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0791565776159782}
episode index:1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.      , 1.      , 5.938018], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5176176253126985
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7642663, 3.5670333, 2.2690215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9516457627102717}
episode index:1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       ,  7.       ,  2.9005027], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.517878535863323
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2146635, 2.9573083, 4.2531767], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2154135154851482}
episode index:1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 5.      , 2.526826], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5181165856296409
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.139633 , 4.26581  , 4.2475567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7032436028259548}
episode index:1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 3.       , 4.0020227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5184003399377409
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.695777 , 1.4837538, 3.7240992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.274788334328987}
episode index:1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.       , 10.       ,  5.8347993], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.602325267042628}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5184415877995197
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0961678, 2.5918927, 4.208811 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9470820700834803}
episode index:1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.      ,  9.      ,  4.793417], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5185240007921771
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2385068, 4.264737 , 3.7760615], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2870297186850768}
episode index:1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  3.       ,  1.8229108], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5186749726679352
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.378495 , 3.8337064, 4.2194157], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.610998258172183}
episode index:1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  4.       ,  2.3741024], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5183638299128854
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.6193228, 11.247506 ,  0.8778274], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.256286859419903}
episode index:1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.9910868, 11.188608 ,  0.1650711], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.250527832670135}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.518377811475268
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.013825 , 2.0383503, 4.060297 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3973586046065012}
episode index:1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8602424, 4.6434574, 2.001113 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8549849587113114}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5186663807913403
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8602424, 4.6434574, 2.001113 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8549849587113114}
episode index:1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.      , 12.      ,  4.883833], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5189310093118246
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4936392, 4.312104 , 5.244118 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9976837249376018}
episode index:1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  5.      ,  5.980116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.518755675467553
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2385535 , 4.610653  , 0.10696094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0318015828582756}
episode index:1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 4.       , 2.7012572], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5190085011098576
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8818102 , 4.677341  , 0.33377796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.015892167941041}
episode index:1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  1.        ,  0.24885496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.18033988749895}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5188684528565184
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9226391, 4.8472567, 4.4434037], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8488758405006684}
episode index:1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  9.       ,  1.5140427], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5185584955967475
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.916756 , 13.277786 ,  1.0707074], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.318591556417655}
episode index:1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  7.       ,  3.7534082], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5185326935349021
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1105129, 3.7933955, 5.2162294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.049301864027449}
episode index:1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 9.       , 2.1012254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5185275939243499
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4436483, 4.220986 , 6.253688 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9781397920823616}
episode index:1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  3.       ,  2.7269998], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.000000000000002}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5183491262293243
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5460904, 3.6030164, 4.1719084], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5740018991884248}
episode index:1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 11.        ,  0.79136574], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5180402173340745
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.852962 , 10.862954 ,  1.3063924], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.112832796158811}
episode index:1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 12.       ,  1.3016727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5179154432145393
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9078581, 4.690434 , 3.9148746], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.012545873871719}
episode index:1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 2.1411133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5178409167771245
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.504765 , 3.073688 , 4.5154524], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5006871254840979}
episode index:1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1067843, 3.6447744, 2.91181  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2809003310358056}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5181277455000411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1067843, 3.6447744, 2.91181  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2809003310358056}
episode index:1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8231515, 3.8222644, 0.2404645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4356501182384291}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5184142331662124
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8231515, 3.8222644, 0.2404645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4356501182384291}
episode index:1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.850657 ,  6.6367683,  2.3312879], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.32015257278836}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5186435664055722
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7088768, 4.4977455, 3.3837652], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5257766264946486}
episode index:1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9998891, 2.2678852, 5.774776 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2394409264037387}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.518929407518158
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9998891, 2.2678852, 5.774776 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2394409264037387}
episode index:1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 0.      , 5.478762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.51910684225411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4566963, 1.3704145, 1.0622725], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6923712362671477}
episode index:1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.9303198, 10.770453 ,  1.6688128], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.843733972178252}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5192246498996285
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.561614 , 2.6730366, 4.4540415], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.649857989980479}
episode index:1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        , 13.        ,  0.36535805], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5192191733710866
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7530804, 3.9854906, 5.01762  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.015953222123537}
episode index:1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 1.      , 2.777119], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.51932414614637
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5997281, 2.177175 , 1.3896773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6241312384187159}
episode index:1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  2.      ,  3.222402], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5192011958416352
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6770172 , 4.619428  , 0.09484785], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6513223306477705}
episode index:1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 7.       , 4.5831227], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5194623762050424
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6273203, 3.356243 , 4.1022286], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6658572343722269}
episode index:1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  9.       ,  1.2347593], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5193105430950751
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1395764, 1.8925258, 3.5726814], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5890668873668161}
episode index:1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  4.8386235], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5195381267427782
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.852097 , 2.3040237, 4.30871  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.97854653111662}
episode index:1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.013922,  7.235576,  2.833113], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.685841747601936}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5196467561392678
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7982202, 4.768587 , 4.164061 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.138264521471795}
episode index:1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 3.       , 3.7962432], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.519863248533351
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5065525, 1.065212 , 5.090279 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4441338184208594}
episode index:1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.06161  , 9.76619  , 1.8944062], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.449924224840577}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.51970845287449
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.362218 , 4.4982753, 4.0880766], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.024960846604929}
episode index:1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 6.      , 6.100377], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5198004461505429
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3010836, 2.5319328, 4.5744934], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7622156306216872}
episode index:1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13286  ,  5.7167897,  3.087101 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.490748395548493}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5199128520321703
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2074544, 4.3358755, 3.8623226], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.553284211372039}
episode index:1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 5.        , 0.42706054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5200511361518237
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0384886, 4.609522 , 3.381997 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6099819908932569}
episode index:1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  1.5087439], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5197450436643889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.081409  , 8.361761  , 0.41600347], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.751586333487419}
episode index:1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 4.9815116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5199931231383516
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6837664, 1.5058317, 2.393019 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6431905840714196}
episode index:1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 10.       ,  2.7671325], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5200256696561872
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1334212, 4.4487724, 4.4929028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6881648029309801}
episode index:1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  9.       ,  0.6671802], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5197201316599145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.29229 , 12.909568,  1.8115  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.303537060484809}
episode index:1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 3.7777016], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5198952266722089
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5294011, 3.4613242, 6.0795245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5412596348727376}
episode index:1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9883374 , 0.21567202, 0.2097384 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.9624219049334104}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5198954879961396
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.912573 , 2.0195482, 3.7159681], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1492372422252677}
episode index:1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  1.      ,  3.653562], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.132745950421556}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5198118171216102
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7465627, 1.3775039, 5.839836 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6421705577886023}
episode index:1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  2.       ,  2.3018923], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.5195913793256149
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9445233, 4.669778 , 3.7323148], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5630702554093716}
episode index:1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 2.       , 1.8383592], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5198669555533094
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.471236 , 3.928836 , 1.7215685], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.739905904206563}
episode index:1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 10.       ,  1.8378121], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.892443989449804}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.519665480346013
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4650021, 3.584979 , 4.6996026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6426865718976238}
episode index:1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 10.       ,  1.3110971], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5197607928765364
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9983892, 3.7951324, 5.755092 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.278850905182988}
episode index:1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  6.       ,  1.4475327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5196446760822021
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4017925, 1.2089031, 0.6578795], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.835610370292535}
episode index:1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 2.       , 2.7026691], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5197359001458947
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9949278, 2.1423492, 6.066476 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3212626613970853}
episode index:1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      , 13.      ,  1.809418], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.5195697089829826
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.321036 , 4.381173 , 4.4951267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5390358029123992}
episode index:1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  7.       ,  1.2875885], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5196056673630965
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0634298, 3.8796506, 3.7941105], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2848925418684125}
episode index:1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 11.       ,  2.9972637], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5197702093246928
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3627602, 3.994019 , 5.620987 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9153662947175998}
episode index:1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  8.       ,  3.9720228], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5194671363163402
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.511541 , 7.7145915, 3.9560218], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.341835890622908}
episode index:1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.    , 12.    ,  2.6215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5196131596318931
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6437068, 4.4632616, 4.221353 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.506014398351559}
episode index:1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 13.        ,  0.06829327], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.5194307451749831
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.5733895, 4.2416143, 5.0138535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0042856052194993}
episode index:1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 10.       ,  5.6062846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0384048104053}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5191283989903644
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.789265, 10.283435,  5.805391], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.303978127518329}
episode index:1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 11.       ,  3.9284115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5193267293892306
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6443734, 4.9494653, 5.9068108], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.053200418395952}
episode index:1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 1.5205505], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5192747307172159
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6590766, 4.4346123, 3.9206789], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9637179070310204}
episode index:1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.      , 13.      ,  5.988772], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.519317549843122
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3615493, 4.563036 , 5.319624 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.264420954896938}
episode index:1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.006953, 11.166632,  2.556981], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.579324299900458}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5193967261908171
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.833113 , 1.8802302, 4.2265887], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.395693958926123}
episode index:1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 4.843337], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5192913040635437
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9313874, 1.1143944, 3.8287094], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1030908001925823}
episode index:1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 6.       , 2.3912346], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5192272256820815
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6106775, 4.7510943, 5.946977 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2352960552680217}
episode index:1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 0.      , 6.160523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5191405604136569
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0341082, 2.164555 , 2.716324 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.836140893103685}
episode index:1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.      , 7.      , 4.667373], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.65685424949238}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5192770486678805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5566278, 4.2152557, 4.9217205], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2936094691869817}
episode index:1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.778416 ,  9.987687 ,  1.4377649], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.018535034245366}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5192434325235552
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6197844, 4.70792  , 3.5985267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7497299555450563}
episode index:1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  3.       ,  4.2310624], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.5190386982429656
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9242373, 4.7244163, 4.7943363], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.956483048851852}
episode index:1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5507417, 5.6217217, 3.70691  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.678944063549211}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5193110876598291
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8619043, 4.550354 , 3.2943504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.923241909608829}
episode index:1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 11.      ,  3.395702], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5195130730500657
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5923946, 1.2946838, 5.7902026], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.211211496728642}
episode index:1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.000518 , 7.       , 4.8015523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.00031072425369}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5197568437469754
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.648989 , 3.2357793, 4.9739437], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6657602633955126}
episode index:1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.0552025,  8.533351 ,  3.1877694], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.966262028699363}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5199583385327848
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.286323 , 4.461299 , 3.2725582], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2521286878701487}
episode index:1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  1.       ,  3.8305697], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5197822947411941
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3799474, 4.822291 , 2.5833893], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8614790247515849}
episode index:1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 12.       ,  0.4624668], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5196534621533383
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8164505, 4.7539043, 3.5403082], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1158851380454946}
episode index:1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.4828005,  6.303114 ,  2.2045574], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.041616737210688}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5197434192639475
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2797931, 4.9175706, 4.0020423], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.576080084410578}
episode index:1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      ,  1.      ,  4.967489], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5194440278934038
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.700136 , 11.851997 ,  5.3750267], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.159599686011145}
episode index:1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 9.       , 2.7540538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.999999999999999}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5196762101999867
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3198454, 3.2431078, 4.4232078], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6976515403742556}
episode index:1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  3.6103156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5195920836581307
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2301402, 1.0852797, 5.380073 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0636952082854574}
episode index:1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.013872  ,  6.7239676 ,  0.94582725], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.53620340338997}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5194398799071464
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5467858, 2.3795671, 3.5753896], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8269895231650559}
episode index:1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 12.      ,  6.261772], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.5192778986858136
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.066172 , 4.1048646, 4.3082957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1068443868563729}
episode index:1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  3.1716342], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5191383086622716
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5153985, 3.1362534, 4.6267343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.490840898319043}
episode index:1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 8.       , 1.5526508], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5191607701030785
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8678865, 4.993214 , 4.8165793], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.7316483500217354}
episode index:1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 9.      , 5.515872], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5191768319919412
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4423895, 2.4327488, 3.5912263], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5499230193161935}
episode index:1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        ,  8.        ,  0.28699827], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.5189939765990889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7731624, 3.7594643, 5.79282  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0837739666349242}
episode index:1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.620185 , 12.447795 ,  2.5037582], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.214399601015467}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5190760901030343
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0214355, 2.6218603, 5.053459 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0143751245716026}
episode index:1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.0696306, 10.310062 ,  5.0753374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.31039402761809}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5193289652003408
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3915477, 4.773855 , 3.385971 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8753070494118869}
episode index:1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.812331, 5.172401, 4.704785], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3193118557342123}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5195983819346279
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.797123 , 3.1724586, 5.3307076], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8155654427492623}
episode index:1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 6.        , 0.13276738], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.519514781190074
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4925666, 3.6748185, 4.416125 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.835465062362012}
episode index:1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.      , 8.      , 1.813474], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.519478814904513
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1855443, 2.066459 , 3.1054242], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9518012289119341}
episode index:1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  5.9954214], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.519284435972336
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.299617 , 2.0776076, 3.8628213], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9698340368459225}
episode index:1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 6.       , 2.2453344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5194456844895072
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.583684 , 4.7785482, 5.858472 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.826623355594107}
episode index:1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.4603662, 8.925823 , 1.9512764], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.41688936225544}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5195588607726743
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0517441, 1.7658445, 4.83424  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3062612438331884}
episode index:1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 1.       , 1.7428305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5195077085494861
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1670334, 1.4647442, 5.222371 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.390978204076759}
episode index:1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  6.        ,  0.48300254], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5194615909857805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.485041 , 4.289858 , 5.2556806], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3780413593119702}
episode index:1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 4.8721433], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.519300981848346
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7901726, 4.477543 , 5.5606737], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3211744460631443}
episode index:1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.        , 12.        ,  0.27927634], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5193296266209138
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1538765, 2.2335923, 5.027502 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9988878468179825}
episode index:1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 11.       ,  1.2790146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.519472320712652
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.859779 , 3.5898876, 4.924416 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9510881296231886}
episode index:1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.       , 7.       , 5.0899806], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5191768302003013
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.060448, 12.289197,  2.24427 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.298780318441006}
episode index:1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.010047 , 4.7378125, 2.4884815], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.657118952648809}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5190918687491639
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2491903, 1.7519557, 3.3751373], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2726783870071585}
episode index:1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.        , 5.        , 0.04477373], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5189850171693752
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2346613, 1.8289229, 5.4765315], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.118452791919581}
episode index:1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 13.       ,  2.8539865], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.5188212186464616
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4027622, 4.3131413, 6.059569 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3735201359317448}
episode index:1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  8.       ,  2.7083416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5187072389899194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6538758, 4.6395807, 3.1629887], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6757168321956717}
episode index:1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7236118, 4.539751 , 3.2475996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7013075523544132}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.518980235451071
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7236118, 4.539751 , 3.2475996], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7013075523544132}
episode index:1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.01242  , 11.77746  ,  3.1640022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.580441750464566}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5192038958887312
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2347376, 4.899235 , 4.473021 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.913686096325069}
episode index:1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 13.       ,  1.3850011], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5193873175870287
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5253708, 4.034624 , 4.1028605], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.801382302095734}
episode index:1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 1.6342981], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5192419751576194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.667486 , 3.6802504, 5.010994 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8009027768647214}
episode index:1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.      , 13.      ,  2.225351], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5192870877955982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7483287, 3.1926723, 4.9199476], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2664137064297771}
episode index:1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.952228 ,  7.7587676,  1.8074589], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.79877055117069}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5194847454454701
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5084329, 4.8662696, 3.2870507], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9299223883154968}
episode index:1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.092874 , 11.3976145,  3.24547  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.129585900007802}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5194912015209419
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.091414 , 4.5928903, 5.363506 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5955111730977176}
episode index:1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.833602 , 9.375341 , 4.0280337], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.633782803189281}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5197296133559023
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7974644, 3.993537 , 5.012325 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0139705865883937}
episode index:1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.775746 , 5.079823 , 5.8459544], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.219785080864065}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5198293782371523
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4208565, 2.7010098, 4.3069987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4519739303046542}
episode index:1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.183327 , 11.174337 ,  4.9009156], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.370294701011254}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5197446718372722
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9924026, 1.4777201, 4.337272 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.507389858656229}
episode index:1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.       , 11.       ,  3.7240949], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5197014136927651
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3939404, 3.701913 , 3.9316413], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8049044060627779}
episode index:1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.159554 , 10.782789 ,  2.9412127], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.28874282784156}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5197817860869541
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.535924 , 4.8277917, 4.684639 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.341867016362321}
episode index:1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.017211, 10.738182,  4.181851], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.142492239121827}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5199497439187909
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8512865, 3.9223084, 3.6741028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4731583427108104}
episode index:1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 11.       ,  1.1622577], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5200010769681231
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0487523, 3.8875494, 4.393863 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.373908782028082}
episode index:1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  0.9100929], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5197084483373026
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.0148276, 10.04149  ,  2.8437252], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.31597465992315}
episode index:1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  9.       ,  0.6941807], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.519598626673369
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3949662, 4.473994 , 4.2683144], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1791720936900125}
episode index:1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 0.       , 2.7073667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.519825240539448
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9352793, 2.8256886, 2.0715451], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0788951474325619}
episode index:1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  6.       ,  0.7789886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5197268062376548
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0839672, 4.409793 , 3.7024138], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.681259080442648}
episode index:1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  8.      ,  3.486886], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5197850873093299
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7951078, 3.4725223, 3.2287178], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9249182475353519}
episode index:1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.223195 , 2.9181309, 2.4044323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.23773639947189615}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.520054568180649
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.223195 , 2.9181309, 2.4044323], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.23773639947189615}
episode index:1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.164489 ,  3.794292 ,  2.2200935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.192707882126312}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5200189849947618
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.375982 , 3.005345 , 4.2227116], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3759921894054847}
episode index:1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.354741 ,  4.8930507,  2.6068163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.498930690383506}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5200191652288736
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0764296, 2.9331002, 5.0582457], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9259902017390104}
episode index:1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 4.       , 2.7159328], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5201026120598768
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4908385, 3.004054 , 1.9803089], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4908440398288063}
episode index:1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.041407 , 9.404861 , 3.0519845], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.804592226955453}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5200802373927501
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9337425, 2.6234446, 4.264465 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0068113532978165}
episode index:1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([-0.40787983, 12.420519  ,  2.4187393 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.017974885584835}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5202201159089295
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1928544, 3.4836626, 4.247862 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9409641481843655}
episode index:1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.       , 11.       ,  1.8833991], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5201923566951022
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0456753, 4.8851023, 5.1539054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1557011301292186}
episode index:1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.       , 1.1229852], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5199015839971173
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.207239 , 13.093137 ,  1.6980531], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.354760231198155}
episode index:1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.108901 ,  4.3490443,  2.375295 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.208257079811311}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5198218829149532
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8518958, 4.4193096, 3.812522 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8255363648296647}
episode index:1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 9.      , 5.750745], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5200167036463067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9961882, 1.5710157, 4.2566714], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7419491791309687}
episode index:1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  3.       ,  3.5293791], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5200541032435674
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0670872, 4.5340214, 5.0221477], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7954241640183601}
episode index:1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 1.       , 3.0745962], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5202838920024427
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4918349, 2.9385815, 1.3478905], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5094152075136642}
episode index:1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.        , 8.        , 0.33325773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5202928018871723
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5494676, 1.6194257, 4.1396813], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4859004432256482}
episode index:1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.287696 , 11.966552 ,  3.9885163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.64682151462949}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5204362718356469
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.223606 , 3.0193832, 4.4320583], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2237596249393978}
episode index:1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 3.8654294], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5204450869544507
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.79689  , 1.9896598, 2.3431332], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.061455853530547}
episode index:1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 7.      , 5.019396], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5203061382469145
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3257396, 4.4546905, 6.2747965], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6033562863872175}
episode index:1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.      , 9.      , 4.709916], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.520509741546953
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9445796, 3.2351284, 4.2759604], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9734043296236337}
episode index:1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 1.      , 2.197362], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5207652113960096
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.326209 , 2.8830843, 1.6906987], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3313526108445517}
episode index:1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6497368 , 0.10848153, 4.2106175 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1912520889274054}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5207955389174435
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3694513, 1.1295506, 0.7288154], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9065873435296383}
episode index:1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.790688 , 10.592974 ,  2.4656038], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.742817827608611}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.520684714672939
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9749129, 1.1336527, 5.8218555], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1293322644118673}
episode index:1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.155524 ,  9.81297  ,  2.0172527], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.412282277589892}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5205138162204174
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.3048024, 2.9871426, 3.8249197], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3048657643870016}
episode index:1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  7.      ,  4.373793], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5205137201064017
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1507907, 3.519967 , 4.874928 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.99575208423934}
episode index:1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 12.       ,  4.0074935], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5207165311660523
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9866037, 4.347574 , 5.642208 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.400531341919907}
episode index:1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 1.1317031], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5206835701889067
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1279703, 3.5204248, 4.7757344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9430226640320798}
episode index:1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 8.       , 3.9562497], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5209325266838186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.151802 , 3.408449 , 3.9633007], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2220795900608188}
episode index:1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.1680264, 12.197583 ,  3.4832551], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.097919645132896}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5210746898452548
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.807001, 3.555319, 5.234974], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8904053258540823}
episode index:1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  4.       ,  3.5885696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5210547322569965
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2605217, 3.2274406, 4.599148 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7542845132886131}
episode index:1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.862479 ,  7.354994 ,  3.6678567], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.98802261345648}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5208928614598037
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0808024, 4.892609 , 4.2737193], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8943332107387574}
episode index:1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.000000e+00, 1.300000e+01, 8.751452e-04], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5211200285793878
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0569487, 3.9410284, 4.8748307], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4151589456753186}
episode index:1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  9.      ,  5.078487], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.521232596391303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4838715, 4.0182514, 4.2726727], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.127371962662487}
episode index:1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  3.1898909], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5210899253045806
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2280805, 3.4213257, 4.681582 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4790992080939484}
episode index:1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 13.       ,  5.7006707], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.19803902718557}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5211678029194066
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.266872 , 3.5867443, 4.1699085], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.396149479260561}
episode index:1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0967911, 7.385349 , 3.1737742], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.780532188324804}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5210648310638225
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2343373, 4.393162 , 6.0614796], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4127327106766516}
episode index:1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 5.9226155], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5212912776295764
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5016832, 4.825433 , 4.756609 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8922273708346795}
episode index:1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  0.       ,  2.7258852], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5212183110218916
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.203543 , 4.429565 , 5.1027846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4439825781513298}
episode index:1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.      , 4.      , 5.320421], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5214144049745483
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.620505 , 2.036334 , 4.5301805], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8853880437233042}
episode index:1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.4487   , 7.7376738, 3.0613306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4989602173452905}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5216402855812218
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4660196, 3.0862517, 5.22252  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4685547040275413}
episode index:1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  2.       ,  6.0863686], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5214936497786462
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1851754, 3.3543265, 3.0914235], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.39979644176145995}
episode index:1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  3.      ,  3.415966], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.000000000000002}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.521390838353982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7914515, 3.7836528, 1.8152132], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.955354186854016}
episode index:1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.411573 ,  5.9571934,  1.6660794], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.733728335296055}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5211045171906904
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.801317, 13.459079,  2.998737], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.669510372646384}
episode index:1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  4.3018513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5210428902188089
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7344613, 3.373406 , 4.4054475], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8239328876405826}
episode index:1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  2.       ,  4.0243773], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5208525155277818
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2871614, 4.312515 , 4.457621 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8383360395804371}
episode index:1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.989878  , 12.799033  ,  0.31733388], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.27138511775281}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5208050258409748
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.8084989, 3.8079326, 4.5273066], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.830317889535335}
episode index:1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      , 13.      ,  2.319717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5206836972741097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5878117, 3.393567 , 4.9889193], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5699072190238603}
episode index:1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 8.       , 2.1908178], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5206461068869204
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.594553, 3.281012, 5.479917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6576176999538166}
episode index:1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.0692697, 9.478195 , 3.013891 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.650226693882084}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5203611336483397
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.5665045, 6.7906156, 5.0157127], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.577740878557003}
episode index:1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  6.      ,  5.792523], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5202767087072154
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.776432, 4.163656, 4.060569], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.123630527184061}
episode index:1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 5.8291783], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5205070003642377
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.222445 , 3.4538283, 1.896194 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3039677767235447}
episode index:1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.305049 ,  2.8753781,  2.006679 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.305735809925325}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5204186076305315
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1413212, 1.4359206, 5.683831 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.784285205252765}
episode index:1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 10.       ,  2.0944479], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5205303289458386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.925679 , 4.110429 , 4.2545986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.22290183094778}
episode index:1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 10.       ,  5.5405536], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5202461966702132
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9942365, 9.711164 , 5.768665 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.711166949339507}
episode index:1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 8.       , 1.9002087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5204557634341732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0571756, 4.2805986, 5.0696154], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.590236038676007}
episode index:1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  6.       ,  5.1447124], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5205153969498597
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7985187, 4.6505184, 4.8963194], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4410818110169727}
episode index:1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9129896, 6.3211884, 4.564956 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.4443929449359114}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.520771246869778
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6191912, 4.3428855, 4.1626973], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4787627109518506}
episode index:1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 9.      , 3.144319], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5206506636151171
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5465288, 4.56421  , 5.0215287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.135259050410966}
episode index:1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 3.6388133], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5208645267527702
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.92061  , 1.6837213, 2.31568  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6062727936492056}
episode index:1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.      , 11.      ,  5.094491], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792298}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5210585727246124
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3046112, 4.6786737, 4.635104 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7060871977499017}
episode index:1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 9.       , 2.3939388], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5210113583440074
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7930164, 4.326288 , 3.4096584], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3423422245788874}
episode index:1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.981951 , 6.278496 , 4.1994953], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.8310136178894045}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5212662434753421
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00055  , 4.535839 , 4.2171335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.833003429048733}
episode index:1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 11.      ,  4.500383], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5212714793794593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2077775, 4.7169456, 3.0889344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.890904240863243}
episode index:1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 11.        ,  0.19643936], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5212825616794524
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6363024, 3.4025793, 5.1447496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4218795216772522}
episode index:1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.       , 8.       , 2.6580367], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5212877830474805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.952159 , 3.6768897, 5.084785 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0661810397253224}
episode index:1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 4.2560277], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5215004888307972
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4509861, 2.265131 , 2.380095 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7144901192534008}
episode index:1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.      , 1.      , 5.104525], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5216147451347667
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2788125, 2.537738 , 0.4441271], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7821819192222705}
episode index:1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.876976  , 3.6906235 , 0.64382964], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.701495367199443}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5218738920767305
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.876976  , 3.6906235 , 0.64382964], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.701495367199443}
episode index:1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 0.       , 5.5673714], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5219608875845452
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1963305, 1.8314245, 1.6005012], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4182571513808053}
episode index:1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 3.3958538], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.32455532033676}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5221727687316767
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7223997, 4.0214944, 4.4858704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0585427275343757}
episode index:1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.       ,  1.5276188], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5220883228541978
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.1438732, 3.3118634, 5.7451754], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9111596274518335}
episode index:1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 5.       , 5.4833927], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.522201952609225
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1074343, 1.3514453, 2.228493 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6520516532243064}
episode index:1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.684947 ,  8.879055 ,  1.4796228], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.19554757502194}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5223037091055313
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7486567, 4.524912 , 4.658793 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3201630700511537}
episode index:1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  7.      ,  5.845976], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5221916710191788
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3670635, 3.9144669, 3.629922 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8715584880768277}
episode index:1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  6.       ,  5.9340158], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5222636988386905
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4492407, 4.8019147, 3.9352496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8842060345342153}
episode index:1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 1.       , 2.7775414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5224898134290693
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.328563  , 2.4949646 , 0.49408802], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.421316631687083}
episode index:1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  4.      ,  3.423633], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5226359721731203
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6802031, 4.309818 , 3.330021 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.859431782872008}
episode index:1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 13.       ,  6.0369277], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5225791213691118
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0265622, 2.8623269, 5.54583  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9782342098386576}
episode index:1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 13.       ,  1.4784943], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5227165700702587
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.2115293, 3.8531554, 5.0818844], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4817817749571345}
episode index:1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  2.5237303], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.522837374981056
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9553766, 1.4826893, 4.3611426], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4750211359088055}
episode index:1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 1.       , 1.9470315], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5228949254147419
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7504554, 2.6888025, 2.137823 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7779026777615579}
episode index:1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  1.1097336], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5228771809779621
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4297194, 4.6129494, 3.384805 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7107967682885443}
episode index:1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 6.1664696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.52308218092102
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4137354, 4.218099 , 2.5842319], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2864456584768968}
episode index:1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 6.       , 6.2580667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.523240518599149
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6906204, 4.0240736, 4.5103793], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9765940787890348}
episode index:1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 0.       , 0.2005502], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5234166953315675
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.286509 , 4.785768 , 2.9989612], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.80860578697245}
episode index:1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  4.       ,  4.0742717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5234476769403585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.302527 , 3.7955585, 5.0238004], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5262665917747704}
episode index:1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 9.       , 2.0350199], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.32455532033676}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5233652543991839
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9467304, 2.0079746, 4.056092 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3712814258448405}
episode index:1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  9.       ,  6.2509146], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5230847799863226
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.4811106, 8.733155 , 2.4450245], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.246997582954174}
episode index:1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  6.       ,  3.8933637], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5230395377377011
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2618215, 4.747322 , 3.785582 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7668290699230695}
episode index:1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.453225 ,  5.923808 ,  1.7892647], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.89505529348739}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5227595379851648
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.114262 , 12.631157 ,  2.0466957], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.860491260020028}
episode index:1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.       , 12.       ,  2.8663464], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5227419514335431
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4566135, 4.022282 , 4.90165  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8512435018302378}
episode index:1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 8.       , 1.6034758], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5228060528510036
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.30786  , 4.061925 , 3.5607553], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1056501629502364}
episode index:1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 2.       , 3.3982992], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5230557556554659
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0655372, 1.4922072, 2.1513045], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5092164675962947}
episode index:1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.      , 6.      , 5.005397], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5232999032218894
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4571676, 4.075076 , 4.411934 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8804574878800018}
episode index:1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.167867 ,  7.8186684,  1.5515933], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.357091520837209}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.523461605688848
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.2907321, 4.981488 , 5.0402493], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.104603641254723}
episode index:1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.252579 ,  9.027109 ,  3.2710967], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.765052794818649}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5236320867922072
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5122552, 4.8106713, 5.206271 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3434835774513765}
episode index:1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       ,  7.       ,  3.7391372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.5234294803123999
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7689867, 2.8367925, 3.6691332], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7861152879613686}
episode index:1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       , 13.       ,  1.8998002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.62049935181331}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5234064056728791
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6843234, 3.5864406, 4.608638 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4404573478080231}
episode index:1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 7.       , 1.9109416], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.523365977760301
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.45792  , 4.3698177, 5.620034 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4443307856347232}
episode index:1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.      , 12.      ,  5.702436], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.5232125436024098
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6562705, 3.9808824, 5.520875 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.924931759121344}
episode index:1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  5.       ,  4.6410356], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.5230385568916172
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.083788 , 2.7719445, 4.0768924], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1075222580401047}
episode index:1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.239213, 4.948488, 2.850215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.309167650450239}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5232922597868875
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.239213, 4.948488, 2.850215], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.309167650450239}
episode index:1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.        ,  4.        ,  0.70578337], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.523130614390598
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6018867, 4.9185452, 2.9076765], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.010742032278114}
episode index:1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 11.       ,  2.9220645], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.5229321610384264
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7846973, 2.441461 , 4.21421  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9631799217836776}
episode index:1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.069309 , 7.4780493, 3.8374395], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.60394912849395}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5231645900607109
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1960791, 4.7096405, 3.9191344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4853573458664897}
episode index:1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.535006 ,  2.927114 ,  1.3181883], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.535201813523816}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.523260281729999
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9964153, 4.57809  , 3.842763 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8701740523401764}
episode index:1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.526008 ,  2.9295895,  1.7198343], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.52622271334312}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5233101649444353
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7322254, 4.237217 , 3.0748258], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4376577627991085}
episode index:1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      ,  2.      ,  5.369468], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405295}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5233170356020508
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7712481, 4.360049 , 4.7710986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8329113141707847}
episode index:1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 0.       , 4.8847337], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5234602773869166
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4984605 , 1.8652296 , 0.59017247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2394219406124434}
episode index:1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.966826, 9.000275, 4.548433], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.077668007945213}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5236969504391481
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.6302605, 3.156273 , 5.81186  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.637733313934257}
episode index:1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 13.       ,  2.0664697], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5238704606671288
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2754269, 3.9209642, 3.8333468], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9612674398982787}
episode index:1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.      ,  1.      ,  4.222352], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5235932805292097
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.920865 ,  8.355998 ,  3.4867306], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.163552467559885}
episode index:1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  9.       ,  5.3229337], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5236363332984368
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1885414, 4.959839 , 4.145711 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9688870585970752}
episode index:1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 11.       ,  1.1741881], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.523654636578167
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1217023, 1.9350841, 4.7226176], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1591776360011767}
episode index:1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.243901 ,  6.5661125,  0.2077797], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.75265762028686}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5233780097231336
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3656178, 8.252381 , 3.2174149], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.265091254803174}
episode index:1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  4.9616194], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5235249223840713
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1078396, 3.439404 , 4.2902822], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1917988208322823}
episode index:1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 1.5073231], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5234665713280027
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.843479, 4.787646, 4.846287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9766472248661637}
episode index:1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  5.       ,  4.8365183], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5233722388448906
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4948325, 3.261736 , 4.2630568], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5175737016171298}
episode index:1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 5.5587626], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5234634570732536
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.576324 , 1.2309357, 0.6141548], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.81909040123025}
episode index:1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 0.       , 1.5953285], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5237092613635206
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9509406, 1.9993982, 1.6253587], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1925722699316283}
episode index:1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 1.        , 0.07708961], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5238598767436458
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.972036 , 4.3789387, 1.6196167], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4063244586657873}
episode index:1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3836434, 4.0371404, 4.407496 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9204866155764724}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5241104768085175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3836434, 4.0371404, 4.407496 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9204866155764724}
episode index:1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  0.       ,  2.8127632], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5238347742957303
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.732222  , 11.603917  ,  0.21286717], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.92475160163849}
episode index:1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       , 13.       ,  1.6424807], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.523689403061029
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3813663, 4.652749 , 5.3803315], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7647343068066563}
episode index:1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 5.       , 3.1567097], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5238616407741313
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.553413 , 3.5404387, 5.177155 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5442434355719785}
episode index:1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.        , 12.        ,  0.03868762], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5239559611807624
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5671837, 4.8465195, 4.969778 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.896566390842686}
episode index:1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 6.       , 1.2362959], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5240731350039387
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.570212 , 3.6083813, 3.6755204], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8338281404645884}
episode index:1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  6.       ,  3.8913305], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5239941167171865
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2001667, 4.5178204, 2.4867663], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5309622296765029}
episode index:1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 6.      , 0.279361], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5241703433757728
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.053159 , 4.2997484, 5.031479 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6728688936783283}
episode index:1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 7.       , 2.4041314], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5241823907023005
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.818465 , 3.1645036, 5.4234595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.192931848584388}
episode index:1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.        , 12.        ,  0.89583504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5241074115128832
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4449196, 4.1350546, 4.326468 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2191400069842242}
episode index:1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.       , 10.       ,  5.8517184], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5240973478884194
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3680482, 4.6223183, 4.951162 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7410570498748834}
episode index:1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.       ,  5.3341975], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5240413685749944
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0891086, 4.5367637, 3.7918065], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4521722138502424}
episode index:1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  4.554488], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.5238878614880402
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9374107, 4.487116 , 3.7342355], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8277334530717975}
episode index:1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 0.       , 0.1519692], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5241161459357726
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.9984655 , 1.7329359 , 0.76169795], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.36628737188753}
episode index:1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 4.8347335], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5243492613245209
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.698311 , 1.022885 , 4.4755454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6063851618749987}
episode index:1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.      ,  7.      ,  4.967188], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.5241982782268269
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.792219 , 4.331604 , 4.3783846], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7977497222492373}
episode index:1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.116863 ,  8.326358 ,  2.2815151], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.157054403866464}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.524369081719973
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4431176, 3.5506217, 3.7155404], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5445946945198104}
episode index:1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  6.       ,  5.2257667], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5242271889611672
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.7476025, 4.7442284, 4.673071 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1472848375766325}
episode index:1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 13.      ,  5.918463], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5239538692588933
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.5772586, 13.2579   ,  5.6718926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.274129878955812}
episode index:1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.      , 10.      ,  2.905103], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5236808344130054
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.6001673 , 5.70442   , 0.26656556], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.7516340275860434}
episode index:1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.48202  ,  7.302205 ,  2.2538595], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.630735631357625}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.5235281547171229
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5260115, 3.750042 , 5.0652246], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9161064390452103}
episode index:1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.      , 7.      , 6.012377], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5236729243344169
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6300597, 4.213655 , 4.2589374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8302171510709027}
episode index:1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.      ,  6.      ,  4.196669], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5238133724922261
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7936244, 4.547175 , 3.7651772], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5608783276144453}
episode index:1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.320378 ,  9.085757 ,  2.8457115], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.741476009351029}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5235409786427762
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.4357767, 4.3484797, 3.3260098], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.8971777143477815}
episode index:1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.180533 ,  2.8244107,  2.0236454], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.18204753988843}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5235110097220521
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.714494 , 4.8090625, 4.760087 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.9450472286201033}
episode index:1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.507975,  7.668136,  4.404917], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.350690021263365}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5235039836111572
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0970562, 2.0286593, 5.041619 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.136515379101071}
episode index:1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       ,  3.       ,  6.1547947], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.523444438538889
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0527322, 4.673997 , 2.9127553], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5679013760585443}
episode index:1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.582504 ,  8.589076 ,  4.3766384], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.767976785705136}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5231728015702648
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.16201  , 12.391341 ,  2.4416282], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.442495734382412}
episode index:1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 8.       , 1.5792409], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5231824033723377
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.204475, 4.689085, 4.479414], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8670480184709892}
episode index:1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 11.       ,  5.6093583], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5230898319778199
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.7566528, 4.790248 , 5.485187 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.806711224603668}
episode index:1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 9.       , 2.3634326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5230460639311008
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.011117 , 4.3615274, 4.960442 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6959111203561967}
episode index:1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.       ,  9.       ,  0.6006954], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5231468844741702
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6139536, 4.281971 , 4.083469 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4214037424444042}
episode index:1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.502809 , 2.673946 , 4.4941926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5992730697361974}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5233937028569476
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.502809 , 2.673946 , 4.4941926], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5992730697361974}
episode index:1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.479187 ,  4.9309983,  1.5816135], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.655614245977157}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5235678697745805
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.004931, 3.246076, 4.206983], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0250443009148158}
episode index:1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.1463296, 11.005361 ,  6.1226945], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.006697868242794}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5237463485455186
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2680488, 4.3751874, 4.001133 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4010676344736896}
episode index:1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.        , 4.        , 0.04842308], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.523952549241065
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.135503 , 2.1964908, 4.2804317], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3910405223347049}
episode index:1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.584251 ,  7.2207155,  1.068714 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.394771489729095}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5238995439068262
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4118705, 3.5877385, 4.529575 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7176864585099955}
episode index:1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 2.       , 5.0809984], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5239959061594672
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.914512 , 2.0069838, 2.864921 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.349967891787281}
episode index:1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.       , 10.       ,  1.8951432], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5240885025417377
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4394403, 4.822931 , 3.2804434], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.875149189279227}
episode index:1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.061128, 12.509312,  3.742768], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.135107272907646}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5241209668426122
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8777099, 3.668386 , 4.4308205], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1032290921379238}
episode index:1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.507377 ,  0.6855364,  5.4467087], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.719717489523857}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5241722062385136
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4488344, 3.721746 , 4.5938597], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6186532724874314}
episode index:1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  0.       ,  2.4565163], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505136}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5240643427490455
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5654383, 4.6821365, 3.5380292], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7373621445272656}
episode index:1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  6.       ,  0.8253719], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.5238796868545633
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0607169, 4.8123856, 3.3453002], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.654347468394832}
episode index:1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.       , 6.       , 0.5160639], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.523764144242344
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3081472, 4.432832 , 4.637319 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4655925615598369}
episode index:1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 0.       , 0.6260091], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5239988849088859
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.7249794, 1.8639755, 0.6797587], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.065455286525269}
episode index:1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.      , 13.      ,  0.921986], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5239642375375928
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1553752, 4.725815 , 4.5315657], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.5260794301853373}
episode index:1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.233065 , 8.063016 , 3.1473353], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.030269319027484}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5241504763013023
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4049273, 3.502581 , 4.13007  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6454096550826394}
episode index:1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 7.      , 5.478696], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.524151218988115
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0210187, 2.0912936, 5.2606344], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.1776395898541754}
episode index:1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0838585, 3.4269369, 3.3788311], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.43509462524130327}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5243954945430492
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0838585, 3.4269369, 3.3788311], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.43509462524130327}
episode index:1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  4.1919775], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5241264357977732
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.727172  , 7.979756  , 0.12786692], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.589365359297337}
episode index:1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 10.       ,  5.2678204], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5242331931997509
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8125987, 3.284734 , 4.094128 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8348262923973444}
episode index:1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.94886 , 5.000654, 4.59821 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.792968540196462}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5244719255456249
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.720998, 3.013677, 5.666031], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7210521550413256}
episode index:1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 2.       , 3.5407686], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5246527908619433
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9069397 , 2.6029768 , 0.27247575], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1629308420085784}
episode index:1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       , 11.       ,  2.0753958], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5246728893768386
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1102152, 4.842244 , 4.3155947], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8455381028954783}
episode index:1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.562578 ,  7.390618 ,  2.7381287], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.368134044524247}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5245993859114997
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5809157, 4.1584835, 4.6017256], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2959734064418884}
episode index:1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  7.       ,  4.8477893], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5245998959379007
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0090346, 3.5259283, 3.730842 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1378714450161496}
episode index:1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.     ,  4.     ,  5.13022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5243316955821042
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4030664, 6.3765135, 2.0664425], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.4288734323468364}
episode index:1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.      , 10.      ,  5.537941], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.28010988928052}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5245546206277956
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4381037, 4.465537 , 4.457535 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0532756974197883}
episode index:1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.        , 0.        , 0.32552934], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5244380894565859
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.4997826, 4.34486  , 4.012687 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0144469118092396}
episode index:1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  9.       ,  1.8803911], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5245963720864958
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2060235, 4.9236846, 3.0858855], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6303829139504096}
episode index:1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  2.4352942], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5245136045588294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9361683, 2.4248285, 5.363618 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2093635015369117}
episode index:1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 12.       ,  3.4049466], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5246674316976754
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.468545 , 4.9948173, 3.418898 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4770789628394274}
episode index:1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.151327 ,  1.7631571,  2.1709015], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.219709315218443}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5245774345941049
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.3661699, 2.1304772, 4.994472 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8508026951393795}
episode index:1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      , 13.      ,  4.654113], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.5243872030609206
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8273187, 1.4759371, 4.714553 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.379466569078372}
episode index:1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([0.       , 9.       , 2.3737237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5242804932697392
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.6249398 , 4.999064  , 0.88523835], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.426323825525655}
episode index:1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([6.      , 1.      , 1.430003], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5244880173687912
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8399224, 4.811735 , 2.0662012], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.58218876282779}
episode index:1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.     , 11.     ,  3.02671], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5244511687588714
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.916326 , 3.1958218, 5.1692357], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.926305187929911}
episode index:1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.136487 ,  9.19602  ,  3.5283036], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.626627692886814}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5246443212277326
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3961039, 3.250567 , 4.6675496], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4687025370176629}
episode index:1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.0129056, 8.773162 , 2.688034 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.030846998933814}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5245875658160996
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4748175, 4.2769947, 3.1584218], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3807722912449347}
episode index:1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.195419 ,  4.99043  ,  1.1448972], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.38789613145776}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5246712943686912
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.0759983, 1.107252 , 5.393775 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.106246451591131}
episode index:1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       , 11.       ,  1.3060352], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5245383204056868
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8269506, 3.1415834, 3.9673872], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1815628985384283}
episode index:1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  6.       ,  5.6163816], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5245853786606521
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.0374988, 3.4795442, 5.6750126], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0202409359812803}
episode index:1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.       , 7.       , 0.3217157], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5244140226251831
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.224638 , 4.563013 , 3.0690374], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.744762485882025}
episode index:1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 3.       , 4.2876015], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5246449835868531
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.531457 , 1.0556576, 4.413441 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.4750409394521857}
episode index:1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 13.       ,  5.7892513], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5248282358098162
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.942878 , 4.4278073, 3.7773993], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.411100952221249}
episode index:1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       , 11.       ,  3.4384518], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.5246618881025823
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.065136 , 4.0477204, 4.3806705], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4041683963607716}
episode index:1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.      , 11.      ,  5.270322], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422205101855958}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5243963709527328
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.978559 , 7.537615 , 1.9862247], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.641931281962233}
episode index:1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.015109 , 11.245376 ,  3.1158054], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.499051813610604}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5246025768085518
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.982114 , 4.6563663, 4.169984 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.583084347681532}
episode index:1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.961767 , 3.3891919, 0.8971445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3910653065429869}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5248429192874151
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.961767 , 3.3891919, 0.8971445], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3910653065429869}
episode index:1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 12.       ,  4.0027056], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5247664267968476
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.6442544, 3.1399064, 4.2583356], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.38226784029283445}
episode index:1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.       , 12.       ,  3.0238492], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5249314224252809
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8209032, 4.0407543, 5.213989 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.572717030198766}
episode index:1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.       ,  8.       ,  0.6976265], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5250551537344076
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.9777827, 3.6397994, 3.9809704], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2059317440066326}
episode index:1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.004856 , 2.1392846, 3.410018 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3230898617756977}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5252947828193044
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.004856 , 2.1392846, 3.410018 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3230898617756977}
episode index:1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5507659, 2.9488943, 2.0303001], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4501349434378823}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5255341702208075
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.5507659, 2.9488943, 2.0303001], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4501349434378823}
episode index:1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.       , 13.       ,  4.6195517], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5254057529271035
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8975239, 4.359724 , 4.653469 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7505150879573215}
episode index:1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([18.      ,  6.      ,  1.107619], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.297058540778353}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5254713718024924
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1910877, 3.7638984, 4.3626757], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4150020095712892}
episode index:1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.     ,  5.     ,  4.93326], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5254254301886905
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.145441, 4.998083, 2.76302 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.3031220862358763}
episode index:1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.       ,  4.       ,  2.9333403], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5255852277544656
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6719427, 4.6842675, 4.849965 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8133571321375006}
episode index:1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.7416787, 10.44545  ,  3.3252237], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.646448057960595}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5253208488672652
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.480065  , 11.46142   ,  0.10640028], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.589890653234967}
episode index:1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.045664 ,  2.9994786,  0.6778372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.045663848646376}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5253733892304305
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4668684, 4.21908  , 4.4537535], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3054202694580548}
episode index:1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.006918  ,  0.26397312,  4.784392  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.271635984042572}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5253761692071767
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.57965  , 3.8822758, 4.067361 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8093381409927156}
episode index:1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.02866  ,  6.7482843,  1.0287379], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.775803478390372}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5253017646254169
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5729642, 1.9109144, 6.1701913], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.2306077209561705}
episode index:1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.       , 7.       , 2.8495543], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5254285314902631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1831408, 3.086331 , 4.6162033], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.20246866106475145}
episode index:1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.       , 0.       , 3.5683522], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.6055512754639887}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5255323322118708
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.8361299 , 1.8168342 , 0.92887545], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.6596610751225993}
episode index:1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.117025 , 10.325902 ,  3.4507504], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.213759841013577}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5253855579544667
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8578987, 1.0721647, 2.7351022], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6773749724505644}
episode index:1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.       ,  3.       ,  5.6765366], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5253487950934702
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9996212, 3.493053 , 3.9074562], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.49305310498950644}
episode index:1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([17.998476  ,  7.078082  ,  0.03429653], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 15.54300603695923}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5250855942943252
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.8335958, 9.704929 , 2.9549375], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.756549377579097}
episode index:1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.       , 7.       , 1.0463897], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.525240540797658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5531743, 3.220622 , 4.15553  ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5955466845328788}
episode index:1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.2152132e+01, 5.7944059e-03, 5.7187090e+00], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.62947495519924}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.5250683117747994
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4836783, 4.992902 , 3.792853 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.504174308327138}
episode index:1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.033231 , 12.3630705,  3.281446 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.470634762921257}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.525240236487653
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.1069437, 3.1396976, 4.274556 ], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8982037457676488}
episode index:1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 0.       , 12.       ,  0.7068458], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5249776163694091
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.4630405, 5.7216997, 3.1343372], dtype=float32), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1256829542247484}
episode index:0
done in step count: 4
reward sum = 1.0
episode index:1
done in step count: 119
reward sum = 1.0
episode index:2
done in step count: 26
reward sum = 1.0
episode index:3
done in step count: 0
reward sum = 1.0
episode index:4
done in step count: 116
reward sum = 1.0
episode index:5
done in step count: 72
reward sum = 1.0
episode index:6
done in step count: 43
reward sum = 1.0
episode index:7
done in step count: 0
reward sum = 1.0
episode index:8
done in step count: 40
reward sum = 1.0
episode index:9
done in step count: 27
reward sum = 1.0
episode index:10
done in step count: 13
reward sum = 1.0
episode index:11
done in step count: 34
reward sum = 1.0
episode index:12
done in step count: 75
reward sum = 1.0
episode index:13
done in step count: 74
reward sum = 1.0
episode index:14
done in step count: 43
reward sum = 1.0
episode index:15
done in step count: 43
reward sum = 1.0
episode index:16
done in step count: 13
reward sum = 1.0
episode index:17
done in step count: 17
reward sum = 1.0
episode index:18
done in step count: 11
reward sum = 1.0
episode index:19
done in step count: 18
reward sum = 1.0
episode index:20
reward sum = 0.0
episode index:21
done in step count: 125
reward sum = 1.0
episode index:22
reward sum = 0.0
episode index:23
done in step count: 41
reward sum = 1.0
episode index:24
done in step count: 7
reward sum = 1.0
episode index:25
done in step count: 129
reward sum = 1.0
episode index:26
done in step count: 76
reward sum = 1.0
episode index:27
done in step count: 34
reward sum = 1.0
episode index:28
done in step count: 152
reward sum = 1.0
episode index:29
done in step count: 1
reward sum = 1.0
episode index:30
done in step count: 46
reward sum = 1.0
episode index:31
done in step count: 60
reward sum = 1.0
episode index:32
done in step count: 104
reward sum = 1.0
episode index:33
done in step count: 17
reward sum = 1.0
episode index:34
done in step count: 124
reward sum = 1.0
episode index:35
done in step count: 83
reward sum = 1.0
episode index:36
done in step count: 37
reward sum = 1.0
episode index:37
done in step count: 101
reward sum = 1.0
episode index:38
reward sum = 0.0
episode index:39
reward sum = 0.0
episode index:40
done in step count: 47
reward sum = 1.0
episode index:41
done in step count: 159
reward sum = 1.0
episode index:42
done in step count: 123
reward sum = 1.0
episode index:43
done in step count: 60
reward sum = 1.0
episode index:44
done in step count: 28
reward sum = 1.0
episode index:45
done in step count: 0
reward sum = 1.0
episode index:46
done in step count: 8
reward sum = 1.0
episode index:47
done in step count: 92
reward sum = 1.0
episode index:48
done in step count: 86
reward sum = 1.0
episode index:49
done in step count: 197
reward sum = 1.0
episode index:50
done in step count: 16
reward sum = 1.0
episode index:51
done in step count: 55
reward sum = 1.0
episode index:52
done in step count: 34
reward sum = 1.0
episode index:53
done in step count: 135
reward sum = 1.0
episode index:54
done in step count: 5
reward sum = 1.0
episode index:55
done in step count: 9
reward sum = 1.0
episode index:56
done in step count: 120
reward sum = 1.0
episode index:57
done in step count: 89
reward sum = 1.0
episode index:58
done in step count: 43
reward sum = 1.0
episode index:59
done in step count: 40
reward sum = 1.0
episode index:60
done in step count: 81
reward sum = 1.0
episode index:61
done in step count: 10
reward sum = 1.0
episode index:62
done in step count: 164
reward sum = 1.0
episode index:63
done in step count: 55
reward sum = 1.0
episode index:64
done in step count: 30
reward sum = 1.0
episode index:65
done in step count: 26
reward sum = 1.0
episode index:66
done in step count: 15
reward sum = 1.0
episode index:67
done in step count: 115
reward sum = 1.0
episode index:68
done in step count: 85
reward sum = 1.0
episode index:69
done in step count: 22
reward sum = 1.0
episode index:70
done in step count: 11
reward sum = 1.0
episode index:71
done in step count: 99
reward sum = 1.0
episode index:72
done in step count: 24
reward sum = 1.0
episode index:73
done in step count: 15
reward sum = 1.0
episode index:74
done in step count: 89
reward sum = 1.0
episode index:75
done in step count: 111
reward sum = 1.0
episode index:76
done in step count: 174
reward sum = 1.0
episode index:77
done in step count: 120
reward sum = 1.0
episode index:78
done in step count: 138
reward sum = 1.0
episode index:79
done in step count: 27
reward sum = 1.0
episode index:80
reward sum = 0.0
episode index:81
reward sum = 0.0
episode index:82
done in step count: 62
reward sum = 1.0
episode index:83
done in step count: 201
reward sum = 1.0
episode index:84
done in step count: 6
reward sum = 1.0
episode index:85
done in step count: 87
reward sum = 1.0
episode index:86
done in step count: 63
reward sum = 1.0
episode index:87
done in step count: 158
reward sum = 1.0
episode index:88
done in step count: 21
reward sum = 1.0
episode index:89
done in step count: 7
reward sum = 1.0
episode index:90
done in step count: 47
reward sum = 1.0
episode index:91
done in step count: 146
reward sum = 1.0
episode index:92
done in step count: 34
reward sum = 1.0
episode index:93
done in step count: 81
reward sum = 1.0
episode index:94
done in step count: 83
reward sum = 1.0
episode index:95
done in step count: 99
reward sum = 1.0
episode index:96
done in step count: 42
reward sum = 1.0
episode index:97
done in step count: 9
reward sum = 1.0
episode index:98
done in step count: 38
reward sum = 1.0
episode index:99
done in step count: 46
reward sum = 1.0

Process finished with exit code 0
