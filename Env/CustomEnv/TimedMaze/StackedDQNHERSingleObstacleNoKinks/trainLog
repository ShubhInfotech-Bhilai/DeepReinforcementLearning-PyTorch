/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/TimedMaze/StackedDQNHERSingleObstacleNoKinks/StackedDQN_CNNTimedMazeGPU.py
episode index:0
target Thresh 3.799999999999999
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9336631, 5.234548 , 0.3846535], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5696012024771592
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2009771, 4.955919 , 4.2830844], dtype=float32)}
episode index:1
target Thresh 3.8758103162712287
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.241663 , 3.3359675, 5.1704764], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7848006012385795
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.241663 , 3.3359675, 5.1704764], dtype=float32)}
episode index:2
target Thresh 3.951242527012645
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0153735, 6.1841965, 6.1062255], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.6229938645964967
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9424987, 1.8353939, 2.6683588], dtype=float32)}
episode index:3
target Thresh 4.026298518033446
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9619552, 5.9975243, 0.6311006], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46724539844737256
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.082214 ,  6.501328 ,  5.6114364], dtype=float32)}
episode index:4
target Thresh 4.100980165737321
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.068688 , 6.991127 , 3.0772843], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.37379631875789804
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.942385, 13.140276,  6.007459], dtype=float32)}
episode index:5
target Thresh 4.175289337169344
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.9402933, 5.184298 , 5.8399363], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.41859886597903473
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6110997, 4.529104 , 4.407821 ], dtype=float32)}
episode index:6
target Thresh 4.249227890062675
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0494442, 7.212806 , 4.048694 ], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4829055726633124
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7132738, 4.329506 , 3.2303176], dtype=float32)}
episode index:7
target Thresh 4.322797672884988
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8809228, 6.0195117, 2.6043873], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5390505445687721
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0889393, 4.9930496, 3.5086105], dtype=float32)}
episode index:8
target Thresh 4.396000524884688
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.9935945, 5.478985 , 4.1519017], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5766583755054605
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0322201, 4.9984703, 4.9235244], dtype=float32)}
episode index:9
target Thresh 4.4688382761368795
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.311659 , 5.1326766, 1.0332763], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.5189925379549145
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.184982  ,  3.0526376 ,  0.28811353], dtype=float32)}
episode index:10
target Thresh 4.541312747589146
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.134221, 6.054187, 2.326914], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.48128523930718364
{'scaleFactor': 20, 'timeStep': 226, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9857562, 1.175617 , 3.5304759], dtype=float32)}
episode index:11
target Thresh 4.613425751107045
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6375268, 6.7058983, 1.7358873], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.4514805555429486
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2446915, 1.9106029, 5.7912235], dtype=float32)}
episode index:12
target Thresh 4.685179089519418
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.084909 , 5.8706117, 4.2073445], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4835778830218739
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.733367 , 3.9053988, 5.4187503], dtype=float32)}
episode index:13
target Thresh 4.756574556663468
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9941123 , 6.1665606 , 0.51678073], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4490366056631686
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.5223848, 13.191083 ,  1.9784018], dtype=float32)}
episode index:14
target Thresh 4.827613937429584
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.003514, 5.974234, 3.706167], dtype=float32)}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.4364398627108058
{'scaleFactor': 20, 'timeStep': 135, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4207473, 1.6472367, 2.3804164], dtype=float32)}
episode index:15
target Thresh 4.8982990078059965
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9081926, 4.9754767, 3.15973  ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47166237129138044
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9081926, 4.9754767, 3.15973  ], dtype=float32)}
episode index:16
target Thresh 4.968631534923135
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9308152, 6.869895 , 2.1251147], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.44391752592129924
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.455173 , 9.215365 , 1.3647486], dtype=float32)}
episode index:17
target Thresh 5.038613277097847
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7446144, 4.9905357, 1.137649 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4748109967034493
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7446144, 4.9905357, 1.137649 ], dtype=float32)}
episode index:18
target Thresh 5.10824598387733
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8755672, 7.885173 , 2.019822 ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.49742000082478377
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9411142, 4.495262 , 3.9977307], dtype=float32)}
episode index:19
target Thresh 5.177531396082881
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8886648, 8.088982 , 4.723586 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.47254900078354456
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.480114 ,  7.623348 ,  5.6438923], dtype=float32)}
episode index:20
target Thresh 5.246471245853414
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9701927, 7.8011756, 3.742589 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4500466674128996
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.22716913, 10.644357  ,  4.4882603 ], dtype=float32)}
episode index:21
target Thresh 5.315067256688763
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.102646, 5.951878, 3.013144], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.46947731993953906
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6968215, 3.219277 , 4.869077 ], dtype=float32)}
episode index:22
target Thresh 5.383321143492771
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7350355, 5.1012125, 3.6065006], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4916783060291243
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8447118, 4.849466 , 3.6541617], dtype=float32)}
episode index:23
target Thresh 5.451234612616163
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.104583, 4.254686, 5.153344], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5128583766112441
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.104583, 4.254686, 5.153344], dtype=float32)}
episode index:24
target Thresh 5.518809361899206
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9746988, 6.135874 , 5.0718017], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5307678819467944
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4152284, 4.8541045, 5.2262397], dtype=float32)}
episode index:25
target Thresh 5.586047080714148
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6512814, 7.952886 , 4.47678  ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5396746062698832
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.674632 , 1.4700336, 4.4031353], dtype=float32)}
episode index:26
target Thresh 5.652949450007468
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4730155, 7.838849 , 2.24058  ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.5196866578895171
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.642014  ,  0.47452664,  5.7636886 ], dtype=float32)}
episode index:27
target Thresh 5.719518142341877
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7674243, 5.7556434, 2.5029807], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.5011264201077485
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.3502245,  4.669608 ,  5.2726007], dtype=float32)}
episode index:28
target Thresh 5.785754821938149
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0047402, 7.630269 , 5.681698 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.48384619872472273
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.141744 , -0.2829193,  4.552119 ], dtype=float32)}
episode index:29
target Thresh 5.851661144716731
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8864611, 6.814173 , 2.4748445], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.49338609696073715
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0100222, 1.2149844, 5.598302 ], dtype=float32)}
episode index:30
target Thresh 5.91723875833912
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9756863, 5.8624473, 3.7200594], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5081475147974875
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4763057, 4.901821 , 4.6566257], dtype=float32)}
episode index:31
target Thresh 5.982489302249073
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3421814, 6.820159 , 5.657917 ], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5163317532664772
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.286738 , 3.647472 , 6.0059814], dtype=float32)}
episode index:32
target Thresh 6.047414407713585
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0839398, 7.803933 , 3.7395556], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5172658408156979
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1951995 , 2.3408387 , 0.58774656], dtype=float32)}
episode index:33
target Thresh 6.112015697863679
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8924623, 8.072333 , 3.7380588], dtype=float32)}
done in step count: 473
reward sum = 0.008618804795743068
running average episode reward sum: 0.5023056338739345
{'scaleFactor': 20, 'timeStep': 474, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3758726, 3.161126 , 3.4331427], dtype=float32)}
episode index:34
target Thresh 6.1762947877349665
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0723124, 7.890298 , 5.6283298], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.514054537119926
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3677166, 4.2697773, 5.0446105], dtype=float32)}
episode index:35
target Thresh 6.2402532843080465
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9533668, 9.886468 , 1.378092 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4997752444221503
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.661073  ,  0.47418737,  5.5768876 ], dtype=float32)}
episode index:36
target Thresh 6.303892786548666
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8782659, 7.8244348, 4.3354897], dtype=float32)}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.48768984040783586
{'scaleFactor': 20, 'timeStep': 294, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.910641 , 4.3587112, 4.902303 ], dtype=float32)}
episode index:37
target Thresh 6.367214885447689
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0964382, 7.3536944, 4.9425054], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.49659717681382926
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.183729 , 4.898432 , 5.6322093], dtype=float32)}
episode index:38
target Thresh 6.4302211640608915
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.871381 , 5.85294  , 6.1129107], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.49906818372633355
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9241753, 1.3131127, 4.8308945], dtype=float32)}
episode index:39
target Thresh 6.492913197548519
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8216667, 4.1464477, 5.824089 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5115914791331753
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8216667, 4.1464477, 5.824089 ], dtype=float32)}
episode index:40
target Thresh 6.555292553214675
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8390462, 6.8768477, 4.2573004], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5154300225340656
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.071692 , 1.6473882, 0.9815057], dtype=float32)}
episode index:41
target Thresh 6.617360790546498
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0412824, 5.9880567, 5.121102 ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.524690785688226
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2441761, 2.4499483, 5.448767 ], dtype=float32)}
episode index:42
target Thresh 6.679119461253155
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0172114, 7.241725 , 5.7390127], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5282033034410646
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.303873 , 1.1217313, 2.718591 ], dtype=float32)}
episode index:43
target Thresh 6.740570109304631
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0341911, 7.850977 , 1.6228023], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.5161986829083131
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.859399  , -0.02191847,  2.697344  ], dtype=float32)}
episode index:44
target Thresh 6.801714270970326
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.004416 , 9.070918 , 3.4337182], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.5047276010659061
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.78967 ,  4.038643,  2.164173], dtype=float32)}
episode index:45
target Thresh 6.862553474857468
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8776302, 6.837271 , 1.4889445], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5069075677196372
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9303321, 1.1892464, 2.029744 ], dtype=float32)}
episode index:46
target Thresh 6.923089241949322
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6246023, 8.058644 , 1.5128391], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.496122300321347
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.7952056, 7.7098994, 2.7743185], dtype=float32)}
episode index:47
target Thresh 6.983323085643219
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.085681 , 6.2235494, 1.0080965], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4997233307015206
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2873678, 3.5677319, 5.962437 ], dtype=float32)}
episode index:48
target Thresh 7.043256511788387
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.293802 , 10.483865 ,  3.2997968], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.49803756640012675
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0321243, 1.6799605, 4.5239887], dtype=float32)}
episode index:49
target Thresh 7.102891018723604
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.309276, 8.042646, 4.588607], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5010588676394923
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0600772, 2.126635 , 5.269799 ], dtype=float32)}
episode index:50
target Thresh 7.162228097314644
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8777355, 9.817272 , 4.4353323], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.504483773157547
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.517766 , 1.028194 , 5.3132067], dtype=float32)}
episode index:51
target Thresh 7.221269230991568
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9829915, 5.917632 , 0.8438224], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4947821621352865
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.239157 ,  4.0913286,  4.2957006], dtype=float32)}
episode index:52
target Thresh 7.280015895785792
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.331513 , 8.574952 , 5.6140428], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.4877792729277596
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3738666, 1.5752574, 4.871291 ], dtype=float32)}
episode index:53
target Thresh 7.338469560366993
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.701532,  8.948624,  5.788124], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.4813816472769138
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.102127 , 3.9315166, 5.391149 ], dtype=float32)}
episode index:54
target Thresh 7.3966316860798305
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.054713 , 9.6300125, 5.6934757], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4835193628985365
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5802324, 1.0478302, 1.1444246], dtype=float32)}
episode index:55
target Thresh 7.4545037269804775
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1078103, 9.543517 , 2.1248372], dtype=float32)}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.4761425751057976
{'scaleFactor': 20, 'timeStep': 265, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1856956, 4.7671566, 3.8651078], dtype=float32)}
episode index:56
target Thresh 7.512087129872971
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9826894, 3.531417 , 5.8582096], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4853330562442924
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9826894, 3.531417 , 5.8582096], dtype=float32)}
episode index:57
target Thresh 7.569383334345384
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0161505, 4.8352976, 3.053734 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49420662424008044
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0161505, 4.8352976, 3.053734 ], dtype=float32)}
episode index:58
target Thresh 7.626393772805808
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.068275 , 6.2545466, 1.8873837], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.48583024077838416
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.2116419, 11.43317  ,  3.542525 ], dtype=float32)}
episode index:59
target Thresh 7.683119870518176
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.591725 , 11.114537 ,  0.8509076], dtype=float32)}
done in step count: 422
reward sum = 0.014389712604426484
running average episode reward sum: 0.47797289864215153
{'scaleFactor': 20, 'timeStep': 423, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8469472, 3.8231626, 4.794061 ], dtype=float32)}
episode index:60
target Thresh 7.739563045637888
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7686248, 4.851965 , 5.4914856], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.48653071997588676
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7686248, 4.851965 , 5.4914856], dtype=float32)}
episode index:61
target Thresh 7.795724709247258
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.678535, 6.143066, 6.007266], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.481786439216501
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.338837 , 3.3193011, 0.5231864], dtype=float32)}
episode index:62
target Thresh 7.851606265390803
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5285816, 7.234315 , 3.419518 ], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.47848013305468695
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.699667 , 1.070607 , 2.4666429], dtype=float32)}
episode index:63
target Thresh 7.907209111110336
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4523463, 6.1880975, 4.36305  ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4864726309757074
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4289101, 4.196574 , 4.716528 ], dtype=float32)}
episode index:64
target Thresh 7.962534636479896
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1892922, 8.769736 , 3.390884 ], dtype=float32)}
done in step count: 307
reward sum = 0.045709317994222766
running average episode reward sum: 0.47969165692983845
{'scaleFactor': 20, 'timeStep': 308, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2872344, 4.852309 , 4.2855425], dtype=float32)}
episode index:65
target Thresh 8.017584224640503
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.00424  , 9.377351 , 3.4905035], dtype=float32)}
done in step count: 478
reward sum = 0.008196397602782058
running average episode reward sum: 0.47254778936427694
{'scaleFactor': 20, 'timeStep': 479, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.760593 , 4.737259 , 4.1806073], dtype=float32)}
episode index:66
target Thresh 8.07235925183472
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.593832 , 3.9052026, 5.829429 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.48042021041854144
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.593832 , 3.9052026, 5.829429 ], dtype=float32)}
episode index:67
target Thresh 8.126861087441089
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4466057, 7.442828 , 5.558394 ], dtype=float32)}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.47463412731089
{'scaleFactor': 20, 'timeStep': 244, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2371807, 4.6899767, 5.917961 ], dtype=float32)}
episode index:68
target Thresh 8.181091094008332
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.175172 , 12.65468  ,  2.2536118], dtype=float32)}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.4709327235725651
{'scaleFactor': 20, 'timeStep': 152, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4929866, 4.8912373, 4.8426714], dtype=float32)}
episode index:69
target Thresh 8.23505062728944
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.2896037, 11.099509 ,  1.5337873], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4642051132358142
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.517344  , -0.35123348,  3.8635893 ], dtype=float32)}
episode index:70
target Thresh 8.288741036275557
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.151657 ,  9.800892 ,  4.8191075], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4576670130493943
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.512093 ,  7.2699757,  5.2369413], dtype=float32)}
episode index:71
target Thresh 8.3421636632297
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.056801 ,  9.832492 ,  1.3555634], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4614814068911304
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4522967, 4.9628153, 4.764826 ], dtype=float32)}
episode index:72
target Thresh 8.395319843720328
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9132478, 7.2576447, 1.5919585], dtype=float32)}
done in step count: 372
reward sum = 0.02378441041510293
running average episode reward sum: 0.4554855576243355
{'scaleFactor': 20, 'timeStep': 373, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2100458, 4.2806926, 5.604142 ], dtype=float32)}
episode index:73
target Thresh 8.44821090665472
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.0148206, 9.928183 , 2.1626291], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.45537795838297707
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6264024, 1.9662718, 3.6189115], dtype=float32)}
episode index:74
target Thresh 8.500838174312209
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.513266 , 7.64389  , 4.5180807], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.454490037845527
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.521862 , 2.3786988, 2.6786234], dtype=float32)}
episode index:75
target Thresh 8.553202962377222
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7193453, 9.142224 , 4.441507 ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.46114932695282274
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9751387, 4.550692 , 4.475457 ], dtype=float32)}
episode index:76
target Thresh 8.605306579972192
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.679335 , 7.974019 , 3.4880834], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.46632996367604956
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0107923, 2.3272436, 4.8053255], dtype=float32)}
episode index:77
target Thresh 8.657150329690268
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8187504, 11.068565 ,  5.2015805], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4724216327238054
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6139994, 3.0098286, 4.5423474], dtype=float32)}
episode index:78
target Thresh 8.708735507627898
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.0541177 , 9.808675  , 0.53923506], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.47336764550439975
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6910398, 3.3197622, 3.9039874], dtype=float32)}
episode index:79
target Thresh 8.76006340341721
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.687142 , 6.2395263, 2.7474165], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.470938576042908
{'scaleFactor': 20, 'timeStep': 128, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5449758, 1.4022231, 4.237948 ], dtype=float32)}
episode index:80
target Thresh 8.81113530025828
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1134582, 7.8624935, 3.2433996], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4753241321884966
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5803113, 3.4576373, 5.0628133], dtype=float32)}
episode index:81
target Thresh 8.86195247495119
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9815147, 5.0617013, 1.1337295], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4786393512390222
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0623802, 3.9924345, 4.3223886], dtype=float32)}
episode index:82
target Thresh 8.912516197927946
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6696815, 7.466854 , 0.5307147], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4842157463976003
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.549063 , 3.6934624, 4.272831 ], dtype=float32)}
episode index:83
target Thresh 8.96282773328426
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.9163585, 9.897313 , 0.2507186], dtype=float32)}
done in step count: 428
reward sum = 0.013547628772652897
running average episode reward sum: 0.47861255452111284
{'scaleFactor': 20, 'timeStep': 429, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.307403 , 1.0325061, 4.2818904], dtype=float32)}
episode index:84
target Thresh 9.012888338811136
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.103007 , 10.732024 ,  1.8095698], dtype=float32)}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.4744509693180286
{'scaleFactor': 20, 'timeStep': 208, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.635136, 4.065616, 4.958717], dtype=float32)}
episode index:85
target Thresh 9.062699266026321
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.4997807, 11.828118 ,  1.3633199], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46893409758177246
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.410251 , 7.9088345, 4.821132 ], dtype=float32)}
episode index:86
target Thresh 9.11226176020559
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8578367, 7.0189753, 0.5768924], dtype=float32)}
done in step count: 318
reward sum = 0.040925300976303945
running average episode reward sum: 0.4640144562414797
{'scaleFactor': 20, 'timeStep': 319, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1393368, 2.4504192, 2.036124 ], dtype=float32)}
episode index:87
target Thresh 9.161577060413872
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9071246, 2.9689386, 4.543609 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47010520105691744
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9071246, 2.9689386, 4.543609 ], dtype=float32)}
episode index:88
target Thresh 9.21064639953625
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.946898 , 5.9531674, 3.0192823], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.47122313365714485
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2912383, 4.924496 , 2.5322096], dtype=float32)}
episode index:89
target Thresh 9.259471004308754
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.01807  , 9.928021 , 4.8986707], dtype=float32)}
done in step count: 372
reward sum = 0.02378441041510293
running average episode reward sum: 0.46625159228778884
{'scaleFactor': 20, 'timeStep': 373, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5505295, 4.838534 , 5.3905997], dtype=float32)}
episode index:90
target Thresh 9.308052095349044
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4357667, 4.4408636, 3.869206 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47211695940550547
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4357667, 4.4408636, 3.869206 ], dtype=float32)}
episode index:91
target Thresh 9.35639088718693
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.85653   , 11.089145  ,  0.87429667], dtype=float32)}
done in step count: 228
reward sum = 0.10111704470857531
running average episode reward sum: 0.4680843516370605
{'scaleFactor': 20, 'timeStep': 229, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.879877 , 3.4494987, 4.9568715], dtype=float32)}
episode index:92
target Thresh 9.404488588294726
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.1302147, 10.987584 ,  2.301742 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46305118656569433
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.893148 , 12.358665 ,  1.4497442], dtype=float32)}
episode index:93
target Thresh 9.45234640111746
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.1614475, 9.033907 , 4.703575 ], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4679415430323137
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4683845, 3.6688032, 5.1391435], dtype=float32)}
episode index:94
target Thresh 9.499965522102947
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.904065  , 10.909593  ,  0.96293783], dtype=float32)}
done in step count: 412
reward sum = 0.015911098861934425
running average episode reward sum: 0.46318332783052024
{'scaleFactor': 20, 'timeStep': 413, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1734252, 1.9179559, 5.587649 ], dtype=float32)}
episode index:95
target Thresh 9.547347141731692
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.272735 ,  9.003695 ,  4.8202677], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.45835850149895235
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.097608 ,  5.2084537,  3.239583 ], dtype=float32)}
episode index:96
target Thresh 9.59449244454666
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.45946  , 7.7990437, 4.673757 ], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.45829339993985463
{'scaleFactor': 20, 'timeStep': 80, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6369238, 1.5153229, 0.4974556], dtype=float32)}
episode index:97
target Thresh 9.641402609182865
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.185269 , 10.392826 ,  3.1758523], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.46257123282821294
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.448912 , 4.640652 , 5.1929817], dtype=float32)}
episode index:98
target Thresh 9.688078808396874
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4218621, 9.870664 , 3.3669057], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4673135976269885
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2206364, 4.7019844, 3.9667077], dtype=float32)}
episode index:99
target Thresh 9.734522209096099
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.0383143, 10.989802 ,  3.899902 ], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.46506462811116445
{'scaleFactor': 20, 'timeStep': 142, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.454896 , 1.2493228, 1.0759466], dtype=float32)}
episode index:100
target Thresh 9.78073397236797
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.04816 ,  9.95508 ,  6.146945], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46046002783283607
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.00388  , 12.84178  ,  0.5937008], dtype=float32)}
episode index:101
target Thresh 9.826715253508985
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.150033 , 9.810364 , 4.2333403], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4614741960938121
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4534122, 4.9331074, 1.1270498], dtype=float32)}
episode index:102
target Thresh 9.872467202053558
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9660387, 9.976496 , 4.4055524], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4618956688384068
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0987082, 1.6480191, 5.5826693], dtype=float32)}
episode index:103
target Thresh 9.917990961802792
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.203205, 8.028668, 4.993003], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4639517590328479
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.77976  , 4.03752  , 3.1154027], dtype=float32)}
episode index:104
target Thresh 9.963287670853045
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2942467, 5.884833 , 3.231903 ], dtype=float32)}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.4605059177944261
{'scaleFactor': 20, 'timeStep': 228, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0325539, 4.6323476, 0.656385 ], dtype=float32)}
episode index:105
target Thresh 10.008358461624411
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.9208345, 9.082409 , 5.5159364], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.46419414282556243
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9218433, 4.663288 , 3.7935274], dtype=float32)}
episode index:106
target Thresh 10.053204460889
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.13741 , 5.384899, 6.080923], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.46257072554971573
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8104546, 3.6970832, 5.0584188], dtype=float32)}
episode index:107
target Thresh 10.097826789799134
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.899494  , 10.838769  ,  0.50236905], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4582876632761072
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.806012  , 12.653409  ,  0.33329105], dtype=float32)}
episode index:108
target Thresh 10.142226563915358
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0269363, 7.930885 , 0.9815513], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4592566314153392
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3160851, 1.9772066, 5.1851473], dtype=float32)}
episode index:109
target Thresh 10.186404893234334
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7732577, 8.495258 , 3.9867048], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.46390247112974514
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5003743, 3.607569 , 4.543853 ], dtype=float32)}
episode index:110
target Thresh 10.230362882216603
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.078987 , 5.9275184, 0.3156193], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.46396268888160014
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1393613, 4.463858 , 3.9503913], dtype=float32)}
episode index:111
target Thresh 10.274101629814172
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.304142  , 10.1008835 ,  0.41425565], dtype=float32)}
done in step count: 388
reward sum = 0.020251421078849283
running average episode reward sum: 0.46000098113336124
{'scaleFactor': 20, 'timeStep': 389, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9712734, 1.4401987, 5.8375435], dtype=float32)}
episode index:112
target Thresh 10.317622229498014
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7049174, 4.1246014, 4.2619185], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.464779733512712
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7049174, 4.1246014, 4.2619185], dtype=float32)}
episode index:113
target Thresh 10.360925769285384
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.670058 , 9.999848 , 2.8886766], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4660097890708245
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3519325, 4.5544643, 4.249176 ], dtype=float32)}
episode index:114
target Thresh 10.404013331767038
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.962566  , 6.852973  , 0.67011005], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4657334546162244
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3653245, 1.1640564, 5.1265492], dtype=float32)}
episode index:115
target Thresh 10.446885994134272
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.18486  , 11.193238 ,  1.0958105], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.4643783771681074
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.818019, 4.983588, 5.009973], dtype=float32)}
episode index:116
target Thresh 10.48954482820589
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8260779, 9.927523 , 1.9096346], dtype=float32)}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.46272347017959364
{'scaleFactor': 20, 'timeStep': 131, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.888807 , 4.9087286, 3.1087937], dtype=float32)}
episode index:117
target Thresh 10.531990900454955
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.731563  , 9.860373  , 0.55032885], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4659456712236981
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8216276, 4.991813 , 4.05306  ], dtype=float32)}
episode index:118
target Thresh 10.574225272035491
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2215176, 6.049833 , 2.44954  ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4620301613814822
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.6206527, 12.562331 ,  0.5313307], dtype=float32)}
episode index:119
target Thresh 10.616248998808986
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.35437  , 9.93292  , 3.9804838], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4662657350366365
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6446033, 4.2722235, 4.7592587], dtype=float32)}
episode index:120
target Thresh 10.658063131370797
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.071352 , 7.107762 , 0.4135893], dtype=float32)}
done in step count: 247
reward sum = 0.08353972967320515
running average episode reward sum: 0.4631027101989222
{'scaleFactor': 20, 'timeStep': 248, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5273547, 4.90149  , 2.1368937], dtype=float32)}
episode index:121
target Thresh 10.699668715076417
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.884402 , 10.046497 ,  0.5402862], dtype=float32)}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4618105488524417
{'scaleFactor': 20, 'timeStep': 119, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2377932, 1.5002048, 2.1620004], dtype=float32)}
episode index:122
target Thresh 10.741066790067602
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.648767 , 9.808381 , 3.5895374], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4657876179666495
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.63828 , 4.213276, 4.843921], dtype=float32)}
episode index:123
target Thresh 10.782258391298386
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.790717 , 7.6375246, 5.7077947], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.46726596482472815
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1806395, 1.5990424, 2.1890953], dtype=float32)}
episode index:124
target Thresh 10.823244548560947
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.81003 ,  9.711108,  4.938017], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4635278371061303
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.413229  , -0.11567415,  1.0105749 ], dtype=float32)}
episode index:125
target Thresh 10.864026286511349
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.165333 , 9.319101 , 1.8357418], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.46097846925435226
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.010726 , 1.3912587, 6.1510158], dtype=float32)}
episode index:126
target Thresh 10.904604624695162
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9483433, 7.886168 , 2.0714169], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.46148727254782634
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5010953, 2.731347 , 4.8830767], dtype=float32)}
episode index:127
target Thresh 10.944980577572958
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.964956, 10.509799,  4.783848], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.46377814766232484
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1271709, 3.8077457, 5.175602 ], dtype=float32)}
episode index:128
target Thresh 10.985155154545662
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.029128 , 10.896564 ,  1.7701232], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.46645992844185147
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.795382 , 4.914613 , 5.2299733], dtype=float32)}
episode index:129
target Thresh 11.025129359979788
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3781765, 5.1414437, 1.0446994], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.47041100591537566
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2028363 , 4.8880105 , 0.29346114], dtype=float32)}
episode index:130
target Thresh 11.064904193232556
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.952053 ,  9.953468 ,  4.6663547], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.47229894123356575
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7296631, 4.968339 , 5.052269 ], dtype=float32)}
episode index:131
target Thresh 11.104480648676866
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.023072 ,  8.1306505,  3.810432 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46872091895149326
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.133509  , 13.1309805 ,  0.47947338], dtype=float32)}
episode index:132
target Thresh 11.143859715726173
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.382231 , 10.470233 ,  1.7756087], dtype=float32)}
done in step count: 403
reward sum = 0.017417403892223023
running average episode reward sum: 0.46532765943976945
{'scaleFactor': 20, 'timeStep': 404, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6504219, 4.4963207, 4.7710605], dtype=float32)}
episode index:133
target Thresh 11.183042378859195
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.330258 , 11.073917 ,  3.1753516], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.46771833467385493
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9798037, 2.3971512, 5.26662  ], dtype=float32)}
episode index:134
target Thresh 11.222029617644555
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.1235762, 12.777933 ,  2.2955875], dtype=float32)}
done in step count: 245
reward sum = 0.08523592457219176
running average episode reward sum: 0.46488513163606476
{'scaleFactor': 20, 'timeStep': 246, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8791423, 3.6173341, 5.203489 ], dtype=float32)}
episode index:135
target Thresh 11.260822406765254
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1570168, 6.710293 , 3.2123675], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.46450325325001934
{'scaleFactor': 20, 'timeStep': 89, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2242659, 3.1752644, 5.410935 ], dtype=float32)}
episode index:136
target Thresh 11.299421716043039
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9858046, 5.7522173, 5.2754016], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4683389959270265
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7561707, 3.714289 , 5.3023863], dtype=float32)}
episode index:137
target Thresh 11.33782851046265
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7630868, 8.204346 , 5.3315973], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4689501878200364
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6924424 , 1.2176516 , 0.56499475], dtype=float32)}
episode index:138
target Thresh 11.376043750195954
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8973905, 10.827901 ,  6.017045 ], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.47117228257959826
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7879841, 1.1763661, 4.2393785], dtype=float32)}
episode index:139
target Thresh 11.414068390625932
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.605686 , 8.924871 , 5.0994544], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4724432136209468
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9997797, 2.2623978, 4.0973597], dtype=float32)}
episode index:140
target Thresh 11.451903382370576
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.9539137, 10.027382 ,  2.174843 ], dtype=float32)}
done in step count: 267
reward sum = 0.06832772446471172
running average episode reward sum: 0.4695771463219664
{'scaleFactor': 20, 'timeStep': 268, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6787918, 4.143053 , 5.8003488], dtype=float32)}
episode index:141
target Thresh 11.489549671306646
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8794575, 6.91572  , 1.3643444], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4723270139861869
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5227568, 2.961236 , 5.717725 ], dtype=float32)}
episode index:142
target Thresh 11.52700819859333
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.006503 , 8.203332 , 4.7921495], dtype=float32)}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.4699993815021942
{'scaleFactor': 20, 'timeStep': 197, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3283247, 3.7025623, 3.7304711], dtype=float32)}
episode index:143
target Thresh 11.564279900695759
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8388801, 12.849083 ,  2.0611746], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.47038547251624535
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.005223, 4.975379, 2.35317 ], dtype=float32)}
episode index:144
target Thresh 11.60136570940843
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.000089 , 4.8169484, 1.4050261], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47403798649889195
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.000089 , 4.8169484, 1.4050261], dtype=float32)}
episode index:145
target Thresh 11.638266551878491
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.10087633,  5.5596323 ,  3.6536026 ], dtype=float32)}
done in step count: 462
reward sum = 0.009626311346295479
running average episode reward sum: 0.4708570846142851
{'scaleFactor': 20, 'timeStep': 463, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6865537, 2.8082957, 5.292875 ], dtype=float32)}
episode index:146
target Thresh 11.674983350628924
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9202936, 8.166441 , 1.1521773], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.46889857736050805
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0150032 , 2.9346743 , 0.23276079], dtype=float32)}
episode index:147
target Thresh 11.711517023581608
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1192625, 4.1949244, 4.67546  ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.47248710048645054
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1192625, 4.1949244, 4.67546  ], dtype=float32)}
episode index:148
target Thresh 11.747868484080277
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.943061, 10.40022 ,  2.494141], dtype=float32)}
done in step count: 296
reward sum = 0.0510525689892109
running average episode reward sum: 0.46965868081197243
{'scaleFactor': 20, 'timeStep': 297, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3815166 , 4.3400965 , 0.31585246], dtype=float32)}
episode index:149
target Thresh 11.784038640913332
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.619955,  8.446985,  4.168066], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.46790367318734205
{'scaleFactor': 20, 'timeStep': 158, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.120921 , 3.2327724, 4.7684107], dtype=float32)}
episode index:150
target Thresh 11.820028398336575
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.846464 ,  6.746504 ,  3.8502464], dtype=float32)}
done in step count: 335
reward sum = 0.03449770389516398
running average episode reward sum: 0.4650334349801091
{'scaleFactor': 20, 'timeStep': 336, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1155543, 3.8867357, 3.8993204], dtype=float32)}
episode index:151
target Thresh 11.855838656095822
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.3398783, 10.273043 ,  3.3301752], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4672478899443126
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.081837 , 3.2722554, 4.067253 ], dtype=float32)}
episode index:152
target Thresh 11.891470309449378
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.077143 ,  8.274985 ,  5.5886354], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4678062924751497
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4583936, 4.104268 , 4.954529 ], dtype=float32)}
episode index:153
target Thresh 11.926924249190435
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7583268, 7.1641507, 6.047256 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.46476858927725917
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.122009 ,  4.9800878,  4.848322 ], dtype=float32)}
episode index:154
target Thresh 11.962201361669333
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9934676, 6.263799 , 3.2778673], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.46595784114236327
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.935545 , 4.7913404, 4.8958645], dtype=float32)}
episode index:155
target Thresh 11.99730252881572
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.2737062, 6.3083463, 2.2624254], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.46692791998723454
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3241556, 3.9978337, 4.9022794], dtype=float32)}
episode index:156
target Thresh 12.032228628160603
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.95651  , 11.126718 ,  6.2291417], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.46486027392223367
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8936198 , 4.894553  , 0.12070973], dtype=float32)}
episode index:157
target Thresh 12.066980532858285
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.035685 , 10.0844345,  1.6100128], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.4643541714220517
{'scaleFactor': 20, 'timeStep': 96, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7830315, 1.8569936, 4.1258383], dtype=float32)}
episode index:158
target Thresh 12.101559111708191
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.841101 ,  7.9098845,  6.0476193], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.46608590854133614
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2452493, 4.4554424, 4.988814 ], dtype=float32)}
episode index:159
target Thresh 12.135965229176598
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.130451  , 5.6470003 , 0.69685435], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4683364255119252
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.848715 , 3.9252262, 4.243944 ], dtype=float32)}
episode index:160
target Thresh 12.170199745418232
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.028358 , 11.14067  ,  0.5371992], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4690736948064939
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0544176, 4.8441315, 4.863312 ], dtype=float32)}
episode index:161
target Thresh 12.204263516297782
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.03765  ,  7.4933476,  2.5088534], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.47060861355829503
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0989021, 2.9138944, 5.360687 ], dtype=float32)}
episode index:162
target Thresh 12.238157393411294
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.031092 ,  7.9031057,  1.4090183], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.4682603705867113
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2412575, 4.598242 , 1.278672 ], dtype=float32)}
episode index:163
target Thresh 12.271882224107463
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.8555   ,  3.0449662,  3.131823 ], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.4661589599985994
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.151341 , 4.2917843, 4.028404 ], dtype=float32)}
episode index:164
target Thresh 12.305438851508809
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.802278 , 9.981599 , 5.4823475], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4633337541804261
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.32256156, 0.55257064, 5.3910027 ], dtype=float32)}
episode index:165
target Thresh 12.338828114532772
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.014641, 10.796566,  2.972842], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4605425869865681
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.864305  ,  0.07106614,  5.0799108 ], dtype=float32)}
episode index:166
target Thresh 12.372050847912657
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.0803902 , 11.177652  ,  0.03999704], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.4589253347277565
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.248345 , 3.8019767, 5.8363853], dtype=float32)}
episode index:167
target Thresh 12.405107882218537
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.53496  , 12.215632 ,  2.2818484], dtype=float32)}
done in step count: 479
reward sum = 0.008114433626754238
running average episode reward sum: 0.45624193650691724
{'scaleFactor': 20, 'timeStep': 480, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7972765, 1.6585194, 1.0697696], dtype=float32)}
episode index:168
target Thresh 12.438000043877988
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.752187, 7.082934, 5.353309], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.45922628013705385
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2528334, 1.5638151, 3.5572274], dtype=float32)}
episode index:169
target Thresh 12.470728155196763
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.850696 , 10.827456 ,  1.8950074], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.45952486523183333
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1263094, 4.2047296, 4.093432 ], dtype=float32)}
episode index:170
target Thresh 12.503293034379356
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7724214, 8.392663 , 2.2133992], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.45885286124420577
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7364511, 3.637518 , 4.487498 ], dtype=float32)}
episode index:171
target Thresh 12.535695495549433
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.111607 , 11.01765  ,  2.7194662], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.458356416007055
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0131403, 3.9890556, 1.8190508], dtype=float32)}
episode index:172
target Thresh 12.567936348770218
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.002519  , 7.880604  , 0.48353398], dtype=float32)}
done in step count: 348
reward sum = 0.030272460413199778
running average episode reward sum: 0.4558819422752986
{'scaleFactor': 20, 'timeStep': 349, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.356945 , 3.0672433, 5.240253 ], dtype=float32)}
episode index:173
target Thresh 12.600016400064717
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5410852, 8.901013 , 5.588074 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4532619311127969
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.946538  ,  0.10643455,  4.663661  ], dtype=float32)}
episode index:174
target Thresh 12.631936451435887
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.955124 ,  4.371566 ,  5.5169096], dtype=float32)}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.4509817249813184
{'scaleFactor': 20, 'timeStep': 291, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.800712, 2.682825, 3.651867], dtype=float32)}
episode index:175
target Thresh 12.663697300886673
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.94809294, 3.3700833 , 4.334279  ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4527508215118058
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7281964 , 3.3904054 , 0.38451666], dtype=float32)}
episode index:176
target Thresh 12.695299742439962
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7634287, 5.2244735, 5.118922 ], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4530453699144909
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9857984 , 4.339248  , 0.31061924], dtype=float32)}
episode index:177
target Thresh 12.726744566158446
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.329824 ,  6.6984463,  2.5260205], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.45152801681558397
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6547426, 2.3624368, 5.4824066], dtype=float32)}
episode index:178
target Thresh 12.758032558164349
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.7587404, 10.390357 ,  3.4620867], dtype=float32)}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.44964925883736123
{'scaleFactor': 20, 'timeStep': 216, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.022655 , 1.7730324, 1.7030901], dtype=float32)}
episode index:179
target Thresh 12.789164500659101
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6367226, 8.1866455, 5.315469 ], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.450252685946864
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.766937 , 4.9577894, 3.223416 ], dtype=float32)}
episode index:180
target Thresh 12.820141171942895
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.833467 ,  4.227668 ,  2.9569044], dtype=float32)}
done in step count: 403
reward sum = 0.017417403892223023
running average episode reward sum: 0.4478613307973909
{'scaleFactor': 20, 'timeStep': 404, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.835678 , 1.6522073, 4.0408425], dtype=float32)}
episode index:181
target Thresh 12.850963346434114
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1907313, 6.9534364, 6.195007 ], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.44588321914020823
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.157399 , 4.370167 , 3.7033038], dtype=float32)}
episode index:182
target Thresh 12.881631794688733
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.813721 , 10.897535 ,  0.5177762], dtype=float32)}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.4449262302897808
{'scaleFactor': 20, 'timeStep': 131, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.385925 , 3.4622006, 4.5235596], dtype=float32)}
episode index:183
target Thresh 12.912147283419554
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.675214 ,  4.022805 ,  2.8473108], dtype=float32)}
done in step count: 401
reward sum = 0.017771047742294686
running average episode reward sum: 0.44260473473245754
{'scaleFactor': 20, 'timeStep': 402, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8182105, 4.9987297, 3.684904 ], dtype=float32)}
episode index:184
target Thresh 12.942510575515385
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.10644 ,  8.980302,  5.847822], dtype=float32)}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.4410050526075908
{'scaleFactor': 20, 'timeStep': 192, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4262514, 2.2253246, 4.657805 ], dtype=float32)}
episode index:185
target Thresh 12.972722430060108
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0582488, 5.2575955, 4.001506 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.44395663834625965
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3189361, 4.5204906, 2.9573684], dtype=float32)}
episode index:186
target Thresh 13.002783602351663
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.486466 ,  0.1285342,  5.1189094], dtype=float32)}
done in step count: 314
reward sum = 0.04260407137887648
running average episode reward sum: 0.4418103679346693
{'scaleFactor': 20, 'timeStep': 315, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1422095, 4.519411 , 4.3568273], dtype=float32)}
episode index:187
target Thresh 13.03269484392092
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.269106 , 11.121937 ,  4.1201363], dtype=float32)}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4408162157685078
{'scaleFactor': 20, 'timeStep': 137, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5337334, 2.266685 , 1.9442447], dtype=float32)}
episode index:188
target Thresh 13.062456902550478
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.052097 , 4.0309954, 4.7902403], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.44377486012952105
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.052097 , 4.0309954, 4.7902403], dtype=float32)}
episode index:189
target Thresh 13.092070522293353
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.182434 ,  8.817817 ,  4.0413156], dtype=float32)}
done in step count: 233
reward sum = 0.09616130339314856
running average episode reward sum: 0.4419453150940664
{'scaleFactor': 20, 'timeStep': 234, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.891232 , 2.6871374, 3.3084009], dtype=float32)}
episode index:190
target Thresh 13.12153644349158
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7179046, 5.474576 , 0.391024 ], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4419745187520232
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.914885, 4.290829, 5.871985], dtype=float32)}
episode index:191
target Thresh 13.150855402794724
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.82818  ,  6.7069764,  4.868166 ], dtype=float32)}
done in step count: 351
reward sum = 0.02937333806646733
running average episode reward sum: 0.43982555426928593
{'scaleFactor': 20, 'timeStep': 352, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8604871, 1.0037549, 3.2007337], dtype=float32)}
episode index:192
target Thresh 13.180028133178295
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.2492046, 10.738208 ,  3.1187072], dtype=float32)}
done in step count: 206
reward sum = 0.12613920430197118
running average episode reward sum: 0.4382002363938077
{'scaleFactor': 20, 'timeStep': 207, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7827387, 4.694195 , 4.3723936], dtype=float32)}
episode index:193
target Thresh 13.209055363962072
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.842407  ,  6.723435  ,  0.03349226], dtype=float32)}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.43656605381629854
{'scaleFactor': 20, 'timeStep': 211, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.901808 , 1.6445997, 4.5148096], dtype=float32)}
episode index:194
target Thresh 13.237937820828337
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.8664827, 11.047208 ,  5.530882 ], dtype=float32)}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4362618004476176
{'scaleFactor': 20, 'timeStep': 98, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1967   , 3.2691658, 4.8184147], dtype=float32)}
episode index:195
target Thresh 13.266676225840015
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6073371, 8.316278 , 2.132521 ], dtype=float32)}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.43596065170514764
{'scaleFactor': 20, 'timeStep': 98, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.947712 , 1.3768315, 1.8872843], dtype=float32)}
episode index:196
target Thresh 13.295271297458726
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.018383 ,  2.7414806,  5.7269425], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.4359741382539028
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9137836, 3.455093 , 4.963209 ], dtype=float32)}
episode index:197
target Thresh 13.323723750562753
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.1277869, 11.2399645,  3.0962696], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4360324669180943
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1687405, 2.7144792, 5.706548 ], dtype=float32)}
episode index:198
target Thresh 13.352034296464904
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0868256 , 5.9712353 , 0.78827035], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.43825100237578707
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1986434, 4.857699 , 4.6780934], dtype=float32)}
episode index:199
target Thresh 13.380203642930304
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.124079 ,  4.981155 ,  3.3917804], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.4378534888523176
{'scaleFactor': 20, 'timeStep': 103, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2775692, 4.067384 , 4.045958 ], dtype=float32)}
episode index:200
target Thresh 13.408232494194078
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.8913755, 9.978067 , 5.2866125], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.437389601760254
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.756198 , 4.3340282, 4.5172863], dtype=float32)}
episode index:201
target Thresh 13.436121550978966
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9041753, 11.353281 ,  2.8824372], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4383110660826844
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8604207, 3.8899984, 5.14057  ], dtype=float32)}
episode index:202
target Thresh 13.463871510512845
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.954822 ,  8.017361 ,  3.3600051], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4394473256515859
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6234345, 4.504569 , 4.483093 ], dtype=float32)}
episode index:203
target Thresh 13.491483066546145
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0240743, 8.032599 , 0.5053002], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.44028884483727837
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4305863, 2.2721882, 1.8889905], dtype=float32)}
episode index:204
target Thresh 13.518956909369209
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.066415 , 10.947663 ,  1.2486079], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.43814109437465754
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.94540465, 7.030152  , 4.6065664 ], dtype=float32)}
episode index:205
target Thresh 13.546293725829536
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.080025  , 10.065704  ,  0.36843225], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.43979002770001907
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7019935, 3.9094682, 4.2473207], dtype=float32)}
episode index:206
target Thresh 13.573494199348959
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.059786 , 10.8777075,  1.4984479], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4376654381942219
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.433231 ,  6.0942764,  6.0056024], dtype=float32)}
episode index:207
target Thresh 13.600559009940739
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0071108 , 6.0926743 , 0.36866117], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.43791651913080837
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0806776, 4.794634 , 4.3273067], dtype=float32)}
episode index:208
target Thresh 13.62748883422654
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.125601 , 10.964068 ,  0.7981555], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4391200242348192
{'scaleFactor': 20, 'timeStep': 38, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1653936, 1.522192 , 5.246459 ], dtype=float32)}
episode index:209
target Thresh 13.654284345453384
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.134158, 11.051524,  3.051074], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4408848234918975
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3014767, 3.1862059, 5.884136 ], dtype=float32)}
episode index:210
target Thresh 13.680946213510438
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.061855, 10.034882,  1.846245], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.44178026807820714
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0128227, 2.809908 , 4.491598 ], dtype=float32)}
episode index:211
target Thresh 13.707475104945797
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.352949 , 8.986499 , 1.0942132], dtype=float32)}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.44038820804780104
{'scaleFactor': 20, 'timeStep': 192, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0475206, 1.5870618, 3.0546927], dtype=float32)}
episode index:212
target Thresh 13.733871682983127
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9097542 , 9.115392  , 0.20396927], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.44057484301739547
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.079202 , 1.075592 , 1.2489237], dtype=float32)}
episode index:213
target Thresh 13.760136607538252
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.91332  , 11.784665 ,  2.5022676], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.44094757983810995
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6128597, 4.2077823, 5.131253 ], dtype=float32)}
episode index:214
target Thresh 13.786270535235658
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1691244, 9.10187  , 1.858819 ], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4423027230465578
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9992056, 2.8202488, 4.1643243], dtype=float32)}
episode index:215
target Thresh 13.812274119424893
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.645167 ,  5.3915176,  5.3864136], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.44245567359729465
{'scaleFactor': 20, 'timeStep': 75, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.124092, 3.862979, 4.014501], dtype=float32)}
episode index:216
target Thresh 13.838148010196925
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.431953 , 7.2345304, 1.4624846], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4404167073595191
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.536571 , 11.493574 ,  0.2701695], dtype=float32)}
episode index:217
target Thresh 13.863892854400362
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8509033, 10.552216 ,  1.2528946], dtype=float32)}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.4388034762822521
{'scaleFactor': 20, 'timeStep': 242, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.685117 , 1.6539961, 4.783902 ], dtype=float32)}
episode index:218
target Thresh 13.889509295657653
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.4958378, 12.961542 ,  1.7973351], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4387825988873186
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0534444, 1.0294114, 5.4989505], dtype=float32)}
episode index:219
target Thresh 13.914997974381166
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.969838 ,  2.1348228,  3.5623531], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.4377263486065463
{'scaleFactor': 20, 'timeStep': 158, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2587811, 1.8017278, 4.322416 ], dtype=float32)}
episode index:220
target Thresh 13.940359527789191
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.877173 ,  7.6824546,  3.471459 ], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.438053314206741
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0101886, 2.6948187, 5.39555  ], dtype=float32)}
episode index:221
target Thresh 13.965594589921889
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.666979 , 10.143652 ,  3.3426447], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4401950436359162
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2955272, 4.628089 , 4.695831 ], dtype=float32)}
episode index:222
target Thresh 13.990703791657126
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.809098 , 8.842328 , 5.8082867], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4416054662528118
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9753647, 1.1239815, 4.887123 ], dtype=float32)}
episode index:223
target Thresh 14.015687760726253
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.228644 ,  9.885971 ,  1.5442308], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.44140489634285285
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4322038, 3.7428622, 4.114148 ], dtype=float32)}
episode index:224
target Thresh 14.0405471217298
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.017616 ,  6.2876263,  1.559277 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4394430968035513
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.246462 ,  1.6930611,  5.757415 ], dtype=float32)}
episode index:225
target Thresh 14.065282496153083
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.480381 ,  3.7578292,  1.755163 ], dtype=float32)}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.4376367074594283
{'scaleFactor': 20, 'timeStep': 346, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.257953 , 4.486872 , 3.9719622], dtype=float32)}
episode index:226
target Thresh 14.089894502381755
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.173677, 9.444321, 5.653151], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.43957452382744383
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0065846, 3.7318602, 4.710684 ], dtype=float32)}
episode index:227
target Thresh 14.11438375571725
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0318038, 8.0985   , 0.7641862], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.44149534180626626
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.747782 , 3.2068443, 3.89103  ], dtype=float32)}
episode index:228
target Thresh 14.13875086839218
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5732276, 7.3069143, 4.666002 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4438473272132258
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.157609 , 3.6169767, 4.909134 ], dtype=float32)}
episode index:229
target Thresh 14.162996449585629
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.969422  ,  7.0457563 ,  0.48657632], dtype=float32)}
done in step count: 261
reward sum = 0.07257479035344933
running average episode reward sum: 0.4422330987920963
{'scaleFactor': 20, 'timeStep': 262, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8646793, 3.5617723, 5.3509326], dtype=float32)}
episode index:230
target Thresh 14.187121105438388
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.080575  , 8.975576  , 0.37162477], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.44207075625480724
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2443444, 2.585638 , 5.497381 ], dtype=float32)}
episode index:231
target Thresh 14.211125439068116
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.098828 , 10.081296 ,  1.2707633], dtype=float32)}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.44107303078041893
{'scaleFactor': 20, 'timeStep': 156, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9836583, 4.852231 , 6.1277456], dtype=float32)}
episode index:232
target Thresh 14.2350100505844
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.254683,  8.91993 ,  6.172197], dtype=float32)}
done in step count: 315
reward sum = 0.04217803066508771
running average episode reward sum: 0.43936103507176943
{'scaleFactor': 20, 'timeStep': 316, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.93966  , 4.3076897, 1.8465856], dtype=float32)}
episode index:233
target Thresh 14.25877553710377
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.0109954, 11.070831 ,  3.0812232], dtype=float32)}
done in step count: 385
reward sum = 0.02087132015888843
running average episode reward sum: 0.43757261748667164
{'scaleFactor': 20, 'timeStep': 386, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1275933, 3.7433553, 4.908141 ], dtype=float32)}
episode index:234
target Thresh 14.282422492764628
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.321448  ,  0.14767969,  4.449822  ], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.43589744970368616
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0090408, 1.4458365, 3.1998377], dtype=float32)}
episode index:235
target Thresh 14.305951508742098
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6841488, 6.029647 , 4.1605587], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.43824534186595865
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8434052, 4.14969  , 3.5452695], dtype=float32)}
episode index:236
target Thresh 14.329363173262806
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.428746 , 9.802031 , 1.2876863], dtype=float32)}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.4367451688301381
{'scaleFactor': 20, 'timeStep': 249, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5036044, 4.798006 , 6.1973505], dtype=float32)}
episode index:237
target Thresh 14.352658071619581
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.117152 ,  8.008898 ,  1.2310382], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.43491010509555766
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.673063 ,  8.282554 ,  1.0875463], dtype=float32)}
episode index:238
target Thresh 14.375836786186097
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.208328 , 6.1305647, 4.9372487], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.43679912085547634
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.89084   , 4.2207894 , 0.38443935], dtype=float32)}
episode index:239
target Thresh 14.398899896431429
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.2790718, 10.224845 ,  4.3563604], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.43664864390587665
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.452994 , 1.830143 , 1.9643023], dtype=float32)}
episode index:240
target Thresh 14.421847978934528
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.4387387, 6.8942404, 3.015072 ], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.43847798987721726
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4089216, 3.981382 , 5.303838 ], dtype=float32)}
episode index:241
target Thresh 14.444681607398657
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9185725, 5.1034813, 5.9238605], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4407570064479725
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.976974  , 4.7503967 , 0.01180905], dtype=float32)}
episode index:242
target Thresh 14.467401352665712
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.702122 ,  4.1829715,  5.855257 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4389431916066228
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.27092054, 10.49571   ,  4.3393955 ], dtype=float32)}
episode index:243
target Thresh 14.490007782730512
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.521829 ,  9.703116 ,  1.5468125], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.43994159899762186
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7585206, 4.985871 , 3.7992394], dtype=float32)}
episode index:244
target Thresh 14.512501462754983
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.501976 , 12.257654 ,  2.6237664], dtype=float32)}
done in step count: 447
reward sum = 0.011192625819338041
running average episode reward sum: 0.4381916031887309
{'scaleFactor': 20, 'timeStep': 448, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7409177, 2.936852 , 3.6849313], dtype=float32)}
episode index:245
target Thresh 14.5348829550823
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8256142, 5.959898 , 2.0173311], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.438193334483939
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5469124, 1.4548262, 2.0196261], dtype=float32)}
episode index:246
target Thresh 14.557152819250938
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.094664,  4.073402,  5.212937], dtype=float32)}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.43742064359895894
{'scaleFactor': 20, 'timeStep': 140, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0863085, 2.267701 , 5.6656337], dtype=float32)}
episode index:247
target Thresh 14.579311612008656
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.019035 ,  8.111015 ,  5.1405396], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4356568506812212
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.499455, 8.371581, 4.690289], dtype=float32)}
episode index:248
target Thresh 14.601359887326431
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.790842 , 10.043096 ,  3.4958363], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4375392813010106
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2334671, 4.9881883, 4.2316704], dtype=float32)}
episode index:249
target Thresh 14.623298196412293
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3002353, 6.38569  , 3.1900356], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4388080013246211
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5759244, 4.9368773, 6.012823 ], dtype=float32)}
episode index:250
target Thresh 14.64512708772511
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.969742 , 9.8448925, 0.185409 ], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.4385476677753368
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0731598, 3.7072487, 5.0746665], dtype=float32)}
episode index:251
target Thresh 14.666847106988305
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1279984, 5.2094455, 3.045937 ], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.43813429154166456
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8401442, 4.394956 , 3.9045758], dtype=float32)}
episode index:252
target Thresh 14.68845879720349
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.723352 ,  3.1313214,  5.417026 ], dtype=float32)}
done in step count: 312
reward sum = 0.04346910660022087
running average episode reward sum: 0.43657435009920825
{'scaleFactor': 20, 'timeStep': 313, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3338583, 3.569323 , 5.1957054], dtype=float32)}
episode index:253
target Thresh 14.709962698664045
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.129711 ,  3.144772 ,  3.9965131], dtype=float32)}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.4350846013664425
{'scaleFactor': 20, 'timeStep': 284, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0130349, 2.666201 , 4.550585 ], dtype=float32)}
episode index:254
target Thresh 14.731359348968628
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.264444 , 11.128225 ,  2.8410773], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4347029143057855
{'scaleFactor': 20, 'timeStep': 109, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1905093, 4.142564 , 4.802685 ], dtype=float32)}
episode index:255
target Thresh 14.752649283034609
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.025134  ,  5.2790594 ,  0.19371575], dtype=float32)}
done in step count: 423
reward sum = 0.014245815478382218
running average episode reward sum: 0.4330605037634909
{'scaleFactor': 20, 'timeStep': 424, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5571146, 1.2010058, 4.5135956], dtype=float32)}
episode index:256
target Thresh 14.77383303311145
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.122596 ,  9.899946 ,  6.0215893], dtype=float32)}
done in step count: 358
reward sum = 0.027377870564111503
running average episode reward sum: 0.4314819721168007
{'scaleFactor': 20, 'timeStep': 359, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5525384, 1.6478459, 2.139266 ], dtype=float32)}
episode index:257
target Thresh 14.794911128794004
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.06489  ,  4.1764936,  0.6698681], dtype=float32)}
done in step count: 334
reward sum = 0.03484616555067069
running average episode reward sum: 0.42994462402933514
{'scaleFactor': 20, 'timeStep': 335, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3617773, 1.4391894, 1.6458741], dtype=float32)}
episode index:258
target Thresh 14.815884097035765
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9759636, 9.037541 , 1.003471 ], dtype=float32)}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.4288396059332899
{'scaleFactor': 20, 'timeStep': 194, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1265175, 1.1617874, 3.0674148], dtype=float32)}
episode index:259
target Thresh 14.836752462162027
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1705275, 9.795109 , 2.6709986], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4301519272404894
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.47234  , 3.7839217, 4.968964 ], dtype=float32)}
episode index:260
target Thresh 14.857516745883007
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.014755 , 8.162383 , 3.4765797], dtype=float32)}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.4288685853367298
{'scaleFactor': 20, 'timeStep': 235, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.740134 , 4.5638843, 0.9783016], dtype=float32)}
episode index:261
target Thresh 14.878177467306882
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.300076 ,  6.7129445,  2.707904 ], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.43032224672178526
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9510006, 4.062603 , 5.5700192], dtype=float32)}
episode index:262
target Thresh 14.89873514295276
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.868172  , 10.879459  ,  0.22388601], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.42970522949819245
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7059513, 4.121992 , 4.3542643], dtype=float32)}
episode index:263
target Thresh 14.919190286763603
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.237302 ,  8.04725  ,  6.1688604], dtype=float32)}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.42827685853756486
{'scaleFactor': 20, 'timeStep': 294, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3923393, 4.088694 , 5.72893  ], dtype=float32)}
episode index:264
target Thresh 14.939543410119073
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.95674  , 11.182154 ,  2.7956314], dtype=float32)}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.42706195787774465
{'scaleFactor': 20, 'timeStep': 224, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6409261, 2.771698 , 1.3683257], dtype=float32)}
episode index:265
target Thresh 14.959795021848315
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.156649  ,  4.9784546 ,  0.20948094], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4269933797444544
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1075878, 4.1136694, 5.965539 ], dtype=float32)}
episode index:266
target Thresh 14.979945628242676
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.798136 ,  5.2084265,  1.649333 ], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4270702705085718
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8337314, 1.0291114, 4.852715 ], dtype=float32)}
episode index:267
target Thresh 14.999995733068364
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.928556 , 10.946838 ,  1.3480166], dtype=float32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.42745771555501416
{'scaleFactor': 20, 'timeStep': 64, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.655777 , 4.2054605, 4.609516 ], dtype=float32)}
episode index:268
target Thresh 15.019945837579046
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.9885893, 10.961958 ,  0.0599331], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4268853417091673
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8753917, 3.8910444, 4.755872 ], dtype=float32)}
episode index:269
target Thresh 15.039796440528372
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.251397 , 11.19911  ,  2.9620512], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4282435970496654
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2484653, 2.2511425, 2.5506322], dtype=float32)}
episode index:270
target Thresh 15.059548038182449
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5119495, 9.571342 , 3.7262363], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4280979893781693
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8732433, 3.4560418, 5.1088896], dtype=float32)}
episode index:271
target Thresh 15.079201124332249
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.241906 , 10.082853 ,  2.7651591], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.42836173585067305
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1973997, 4.2115254, 4.1719522], dtype=float32)}
episode index:272
target Thresh 15.09875619030595
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.05905  , 3.7647252, 4.534534 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4304556489061651
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.05905  , 3.7647252, 4.534534 ], dtype=float32)}
episode index:273
target Thresh 15.118213724981219
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9069624, 10.991525 ,  5.5065007], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.4293358437427717
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8402257, 1.0133317, 2.149883 ], dtype=float32)}
episode index:274
target Thresh 15.137574214797436
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.510175  , 12.780533  ,  0.95293546], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.430207247069415
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.549518 , 3.8522744, 5.101865 ], dtype=float32)}
episode index:275
target Thresh 15.156838143767853
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.7491527, 10.835073 ,  2.711143 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42864852515974317
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.21714 , 11.148303,  6.277712], dtype=float32)}
episode index:276
target Thresh 15.176005993491701
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3819966, 5.5326967, 3.9903555], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.43063932470790295
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1638445, 3.93744  , 4.452224 ], dtype=float32)}
episode index:277
target Thresh 15.195078243166218
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.915319 ,  4.9677167,  5.710258 ], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4315209424214007
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9305954, 3.438186 , 4.9753003], dtype=float32)}
episode index:278
target Thresh 15.214055369598643
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3324518, 9.780199 , 4.13267  ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4334172688284925
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.378263 , 4.6095448, 5.055261 ], dtype=float32)}
episode index:279
target Thresh 15.232937847218123
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9470346, 8.984687 , 4.10525  ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4353000500469622
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5753236, 3.5964901, 4.031777 ], dtype=float32)}
episode index:280
target Thresh 15.251726148087581
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.00316  , 11.0309925,  4.421647 ], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.43584003841667934
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6377733, 3.338253 , 5.8344646], dtype=float32)}
episode index:281
target Thresh 15.27042074191552
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0394409 , 7.060567  , 0.39855468], dtype=float32)}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.43451808454013846
{'scaleFactor': 20, 'timeStep': 276, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4011624, 4.6497397, 4.996717 ], dtype=float32)}
episode index:282
target Thresh 15.289022096067756
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.159317,  7.296924,  4.018236], dtype=float32)}
done in step count: 310
reward sum = 0.044351705540476356
running average episode reward sum: 0.43313940475568735
{'scaleFactor': 20, 'timeStep': 311, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2658987, 2.5276928, 3.9346037], dtype=float32)}
episode index:283
target Thresh 15.307530675579114
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0068648, 5.991966 , 3.8557634], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.43506532234457573
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2095288, 4.2771387, 4.399431 ], dtype=float32)}
episode index:284
target Thresh 15.325946943165045
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4894767, 5.3480415, 5.528659 ], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4354780176246382
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8123322, 2.5386403, 3.8156881], dtype=float32)}
episode index:285
target Thresh 15.3442713592332
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1279526, 5.7489276, 3.24138  ], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4362022609269462
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.333549 , 4.628953 , 3.6798844], dtype=float32)}
episode index:286
target Thresh 15.362504381894931
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.662179  ,  0.47734118,  5.669342  ], dtype=float32)}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.4353663349795458
{'scaleFactor': 20, 'timeStep': 163, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9184215, 1.7502611, 5.4302516], dtype=float32)}
episode index:287
target Thresh 15.380646466976758
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.001993 , 8.991235 , 3.6919398], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.43684096004781575
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1598299, 4.024012 , 4.443182 ], dtype=float32)}
episode index:288
target Thresh 15.398698068031749
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.080643 , 11.028374 ,  1.0378283], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.43771503660775085
{'scaleFactor': 20, 'timeStep': 38, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0784092, 3.3669538, 5.234239 ], dtype=float32)}
episode index:289
target Thresh 15.416659636350873
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.226835,  8.902534,  0.712246], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4378613346076256
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0073589, 4.780065 , 2.9220722], dtype=float32)}
episode index:290
target Thresh 15.434531620974276
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9715946, 6.9882364, 1.5604465], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.436881329101331
{'scaleFactor': 20, 'timeStep': 188, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1389842, 3.2084203, 1.1831819], dtype=float32)}
episode index:291
target Thresh 15.4523144687025
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.167699 ,  1.5931066,  3.5821207], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.436158955219251
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0438986, 3.8133879, 4.6481714], dtype=float32)}
episode index:292
target Thresh 15.470008624107667
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.292309 ,  4.1773987,  4.299731 ], dtype=float32)}
done in step count: 390
reward sum = 0.019848417799380184
running average episode reward sum: 0.43473810014273273
{'scaleFactor': 20, 'timeStep': 391, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.081306 , 4.757958 , 3.0915003], dtype=float32)}
episode index:293
target Thresh 15.487614529544581
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7850714, 5.16033  , 4.794606 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4366267460606146
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.776492 , 2.8138945, 4.190158 ], dtype=float32)}
episode index:294
target Thresh 15.505132625161798
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.91129  , 11.143556 ,  2.0861833], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.43713661058901077
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7452102, 4.132483 , 5.401729 ], dtype=float32)}
episode index:295
target Thresh 15.52256334891262
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.20105064, 10.409991  ,  3.2567234 ], dtype=float32)}
done in step count: 251
reward sum = 0.08024793100055946
running average episode reward sum: 0.43593090559040115
{'scaleFactor': 20, 'timeStep': 252, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4277642, 3.482037 , 4.837451 ], dtype=float32)}
episode index:296
target Thresh 15.539907136566047
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.1098385,  5.006661 ,  3.058563 ], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.436163077250996
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9227266, 1.0288117, 5.921703 ], dtype=float32)}
episode index:297
target Thresh 15.557164421717674
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.092147,  4.08727 ,  3.372285], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.4359032961115024
{'scaleFactor': 20, 'timeStep': 103, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.514639 , 1.0697176, 3.0580735], dtype=float32)}
episode index:298
target Thresh 15.57433563580053
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.8439536 , 8.510281  , 0.17167038], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.43712646431694574
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8518434, 4.7054877, 3.0033808], dtype=float32)}
episode index:299
target Thresh 15.591421208095863
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.848662 ,  5.120192 ,  1.6331667], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4370458750063356
{'scaleFactor': 20, 'timeStep': 89, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2237694, 4.7793994, 4.8898764], dtype=float32)}
episode index:300
target Thresh 15.608421565743868
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.033181 ,  4.2158756,  3.9907632], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.43739351354773265
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.861348 , 1.2389187, 5.3460703], dtype=float32)}
episode index:301
target Thresh 15.625337133754368
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.97454   , 10.297896  ,  0.52776897], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.43825119468846724
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8921986, 4.557525 , 5.465446 ], dtype=float32)}
episode index:302
target Thresh 15.642168335017452
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.074404  ,  6.7880206 ,  0.87071383], dtype=float32)}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.43754306755741174
{'scaleFactor': 20, 'timeStep': 150, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1629949, 3.5089858, 6.0050807], dtype=float32)}
episode index:303
target Thresh 15.65891559031402
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.925476 , 10.35848  ,  3.5453076], dtype=float32)}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.43626837530513835
{'scaleFactor': 20, 'timeStep': 299, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5107188, 4.691482 , 4.7067637], dtype=float32)}
episode index:304
target Thresh 15.675579318326335
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.32504 , 9.20256 , 4.870498], dtype=float32)}
done in step count: 306
reward sum = 0.046171028276992696
running average episode reward sum: 0.43498936760996415
{'scaleFactor': 20, 'timeStep': 307, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0353196, 4.058044 , 5.099046 ], dtype=float32)}
episode index:305
target Thresh 15.69215993564846
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.2320795, 8.126143 , 5.1004014], dtype=float32)}
done in step count: 263
reward sum = 0.07113055202541568
running average episode reward sum: 0.4338002865132826
{'scaleFactor': 20, 'timeStep': 264, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0260043, 2.9414382, 5.218721 ], dtype=float32)}
episode index:306
target Thresh 15.708657856796693
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.9374733, 10.215223 ,  0.5508784], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.43521704718512527
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0517699, 4.3512726, 3.685471 ], dtype=float32)}
episode index:307
target Thresh 15.72507349421992
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.1299925, 10.337404 ,  5.4523597], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.434674286372566
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4564323, 3.6326427, 5.159357 ], dtype=float32)}
episode index:308
target Thresh 15.741407258309936
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0324526, 7.921395 , 1.5256912], dtype=float32)}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.4341008569887193
{'scaleFactor': 20, 'timeStep': 136, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9862022 , 3.3905342 , 0.09783237], dtype=float32)}
episode index:309
target Thresh 15.757659557411689
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5265384, 2.6905239, 4.376107 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4359263380952073
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5265384, 2.6905239, 4.376107 ], dtype=float32)}
episode index:310
target Thresh 15.773830797833506
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9400389, 5.0562735, 5.6492324], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4377079254325218
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5121944, 3.7108388, 5.7769322], dtype=float32)}
episode index:311
target Thresh 15.789921383857237
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.064806 ,  1.7375487,  4.019394 ], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.43680430948333654
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5596566, 4.1422796, 4.284649 ], dtype=float32)}
episode index:312
target Thresh 15.805931717748377
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.1747265 , 11.45219   ,  0.74123204], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4379957585527867
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.906195 , 2.8935816, 4.660729 ], dtype=float32)}
episode index:313
target Thresh 15.8218621997661
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.820313  , 10.362109  ,  0.24431798], dtype=float32)}
done in step count: 495
reward sum = 0.00690909756953347
running average episode reward sum: 0.4366228710974261
{'scaleFactor': 20, 'timeStep': 496, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.85146  , 4.178628 , 1.5230947], dtype=float32)}
episode index:314
target Thresh 15.837713228173289
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.294535 , 10.232658 ,  0.2720472], dtype=float32)}
done in step count: 377
reward sum = 0.022618737647500813
running average episode reward sum: 0.4353085722610771
{'scaleFactor': 20, 'timeStep': 378, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.067638 , 4.0412064, 5.404368 ], dtype=float32)}
episode index:315
target Thresh 15.85348519924648
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8802153, 8.969052 , 2.7755756], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.43685109163502284
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7868513, 4.9081993, 5.268252 ], dtype=float32)}
episode index:316
target Thresh 15.869178507285772
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.044717 ,  2.0183735,  6.21475  ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4354730124816001
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.242607  ,  0.38365665,  3.94614   ], dtype=float32)}
episode index:317
target Thresh 15.884793544624682
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.6396523, 7.914277 , 4.7823834], dtype=float32)}
done in step count: 286
reward sum = 0.05645022209082803
running average episode reward sum: 0.4342811169143335
{'scaleFactor': 20, 'timeStep': 287, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6229577, 4.8132496, 3.895352 ], dtype=float32)}
episode index:318
target Thresh 15.900330701639957
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.305834, 10.984939,  2.085249], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4340109350885595
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2618284, 4.944701 , 5.9050536], dtype=float32)}
episode index:319
target Thresh 15.91579036676133
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.187699 ,  7.838563 ,  2.6385891], dtype=float32)}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.43332613356548927
{'scaleFactor': 20, 'timeStep': 154, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5913324, 3.7516477, 2.350699 ], dtype=float32)}
episode index:320
target Thresh 15.931172926481239
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.3217435, 9.753244 , 3.703078 ], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4342121285780524
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0923321, 1.6503773, 5.145632 ], dtype=float32)}
episode index:321
target Thresh 15.946478765364471
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.990169 , 10.167318 ,  0.6222969], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4355079224989122
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6334391, 4.87249  , 5.360907 ], dtype=float32)}
episode index:322
target Thresh 15.961708266057805
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.935992 ,  5.0361576,  4.822535 ], dtype=float32)}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.4343224964103474
{'scaleFactor': 20, 'timeStep': 294, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3122861, 4.5433416, 4.051355 ], dtype=float32)}
episode index:323
target Thresh 15.97686180929954
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.8546815, 9.974753 , 2.455187 ], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.43569039309734936
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9097186, 4.8428774, 4.08727  ], dtype=float32)}
episode index:324
target Thresh 15.991939773929056
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.959957 ,  6.3642383,  4.3133855], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.43448490939085
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2306218, 2.0962822, 5.83543  ], dtype=float32)}
episode index:325
target Thresh 16.00694253689625
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.0200505, 10.068155 ,  0.9208979], dtype=float32)}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.43362520706693286
{'scaleFactor': 20, 'timeStep': 187, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2505631, 1.9393194, 2.5038297], dtype=float32)}
episode index:326
target Thresh 16.02187047327098
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.953123 , 10.980638 ,  4.3936644], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4346777946887439
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1996026, 4.0072746, 4.169322 ], dtype=float32)}
episode index:327
target Thresh 16.036723956252427
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1073236, 5.0808764, 3.6253908], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.43634066726591236
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6597928, 3.602467 , 3.9900885], dtype=float32)}
episode index:328
target Thresh 16.051503357178444
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.8409195,  7.967084 ,  5.258264 ], dtype=float32)}
done in step count: 461
reward sum = 0.009723546814439878
running average episode reward sum: 0.4350439586931115
{'scaleFactor': 20, 'timeStep': 462, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3304567, 1.0710356, 1.845325 ], dtype=float32)}
episode index:329
target Thresh 16.06620904553482
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.599015, 9.051931, 5.572092], dtype=float32)}
done in step count: 240
reward sum = 0.08962861870232462
running average episode reward sum: 0.4339972455416243
{'scaleFactor': 20, 'timeStep': 241, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6182208, 4.688074 , 4.5135894], dtype=float32)}
episode index:330
target Thresh 16.080841388964537
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.749216 , 9.065733 , 5.5019507], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.433411128423928
{'scaleFactor': 20, 'timeStep': 143, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3603737, 3.7265925, 4.304923 ], dtype=float32)}
episode index:331
target Thresh 16.095400753276937
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.619152 ,  3.0981228,  4.52701  ], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4328657956970166
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1408467, 3.8025832, 5.5505176], dtype=float32)}
episode index:332
target Thresh 16.109887502456885
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.112773 ,  9.780219 ,  3.2352905], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.4321378547482719
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0812945, 3.0934176, 4.652721 ], dtype=float32)}
episode index:333
target Thresh 16.124301998673875
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.010675  ,  6.9238772 ,  0.30411887], dtype=float32)}
done in step count: 429
reward sum = 0.013412152484926368
running average episode reward sum: 0.4308841849810164
{'scaleFactor': 20, 'timeStep': 430, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8539577, 3.8290284, 4.418275 ], dtype=float32)}
episode index:334
target Thresh 16.13864460229105
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8828728, 6.8574047, 2.0251436], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4319432714163185
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2082148 , 4.6752295 , 0.08806363], dtype=float32)}
episode index:335
target Thresh 16.152915671874254
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.118026 ,  8.135309 ,  1.7510881], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4317471079337498
{'scaleFactor': 20, 'timeStep': 101, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9322443 , 1.2456899 , 0.54166496], dtype=float32)}
episode index:336
target Thresh 16.16711556420097
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.951401 , 9.112147 , 6.1647673], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.4314581754380708
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4464972, 4.739909 , 6.0703306], dtype=float32)}
episode index:337
target Thresh 16.18124463426924
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.3313875 , 10.981981  ,  0.25371665], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4311037461659142
{'scaleFactor': 20, 'timeStep': 117, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0267768, 3.3351657, 5.2452   ], dtype=float32)}
episode index:338
target Thresh 16.195303235306564
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.337212 , 10.8147135,  0.8496605], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.4313364364316476
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1404643, 4.2258835, 4.4994917], dtype=float32)}
episode index:339
target Thresh 16.209291718778687
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.910478,  9.895058,  2.898966], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4315379087653757
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5953089, 1.8160068, 5.278007 ], dtype=float32)}
episode index:340
target Thresh 16.223210434398432
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.7227   ,  9.794851 ,  1.8063328], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4308089310807531
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4674864, 3.8303823, 4.9401464], dtype=float32)}
episode index:341
target Thresh 16.237059730134412
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.0189066, 11.098574 ,  5.891321 ], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4307091324706398
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0492435, 2.1152813, 4.7622232], dtype=float32)}
episode index:342
target Thresh 16.250839952219742
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.935495 ,  1.8970599,  2.5288491], dtype=float32)}
done in step count: 422
reward sum = 0.014389712604426484
running average episode reward sum: 0.4294953732290473
{'scaleFactor': 20, 'timeStep': 423, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0291154 , 4.532177  , 0.01700848], dtype=float32)}
episode index:343
target Thresh 16.264551445160695
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.646225,  9.736864,  5.468644], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4290498973408791
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.157229 , 3.607647 , 4.2613864], dtype=float32)}
episode index:344
target Thresh 16.2781945517453
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.08335  , 8.835263 , 6.2766433], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4289560651932882
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0981375, 1.8669909, 1.1301068], dtype=float32)}
episode index:345
target Thresh 16.291769613051944
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.5862255,  1.88431  ,  5.2910743], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4289463370377835
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5985181, 3.853641 , 4.115137 ], dtype=float32)}
episode index:346
target Thresh 16.305276968457857
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.939013  ,  7.0593143 ,  0.49473733], dtype=float32)}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.42833567006266826
{'scaleFactor': 20, 'timeStep': 153, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4715358, 3.3993974, 4.355094 ], dtype=float32)}
episode index:347
target Thresh 16.31871695564763
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.123885, 11.25271 ,  5.542634], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42886090445769753
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7283474, 4.8989673, 5.7727156], dtype=float32)}
episode index:348
target Thresh 16.332089910621647
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.963652  , 11.240737  ,  0.86961555], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.42846442190713097
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4886975, 3.2706077, 3.4915333], dtype=float32)}
episode index:349
target Thresh 16.345396167704475
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.048853,  8.01124 ,  4.965879], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.42880354253708425
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4558492, 4.9330435, 3.911252 ], dtype=float32)}
episode index:350
target Thresh 16.358636059553234
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.337922 , 10.99996  ,  1.8743415], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42758188002273356
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.11338   , 0.09864146, 4.367183  ], dtype=float32)}
episode index:351
target Thresh 16.371809917165912
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.010588 ,  7.9575834,  1.5325464], dtype=float32)}
done in step count: 381
reward sum = 0.02172746913542607
running average episode reward sum: 0.4264288845372582
{'scaleFactor': 20, 'timeStep': 382, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4073427, 2.5719323, 5.5089436], dtype=float32)}
episode index:352
target Thresh 16.384918069889633
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.270528 ,  8.604784 ,  3.5816011], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4276819069968382
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8059134, 3.6462345, 4.447593 ], dtype=float32)}
episode index:353
target Thresh 16.397960845428898
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9327074 , 5.094612  , 0.81247514], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4292424100844177
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.132061 , 4.77593  , 5.8951583], dtype=float32)}
episode index:354
target Thresh 16.410938569853776
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.080614 ,  9.149881 ,  6.2402773], dtype=float32)}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.42830965334435933
{'scaleFactor': 20, 'timeStep': 232, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.065635 , 1.1474582, 3.5953262], dtype=float32)}
episode index:355
target Thresh 16.423851567608054
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.788139,  8.713756,  4.987257], dtype=float32)}
done in step count: 338
reward sum = 0.03347308759177372
running average episode reward sum: 0.4272005618675262
{'scaleFactor': 20, 'timeStep': 339, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.786669 , 1.6587908, 2.1695487], dtype=float32)}
episode index:356
target Thresh 16.436700161517347
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.39248  , 10.8179   ,  1.7157552], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42807591147962915
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4457474, 4.305189 , 5.207029 ], dtype=float32)}
episode index:357
target Thresh 16.449484672797173
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.376176 ,  8.614903 ,  3.2039938], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.426880168710133
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.052422  ,  0.41592973,  5.1268907 ], dtype=float32)}
episode index:358
target Thresh 16.462205421060983
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.870531 ,  8.686354 ,  3.5987873], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.426731377935047
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4253721, 4.410536 , 5.7674804], dtype=float32)}
episode index:359
target Thresh 16.474862724328137
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.064814 , 9.107192 , 3.5597122], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4282143352463385
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.329734 , 3.664104 , 4.5321527], dtype=float32)}
episode index:360
target Thresh 16.487456899031883
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8356657, 7.04655  , 1.4994441], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4278914730474543
{'scaleFactor': 20, 'timeStep': 117, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.127122 , 4.7773743, 4.2771735], dtype=float32)}
episode index:361
target Thresh 16.499988260027244
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.784619 ,  1.9322097,  2.0400152], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.4269973315551406
{'scaleFactor': 20, 'timeStep': 226, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7014894, 4.163506 , 5.240381 ], dtype=float32)}
episode index:362
target Thresh 16.5124571205989
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.804449 ,  4.0386806,  0.2676458], dtype=float32)}
done in step count: 382
reward sum = 0.021510194444071807
running average episode reward sum: 0.4258802871002891
{'scaleFactor': 20, 'timeStep': 383, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8959925, 3.0259392, 3.8599308], dtype=float32)}
episode index:363
target Thresh 16.524863792469006
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.203472, 10.060509,  6.020181], dtype=float32)}
done in step count: 279
reward sum = 0.06056466128430853
running average episode reward sum: 0.42487667274365176
{'scaleFactor': 20, 'timeStep': 280, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.949942 , 4.298395 , 5.5613217], dtype=float32)}
episode index:364
target Thresh 16.53720858580501
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.29346  ,  7.6431503,  1.9796598], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4255639395280809
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9806502, 3.5784693, 4.7696314], dtype=float32)}
episode index:365
target Thresh 16.549491809227394
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3685255, 3.6427188, 5.2486115], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4271334369610643
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3685255, 3.6427188, 5.2486115], dtype=float32)}
episode index:366
target Thresh 16.561713769817374
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1467223, 8.302028 , 4.285276 ], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4286134521192086
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3705823, 4.762411 , 4.379897 ], dtype=float32)}
episode index:367
target Thresh 16.573874773124608
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.0530405,  9.102716 ,  2.871146 ], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42898109271250523
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4052045, 4.732696 , 4.660186 ], dtype=float32)}
episode index:368
target Thresh 16.585975123174805
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.810302  ,  9.982862  ,  0.03914135], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.43003509229213865
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4912434, 4.8981414, 3.6441827], dtype=float32)}
episode index:369
target Thresh 16.598015122477353
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.813046 , 11.222806 ,  1.9885632], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.43089221932467775
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8276322, 3.8968127, 4.5051365], dtype=float32)}
episode index:370
target Thresh 16.60999507203286
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.980223 ,  6.6565876,  2.113999 ], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4317650685642437
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.179945 , 4.351678 , 3.1037269], dtype=float32)}
episode index:371
target Thresh 16.621915271340686
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.959168  ,  6.9426613 ,  0.28647992], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.43084055227560364
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.115217, 4.823618, 4.165967], dtype=float32)}
episode index:372
target Thresh 16.63377601840644
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.780734 ,  3.1837232,  5.10155  ], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.43083800123337385
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5500736, 3.8533142, 5.9730277], dtype=float32)}
episode index:373
target Thresh 16.645577609749413
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.608569 , 5.0759335, 4.888807 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.43233308679157345
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5605493, 4.9105096, 5.3981   ], dtype=float32)}
episode index:374
target Thresh 16.657320340410003
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.1046205, 9.93852  , 5.098208 ], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.43311347945601686
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7468128, 4.476686 , 4.139305 ], dtype=float32)}
episode index:375
target Thresh 16.669004503957087
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.796572, 12.177154,  2.746802], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.43353862032555274
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0491186, 4.1309366, 4.4429383], dtype=float32)}
episode index:376
target Thresh 16.680630392495363
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9931126, 9.986077 , 3.0698848], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.434811773182736
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1022327, 4.6172733, 5.43876  ], dtype=float32)}
episode index:377
target Thresh 16.692198296672657
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.517298 ,  5.9441204,  1.4912472], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.43482184124788725
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1779199, 4.475308 , 4.8052998], dtype=float32)}
episode index:378
target Thresh 16.70370850568716
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.176866  ,  9.916697  ,  0.38893318], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.43522346378268834
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5977516, 1.6100976, 4.4463286], dtype=float32)}
episode index:379
target Thresh 16.715161307294714
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.006416 , 11.04774  ,  1.9074731], dtype=float32)}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.43432122352103875
{'scaleFactor': 20, 'timeStep': 238, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7928224, 3.1112216, 2.6171336], dtype=float32)}
episode index:380
target Thresh 16.726556987815947
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.148836 , 4.2800922, 4.203589 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.43580594471914624
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.148836 , 4.2800922, 4.203589 ], dtype=float32)}
episode index:381
target Thresh 16.737895832143465
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.548374 ,  5.2490644,  4.1011868], dtype=float32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4360548965469891
{'scaleFactor': 20, 'timeStep': 64, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4453942, 2.2589083, 5.647565 ], dtype=float32)}
episode index:382
target Thresh 16.74917812374897
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9490938 , 8.200905  , 0.23935774], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.43720754961866526
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.975463 , 3.2796342, 4.813286 ], dtype=float32)}
episode index:383
target Thresh 16.760404144690337
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.778517 ,  2.178702 ,  2.0546875], dtype=float32)}
done in step count: 256
reward sum = 0.0763149839065938
running average episode reward sum: 0.4362677252287901
{'scaleFactor': 20, 'timeStep': 257, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3363054, 1.3026209, 5.200748 ], dtype=float32)}
episode index:384
target Thresh 16.771574175618678
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.976324,  8.083948,  2.113953], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.43696169917624667
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9536147, 3.3531137, 4.4050145], dtype=float32)}
episode index:385
target Thresh 16.782688495785344
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1148596, 9.961958 , 1.8662479], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4373658565524779
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0592273, 2.8419604, 5.33788  ], dtype=float32)}
episode index:386
target Thresh 16.793747383048917
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2522793, 8.8448925, 2.8918164], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4372815312711493
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6726542 , 4.842966  , 0.06843822], dtype=float32)}
episode index:387
target Thresh 16.804751113882162
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.9351726, 11.100317 ,  2.7024038], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.43657248064934484
{'scaleFactor': 20, 'timeStep': 182, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4487357, 1.1161067, 2.7686105], dtype=float32)}
episode index:388
target Thresh 16.815699963378915
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.371284 ,  4.3025336,  2.8432326], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4373138890177472
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2086831, 3.6274254, 5.058369 ], dtype=float32)}
episode index:389
target Thresh 16.82659420526099
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.943377 ,  4.129301 ,  3.1591141], dtype=float32)}
done in step count: 245
reward sum = 0.08523592457219176
running average episode reward sum: 0.4364111250063484
{'scaleFactor': 20, 'timeStep': 246, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.939318 , 3.4450212, 4.2938795], dtype=float32)}
episode index:390
target Thresh 16.837434111884992
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.817163 , 8.2427025, 4.332972 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.43780163363804564
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.969259 , 4.2342534, 4.2826385], dtype=float32)}
episode index:391
target Thresh 16.848219954249164
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.914566 ,  9.54702  ,  3.0289233], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.43797276694199727
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.611686 , 4.821416 , 5.0023255], dtype=float32)}
episode index:392
target Thresh 16.858952002000116
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.1714525,  3.340935 ,  4.9900117], dtype=float32)}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.4371182266418867
{'scaleFactor': 20, 'timeStep': 228, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8706958, 4.4003754, 4.8506165], dtype=float32)}
episode index:393
target Thresh 16.869630523439607
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7050655 , 4.896547  , 0.11923608], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.43854686058442
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7050655 , 4.896547  , 0.11923608], dtype=float32)}
episode index:394
target Thresh 16.880255785531226
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.907867 , 10.839743 ,  0.7061255], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4391302147565346
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.063353 , 3.2829702, 5.6016293], dtype=float32)}
episode index:395
target Thresh 16.89082805390708
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6385436, 4.5230017, 5.558982 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.44054655259805847
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6385436, 4.5230017, 5.558982 ], dtype=float32)}
episode index:396
target Thresh 16.90134759287443
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.191857 ,  3.065038 ,  4.6697288], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.4400986081009607
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0336024, 2.0199661, 5.570436 ], dtype=float32)}
episode index:397
target Thresh 16.911814665422295
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.910711 , 12.963277 ,  0.9654077], dtype=float32)}
done in step count: 380
reward sum = 0.021946938520632394
running average episode reward sum: 0.4390479757653318
{'scaleFactor': 20, 'timeStep': 381, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7721024, 3.0684364, 5.138587 ], dtype=float32)}
episode index:398
target Thresh 16.922229533228037
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.7097752, 10.101716 ,  1.0236433], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4399974969729305
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3853811, 4.726679 , 4.476531 ], dtype=float32)}
episode index:399
target Thresh 16.932592456663894
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.273796 , 5.9965534, 2.9032404], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.44125120360400066
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0103393, 4.529531 , 4.2738547], dtype=float32)}
episode index:400
target Thresh 16.942903694803487
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.20362  ,  6.179069 ,  4.3038607], dtype=float32)}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.440355009303374
{'scaleFactor': 20, 'timeStep': 250, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4641755, 1.3679628, 2.5772657], dtype=float32)}
episode index:401
target Thresh 16.95316350542831
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.068906 ,  9.162489 ,  5.4685464], dtype=float32)}
done in step count: 322
reward sum = 0.039312680825886674
running average episode reward sum: 0.4393573915708429
{'scaleFactor': 20, 'timeStep': 323, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6287055, 4.3362155, 5.61626  ], dtype=float32)}
episode index:402
target Thresh 16.963372145034164
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.905549 , 10.734151 ,  2.2524598], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.43936654936254343
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0489576, 3.7380385, 4.771777 ], dtype=float32)}
episode index:403
target Thresh 16.97352986883757
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.800894 ,  1.4102743,  6.2070227], dtype=float32)}
done in step count: 304
reward sum = 0.04710848717170972
running average episode reward sum: 0.43839561356504136
{'scaleFactor': 20, 'timeStep': 305, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8839087, 4.970249 , 4.373901 ], dtype=float32)}
episode index:404
target Thresh 16.98363693078215
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.874366 ,  6.1691546,  4.797055 ], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.43797499900541625
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5989223, 2.1447213, 1.5005027], dtype=float32)}
episode index:405
target Thresh 16.99369358354498
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.44532287, 4.212519  , 3.6834633 ], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4391462853317173
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0344691, 4.898278 , 4.337325 ], dtype=float32)}
episode index:406
target Thresh 17.003700078542902
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.126152  ,  8.898654  ,  0.11980866], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.43949522913708927
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5592881, 4.8237834, 4.9211116], dtype=float32)}
episode index:407
target Thresh 17.013656665938818
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2030234, 6.6529455, 3.1900182], dtype=float32)}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.4386187145780589
{'scaleFactor': 20, 'timeStep': 250, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0487883, 4.0834856, 1.2470592], dtype=float32)}
episode index:408
target Thresh 17.023563594647925
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.766482 ,  1.1568737,  4.0451913], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.43754629718300253
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.480611 , 13.038338 ,  2.0707698], dtype=float32)}
episode index:409
target Thresh 17.03342111234396
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0058584, 5.0144644, 1.3392576], dtype=float32)}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.43685526219424853
{'scaleFactor': 20, 'timeStep': 187, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9172388, 3.04948  , 5.0705943], dtype=float32)}
episode index:410
target Thresh 17.043229465465377
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.034836 , 12.683953 ,  1.9585096], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4375921116132121
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4045343, 4.2977476, 4.7165236], dtype=float32)}
episode index:411
target Thresh 17.052988899221514
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.110251 ,  4.766023 ,  5.7200484], dtype=float32)}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.43660572082053867
{'scaleFactor': 20, 'timeStep': 346, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6396487, 3.8824944, 4.0747204], dtype=float32)}
episode index:412
target Thresh 17.062699657598724
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.128093 ,  5.794451 ,  5.2379074], dtype=float32)}
done in step count: 376
reward sum = 0.022847209744950317
running average episode reward sum: 0.43560388423197793
{'scaleFactor': 20, 'timeStep': 377, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6131645, 4.066461 , 3.9283807], dtype=float32)}
episode index:413
target Thresh 17.072361983366473
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9457265, 5.0125523, 1.7232968], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4369190922410794
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9558108, 4.9884677, 2.200643 ], dtype=float32)}
episode index:414
target Thresh 17.08197611808341
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.345641 , 11.281095 ,  2.2794144], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4380237167278689
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.282006 , 4.2419777, 4.95469  ], dtype=float32)}
episode index:415
target Thresh 17.091542302103402
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.953683  ,  5.2083216 ,  0.13076442], dtype=float32)}
done in step count: 347
reward sum = 0.030578242841615935
running average episode reward sum: 0.4370442804925654
{'scaleFactor': 20, 'timeStep': 348, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6186268, 2.0867896, 3.1057985], dtype=float32)}
episode index:416
target Thresh 17.101060774581548
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.095875 ,  6.069022 ,  3.5398517], dtype=float32)}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.43647169050718493
{'scaleFactor': 20, 'timeStep': 162, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5398567, 4.120748 , 5.8483844], dtype=float32)}
episode index:417
target Thresh 17.11053177348015
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.4401464, 12.493916 ,  0.4249766], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.43662328222821845
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6505482, 3.8877394, 3.7064588], dtype=float32)}
episode index:418
target Thresh 17.119955535574686
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.519188 , 11.977633 ,  1.3133804], dtype=float32)}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.4358324507714646
{'scaleFactor': 20, 'timeStep': 225, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8609512, 1.501647 , 5.4728527], dtype=float32)}
episode index:419
target Thresh 17.12933229645969
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.009056 ,  3.219231 ,  4.9842806], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4353956131817453
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9677544, 2.0143175, 3.6274133], dtype=float32)}
episode index:420
target Thresh 17.138662290554674
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.2967143, 10.25808  ,  2.562902 ], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.43622763818798155
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1628537, 3.2072434, 4.8899813], dtype=float32)}
episode index:421
target Thresh 17.147945751109976
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9650578, 11.321889 ,  2.6233668], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.43700042272864303
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1998692, 4.562273 , 5.999801 ], dtype=float32)}
episode index:422
target Thresh 17.157182910212597
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9152026, 7.963445 , 2.1135054], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4372479041783787
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2254891, 4.554164 , 4.6162896], dtype=float32)}
episode index:423
target Thresh 17.166373998791993
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.019772, 9.96132 , 3.809684], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4381651700266269
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1039658 , 1.0764748 , 0.98726135], dtype=float32)}
episode index:424
target Thresh 17.17551924662586
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.831115, 10.918951,  4.9713  ], dtype=float32)}
done in step count: 301
reward sum = 0.04855048513057287
running average episode reward sum: 0.43724842959157734
{'scaleFactor': 20, 'timeStep': 302, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4177766, 4.994508 , 4.0260196], dtype=float32)}
episode index:425
target Thresh 17.184618882345863
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0458455, 6.8547664, 1.6289932], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.43748087042635575
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.94366  , 3.6351523, 4.8509765], dtype=float32)}
episode index:426
target Thresh 17.193673133443376
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.172959 ,  5.1333923,  4.33231  ], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.43712308799742566
{'scaleFactor': 20, 'timeStep': 126, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8338013, 2.8119311, 5.8550215], dtype=float32)}
episode index:427
target Thresh 17.20268222627514
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.110459, 8.966301, 5.294868], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.438172765062189
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.52672 , 3.48547 , 5.605885], dtype=float32)}
episode index:428
target Thresh 17.211646386068956
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.944401, 10.06245 ,  5.89798 ], dtype=float32)}
done in step count: 286
reward sum = 0.05645022209082803
running average episode reward sum: 0.4372829689247266
{'scaleFactor': 20, 'timeStep': 287, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2160769, 2.640158 , 4.7927403], dtype=float32)}
episode index:429
target Thresh 17.22056583692928
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.5932026, 8.040425 , 4.776103 ], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4384776365549017
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.330893 , 3.5998907, 4.687683 ], dtype=float32)}
episode index:430
target Thresh 17.229440801842845
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.523694, 8.543074, 3.5926  ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.43922906364954717
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7640102, 3.8980422, 5.5859194], dtype=float32)}
episode index:431
target Thresh 17.238271502684242
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.295907 ,  8.679878 ,  1.5409095], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.44003102910593067
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.045947 , 3.6208243, 4.637479 ], dtype=float32)}
episode index:432
target Thresh 17.24705816022145
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.005743 ,  4.092883 ,  2.9310453], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4396464058309105
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0152262, 1.5697718, 4.7236123], dtype=float32)}
episode index:433
target Thresh 17.255800994121362
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.018282, 9.087108, 6.214681], dtype=float32)}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.4387786699769963
{'scaleFactor': 20, 'timeStep': 276, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4603434, 4.8676815, 3.0391552], dtype=float32)}
episode index:434
target Thresh 17.264500222955284
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.097335 , 11.044023 ,  4.1549153], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.43904052010845696
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2834094, 3.5910716, 5.43273  ], dtype=float32)}
episode index:435
target Thresh 17.27315606420439
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.121014 ,  2.8845026,  2.3651755], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.43958338370697037
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.750191, 4.209296, 4.489562], dtype=float32)}
episode index:436
target Thresh 17.28176873426516
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6724277 , 6.176202  , 0.26641196], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4392096269197672
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7918069, 4.266114 , 3.9942253], dtype=float32)}
episode index:437
target Thresh 17.2903384484548
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.003384, 9.882022, 5.062967], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4389184201949484
{'scaleFactor': 20, 'timeStep': 117, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7915363, 4.084907 , 1.6679521], dtype=float32)}
episode index:438
target Thresh 17.2988654210166
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.051659, 10.651519,  2.362108], dtype=float32)}
done in step count: 288
reward sum = 0.05532686267122055
running average episode reward sum: 0.4380446353258739
{'scaleFactor': 20, 'timeStep': 289, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3922105, 4.0984297, 4.799092 ], dtype=float32)}
episode index:439
target Thresh 17.307349865125328
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.091977, 10.699884,  5.669489], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.43821984551246007
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.724868 , 3.4894187, 5.158032 ], dtype=float32)}
episode index:440
target Thresh 17.315791992892525
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.898323, 11.212155,  4.531063], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.43722614971764723
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.032462, 13.219772,  6.166686], dtype=float32)}
episode index:441
target Thresh 17.324192015371825
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.999713 , 9.81697  , 0.9570375], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4367481452059195
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2169778, 4.6467113, 4.858196 ], dtype=float32)}
episode index:442
target Thresh 17.332550142564227
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.8859863, 10.998992 ,  3.0673985], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.4366310976521668
{'scaleFactor': 20, 'timeStep': 96, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.301695 , 4.8885164, 3.0711424], dtype=float32)}
episode index:443
target Thresh 17.340866583423345
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.918024  , 10.002293  ,  0.53493446], dtype=float32)}
done in step count: 350
reward sum = 0.029670038450977102
running average episode reward sum: 0.43571451869000194
{'scaleFactor': 20, 'timeStep': 351, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2466292, 1.8971078, 4.7791753], dtype=float32)}
episode index:444
target Thresh 17.34914154586064
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.901906 , 10.702474 ,  3.6932585], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.43664877318978823
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3655918, 4.70001  , 4.3845143], dtype=float32)}
episode index:445
target Thresh 17.357375236750595
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.848961  ,  0.33778965,  4.4125476 ], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.43626473165740687
{'scaleFactor': 20, 'timeStep': 133, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.776539 , 3.8985558, 4.465125 ], dtype=float32)}
episode index:446
target Thresh 17.365567861935915
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.1499405 , 9.991689  , 0.68772584], dtype=float32)}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.4358044880034049
{'scaleFactor': 20, 'timeStep': 147, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1337724, 1.9407092, 4.1668153], dtype=float32)}
episode index:447
target Thresh 17.37371962623266
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.4616365, 9.306424 , 3.4776359], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.43699755611054014
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.515609 , 4.2008595, 4.6296506], dtype=float32)}
episode index:448
target Thresh 17.381830733435358
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0348897, 8.581679 , 0.5810403], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.43698172416714004
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.816598 , 1.0130284, 0.8236797], dtype=float32)}
episode index:449
target Thresh 17.389901386322112
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.0376005,  9.862414 ,  5.0779705], dtype=float32)}
done in step count: 303
reward sum = 0.047584330476474465
running average episode reward sum: 0.43611639662560525
{'scaleFactor': 20, 'timeStep': 304, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0350611 , 4.4926224 , 0.72228444], dtype=float32)}
episode index:450
target Thresh 17.39793178665966
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.9650116, 10.900814 ,  5.4390416], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.43624660119982833
{'scaleFactor': 20, 'timeStep': 71, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.213    , 2.7194824, 3.4380283], dtype=float32)}
episode index:451
target Thresh 17.40592213520844
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.826085  ,  7.883488  ,  0.21193269], dtype=float32)}
done in step count: 334
reward sum = 0.03484616555067069
running average episode reward sum: 0.4353585471386576
{'scaleFactor': 20, 'timeStep': 335, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4740839, 4.0762506, 5.362948 ], dtype=float32)}
episode index:452
target Thresh 17.41387263172757
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.715616 ,  3.6504931,  1.3100606], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4355461453185951
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8076973, 3.9308996, 4.7424936], dtype=float32)}
episode index:453
target Thresh 17.421783474979886
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.237439 ,  4.1368914,  4.625463 ], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.4356337970734125
{'scaleFactor': 20, 'timeStep': 75, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.45172 , 2.339623, 3.565854], dtype=float32)}
episode index:454
target Thresh 17.429654862736875
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.4764135, 10.514166 ,  4.144772 ], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.43623801560132214
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0279614, 3.5107672, 5.468938 ], dtype=float32)}
episode index:455
target Thresh 17.437486991783647
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9214259, 4.5631466, 0.8612388], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4374743357425473
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9214259, 4.5631466, 0.8612388], dtype=float32)}
episode index:456
target Thresh 17.44528005792383
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.496301, 9.166463, 4.739337], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.43777604343233806
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9160547 , 1.3628128 , 0.48818234], dtype=float32)}
episode index:457
target Thresh 17.453034255984488
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.92911   ,  4.81733   ,  0.27977028], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4378791336656172
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6325743, 3.6799033, 4.324659 ], dtype=float32)}
episode index:458
target Thresh 17.460749779820972
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.7197986, 10.218541 ,  2.4697106], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.43825655873286606
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2776623, 4.0772667, 5.011219 ], dtype=float32)}
episode index:459
target Thresh 17.46842682232178
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.941562 , 9.966382 , 3.1320531], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.43854209056709287
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0022186, 4.291259 , 3.6850266], dtype=float32)}
episode index:460
target Thresh 17.476065575413383
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.733519 ,  9.935695 ,  1.2756304], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4386323278035448
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7767577, 4.110056 , 4.1106534], dtype=float32)}
episode index:461
target Thresh 17.483666230064998
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.0071008, 11.116757 ,  4.710226 ], dtype=float32)}
done in step count: 476
reward sum = 0.008362817674504702
running average episode reward sum: 0.43770100851755117
{'scaleFactor': 20, 'timeStep': 477, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6706088, 3.9472032, 4.2076073], dtype=float32)}
episode index:462
target Thresh 17.491228976293385
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.827353 , 12.490381 ,  2.6403801], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4374087912975759
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4422824, 1.7732111, 1.3075995], dtype=float32)}
episode index:463
target Thresh 17.4987540031676
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.7663803, 9.880342 , 0.762683 ], dtype=float32)}
done in step count: 214
reward sum = 0.11639428152900338
running average episode reward sum: 0.43671694968169544
{'scaleFactor': 20, 'timeStep': 215, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0558753, 4.9947248, 3.6030142], dtype=float32)}
episode index:464
target Thresh 17.506241498813704
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2863152, 8.892651 , 6.2478952], dtype=float32)}
done in step count: 363
reward sum = 0.026036082493920136
running average episode reward sum: 0.43583376502107657
{'scaleFactor': 20, 'timeStep': 364, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5260298, 1.1147274, 2.7024112], dtype=float32)}
episode index:465
target Thresh 17.513691650419478
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.20632 ,  9.128171,  4.83293 ], dtype=float32)}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.43550335070888646
{'scaleFactor': 20, 'timeStep': 127, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3770642, 3.658938 , 5.280252 ], dtype=float32)}
episode index:466
target Thresh 17.521104644239102
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2736652e+00, 1.0201752e+01, 3.8481951e-03], dtype=float32)}
done in step count: 277
reward sum = 0.06179436923202584
running average episode reward sum: 0.4347031173438396
{'scaleFactor': 20, 'timeStep': 278, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1010375, 3.9961212, 4.0916   ], dtype=float32)}
episode index:467
target Thresh 17.528480665597804
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3635974, 7.757868 , 1.1308029], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.43516123595713996
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9420733, 3.6058326, 4.373975 ], dtype=float32)}
episode index:468
target Thresh 17.535819898896506
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.176426 ,  5.8248067,  5.722603 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4342333868399606
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.0945244, 1.2584257, 6.1957793], dtype=float32)}
episode index:469
target Thresh 17.543122527616422
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.117802 ,  8.446014 ,  3.5606334], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.43400676847953046
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6999892, 2.1096923, 5.691082 ], dtype=float32)}
episode index:470
target Thresh 17.550388734323647
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.079947 , 11.074526 ,  1.8890351], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.4337672659446796
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8654402, 3.4791472, 5.258311 ], dtype=float32)}
episode index:471
target Thresh 17.55761870067373
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.177193  , 10.881409  ,  0.56523085], dtype=float32)}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.43345755480786224
{'scaleFactor': 20, 'timeStep': 125, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3771114, 4.83818  , 3.9958625], dtype=float32)}
episode index:472
target Thresh 17.564812607416204
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.014208 ,  2.4236903,  4.589804 ], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.43383315667831673
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.392579 , 3.644009 , 4.5385094], dtype=float32)}
episode index:473
target Thresh 17.571970634399115
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.0259285,  3.0906227,  1.9957217], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4341563710775977
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4393042, 1.2465682, 5.11065  ], dtype=float32)}
episode index:474
target Thresh 17.57909296057351
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.219234  ,  5.8576884 ,  0.76408476], dtype=float32)}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.4335511230156072
{'scaleFactor': 20, 'timeStep': 192, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6546793, 4.855665 , 4.9706306], dtype=float32)}
episode index:475
target Thresh 17.586179763997915
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.2622876, 10.984248 ,  1.813269 ], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.43389861226655374
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0499388, 1.7627832, 4.8015633], dtype=float32)}
episode index:476
target Thresh 17.59323122184278
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.833102 ,  6.6843133,  2.3349206], dtype=float32)}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.43338426893930176
{'scaleFactor': 20, 'timeStep': 167, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9774008, 4.198476 , 4.152036 ], dtype=float32)}
episode index:477
target Thresh 17.600247510394926
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.220464 , 10.060656 ,  4.1557636], dtype=float32)}
done in step count: 235
reward sum = 0.0942476934556249
running average episode reward sum: 0.43267477819561206
{'scaleFactor': 20, 'timeStep': 236, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5044625, 3.5866766, 3.8961754], dtype=float32)}
episode index:478
target Thresh 17.607228805061926
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.1592913, 10.820232 ,  2.1752992], dtype=float32)}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.43203219580325997
{'scaleFactor': 20, 'timeStep': 208, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6758853, 1.7401469, 1.0991414], dtype=float32)}
episode index:479
target Thresh 17.614175280376514
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.810039  ,  8.217875  ,  0.79142064], dtype=float32)}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.4313975731821672
{'scaleFactor': 20, 'timeStep': 206, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0438521, 1.9807723, 4.7760377], dtype=float32)}
episode index:480
target Thresh 17.62108711000093
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.949543  , 10.008741  ,  0.19984254], dtype=float32)}
done in step count: 258
reward sum = 0.07479631572685258
running average episode reward sum: 0.43065619842654285
{'scaleFactor': 20, 'timeStep': 259, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.417376, 4.034561, 5.52578 ], dtype=float32)}
episode index:481
target Thresh 17.627964466731278
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.107195 ,  3.2578278,  6.11506  ], dtype=float32)}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.4300091058092918
{'scaleFactor': 20, 'timeStep': 213, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3495505, 4.0217056, 3.9991517], dtype=float32)}
episode index:482
target Thresh 17.634807522501838
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0687568, 5.1958466, 3.9644496], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.43116850724653966
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3142426, 4.860815 , 3.5875106], dtype=float32)}
episode index:483
target Thresh 17.64161644838935
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.8302207, 9.784699 , 6.2180777], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4308900322055328
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.757008 , 2.2492576, 3.9084196], dtype=float32)}
episode index:484
target Thresh 17.648391414617326
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.09607  ,  4.2325406,  1.4107349], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4314814559176827
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.259983 , 4.561327 , 3.5991268], dtype=float32)}
episode index:485
target Thresh 17.65513259056027
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.603466, 5.396459, 4.213014], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4324545024590225
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0634975, 3.509698 , 3.3533058], dtype=float32)}
episode index:486
target Thresh 17.661840144747938
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.878433 ,  7.447404 ,  4.4196334], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.432603232205076
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7507844, 4.2581034, 4.781879 ], dtype=float32)}
episode index:487
target Thresh 17.668514244869524
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.1832595, 10.801513 ,  3.1640158], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.432872293594927
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2150828, 1.9751337, 1.3358216], dtype=float32)}
episode index:488
target Thresh 17.675155057777882
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.028565 , 10.430459 ,  3.4040272], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.43273560657586424
{'scaleFactor': 20, 'timeStep': 101, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3141407, 2.3202393, 5.1202283], dtype=float32)}
episode index:489
target Thresh 17.681762749493686
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.931587  ,  9.874419  ,  0.75223446], dtype=float32)}
done in step count: 320
reward sum = 0.040110887486875496
running average episode reward sum: 0.431934331638948
{'scaleFactor': 20, 'timeStep': 321, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9282951, 4.694791 , 5.4789896], dtype=float32)}
episode index:490
target Thresh 17.68833748520957
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.029447  ,  6.137073  ,  0.17288488], dtype=float32)}
done in step count: 333
reward sum = 0.035198147020879485
running average episode reward sum: 0.43112631496966475
{'scaleFactor': 20, 'timeStep': 334, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2729032, 4.426314 , 3.8076828], dtype=float32)}
episode index:491
target Thresh 17.694879429294264
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.849448, 10.957418,  1.40973 ], dtype=float32)}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.43046615616624107
{'scaleFactor': 20, 'timeStep': 224, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.662699 , 4.406912 , 4.0573416], dtype=float32)}
episode index:492
target Thresh 17.70138874529672
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.801085 ,  6.17149  ,  0.6260189], dtype=float32)}
done in step count: 351
reward sum = 0.02937333806646733
running average episode reward sum: 0.4296525804702983
{'scaleFactor': 20, 'timeStep': 352, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6467943, 4.9643416, 2.7359045], dtype=float32)}
episode index:493
target Thresh 17.707865595950167
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.8493176, 9.434633 , 4.0185456], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4307079194772411
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7423025, 4.7214084, 4.240216 ], dtype=float32)}
episode index:494
target Thresh 17.714310143176217
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0145378, 5.6804147, 2.6951528], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4314901397158673
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.52433  , 4.6982903, 5.2638702], dtype=float32)}
episode index:495
target Thresh 17.720722548088883
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.271065 ,  5.5151906,  3.1290812], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.43083892386262507
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7969671, 4.2261186, 3.5301962], dtype=float32)}
episode index:496
target Thresh 17.727102970998622
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.9316072, 10.890788 ,  0.966985 ], dtype=float32)}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4303049674678607
{'scaleFactor': 20, 'timeStep': 180, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4041265, 2.8101292, 5.5542183], dtype=float32)}
episode index:497
target Thresh 17.73345157141634
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.945337 , 10.135813 ,  0.1064686], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4301191229566781
{'scaleFactor': 20, 'timeStep': 109, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3463755, 3.5029738, 3.8080359], dtype=float32)}
episode index:498
target Thresh 17.739768508057374
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.134407 , 5.3072844, 1.0638382], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.43112502721509555
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.491511, 3.635467, 4.447236], dtype=float32)}
episode index:499
target Thresh 17.746053938845474
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.083275,  5.99456 ,  2.250744], dtype=float32)}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.43073321041914614
{'scaleFactor': 20, 'timeStep': 145, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5794425, 4.76541  , 4.790457 ], dtype=float32)}
episode index:500
target Thresh 17.75230802091674
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.837711, 8.029472, 5.072148], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.43119538373364685
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8031158, 4.1400633, 4.591602 ], dtype=float32)}
episode index:501
target Thresh 17.75853091062354
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.00491   , 9.13079   , 0.22969478], dtype=float32)}
done in step count: 250
reward sum = 0.08105851616218128
running average episode reward sum: 0.4304978999337037
{'scaleFactor': 20, 'timeStep': 251, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7817726, 4.9328294, 3.5727239], dtype=float32)}
episode index:502
target Thresh 17.764722763538447
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.913364 ,  9.861699 ,  5.3804655], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4296420392976526
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.505839 ,  0.7899441,  2.225373 ], dtype=float32)}
episode index:503
target Thresh 17.770883734458103
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.852277 , 10.865383 ,  1.7638197], dtype=float32)}
done in step count: 327
reward sum = 0.03738596830031274
running average episode reward sum: 0.4288637534424991
{'scaleFactor': 20, 'timeStep': 328, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.597617 , 4.134674 , 4.2315874], dtype=float32)}
episode index:504
target Thresh 17.777013977407105
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.760405 , 10.841786 ,  0.1982624], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4285615512925124
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7570233, 3.465117 , 2.0560882], dtype=float32)}
episode index:505
target Thresh 17.783113645641848
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.970013 ,  7.1915016,  5.2324967], dtype=float32)}
done in step count: 331
reward sum = 0.0359128119792669
running average episode reward sum: 0.4277855656416957
{'scaleFactor': 20, 'timeStep': 332, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5302532, 4.1282606, 5.48232  ], dtype=float32)}
episode index:506
target Thresh 17.78918289165435
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.118823, 10.904658,  5.563323], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.42831540322040945
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.334597 , 4.2385664, 3.6719084], dtype=float32)}
episode index:507
target Thresh 17.795221867176082
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8291655, 6.7892804, 3.337162 ], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.42837110074899903
{'scaleFactor': 20, 'timeStep': 79, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0012622, 1.1025958, 1.1425681], dtype=float32)}
episode index:508
target Thresh 17.801230723181742
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.861134 ,  4.942121 ,  1.3286283], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.42816055059932473
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.320053, 2.602854, 5.218049], dtype=float32)}
episode index:509
target Thresh 17.807209609893047
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.308198  , 9.881973  , 0.05091494], dtype=float32)}
done in step count: 218
reward sum = 0.11180788242357734
running average episode reward sum: 0.4275402512499606
{'scaleFactor': 20, 'timeStep': 219, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0706868, 3.263887 , 5.0273833], dtype=float32)}
episode index:510
target Thresh 17.81315867678247
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.012662, 8.106467, 1.494895], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4279866680196752
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0621092, 4.3576374, 5.088048 ], dtype=float32)}
episode index:511
target Thresh 17.819078072577003
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.726932  ,  4.8656044 ,  0.97212994], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4271507565586993
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.8276505, 10.598395 ,  2.9870336], dtype=float32)}
episode index:512
target Thresh 17.824967945261843
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8297296, 9.955744 , 2.2202451], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.428098839387013
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1200926, 3.8466635, 4.4615993], dtype=float32)}
episode index:513
target Thresh 17.830828442084115
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.266247 , 10.970969 ,  2.5619469], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.42760446387940454
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4172535, 2.5257244, 6.2356715], dtype=float32)}
episode index:514
target Thresh 17.836659709556546
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.66152  ,  6.9314313,  4.812132 ], dtype=float32)}
done in step count: 423
reward sum = 0.014245815478382218
running average episode reward sum: 0.4268018257271695
{'scaleFactor': 20, 'timeStep': 424, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3952172, 3.7519543, 4.1698337], dtype=float32)}
episode index:515
target Thresh 17.842461893461124
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.083411 ,  7.9846444,  4.8528547], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4275439304606852
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2959938, 4.9629655, 3.783547 ], dtype=float32)}
episode index:516
target Thresh 17.848235138852747
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.147511 ,  9.988314 ,  1.6625953], dtype=float32)}
done in step count: 362
reward sum = 0.026299073226181958
running average episode reward sum: 0.42676782822232057
{'scaleFactor': 20, 'timeStep': 363, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.976274 , 2.252867 , 2.5756402], dtype=float32)}
episode index:517
target Thresh 17.85397959006286
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.9397593, 9.3232   , 5.7991624], dtype=float32)}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.42612773529208287
{'scaleFactor': 20, 'timeStep': 235, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0995853, 4.8684034, 6.0663953], dtype=float32)}
episode index:518
target Thresh 17.85969539070303
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.087294 ,  2.0629084,  4.9946456], dtype=float32)}
done in step count: 390
reward sum = 0.019848417799380184
running average episode reward sum: 0.42534492350500636
{'scaleFactor': 20, 'timeStep': 391, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7361937, 4.8365974, 4.7749567], dtype=float32)}
episode index:519
target Thresh 17.865382683668578
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.9941542, 10.042916 ,  3.9534576], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42637425251749667
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0002275, 4.6244745, 4.77842  ], dtype=float32)}
episode index:520
target Thresh 17.871041611142125
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.6574508, 12.450607 ,  2.5356407], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.426230749550204
{'scaleFactor': 20, 'timeStep': 105, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5650358, 2.48599  , 3.9663389], dtype=float32)}
episode index:521
target Thresh 17.876672314597148
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.537654 ,  8.742583 ,  4.8956532], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.4260547842385943
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.421995 , 4.9486346, 5.316425 ], dtype=float32)}
episode index:522
target Thresh 17.882274934801533
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6305475, 6.0682573, 4.2188177], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.42693495648998536
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1319125, 1.6154938, 5.8267875], dtype=float32)}
episode index:523
target Thresh 17.88784961182107
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.948686 ,  4.938776 ,  0.7623241], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4278117693053024
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.867412 , 4.6204786, 3.6635168], dtype=float32)}
episode index:524
target Thresh 17.89339648502298
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.257593 ,  7.975946 ,  1.7470429], dtype=float32)}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4274404411027171
{'scaleFactor': 20, 'timeStep': 146, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6174896, 3.524674 , 5.9355116], dtype=float32)}
episode index:525
target Thresh 17.898915693079378
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.948535 , 10.789608 ,  1.6867278], dtype=float32)}
done in step count: 373
reward sum = 0.0235465663109519
running average episode reward sum: 0.4266725820251662
{'scaleFactor': 20, 'timeStep': 374, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7869956, 3.8652825, 4.057702 ], dtype=float32)}
episode index:526
target Thresh 17.904407373970756
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.802023 , 8.022158 , 4.3625755], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4277227289283443
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9967456, 4.4883137, 4.605542 ], dtype=float32)}
episode index:527
target Thresh 17.90987166498942
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.037783 , 11.023668 ,  1.2524207], dtype=float32)}
done in step count: 434
reward sum = 0.012754823560906535
running average episode reward sum: 0.42693680486514834
{'scaleFactor': 20, 'timeStep': 435, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.147984 , 4.2236223, 4.5703897], dtype=float32)}
episode index:528
target Thresh 17.915308702742934
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.175688, 9.154803, 5.923628], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.42663137848496413
{'scaleFactor': 20, 'timeStep': 133, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4430044, 4.6994605, 4.646091 ], dtype=float32)}
episode index:529
target Thresh 17.92071862315752
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.915079 ,  5.1549892,  3.244551 ], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42671431294364465
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4869845, 4.2141814, 4.922376 ], dtype=float32)}
episode index:530
target Thresh 17.92610156148147
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.871463 ,  9.551971 ,  2.3431668], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.4262955354460978
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3227966, 3.3943758, 4.455363 ], dtype=float32)}
episode index:531
target Thresh 17.93145765228853
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.156586 ,  6.1943836,  4.464858 ], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.426361186634015
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8582923, 4.3744226, 3.9104424], dtype=float32)}
episode index:532
target Thresh 17.936787029481238
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.609566, 9.405258, 5.104215], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42692144770216484
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1449509, 4.8861513, 4.8701506], dtype=float32)}
episode index:533
target Thresh 17.94208982629431
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.7072096, 11.691824 ,  2.2148533], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.4271994126876981
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7004236, 4.1082993, 5.394409 ], dtype=float32)}
episode index:534
target Thresh 17.94736617529794
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.85623  ,  2.1430306,  0.5246357], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.4273541534980941
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7146254, 4.289105 , 4.790478 ], dtype=float32)}
episode index:535
target Thresh 17.95261620840113
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.498041 , 12.23172  ,  2.7103107], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42811377963233294
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.68722 , 4.908368, 5.981381], dtype=float32)}
episode index:536
target Thresh 17.957840056854977
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0201795, 9.842042 , 1.1190803], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4276572484194404
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6992528, 4.0815144, 4.228734 ], dtype=float32)}
episode index:537
target Thresh 17.963037851255965
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.097139,  5.141177,  2.44152 ], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.4271138948069638
{'scaleFactor': 20, 'timeStep': 200, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2875328, 2.5645576, 5.78296  ], dtype=float32)}
episode index:538
target Thresh 17.968209721549226
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.185962 , 5.9645114, 5.776679 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.428158210401014
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8980317, 4.461213 , 5.582367 ], dtype=float32)}
episode index:539
target Thresh 17.97335579703179
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.016045 ,  8.974915 ,  1.8805585], dtype=float32)}
done in step count: 278
reward sum = 0.06117642553970558
running average episode reward sum: 0.4274786145031227
{'scaleFactor': 20, 'timeStep': 279, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1248914, 3.4862075, 4.8369284], dtype=float32)}
episode index:540
target Thresh 17.978476206355804
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.095344,  7.209933,  5.538356], dtype=float32)}
done in step count: 353
reward sum = 0.02878880863894463
running average episode reward sum: 0.4267416647695475
{'scaleFactor': 20, 'timeStep': 354, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.537925 , 1.0841475, 2.9601107], dtype=float32)}
episode index:541
target Thresh 17.983571077531774
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0216765, 6.7810225, 1.3199047], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42733286482409005
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7614682, 1.9863017, 6.273842 ], dtype=float32)}
episode index:542
target Thresh 17.988640537931744
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.207966 ,  6.8933654,  4.4657865], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.42668784268321674
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6150901, 3.10639  , 4.526569 ], dtype=float32)}
episode index:543
target Thresh 17.99368471429249
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1075835, 9.372375 , 4.72107  ], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42768712789887264
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.104661 , 3.0968363, 4.623922 ], dtype=float32)}
episode index:544
target Thresh 17.998703732718674
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.519373 ,  7.0469065,  2.2948089], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42854520336008334
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5837623, 4.128858 , 4.14794  ], dtype=float32)}
episode index:545
target Thresh 18.00369771868603
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.4548397, 9.6582575, 5.7236195], dtype=float32)}
done in step count: 268
reward sum = 0.0676444472200646
running average episode reward sum: 0.4278842129642225
{'scaleFactor': 20, 'timeStep': 269, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4425735, 2.6146717, 1.5861654], dtype=float32)}
episode index:546
target Thresh 18.008666797044462
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.984233 , 11.159669 ,  0.6636096], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.4273003059505909
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6458125, 4.978534 , 1.8418206], dtype=float32)}
episode index:547
target Thresh 18.013611092021186
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.961933 , 10.611874 ,  2.7541432], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.42779138790697585
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.269553 , 1.3854555, 5.484303 ], dtype=float32)}
episode index:548
target Thresh 18.018530727223837
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.651997 ,  1.9870005,  5.896739 ], dtype=float32)}
done in step count: 387
reward sum = 0.02045598088772655
running average episode reward sum: 0.4270494290599463
{'scaleFactor': 20, 'timeStep': 388, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.829698, 4.818685, 4.834874], dtype=float32)}
episode index:549
target Thresh 18.023425825643553
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6803167, 5.902279 , 2.6968617], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42761788532236145
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.825322 , 4.8735123, 6.214695 ], dtype=float32)}
episode index:550
target Thresh 18.028296509658045
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.314444 , 10.9024515,  4.765864 ], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4274365874496127
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.345296 , 4.8644166, 6.0078497], dtype=float32)}
episode index:551
target Thresh 18.03314290103467
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9856368, 8.900948 , 6.1509347], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42828423539673066
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.218439 , 3.3250034, 4.7354283], dtype=float32)}
episode index:552
target Thresh 18.03796512093346
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.950287, 9.583427, 6.169865], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.428648682767086
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0827742, 4.43127  , 4.8052683], dtype=float32)}
episode index:553
target Thresh 18.04276328991017
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.379169,  6.033415,  2.23472 ], dtype=float32)}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.42806687681206457
{'scaleFactor': 20, 'timeStep': 224, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2030191, 4.5127926, 3.7118425], dtype=float32)}
episode index:554
target Thresh 18.04753752791927
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.176918 , 5.198981 , 4.3519993], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4290793689259167
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7086309, 3.401939 , 4.4079986], dtype=float32)}
episode index:555
target Thresh 18.052287954316963
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.027484 ,  4.534285 ,  2.6786075], dtype=float32)}
done in step count: 404
reward sum = 0.01724322985330079
running average episode reward sum: 0.4283386564455703
{'scaleFactor': 20, 'timeStep': 405, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9182187, 2.896352 , 4.62975  ], dtype=float32)}
episode index:556
target Thresh 18.057014687864154
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.433435, 9.702441, 3.529518], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.42860259916286175
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.919024 , 4.3272743, 4.153811 ], dtype=float32)}
episode index:557
target Thresh 18.061717846729426
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1202726, 6.9492493, 0.6092831], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42916012205573884
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1692104, 3.4993253, 3.9235609], dtype=float32)}
episode index:558
target Thresh 18.066397548492
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.069057  ,  9.470068  ,  0.98839796], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4291537356538299
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.929779 , 1.5958023, 4.539444 ], dtype=float32)}
episode index:559
target Thresh 18.07105391014466
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.555778 ,  7.0949016,  1.6918707], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42930727740699054
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2937102, 4.4843874, 4.4397926], dtype=float32)}
episode index:560
target Thresh 18.075687048096686
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.857171  , 10.8101015 ,  0.97422737], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.4290756786795491
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.132048 , 4.5551505, 0.3903365], dtype=float32)}
episode index:561
target Thresh 18.080297078176777
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.293112 ,  4.496301 ,  0.8665822], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.42867579928998806
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0504456 , 3.0658064 , 0.38420027], dtype=float32)}
episode index:562
target Thresh 18.084884115635923
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.025469 , 11.082828 ,  2.2406447], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4286189644891568
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1649781, 3.5909052, 5.388175 ], dtype=float32)}
episode index:563
target Thresh 18.089448275150293
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.154341  , 10.084672  ,  0.14120358], dtype=float32)}
done in step count: 471
reward sum = 0.008793801444488386
running average episode reward sum: 0.42787459363269464
{'scaleFactor': 20, 'timeStep': 472, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0789384, 3.2635794, 5.2973456], dtype=float32)}
episode index:564
target Thresh 18.093989670824122
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.90835   ,  7.154716  ,  0.34368855], dtype=float32)}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.4274863066735832
{'scaleFactor': 20, 'timeStep': 157, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.932411 , 1.1724706, 1.1018939], dtype=float32)}
episode index:565
target Thresh 18.09850841619253
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.906752  ,  0.83027506,  4.196522  ], dtype=float32)}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.4271068709784515
{'scaleFactor': 20, 'timeStep': 155, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.333377 , 4.6791196, 4.4403076], dtype=float32)}
episode index:566
target Thresh 18.103004624224393
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.029264 ,  2.9566572,  1.6625414], dtype=float32)}
done in step count: 368
reward sum = 0.024760055390093627
running average episode reward sum: 0.4263972646017524
{'scaleFactor': 20, 'timeStep': 369, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.54827  , 3.9853244, 4.7636704], dtype=float32)}
episode index:567
target Thresh 18.107478407325143
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7483919, 6.051384 , 4.631261 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4273895229387212
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1654791, 4.191036 , 4.700629 ], dtype=float32)}
episode index:568
target Thresh 18.111929877339588
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.892344 ,  5.1928897,  2.9008558], dtype=float32)}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.42715408040548125
{'scaleFactor': 20, 'timeStep': 123, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2247477, 4.2093863, 4.7464123], dtype=float32)}
episode index:569
target Thresh 18.116359145554714
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.7810955,  8.744728 ,  5.848487 ], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.42704042766417427
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6957984, 3.4808068, 3.5410452], dtype=float32)}
episode index:570
target Thresh 18.120766322702455
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.735706 ,  2.7439163,  4.543303 ], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.42681161183183636
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.448214, 4.964155, 4.433975], dtype=float32)}
episode index:571
target Thresh 18.125151518962472
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.321748, 11.021021,  2.36334 ], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.42681699190472455
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.287915 , 4.7946157, 0.6738727], dtype=float32)}
episode index:572
target Thresh 18.1295148439649
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.039541  , 10.223909  ,  0.39237005], dtype=float32)}
done in step count: 277
reward sum = 0.06179436923202584
running average episode reward sum: 0.42617995416882104
{'scaleFactor': 20, 'timeStep': 278, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0050063, 3.8911605, 4.458331 ], dtype=float32)}
episode index:573
target Thresh 18.133856406793083
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.808824 ,  7.832556 ,  3.4087753], dtype=float32)}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.425598407496673
{'scaleFactor': 20, 'timeStep': 238, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.286831 , 4.969384 , 4.3130746], dtype=float32)}
episode index:574
target Thresh 18.13817631598633
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.295359 ,  4.266623 ,  5.1257467], dtype=float32)}
done in step count: 324
reward sum = 0.03853035847745152
running average episode reward sum: 0.42492524567229173
{'scaleFactor': 20, 'timeStep': 325, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2832146, 4.9825044, 3.6622891], dtype=float32)}
episode index:575
target Thresh 18.142474679542584
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9313636, 7.105632 , 2.831637 ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.42551103988874106
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9117696, 3.5143013, 5.932055 ], dtype=float32)}
episode index:576
target Thresh 18.146751604921167
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.99409   ,  5.908449  ,  0.03694027], dtype=float32)}
done in step count: 247
reward sum = 0.08353972967320515
running average episode reward sum: 0.42491836864053384
{'scaleFactor': 20, 'timeStep': 248, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.990882 , 4.042869 , 5.2246237], dtype=float32)}
episode index:577
target Thresh 18.15100719904543
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.042169 , 11.7520075,  2.670094 ], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42562701118864726
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.299522 , 3.1484437, 5.987527 ], dtype=float32)}
episode index:578
target Thresh 18.155241568305456
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.925632, 8.882424, 2.113238], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42630452401491425
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6103697, 2.9175012, 3.945473 ], dtype=float32)}
episode index:579
target Thresh 18.15945481856069
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.027369 ,  4.987833 ,  3.0766602], dtype=float32)}
done in step count: 359
reward sum = 0.02710409185847039
running average episode reward sum: 0.42561624740774795
{'scaleFactor': 20, 'timeStep': 360, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.340437 , 4.2903976, 5.2034707], dtype=float32)}
episode index:580
target Thresh 18.163647055142608
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.2207155,  9.216802 ,  4.539706 ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4261958110341496
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2299576, 3.8208013, 3.857555 ], dtype=float32)}
episode index:581
target Thresh 18.167818382857348
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.990366  ,  5.986045  ,  0.61076283], dtype=float32)}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.4259676785779487
{'scaleFactor': 20, 'timeStep': 123, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2868539, 3.2646747, 5.144095 ], dtype=float32)}
episode index:582
target Thresh 18.171968905988315
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.105954, 10.854883,  3.242158], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.42617554987093814
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.401387 , 3.3446727, 3.0841868], dtype=float32)}
episode index:583
target Thresh 18.176098728298804
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.053088 ,  3.7607114,  1.8050013], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42641138829659125
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2114717, 3.4591725, 5.2378135], dtype=float32)}
episode index:584
target Thresh 18.18020795303459
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.052843,  5.07171 ,  4.973714], dtype=float32)}
done in step count: 388
reward sum = 0.020251421078849283
running average episode reward sum: 0.42571709775433875
{'scaleFactor': 20, 'timeStep': 389, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.443277 , 1.4570014, 4.7577615], dtype=float32)}
episode index:585
target Thresh 18.1842966829265
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.483162 ,  6.340387 ,  2.0212553], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4264147029824885
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0155807, 1.249542 , 2.8092246], dtype=float32)}
episode index:586
target Thresh 18.188365020193004
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.244693 ,  6.4025054,  2.4241   ], dtype=float32)}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.42580823712818305
{'scaleFactor': 20, 'timeStep': 265, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1371288, 2.108998 , 1.2151678], dtype=float32)}
episode index:587
target Thresh 18.192413066542738
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.814032 ,  9.798776 ,  4.0681562], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4250840734595977
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.7856717, 12.12575  ,  1.3824356], dtype=float32)}
episode index:588
target Thresh 18.196440923177075
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.834912 ,  7.9954967,  5.0581617], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42436236875083777
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.565295  , -0.47213238,  3.752458  ], dtype=float32)}
episode index:589
target Thresh 18.200448690792637
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.006845 ,  4.15902  ,  2.8148954], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.42410665143265364
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1379747 , 4.9986305 , 0.33821172], dtype=float32)}
episode index:590
target Thresh 18.20443646958383
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2787976, 6.8713145, 5.380701 ], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4250308347635629
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8495364, 4.859006 , 4.637438 ], dtype=float32)}
episode index:591
target Thresh 18.20840435924532
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.841706,  9.67866 ,  4.144974], dtype=float32)}
done in step count: 289
reward sum = 0.05477359404450834
running average episode reward sum: 0.42440540023532125
{'scaleFactor': 20, 'timeStep': 290, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2925737, 3.6184192, 4.9139004], dtype=float32)}
episode index:592
target Thresh 18.212352458974568
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.714198, 9.299591, 4.390616], dtype=float32)}
done in step count: 483
reward sum = 0.0077946925652699495
running average episode reward sum: 0.4237028526675809
{'scaleFactor': 20, 'timeStep': 484, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0156348, 4.76305  , 4.563352 ], dtype=float32)}
episode index:593
target Thresh 18.216280867474264
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.112138, 9.217201, 5.94969 ], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4242223821574577
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.814886, 2.801563, 3.74189 ], dtype=float32)}
episode index:594
target Thresh 18.220189682954828
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.060406 ,  8.742713 ,  1.8954442], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4239879038231985
{'scaleFactor': 20, 'timeStep': 126, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2714425, 4.4794106, 4.1336555], dtype=float32)}
episode index:595
target Thresh 18.22407900313685
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.8807335, 10.201777 ,  5.8371453], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4242322214383898
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.71829  , 4.3757987, 3.8374698], dtype=float32)}
episode index:596
target Thresh 18.227948925253543
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.164454 ,  8.045085 ,  1.3199611], dtype=float32)}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4239116724291932
{'scaleFactor': 20, 'timeStep': 146, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2701693, 3.7729254, 4.1656017], dtype=float32)}
episode index:597
target Thresh 18.231799546053153
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.84985  , 10.011917 ,  2.7279458], dtype=float32)}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.4236377155545252
{'scaleFactor': 20, 'timeStep': 135, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4206123, 2.9012086, 4.8714256], dtype=float32)}
episode index:598
target Thresh 18.235630961801405
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.48897  ,  3.1733491,  6.1939254], dtype=float32)}
done in step count: 320
reward sum = 0.040110887486875496
running average episode reward sum: 0.42299743704356085
{'scaleFactor': 20, 'timeStep': 321, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7927558, 1.528648 , 3.8208213], dtype=float32)}
episode index:599
target Thresh 18.23944326828389
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.997867,  9.310745,  3.372348], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.42273029562723863
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.587758 , 3.8553367, 4.7485948], dtype=float32)}
episode index:600
target Thresh 18.24323656080847
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.5523534, 8.886413 , 3.8120642], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.42294652388270476
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4105744, 3.417982 , 4.4531717], dtype=float32)}
episode index:601
target Thresh 18.24701093420765
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.100786, 10.733171,  2.93005 ], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.42358901781017744
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5569067, 3.090753 , 5.9676523], dtype=float32)}
episode index:602
target Thresh 18.250766482840973
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.2264607, 10.79516  ,  4.9406524], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42446364638744083
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.669999 , 2.9671588, 4.703858 ], dtype=float32)}
episode index:603
target Thresh 18.254503300597346
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8230536, 8.179636 , 3.4831426], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4249491875897767
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1088904, 4.5836334, 4.188397 ], dtype=float32)}
episode index:604
target Thresh 18.258221480897404
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.150398 , 11.576242 ,  2.6531231], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.4249573525913207
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.476255  , 2.6531503 , 0.20128727], dtype=float32)}
episode index:605
target Thresh 18.26192111669585
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.943933 , 7.15101  , 5.2143164], dtype=float32)}
done in step count: 355
reward sum = 0.02821591134702963
running average episode reward sum: 0.42430266374438297
{'scaleFactor': 20, 'timeStep': 356, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.750519 , 2.2723575, 4.167411 ], dtype=float32)}
episode index:606
target Thresh 18.265602300483767
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.44542  ,  8.524929 ,  4.1704364], dtype=float32)}
done in step count: 344
reward sum = 0.031514247506815876
running average episode reward sum: 0.42365556585931285
{'scaleFactor': 20, 'timeStep': 345, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1928722, 2.6701574, 4.908257 ], dtype=float32)}
episode index:607
target Thresh 18.269265124290953
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.145213 ,  7.972543 ,  3.2032106], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4238145871698244
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4912844, 1.3343468, 0.4239151], dtype=float32)}
episode index:608
target Thresh 18.272909679688183
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9739895,  2.0105267,  2.2460802], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.42371369625141825
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.871038 , 1.711069 , 3.8013542], dtype=float32)}
episode index:609
target Thresh 18.27653605778954
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6416354, 8.970739 , 4.379026 ], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4246097377329733
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3044915, 4.1818786, 4.161867 ], dtype=float32)}
episode index:610
target Thresh 18.280144349254655
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8630714, 9.958047 , 1.8225908], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42490498540793986
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6977863, 4.314993 , 3.6668022], dtype=float32)}
episode index:611
target Thresh 18.283734644291016
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.315012 ,  7.1505313,  3.947372 ], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.42449499332145013
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9711957, 4.81421  , 3.7383542], dtype=float32)}
episode index:612
target Thresh 18.287307032656177
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4033113, 5.510116 , 5.1125617], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42538537506154567
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6851077, 3.623382 , 4.7473783], dtype=float32)}
episode index:613
target Thresh 18.290861603660034
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2090685, 6.9688354, 5.8379784], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4256390901088691
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8765397, 1.5604634, 2.5576005], dtype=float32)}
episode index:614
target Thresh 18.294398446167044
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.122568, 10.902889,  4.168727], dtype=float32)}
done in step count: 327
reward sum = 0.03738596830031274
running average episode reward sum: 0.4250077842197495
{'scaleFactor': 20, 'timeStep': 328, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7518084, 4.9762325, 4.522135 ], dtype=float32)}
episode index:615
target Thresh 18.297917648598464
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.122503 , 11.075433 ,  1.2238834], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4248772718806712
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4044714, 3.1270123, 3.615508 ], dtype=float32)}
episode index:616
target Thresh 18.301419298934526
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.30084  , 11.806605 ,  1.8475488], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4255548503596068
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0082028, 2.735485 , 4.958969 ], dtype=float32)}
episode index:617
target Thresh 18.304903484716682
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.08374512, 10.26278   ,  3.1846707 ], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.42539124304488807
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2520123, 3.8716474, 5.2143655], dtype=float32)}
episode index:618
target Thresh 18.30837029304975
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0765219 , 6.68752   , 0.06857794], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42622498925871055
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.066638 , 4.0982604, 3.8808227], dtype=float32)}
episode index:619
target Thresh 18.311819810604124
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.176736 , 10.824904 ,  3.3816292], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.42646552113083674
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.833764 , 4.3487353, 5.116376 ], dtype=float32)}
episode index:620
target Thresh 18.31525212361792
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.7739549, 10.918815 ,  0.2577383], dtype=float32)}
done in step count: 441
reward sum = 0.011888329059788622
running average episode reward sum: 0.4257979250083391
{'scaleFactor': 20, 'timeStep': 442, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2628567, 4.5881696, 4.834315 ], dtype=float32)}
episode index:621
target Thresh 18.318667317899145
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.705197 , 11.039108 ,  1.9743878], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.4255030998010019
{'scaleFactor': 20, 'timeStep': 142, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3902807, 4.9543424, 4.3319607], dtype=float32)}
episode index:622
target Thresh 18.322065478827827
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.8459   ,  9.695805 ,  1.9861023], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.42520922106303016
{'scaleFactor': 20, 'timeStep': 142, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.61286  , 3.7480476, 4.301826 ], dtype=float32)}
episode index:623
target Thresh 18.325446691358174
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1735854, 4.9626055, 5.426034 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4261303601318394
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1735854, 4.9626055, 5.426034 ], dtype=float32)}
episode index:624
target Thresh 18.328811040020668
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.819781 , 9.948156 , 1.6953893], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.42583641818929985
{'scaleFactor': 20, 'timeStep': 142, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6460319, 3.3280287, 5.40526  ], dtype=float32)}
episode index:625
target Thresh 18.332158608924207
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.013557 ,  6.091466 ,  3.7842503], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.42540501775974293
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9230516, 1.9527122, 5.586531 ], dtype=float32)}
episode index:626
target Thresh 18.335489481758184
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.100991,  9.84534 ,  1.792466], dtype=float32)}
done in step count: 214
reward sum = 0.11639428152900338
running average episode reward sum: 0.42491217767006073
{'scaleFactor': 20, 'timeStep': 215, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8765001, 4.8753166, 5.0938563], dtype=float32)}
episode index:627
target Thresh 18.3388037417946
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9520344, 7.9830666, 2.231696 ], dtype=float32)}
done in step count: 407
reward sum = 0.016731088683427903
running average episode reward sum: 0.4242622077831393
{'scaleFactor': 20, 'timeStep': 408, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3907428, 1.8624642, 2.0023837], dtype=float32)}
episode index:628
target Thresh 18.342101471890118
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.03333  ,  6.7018323,  6.2060776], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4241411122453162
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4922738, 4.072546 , 4.1335497], dtype=float32)}
episode index:629
target Thresh 18.34538275448817
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.1772003, 10.972205 ,  6.063311 ], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.42406067282977483
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.072671 , 4.99559  , 3.6216226], dtype=float32)}
episode index:630
target Thresh 18.348647671620988
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1527414, 6.128905 , 5.958185 ], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4242733597801997
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0959115, 4.2937727, 5.3760138], dtype=float32)}
episode index:631
target Thresh 18.351896304911676
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.8185966, 11.07807  ,  1.2982442], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.42493581204856
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.452045, 4.495492, 4.711129], dtype=float32)}
episode index:632
target Thresh 18.355128735576226
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.193312 ,  7.1149693,  5.5638986], dtype=float32)}
done in step count: 254
reward sum = 0.07786448720191184
running average episode reward sum: 0.42438751611673275
{'scaleFactor': 20, 'timeStep': 255, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3009212, 3.5956604, 4.390073 ], dtype=float32)}
episode index:633
target Thresh 18.358345044425583
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.519359 , 9.598038 , 3.4469604], dtype=float32)}
done in step count: 239
reward sum = 0.0905339582851764
running average episode reward sum: 0.4238609332179448
{'scaleFactor': 20, 'timeStep': 240, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4723537, 3.412776 , 5.4113317], dtype=float32)}
episode index:634
target Thresh 18.361545311867626
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.57230127, 7.1736283 , 3.5253828 ], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4245753585561826
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8707747, 4.3038063, 5.6816726], dtype=float32)}
episode index:635
target Thresh 18.36472961790922
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.764403 ,  3.1381335,  4.4135056], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.42484012441757457
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.126619 , 4.475439 , 5.0298657], dtype=float32)}
episode index:636
target Thresh 18.36789804215817
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1812315, 5.977322 , 3.2178628], dtype=float32)}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.42451050797061773
{'scaleFactor': 20, 'timeStep': 154, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.918756 , 4.8997726, 6.0078607], dtype=float32)}
episode index:637
target Thresh 18.37105066382525
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.118195 ,  5.268921 ,  0.9723506], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42384513099887694
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.62916  ,  1.2508059,  4.864681 ], dtype=float32)}
episode index:638
target Thresh 18.37418756172617
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.000492  , 10.220038  ,  0.15604919], dtype=float32)}
done in step count: 274
reward sum = 0.06368590427489448
running average episode reward sum: 0.4232815015360851
{'scaleFactor': 20, 'timeStep': 275, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7366679, 4.8309608, 4.2859187], dtype=float32)}
episode index:639
target Thresh 18.377308814283538
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.05016  ,  4.8812795,  6.0465627], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4233407835140258
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0750986, 4.3659635, 5.305813 ], dtype=float32)}
episode index:640
target Thresh 18.380414499528833
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.93601  , 8.223507 , 2.0717654], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.4229572916938148
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.234049 , 4.545716 , 5.8454785], dtype=float32)}
episode index:641
target Thresh 18.383504695104346
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.165341 ,  2.2577271,  2.4085417], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.42250927878604716
{'scaleFactor': 20, 'timeStep': 200, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3152521, 4.6545987, 3.6657076], dtype=float32)}
episode index:642
target Thresh 18.386579478265126
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.493912,  9.527107,  5.896733], dtype=float32)}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.42218302128129287
{'scaleFactor': 20, 'timeStep': 155, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.033631 , 4.7852893, 4.9081063], dtype=float32)}
episode index:643
target Thresh 18.389638925880913
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2403688, 7.7329082, 5.3788643], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4230190662948312
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1941853, 4.2876234, 4.6168966], dtype=float32)}
episode index:644
target Thresh 18.39268311443806
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.206233 , 12.202009 ,  1.3893118], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4235100450655188
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.678714 , 4.5326276, 4.836726 ], dtype=float32)}
episode index:645
target Thresh 18.395712120041438
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.063618 , 6.734279 , 2.6156602], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42432657758074244
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.892784 , 3.4345832, 5.185834 ], dtype=float32)}
episode index:646
target Thresh 18.39872601841634
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.8391943, 9.4020815, 3.930365 ], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.42458722652791514
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7491555, 1.794311 , 1.9631301], dtype=float32)}
episode index:647
target Thresh 18.401724884910386
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0998218, 6.2055845, 0.6772558], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4249236838976016
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3614736, 4.327511 , 3.9553366], dtype=float32)}
episode index:648
target Thresh 18.404708794495395
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.953057 , 10.036892 ,  6.0956635], dtype=float32)}
done in step count: 256
reward sum = 0.0763149839065938
running average episode reward sum: 0.4243865364399883
{'scaleFactor': 20, 'timeStep': 257, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1309466, 2.3541517, 0.7685779], dtype=float32)}
episode index:649
target Thresh 18.407677821769266
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.16336  ,  6.13618  ,  4.8644896], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4241988716695714
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.153736 , 4.183777 , 5.0758324], dtype=float32)}
episode index:650
target Thresh 18.410632040957825
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.403749 , 7.473138 , 2.5341787], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4248551833430358
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.54318  , 3.4715047, 5.490613 ], dtype=float32)}
episode index:651
target Thresh 18.413571525916716
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.913777 ,  5.869511 ,  4.4311595], dtype=float32)}
done in step count: 401
reward sum = 0.017771047742294686
running average episode reward sum: 0.42423082117186905
{'scaleFactor': 20, 'timeStep': 402, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9014368, 4.855423 , 3.5029128], dtype=float32)}
episode index:652
target Thresh 18.416496350133208
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.106924 ,  9.875608 ,  1.9217625], dtype=float32)}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.4238502951080395
{'scaleFactor': 20, 'timeStep': 174, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0767727, 3.7990332, 5.1277094], dtype=float32)}
episode index:653
target Thresh 18.419406586728066
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.345119, 11.143944,  3.026068], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4239074383378714
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0864685, 4.369217 , 6.282876 ], dtype=float32)}
episode index:654
target Thresh 18.42230230845735
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.069348 , 7.5243673, 3.0435066], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4244597600210307
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0296254, 3.9510944, 5.126514 ], dtype=float32)}
episode index:655
target Thresh 18.42518358771426
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.522499,  5.121772,  2.121238], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4246301997545462
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6679665, 4.561393 , 4.9537964], dtype=float32)}
episode index:656
target Thresh 18.428050496530922
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.784125 , 10.99288  ,  0.6525096], dtype=float32)}
done in step count: 291
reward sum = 0.05368359952302263
running average episode reward sum: 0.42406559305708574
{'scaleFactor': 20, 'timeStep': 292, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1435978, 4.1424484, 5.067124 ], dtype=float32)}
episode index:657
target Thresh 18.43090310658021
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4243065, 5.9025116, 3.7212842], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4248519373676388
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5447168, 4.403512 , 4.5083923], dtype=float32)}
episode index:658
target Thresh 18.433741489177518
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9285738, 7.251891 , 1.7129172], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.4244436335920987
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2782197, 2.6116447, 1.3523473], dtype=float32)}
episode index:659
target Thresh 18.436565715282565
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.944995, 11.144284,  2.304363], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.42459689549199786
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0337958, 4.145742 , 5.97855  ], dtype=float32)}
episode index:660
target Thresh 18.43937585550115
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0077453, 7.330336 , 1.8538487], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42530906093642556
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5761957, 4.963212 , 4.4114027], dtype=float32)}
episode index:661
target Thresh 18.442171980086925
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.943607 , 10.960594 ,  1.7542264], dtype=float32)}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.4249374697319819
{'scaleFactor': 20, 'timeStep': 172, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0268953, 3.7753186, 5.2479687], dtype=float32)}
episode index:662
target Thresh 18.444954158943148
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8117902, 5.126318 , 3.3733318], dtype=float32)}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.4248888627314175
{'scaleFactor': 20, 'timeStep': 94, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9037924, 2.9597583, 3.8981214], dtype=float32)}
episode index:663
target Thresh 18.44772246162443
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.163034 , 10.236004 ,  3.0229225], dtype=float32)}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.42438808457121335
{'scaleFactor': 20, 'timeStep': 238, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2443314, 4.971597 , 4.196808 ], dtype=float32)}
episode index:664
target Thresh 18.450476957338495
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.1853266, 11.031806 ,  5.617214 ], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42500481491238457
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7452193, 3.9552188, 5.274192 ], dtype=float32)}
episode index:665
target Thresh 18.45321771494787
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.280495 , 9.302757 , 4.2863226], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42548854956616716
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1716166, 3.7540212, 5.5547853], dtype=float32)}
episode index:666
target Thresh 18.45594480297164
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2059631, 5.891756 , 5.0692506], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4263348935698161
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7600083, 4.0203934, 4.566731 ], dtype=float32)}
episode index:667
target Thresh 18.45865828958715
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0813503, 4.863521 , 1.214905 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4271936736692625
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0813503, 4.863521 , 1.214905 ], dtype=float32)}
episode index:668
target Thresh 18.4613582426317
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.872457, 10.094684,  1.835822], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.4266207506719767
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0707417, 4.28656  , 2.5733118], dtype=float32)}
episode index:669
target Thresh 18.464044729604264
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.2966795,  9.582784 ,  3.0902407], dtype=float32)}
done in step count: 268
reward sum = 0.0676444472200646
running average episode reward sum: 0.42608496514443656
{'scaleFactor': 20, 'timeStep': 269, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9033854, 3.4185824, 4.7641397], dtype=float32)}
episode index:670
target Thresh 18.466717817667156
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.191247  , 7.4354715 , 0.01800745], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42544996519638223
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.04966368,  6.2243996 ,  3.8710947 ], dtype=float32)}
episode index:671
target Thresh 18.469377573647712
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.985801 , 10.159519 ,  4.2452106], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4248168551291257
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.323834 , 9.366274 , 1.8542228], dtype=float32)}
episode index:672
target Thresh 18.472024064039974
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.924665 ,  7.1323733,  1.1705642], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4250491724530321
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.103971 , 4.2689347, 4.188448 ], dtype=float32)}
episode index:673
target Thresh 18.47465735500634
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9648561, 4.7941165, 3.3640747], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4259022152238733
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9648561, 4.7941165, 3.3640747], dtype=float32)}
episode index:674
target Thresh 18.477277512379217
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.7326145,  9.323238 ,  3.8348365], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4252712489790972
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.833652  ,  0.47132117,  3.6321945 ], dtype=float32)}
episode index:675
target Thresh 18.479884601662675
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.910751, 11.086792,  4.317848], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.42583997178862704
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4379958, 4.6788406, 4.5956106], dtype=float32)}
episode index:676
target Thresh 18.48247868803409
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.835394, 7.299487, 4.648203], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42665867197800866
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.612474 , 3.3571134, 4.2459483], dtype=float32)}
episode index:677
target Thresh 18.485059836345748
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.535976 ,  7.7909837,  1.7474827], dtype=float32)}
done in step count: 220
reward sum = 0.10958290556334815
running average episode reward sum: 0.4261910086057156
{'scaleFactor': 20, 'timeStep': 221, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9322019, 2.1768527, 4.5007234], dtype=float32)}
episode index:678
target Thresh 18.487628111126497
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.125368,  4.876722,  5.249836], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.42631441764495553
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4399097, 3.5441363, 4.6674113], dtype=float32)}
episode index:679
target Thresh 18.49018357658334
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1239734, 6.88289  , 2.2243786], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.42639357505514147
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1222044, 3.9293702, 4.693365 ], dtype=float32)}
episode index:680
target Thresh 18.492726296603045
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.960825  , 11.226424  ,  0.66277164], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.42629956395793933
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.866234 , 3.5200472, 4.042685 ], dtype=float32)}
episode index:681
target Thresh 18.49525633475374
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.783875 , 10.70073  ,  0.4265781], dtype=float32)}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.42595096759607637
{'scaleFactor': 20, 'timeStep': 167, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0096147, 2.1118662, 2.0044408], dtype=float32)}
episode index:682
target Thresh 18.49777375428652
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.378066 , 10.556128 ,  2.5729613], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42601631997380635
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0372465, 2.1709433, 4.786447 ], dtype=float32)}
episode index:683
target Thresh 18.500278618136996
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.897722 , 11.047991 ,  1.7839272], dtype=float32)}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4256353934762785
{'scaleFactor': 20, 'timeStep': 180, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0151541, 1.7914097, 4.0566883], dtype=float32)}
episode index:684
target Thresh 18.502770988926898
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.067402 ,  1.9231949,  0.2671919], dtype=float32)}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.42513476419000135
{'scaleFactor': 20, 'timeStep': 249, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0920177, 3.6655035, 4.2790656], dtype=float32)}
episode index:685
target Thresh 18.505250928965626
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6536258, 8.734923 , 2.7769754], dtype=float32)}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.4247245749377617
{'scaleFactor': 20, 'timeStep': 194, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9576064, 3.9100087, 4.899511 ], dtype=float32)}
episode index:686
target Thresh 18.507718500251812
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.9943876, 10.994956 ,  0.5942119], dtype=float32)}
done in step count: 336
reward sum = 0.03415272685621234
running average episode reward sum: 0.4241560569638439
{'scaleFactor': 20, 'timeStep': 337, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5542617, 1.7953165, 5.7799363], dtype=float32)}
episode index:687
target Thresh 18.510173764474864
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.717398 ,  5.23468  ,  1.5850835], dtype=float32)}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.4237682622329485
{'scaleFactor': 20, 'timeStep': 185, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3163174, 1.84304  , 1.4436266], dtype=float32)}
episode index:688
target Thresh 18.512616783016515
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.60893 ,  3.071791,  6.165725], dtype=float32)}
done in step count: 246
reward sum = 0.08438356532646984
running average episode reward sum: 0.4232756864754645
{'scaleFactor': 20, 'timeStep': 247, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4052527, 4.7949214, 4.1542554], dtype=float32)}
episode index:689
target Thresh 18.51504761695236
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.102458,  8.002901,  2.185552], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.42286848618750306
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2871995, 3.5471027, 4.5295696], dtype=float32)}
episode index:690
target Thresh 18.517466327053366
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7915134, 5.170348 , 4.845971 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.423674899376812
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8605254, 4.903266 , 4.7595625], dtype=float32)}
episode index:691
target Thresh 18.51987297378742
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.0390415 , 11.021239  ,  0.36532897], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.423837606495064
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4301934, 1.7057285, 4.935294 ], dtype=float32)}
episode index:692
target Thresh 18.52226761732081
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.992965 ,  4.068032 ,  6.2435846], dtype=float32)}
done in step count: 420
reward sum = 0.014681882057368109
running average episode reward sum: 0.42324719419428813
{'scaleFactor': 20, 'timeStep': 421, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0520635, 4.1757345, 4.384243 ], dtype=float32)}
episode index:693
target Thresh 18.524650317519754
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.843745 ,  9.168174 ,  0.9574878], dtype=float32)}
done in step count: 228
reward sum = 0.10111704470857531
running average episode reward sum: 0.4227830297137612
{'scaleFactor': 20, 'timeStep': 229, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.22714  , 4.559719 , 3.4487576], dtype=float32)}
episode index:694
target Thresh 18.527021133951877
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.802123 , 10.923915 ,  5.9316874], dtype=float32)}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4225165685987605
{'scaleFactor': 20, 'timeStep': 144, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9533211, 1.8409473, 5.6709113], dtype=float32)}
episode index:695
target Thresh 18.52938012588771
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9677005, 11.268996 ,  5.086422 ], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42304975497095143
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1143374, 3.8105972, 5.6029468], dtype=float32)}
episode index:696
target Thresh 18.531727352302184
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.6878095, 7.7936735, 5.888836 ], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4236767400493881
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3893023, 4.2435813, 4.0624404], dtype=float32)}
episode index:697
target Thresh 18.534062871876074
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.096111  , 9.883928  , 0.62230146], dtype=float32)}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.4232103174524172
{'scaleFactor': 20, 'timeStep': 232, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.560212 , 1.6824051, 5.5681524], dtype=float32)}
episode index:698
target Thresh 18.536386742997493
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.7849913, 9.857107 , 2.3813562], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4237065017562123
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7284946, 4.42056  , 5.798295 ], dtype=float32)}
episode index:699
target Thresh 18.538699023763343
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8694584, 9.103344 , 0.5417993], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4242123801242736
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6290495, 1.9287304, 5.5346446], dtype=float32)}
episode index:700
target Thresh 18.540999771980758
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.021742,  4.906143,  6.217601], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.42419042262683887
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9873405, 4.282903 , 5.7367473], dtype=float32)}
episode index:701
target Thresh 18.543289045168567
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.493607 ,  3.0571632,  4.8497224], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4235861627655471
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.2900143, 11.908332 ,  1.9478314], dtype=float32)}
episode index:702
target Thresh 18.545566900558722
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7132864, 9.803245 , 6.0698113], dtype=float32)}
done in step count: 376
reward sum = 0.022847209744950317
running average episode reward sum: 0.42301612158059604
{'scaleFactor': 20, 'timeStep': 377, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3865178, 1.3411407, 2.7672565], dtype=float32)}
episode index:703
target Thresh 18.54783339509772
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.007387 ,  2.6044664,  3.5039942], dtype=float32)}
done in step count: 467
reward sum = 0.009154526307566474
running average episode reward sum: 0.4224282499964014
{'scaleFactor': 20, 'timeStep': 468, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9578247, 3.056621 , 4.9695044], dtype=float32)}
episode index:704
target Thresh 18.55008858544804
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.913515 , 9.962379 , 1.9000853], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42298921267384937
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.973052, 4.976711, 5.217678], dtype=float32)}
episode index:705
target Thresh 18.55233252798957
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.850381 ,  6.4443555,  2.4997532], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42357210863528877
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.075366 , 4.4798794, 3.1956022], dtype=float32)}
episode index:706
target Thresh 18.55456527882098
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.787845, 8.872384, 5.217431], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4236123795569736
{'scaleFactor': 20, 'timeStep': 80, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6138113, 4.9043117, 3.3815756], dtype=float32)}
episode index:707
target Thresh 18.556786893761164
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.24887  , 6.0652094, 1.5731822], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.4234816178461884
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1233561, 2.4415097, 4.977634 ], dtype=float32)}
episode index:708
target Thresh 18.558997428350605
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.0905695, 10.101441 ,  1.7293909], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4234845917609168
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0983194, 4.435834 , 3.3839555], dtype=float32)}
episode index:709
target Thresh 18.561196937852785
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.1960926, 8.330857 , 4.388672 ], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.42399472352013695
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0530095, 1.1623988, 3.0238297], dtype=float32)}
episode index:710
target Thresh 18.56338547725556
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9869031, 11.044109 ,  6.000137 ], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.42383234623056526
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.767056, 4.136195, 5.686752], dtype=float32)}
episode index:711
target Thresh 18.565563101272524
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.32376575, 10.728869  ,  3.3714168 ], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4244695494282737
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.206275 , 3.1731172, 5.777029 ], dtype=float32)}
episode index:712
target Thresh 18.567729864344397
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9370667, 8.696533 , 2.0196705], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4249542248790127
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0663557, 4.6751614, 5.664953 ], dtype=float32)}
episode index:713
target Thresh 18.56988582064036
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.820412 , 10.879032 ,  1.6321721], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42435905089458825
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.346219 , 12.995406 ,  2.5134556], dtype=float32)}
episode index:714
target Thresh 18.572031024059438
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.457327 , 7.551207 , 6.0132403], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4245463335346628
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6581597, 4.5289707, 5.8693795], dtype=float32)}
episode index:715
target Thresh 18.574165528231827
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.231587 ,  7.901887 ,  2.4213843], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.4245359627894094
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.886921 , 2.6083596, 4.8221593], dtype=float32)}
episode index:716
target Thresh 18.576289386520244
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.437527 ,  4.819444 ,  1.3766127], dtype=float32)}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.42406515469500056
{'scaleFactor': 20, 'timeStep': 244, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.095164 , 3.3850508, 5.3441772], dtype=float32)}
episode index:717
target Thresh 18.578402652021257
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.985559 ,  2.8704925,  0.7434025], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.42380878606671246
{'scaleFactor': 20, 'timeStep': 143, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3074642, 4.1426387, 5.6185174], dtype=float32)}
episode index:718
target Thresh 18.580505377566613
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0185368, 9.191486 , 1.2525996], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4241404595784194
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0828314, 2.320811 , 4.018346 ], dtype=float32)}
episode index:719
target Thresh 18.582597615724556
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.772135 , 10.856395 ,  4.9516983], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.42383805274166797
{'scaleFactor': 20, 'timeStep': 158, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.083235 , 4.819062 , 1.2363585], dtype=float32)}
episode index:720
target Thresh 18.58467941880115
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.999983 , 10.603382 ,  1.5823033], dtype=float32)}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.42329347722473326
{'scaleFactor': 20, 'timeStep': 346, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7230573, 4.918957 , 3.6483154], dtype=float32)}
episode index:721
target Thresh 18.58675083884159
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.73324   , 5.9632797 , 0.16577995], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42270719817040536
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.20629367, 7.3013487 , 2.6922293 ], dtype=float32)}
episode index:722
target Thresh 18.588811927631475
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.069735 , 10.963808 ,  6.0690646], dtype=float32)}
done in step count: 428
reward sum = 0.013547628772652897
running average episode reward sum: 0.4221412789872826
{'scaleFactor': 20, 'timeStep': 429, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.730261 , 1.1803464, 4.715095 ], dtype=float32)}
episode index:723
target Thresh 18.59086273669813
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.3292637 , 11.182648  ,  0.38876763], dtype=float32)}
done in step count: 220
reward sum = 0.10958290556334815
running average episode reward sum: 0.4217095685267523
{'scaleFactor': 20, 'timeStep': 221, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.062912 , 3.5257325, 5.1850224], dtype=float32)}
episode index:724
target Thresh 18.5929033173119
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.319985 , 10.41011  ,  3.8448982], dtype=float32)}
done in step count: 466
reward sum = 0.009246996270269165
running average episode reward sum: 0.42114065463398476
{'scaleFactor': 20, 'timeStep': 467, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3297988, 4.5772057, 5.1283064], dtype=float32)}
episode index:725
target Thresh 18.594933720487397
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5911868, 5.8964334, 4.5050287], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42192420745129333
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4037876, 4.08633  , 4.3255744], dtype=float32)}
episode index:726
target Thresh 18.59695399698481
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.898046 ,  3.8795674,  4.8251824], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42134384402976477
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.555552 , -0.3398845,  5.960267 ], dtype=float32)}
episode index:727
target Thresh 18.59896419731115
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.310936 ,  6.0808144,  3.8374615], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.42145166434002496
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.087477 , 1.0431902, 5.891639 ], dtype=float32)}
episode index:728
target Thresh 18.600964371721542
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.099569  ,  9.997726  ,  0.28415188], dtype=float32)}
done in step count: 354
reward sum = 0.02850092055255518
running average episode reward sum: 0.42091263725664024
{'scaleFactor': 20, 'timeStep': 355, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4065757, 4.7150702, 3.6278753], dtype=float32)}
episode index:729
target Thresh 18.602954570220444
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.048906 ,  9.010991 ,  5.7451854], dtype=float32)}
done in step count: 423
reward sum = 0.014245815478382218
running average episode reward sum: 0.4203555594185878
{'scaleFactor': 20, 'timeStep': 424, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1076016, 4.573061 , 5.8170004], dtype=float32)}
episode index:730
target Thresh 18.60493484256292
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.711958  ,  0.83329964,  3.4149547 ], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.4203511344124519
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7950673, 4.68629  , 4.588907 ], dtype=float32)}
episode index:731
target Thresh 18.606905238255884
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.140889 ,  9.833556 ,  5.2857614], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.420262076598644
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0953169, 2.4258695, 3.771366 ], dtype=float32)}
episode index:732
target Thresh 18.60886580655933
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.002602 ,  9.895509 ,  3.5872276], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4208158372360751
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0210311, 3.89971  , 5.1712217], dtype=float32)}
episode index:733
target Thresh 18.610816596487567
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1015353, 7.668041 , 2.9162295], dtype=float32)}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.4205442240889399
{'scaleFactor': 20, 'timeStep': 151, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9525778, 4.047559 , 3.9573565], dtype=float32)}
episode index:734
target Thresh 18.612757656810448
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.650683 , 12.088793 ,  2.8338828], dtype=float32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.4203052051432882
{'scaleFactor': 20, 'timeStep': 141, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0907838, 4.6139593, 5.3133626], dtype=float32)}
episode index:735
target Thresh 18.614689036054582
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8422555 , 5.212692  , 0.25647688], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.42018391150630136
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.313312 , 3.4204621, 4.366062 ], dtype=float32)}
episode index:736
target Thresh 18.616610782504544
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.946698 , 10.095749 ,  1.9841464], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.41987221211452214
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7688043, 1.3147322, 3.6917076], dtype=float32)}
episode index:737
target Thresh 18.618522944204106
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.93901  , 10.003922 ,  1.2017871], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.4195143632489018
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6139567, 3.5363164, 4.657851 ], dtype=float32)}
episode index:738
target Thresh 18.6204255689574
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.333362, 9.971727, 4.226383], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4202206768972808
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1827273, 4.572833 , 5.9324045], dtype=float32)}
episode index:739
target Thresh 18.622318704330155
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.05792 , 10.986564,  4.7819  ], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4201781947907632
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2675653, 4.5962477, 4.098522 ], dtype=float32)}
episode index:740
target Thresh 18.62420239765084
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.073081 , 5.012399 , 0.4183299], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42093382475730734
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.199628 , 4.930516 , 0.5465259], dtype=float32)}
episode index:741
target Thresh 18.6260766960119
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0532193, 8.034026 , 2.700473 ], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4207580224251681
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7956808, 4.901807 , 4.2383404], dtype=float32)}
episode index:742
target Thresh 18.62794164627088
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.430861 , 4.714528 , 4.3245935], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4212177595609984
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6835477, 2.0821474, 0.9895586], dtype=float32)}
episode index:743
target Thresh 18.629797295051638
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.437398, 11.062355,  6.148223], dtype=float32)}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.4207305929893835
{'scaleFactor': 20, 'timeStep': 283, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.825715 , 4.6539764, 5.7427044], dtype=float32)}
episode index:744
target Thresh 18.631643688745495
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.446064 ,  3.0807276,  3.1389728], dtype=float32)}
done in step count: 385
reward sum = 0.02087132015888843
running average episode reward sum: 0.4201938691332352
{'scaleFactor': 20, 'timeStep': 386, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7446856, 1.4358064, 2.950094 ], dtype=float32)}
episode index:745
target Thresh 18.633480873512383
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.344217 , 12.933634 ,  1.6905328], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42044979858417303
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0027864, 2.646627 , 4.9976773], dtype=float32)}
episode index:746
target Thresh 18.63530889528202
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.933361, 6.783961, 2.053909], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.42080990472511665
{'scaleFactor': 20, 'timeStep': 38, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1611114, 3.5102935, 4.766591 ], dtype=float32)}
episode index:747
target Thresh 18.637127799755046
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.855413, 7.841097, 4.842107], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4208516610694233
{'scaleFactor': 20, 'timeStep': 80, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1511118, 2.9696157, 5.500013 ], dtype=float32)}
episode index:748
target Thresh 18.638937632404165
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.099747, 11.270847,  2.200959], dtype=float32)}
done in step count: 430
reward sum = 0.013278030960077104
running average episode reward sum: 0.4203075040198781
{'scaleFactor': 20, 'timeStep': 431, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.84188  , 1.1332003, 3.8336766], dtype=float32)}
episode index:749
target Thresh 18.64073843847529
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.010641 ,  1.8797632,  4.9109087], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4201502999287436
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9628677, 2.6880903, 3.5627809], dtype=float32)}
episode index:750
target Thresh 18.64253026298866
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.888342,  6.466608,  0.285468], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.42012438029228927
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0059319, 4.154537 , 4.46672  ], dtype=float32)}
episode index:751
target Thresh 18.644313150739993
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.998651  ,  9.800502  ,  0.03346649], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4198661672274512
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0552266, 4.202487 , 4.828957 ], dtype=float32)}
episode index:752
target Thresh 18.646087146301564
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1177895, 6.0143824, 4.5702796], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.42052174635129735
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5130255, 2.6202898, 5.492855 ], dtype=float32)}
episode index:753
target Thresh 18.647852294023362
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.38131  ,  5.448068 ,  1.8457626], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4205062270251319
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2846947, 2.1119356, 4.129341 ], dtype=float32)}
episode index:754
target Thresh 18.649608638034174
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.648512, 9.443182, 5.358854], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.421159221754216
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3931112, 4.7787776, 3.2947838], dtype=float32)}
episode index:755
target Thresh 18.65135622224268
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.10504  ,  7.9507847,  5.501279 ], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.42083227811231394
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5580776, 2.0976777, 5.0190334], dtype=float32)}
episode index:756
target Thresh 18.653095090338585
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.343693,  8.232908,  4.758001], dtype=float32)}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.4203175711465536
{'scaleFactor': 20, 'timeStep': 346, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4047098, 3.2654102, 4.143791 ], dtype=float32)}
episode index:757
target Thresh 18.654825285793684
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3782654, 6.7771707, 0.376019 ], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4203964944782487
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3543786, 2.2977853, 5.2501493], dtype=float32)}
episode index:758
target Thresh 18.65654685186295
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.197692, 10.086992,  2.662692], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4207418938201883
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5960937, 4.652012 , 4.002562 ], dtype=float32)}
episode index:759
target Thresh 18.65825983158562
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.605314 ,  3.9923046,  5.4462066], dtype=float32)}
done in step count: 273
reward sum = 0.06432919623726716
running average episode reward sum: 0.42027292974442126
{'scaleFactor': 20, 'timeStep': 274, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0926993, 2.0932214, 5.6369777], dtype=float32)}
episode index:760
target Thresh 18.659964267786282
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.863154 , 10.862015 ,  1.0063853], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4201023851512091
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.742056 , 1.211692 , 2.2825353], dtype=float32)}
episode index:761
target Thresh 18.66166020307593
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.897504 ,  9.310504 ,  4.1541314], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42076202072768776
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0544066, 4.4702563, 4.0293417], dtype=float32)}
episode index:762
target Thresh 18.663347679853032
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.6009946, 9.55395  , 3.1433396], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.42031159323306416
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.42078  , 3.4428234, 1.349627 ], dtype=float32)}
episode index:763
target Thresh 18.665026740304597
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0846863, 9.988632 , 1.226762 ], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4205941506844134
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8456078, 4.836676 , 4.208685 ], dtype=float32)}
episode index:764
target Thresh 18.66669742640722
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.121388 ,  6.8886366,  5.48165  ], dtype=float32)}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.42015803618560793
{'scaleFactor': 20, 'timeStep': 244, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1537538, 4.2762704, 4.161812 ], dtype=float32)}
episode index:765
target Thresh 18.66835977992815
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.1492743, 10.672737 ,  2.6569088], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4204916798055488
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2997804, 3.2449613, 5.1322336], dtype=float32)}
episode index:766
target Thresh 18.670013842426304
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.958748 ,  6.0515413,  4.455636 ], dtype=float32)}
done in step count: 284
reward sum = 0.05759639025694116
running average episode reward sum: 0.4200185438348205
{'scaleFactor': 20, 'timeStep': 285, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9789028, 4.0128164, 4.225288 ], dtype=float32)}
episode index:767
target Thresh 18.671659655253325
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0492375, 7.1822934, 5.578694 ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42072242074388977
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.474122 , 3.6351895, 4.6476426], dtype=float32)}
episode index:768
target Thresh 18.673297259554634
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7651966, 9.018121 , 1.1383312], dtype=float32)}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.4202942361170607
{'scaleFactor': 20, 'timeStep': 239, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.285551 , 3.5162754, 4.3161383], dtype=float32)}
episode index:769
target Thresh 18.67492669627041
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.904939 ,  9.072986 ,  4.9884486], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4203846205805505
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0916859, 3.6929398, 4.2696347], dtype=float32)}
episode index:770
target Thresh 18.676548006136667
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.989355 ,  6.1771626,  5.3883348], dtype=float32)}
done in step count: 442
reward sum = 0.011769445769190735
running average episode reward sum: 0.4198546398090701
{'scaleFactor': 20, 'timeStep': 443, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1008382, 4.0490847, 4.7967143], dtype=float32)}
episode index:771
target Thresh 18.67816122968623
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.107046 ,  5.744108 ,  2.9553952], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4196650472070146
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8944302, 4.1385546, 5.304208 ], dtype=float32)}
episode index:772
target Thresh 18.67976640724977
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.950659  ,  9.952244  ,  0.29731297], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.4192218658294245
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4329035, 3.7966347, 5.2410665], dtype=float32)}
episode index:773
target Thresh 18.681363578956816
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8096113, 6.938054 , 3.4621181], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4196651744192406
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0340874, 3.8323874, 4.6493964], dtype=float32)}
episode index:774
target Thresh 18.682952784736738
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.1288095, 9.88102  , 1.3706932], dtype=float32)}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.4195339794378211
{'scaleFactor': 20, 'timeStep': 115, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5371302, 4.189469 , 5.292729 ], dtype=float32)}
episode index:775
target Thresh 18.684534064319763
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.146105, 8.032015, 5.974631], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.41965054099299093
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4783195, 4.744687 , 3.8029144], dtype=float32)}
episode index:776
target Thresh 18.68610745723797
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0330055, 7.050341 , 3.527699 ], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.41980759959656094
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.759038 , 4.6708775, 3.932489 ], dtype=float32)}
episode index:777
target Thresh 18.687673002826255
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.995083 , 6.309686 , 2.4003654], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4192680011394959
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.45774457, 12.427598  ,  1.7458186 ], dtype=float32)}
episode index:778
target Thresh 18.689230740223348
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7017322, 7.424457 , 2.782927 ], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4191949639337462
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.771825 , 4.7361383, 5.320818 ], dtype=float32)}
episode index:779
target Thresh 18.69078070837276
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9458034, 11.079216 ,  4.4750595], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4198287104511179
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7115759, 1.9404229, 4.9589553], dtype=float32)}
episode index:780
target Thresh 18.692322946023772
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.642614 , 11.085987 ,  2.0325656], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4198876759629216
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5889373, 4.0056725, 5.152896 ], dtype=float32)}
episode index:781
target Thresh 18.693857491732416
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.99773 , 10.129941,  5.465025], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4201976431816186
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4960117, 3.5005076, 4.375465 ], dtype=float32)}
episode index:782
target Thresh 18.695384383862404
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4488995, 6.9881415, 5.0705314], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4209001991928809
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6743276, 3.6813114, 3.846064 ], dtype=float32)}
episode index:783
target Thresh 18.696903660586123
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.158908  , 9.739643  , 0.41787976], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42154030696741546
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.208387 , 3.4045844, 3.7179112], dtype=float32)}
episode index:784
target Thresh 18.698415359885573
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.31816  , 5.4313054, 4.011938 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4222644594426162
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7624886, 3.7881176, 4.0643244], dtype=float32)}
episode index:785
target Thresh 18.69991951955331
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.223719,  9.709148,  0.810097], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.42192147632917254
{'scaleFactor': 20, 'timeStep': 188, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7611644, 4.705717 , 4.369807 ], dtype=float32)}
episode index:786
target Thresh 18.70141617719341
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8911219, 8.951388 , 5.98988  ], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.42254612152758986
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4919314, 4.4911523, 5.1185102], dtype=float32)}
episode index:787
target Thresh 18.702905370222382
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.742379 ,  7.1462913,  5.3838105], dtype=float32)}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.4222666214077944
{'scaleFactor': 20, 'timeStep': 160, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9551768, 3.974022 , 3.6809177], dtype=float32)}
episode index:788
target Thresh 18.704387135870142
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.234661 ,  9.36268  ,  3.0502415], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42281058991183385
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9527087, 3.1782942, 4.296094 ], dtype=float32)}
episode index:789
target Thresh 18.705861511180903
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0755565, 8.092935 , 3.2373054], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.4226146862751314
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.443447 , 4.4529743, 5.2886114], dtype=float32)}
episode index:790
target Thresh 18.70732853301412
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.081854 ,  4.974886 ,  3.6032007], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.4225249195498253
{'scaleFactor': 20, 'timeStep': 105, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1321628, 3.9484234, 3.7709424], dtype=float32)}
episode index:791
target Thresh 18.70878823804542
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.018526 , 10.933371 ,  3.5133772], dtype=float32)}
done in step count: 351
reward sum = 0.02937333806646733
running average episode reward sum: 0.42202851603785135
{'scaleFactor': 20, 'timeStep': 352, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5265765 , 4.8448787 , 0.18952096], dtype=float32)}
episode index:792
target Thresh 18.710240662767504
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.770602 , 10.083696 ,  2.6732206], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4224576638289097
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7654098, 3.3330102, 5.489664 ], dtype=float32)}
episode index:793
target Thresh 18.711685843491065
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.277495,  6.914045,  3.095346], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4230646215256098
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5862713, 4.0292015, 4.2000947], dtype=float32)}
episode index:794
target Thresh 18.7131238163457
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9520035,  9.367115 ,  5.8637924], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.422755763544771
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0846219, 2.3605924, 4.491783 ], dtype=float32)}
episode index:795
target Thresh 18.714554617280797
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.2289715, 11.1207075,  0.0504427], dtype=float32)}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4226201522503441
{'scaleFactor': 20, 'timeStep': 116, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8750196, 4.324531 , 4.705685 ], dtype=float32)}
episode index:796
target Thresh 18.715978282066462
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2155426, 9.17993  , 2.5001183], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4229995251282707
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7118535, 2.8626142, 1.0533926], dtype=float32)}
episode index:797
target Thresh 18.717394846294386
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.194175  , 8.982903  , 0.56259525], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4230246485073408
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.207639 , 3.8436308, 5.1622767], dtype=float32)}
episode index:798
target Thresh 18.718804345378746
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.996781  ,  7.1592474 ,  0.10960448], dtype=float32)}
done in step count: 333
reward sum = 0.035198147020879485
running average episode reward sum: 0.4225392586431525
{'scaleFactor': 20, 'timeStep': 334, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.590214 , 3.8348756, 4.4694147], dtype=float32)}
episode index:799
target Thresh 18.7202068145571
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.859617  ,  6.9620442 ,  0.94768155], dtype=float32)}
done in step count: 451
reward sum = 0.010751591703479103
running average episode reward sum: 0.4220245240594779
{'scaleFactor': 20, 'timeStep': 452, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3554951, 1.7897   , 5.479847 ], dtype=float32)}
episode index:800
target Thresh 18.72160228889124
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.9706736, 10.907739 ,  4.121819 ], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4226849054899904
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0265806, 4.2036276, 3.5479012], dtype=float32)}
episode index:801
target Thresh 18.722990803268104
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.262997 ,  7.826987 ,  2.1760805], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4228401071569489
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7130327, 4.1270165, 4.370471 ], dtype=float32)}
episode index:802
target Thresh 18.724372392400625
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.796976 ,  5.9477515,  1.7067218], dtype=float32)}
done in step count: 262
reward sum = 0.07184904244991483
running average episode reward sum: 0.42240300744996634
{'scaleFactor': 20, 'timeStep': 263, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3666582, 4.7943215, 4.9902735], dtype=float32)}
episode index:803
target Thresh 18.725747090828598
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.101127 , 9.062232 , 3.1229892], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42304862578572633
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2646863, 3.9205964, 4.4035   ], dtype=float32)}
episode index:804
target Thresh 18.727114932919555
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.225119 ,  7.09015  ,  1.8244396], dtype=float32)}
done in step count: 404
reward sum = 0.01724322985330079
running average episode reward sum: 0.4225445197038227
{'scaleFactor': 20, 'timeStep': 405, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5154333, 3.153271 , 3.7347116], dtype=float32)}
episode index:805
target Thresh 18.728475952869626
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.200088, 10.796043,  4.936884], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.42283374389845085
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.695493 , 4.3137636, 3.0837712], dtype=float32)}
episode index:806
target Thresh 18.729830184704376
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.912801,  8.010857,  3.072227], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42294812230430634
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1373756, 3.5719233, 5.4041567], dtype=float32)}
episode index:807
target Thresh 18.731177662279674
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.802883 , 9.8449335, 5.6031103], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4232865692049812
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.018879 , 4.9134874, 3.3450627], dtype=float32)}
episode index:808
target Thresh 18.732518419282524
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.539456 ,  6.2506285,  2.5317514], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.423881248445777
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5797627, 4.262934 , 4.395423 ], dtype=float32)}
episode index:809
target Thresh 18.73385248923193
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.11269  , 11.494999 ,  1.6631649], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42427114860002707
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1783936, 4.7696724, 3.9421058], dtype=float32)}
episode index:810
target Thresh 18.7351799054797
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.07180965, 9.516597  , 2.744037  ], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42479788919496525
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2566602, 4.6686935, 5.873139 ], dtype=float32)}
episode index:811
target Thresh 18.736500701211313
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.096103 ,  8.024793 ,  4.2166142], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4247210100430755
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3356206, 3.2324443, 4.861133 ], dtype=float32)}
episode index:812
target Thresh 18.737814909446737
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1259358 , 8.710014  , 0.61349016], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.42480116903810766
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.162085 , 3.2304475, 4.1219406], dtype=float32)}
episode index:813
target Thresh 18.73912256304124
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.7667062, 10.09283  ,  0.6357272], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42505303938474787
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3170974, 4.437832 , 5.957134 ], dtype=float32)}
episode index:814
target Thresh 18.74042369468623
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.076183 , 9.877604 , 1.4957687], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4245315019131101
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.051585,  8.791187,  6.225654], dtype=float32)}
episode index:815
target Thresh 18.74171833691007
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.239056 , 11.029216 ,  1.4860134], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.4241441925682506
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.081565, 4.651644, 5.956453], dtype=float32)}
episode index:816
target Thresh 18.74300652207888
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6760318, 8.059803 , 1.3817544], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.42402216238134144
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6106693, 3.325211 , 0.214573 ], dtype=float32)}
episode index:817
target Thresh 18.744288282397356
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.093282 , 6.075535 , 1.6497564], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.42433821669995886
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1165717, 3.6559694, 3.925906 ], dtype=float32)}
episode index:818
target Thresh 18.745563649909574
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.9396787, 9.917975 , 3.9629884], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42499298812034964
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9551034, 4.988101 , 3.662341 ], dtype=float32)}
episode index:819
target Thresh 18.746832656499787
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4214582, 8.38682  , 4.521926 ], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4249393955959402
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0416076, 4.2717676, 1.5508462], dtype=float32)}
episode index:820
target Thresh 18.74809533389323
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.922265 , 11.689592 ,  0.4886069], dtype=float32)}
done in step count: 280
reward sum = 0.05995901467146544
running average episode reward sum: 0.4244948397117447
{'scaleFactor': 20, 'timeStep': 281, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1555429, 2.8861287, 4.651373 ], dtype=float32)}
episode index:821
target Thresh 18.749351713656896
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.189829 , 10.50486  ,  3.2067292], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.42505674972634855
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9844904, 1.5272415, 4.0677876], dtype=float32)}
episode index:822
target Thresh 18.750601827200352
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.9018173, 9.940691 , 3.2062223], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4253369471392863
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5284016, 3.484587 , 5.643177 ], dtype=float32)}
episode index:823
target Thresh 18.751845705776496
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1276876, 3.7561502, 3.7421138], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42603435375683574
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1276876, 3.7561502, 3.7421138], dtype=float32)}
episode index:824
target Thresh 18.753083380482362
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.522905 ,  5.7875156,  3.5642133], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.425839604539855
{'scaleFactor': 20, 'timeStep': 133, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1060275, 4.551992 , 2.8875349], dtype=float32)}
episode index:825
target Thresh 18.75431488225988
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.019774 , 7.6452065, 5.1564326], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.42643001330855207
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2901092, 2.535625 , 3.7448087], dtype=float32)}
episode index:826
target Thresh 18.755540241896654
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.944417 , 10.7178335,  4.9976034], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4270759214061234
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4138093, 3.8522015, 3.5375955], dtype=float32)}
episode index:827
target Thresh 18.75675949002675
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.416151,  7.981474,  4.732127], dtype=float32)}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.4266872607544836
{'scaleFactor': 20, 'timeStep': 225, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7461643 , 1.154717  , 0.69022596], dtype=float32)}
episode index:828
target Thresh 18.757972657131422
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.861662 , 10.049227 ,  0.3868423], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.42635306792559236
{'scaleFactor': 20, 'timeStep': 190, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.214688 , 4.7275324, 3.829258 ], dtype=float32)}
episode index:829
target Thresh 18.75917977353992
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.086386 , 10.8985405,  1.9040406], dtype=float32)}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.4259754585123968
{'scaleFactor': 20, 'timeStep': 218, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.913861 , 3.8840556, 3.9798865], dtype=float32)}
episode index:830
target Thresh 18.76038086943021
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.039419 , 10.490247 ,  5.9506903], dtype=float32)}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.42570629433503987
{'scaleFactor': 20, 'timeStep': 160, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9753013, 3.6124182, 5.6133037], dtype=float32)}
episode index:831
target Thresh 18.76157597482976
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.0018835, 10.7780075,  1.528589 ], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.42577755043592824
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8602133, 4.5716896, 4.0316706], dtype=float32)}
episode index:832
target Thresh 18.76276511961626
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.325565 ,  7.2081265,  3.338487 ], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.42622875456450343
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1904883, 4.4513392, 5.1130214], dtype=float32)}
episode index:833
target Thresh 18.763948333518396
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.88989 , 7.112523, 2.454135], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4260659964586826
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.73371  , 4.1783648, 3.5909266], dtype=float32)}
episode index:834
target Thresh 18.765125646116577
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.0055685,  4.9299803,  1.9008322], dtype=float32)}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.4256598893480713
{'scaleFactor': 20, 'timeStep': 244, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9340467, 3.4301343, 5.3259106], dtype=float32)}
episode index:835
target Thresh 18.766297086843675
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.847481  ,  8.017403  ,  0.28503576], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4257024277189685
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.176701 , 1.5635442, 5.877545 ], dtype=float32)}
episode index:836
target Thresh 18.767462684985773
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3282359, 3.0275302, 4.98393  ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4263885657981573
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3282359, 3.0275302, 4.98393  ], dtype=float32)}
episode index:837
target Thresh 18.76862246968288
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0120783, 6.770741 , 3.5853012], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4268958082865782
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6353205, 3.043878 , 5.579743 ], dtype=float32)}
episode index:838
target Thresh 18.769776469929685
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.361713 ,  4.187375 ,  4.0656385], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42638699325882307
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.5473957, 9.926161 , 0.7851223], dtype=float32)}
episode index:839
target Thresh 18.770924714576246
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.839881, 10.191996,  5.896869], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42587938969541966
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.34556  , 11.6387415,  1.9551425], dtype=float32)}
episode index:840
target Thresh 18.77206723232874
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8449292, 10.1908   ,  2.7924056], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42609238217751494
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.127823, 4.964885, 5.327447], dtype=float32)}
episode index:841
target Thresh 18.77320405175017
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.698884 , 10.962797 ,  1.2214432], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42659756672492277
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3813982, 4.33168  , 3.7346044], dtype=float32)}
episode index:842
target Thresh 18.77433520126108
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.4806352, 11.197389 ,  2.967256 ], dtype=float32)}
done in step count: 387
reward sum = 0.02045598088772655
running average episode reward sum: 0.42611578548430923
{'scaleFactor': 20, 'timeStep': 388, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6393538, 2.673441 , 3.539806 ], dtype=float32)}
episode index:843
target Thresh 18.77546070914027
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.696612 , 12.230372 ,  0.8295777], dtype=float32)}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.42579922970672096
{'scaleFactor': 20, 'timeStep': 184, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0036876, 1.0007764, 5.8224454], dtype=float32)}
episode index:844
target Thresh 18.776580603525492
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.943681 , 11.031177 ,  5.0073805], dtype=float32)}
done in step count: 439
reward sum = 0.012129710294652202
running average episode reward sum: 0.42530967997960606
{'scaleFactor': 20, 'timeStep': 440, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4062085, 3.8203816, 5.956888 ], dtype=float32)}
episode index:845
target Thresh 18.777694912414166
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.953925 ,  4.8488765,  4.803605 ], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4255742106514604
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8366263, 4.3710427, 4.7609496], dtype=float32)}
episode index:846
target Thresh 18.778803663664075
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.90737  ,  8.9431   ,  2.1395004], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4259363466125028
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7046607, 4.3403416, 4.455596 ], dtype=float32)}
episode index:847
target Thresh 18.779906884994052
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.780047 ,  9.178734 ,  1.1288304], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.42616199967185403
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8563662 , 4.876189  , 0.35816374], dtype=float32)}
episode index:848
target Thresh 18.78100460398469
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9500427 , 9.666789  , 0.93781257], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42626679957497765
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1747423, 3.977643 , 4.465217 ], dtype=float32)}
episode index:849
target Thresh 18.782096848079025
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.981114 , 9.427804 , 5.6493335], dtype=float32)}
done in step count: 294
reward sum = 0.05208914293358933
running average episode reward sum: 0.42582659056716426
{'scaleFactor': 20, 'timeStep': 295, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.520582, 4.15899 , 4.364981], dtype=float32)}
episode index:850
target Thresh 18.78318364458321
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.960164, 11.274503,  4.102268], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4262587735202506
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.759523, 4.551654, 4.322642], dtype=float32)}
episode index:851
target Thresh 18.78426502066722
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.89004   , 10.666253  ,  0.18258844], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42575846979546156
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.197498 ,  7.299035 ,  3.1116087], dtype=float32)}
episode index:852
target Thresh 18.785341003365506
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.18241  , 11.105562 ,  5.8776293], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4256435393003178
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1173055, 2.875226 , 5.4632945], dtype=float32)}
episode index:853
target Thresh 18.786411619577702
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.5568905, 9.436216 , 4.0034876], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4256238398098286
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4436765, 3.7448268, 4.238739 ], dtype=float32)}
episode index:854
target Thresh 18.787476896069258
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.696023 ,  8.922197 ,  1.9153081], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42572853370177477
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4720683, 4.59137  , 5.764356 ], dtype=float32)}
episode index:855
target Thresh 18.78853685947215
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.948068 ,  3.7586603,  4.0233035], dtype=float32)}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.4254008098378893
{'scaleFactor': 20, 'timeStep': 193, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.427377 , 3.0938802, 4.6551332], dtype=float32)}
episode index:856
target Thresh 18.78959153628551
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.190263 ,  5.372928 ,  3.5392718], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4249044261624659
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.32388  ,  3.8034406,  2.8242543], dtype=float32)}
episode index:857
target Thresh 18.790640952876323
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.90892  , 11.00912  ,  2.9220014], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4248809151444191
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4334836, 3.5431528, 4.3930054], dtype=float32)}
episode index:858
target Thresh 18.791685135480055
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.21186  , 7.9273777, 4.8421283], dtype=float32)}
done in step count: 294
reward sum = 0.05208914293358933
running average episode reward sum: 0.42444693170761955
{'scaleFactor': 20, 'timeStep': 295, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.173949 , 4.221631 , 4.8465385], dtype=float32)}
episode index:859
target Thresh 18.79272411020132
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.304158 ,  4.0029063,  0.9133815], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42487619607033583
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0242524, 2.8445175, 5.658116 ], dtype=float32)}
episode index:860
target Thresh 18.793757903014548
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.892399 ,  2.9091747,  6.040538 ], dtype=float32)}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.4247167389429219
{'scaleFactor': 20, 'timeStep': 125, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0492258, 4.815213 , 4.8546505], dtype=float32)}
episode index:861
target Thresh 18.794786539764605
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2849936, 4.7940845, 6.088902 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42538412091630595
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2849936, 4.7940845, 6.088902 ], dtype=float32)}
episode index:862
target Thresh 18.79581004616747
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.083884 , 9.9335575, 5.992941 ], dtype=float32)}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.425088806095881
{'scaleFactor': 20, 'timeStep': 177, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.533268 , 2.3541431, 4.565869 ], dtype=float32)}
episode index:863
target Thresh 18.796828447810856
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4088035, 5.786079 , 4.4941254], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.425742638496233
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1773226, 3.8470669, 4.437506 ], dtype=float32)}
episode index:864
target Thresh 18.797841770154854
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.7368422, 10.867289 ,  0.4425369], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4260084380130861
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.99355  , 1.6570352, 4.3786616], dtype=float32)}
episode index:865
target Thresh 18.798850038532578
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.160749 , 11.093258 ,  1.8633382], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4261483320135222
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.070383 , 4.8815827, 5.5217714], dtype=float32)}
episode index:866
target Thresh 18.79985327815079
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.872869 ,  7.10142  ,  2.4614263], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.42628159238717084
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8801173, 1.1850834, 5.3317943], dtype=float32)}
episode index:867
target Thresh 18.800851514090528
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.415926 ,  8.379755 ,  4.1790333], dtype=float32)}
done in step count: 441
reward sum = 0.011888329059788622
running average episode reward sum: 0.42580418079347565
{'scaleFactor': 20, 'timeStep': 442, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2404102, 4.044686 , 3.5274954], dtype=float32)}
episode index:868
target Thresh 18.801844771307746
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8278463, 8.777819 , 3.314275 ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4264195914139665
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.654639 , 3.5167127, 4.551036 ], dtype=float32)}
episode index:869
target Thresh 18.802833074633927
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.008844 ,  2.1553435,  3.7921534], dtype=float32)}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.426213751292679
{'scaleFactor': 20, 'timeStep': 140, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1323569, 4.9494996, 3.4344254], dtype=float32)}
episode index:870
target Thresh 18.803816448776708
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.16618 ,  7.293733,  4.96904 ], dtype=float32)}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.42585407678484966
{'scaleFactor': 20, 'timeStep': 218, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3631653, 2.80969  , 5.5711017], dtype=float32)}
episode index:871
target Thresh 18.804794918320486
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.197403 , 9.890749 , 3.5922694], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4263720434662879
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3613269, 1.1520808, 4.979092 ], dtype=float32)}
episode index:872
target Thresh 18.805768507727056
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.6927595, 10.841294 ,  4.7847214], dtype=float32)}
done in step count: 281
reward sum = 0.059359424524750785
running average episode reward sum: 0.42595163954997456
{'scaleFactor': 20, 'timeStep': 282, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.33518 , 4.364271, 4.2517  ], dtype=float32)}
episode index:873
target Thresh 18.806737241336204
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.128859 , 9.359098 , 1.4892453], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4254642806946542
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.71656  ,  5.596766 ,  1.9939827], dtype=float32)}
episode index:874
target Thresh 18.80770114336632
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8384637, 10.040375 ,  1.4046183], dtype=float32)}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.4252723039244477
{'scaleFactor': 20, 'timeStep': 136, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4946566, 4.6898527, 3.7454033], dtype=float32)}
episode index:875
target Thresh 18.808660237915003
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.844036 ,  9.544505 ,  3.6394079], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.42491067695251084
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.550165 , 4.4249153, 5.71461  ], dtype=float32)}
episode index:876
target Thresh 18.80961454895967
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.902341, 5.130445, 2.088076], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.425543732052907
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6642272, 4.9188075, 1.8167373], dtype=float32)}
episode index:877
target Thresh 18.810564100358143
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4867241, 5.9456215, 4.3208194], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4261866207407739
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5415628, 3.9770422, 4.4217706], dtype=float32)}
episode index:878
target Thresh 18.81150891584926
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.811366, 10.981832,  1.480962], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.42600064345580174
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.187821 , 3.1025548, 5.4472556], dtype=float32)}
episode index:879
target Thresh 18.812449019053457
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.322871  , 10.832158  ,  0.12392395], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.42617036403139397
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8465625, 1.989416 , 5.5457597], dtype=float32)}
episode index:880
target Thresh 18.81338443347336
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3349667 , 9.309824  , 0.07633299], dtype=float32)}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.4259305275769951
{'scaleFactor': 20, 'timeStep': 154, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3416538, 4.2494783, 4.879153 ], dtype=float32)}
episode index:881
target Thresh 18.81431518249438
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.050656 ,  9.919476 ,  3.9694493], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4260030442951666
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6857445, 3.5580873, 4.4027023], dtype=float32)}
episode index:882
target Thresh 18.81524128938529
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.9998674, 10.109888 ,  5.709932 ], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.42571566817500395
{'scaleFactor': 20, 'timeStep': 176, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7360045, 4.691101 , 4.326662 ], dtype=float32)}
episode index:883
target Thresh 18.816162777298818
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.02361  , 5.021991 , 3.9912667], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42635399886711367
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5074396, 3.815596 , 3.4744945], dtype=float32)}
episode index:884
target Thresh 18.8170796692722
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.45413  , 11.076519 ,  2.2454736], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4261275515865113
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.913797 , 1.0212086, 4.812253 ], dtype=float32)}
episode index:885
target Thresh 18.817991988227785
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.871079 ,  7.186385 ,  2.9688442], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.426569740509774
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.314201 , 2.9847713, 5.2053065], dtype=float32)}
episode index:886
target Thresh 18.818899756973597
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8263866, 9.2483635, 4.280389 ], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4271827385475307
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2103893, 4.543748 , 4.520797 ], dtype=float32)}
episode index:887
target Thresh 18.8198029982039
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3713698, 9.277036 , 4.0649137], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4275688426097578
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8403025 , 1.669429  , 0.40113804], dtype=float32)}
episode index:888
target Thresh 18.820701734499778
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.0337543, 10.603763 ,  2.517808 ], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42800791808218464
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1425037, 3.8862605, 4.565737 ], dtype=float32)}
episode index:889
target Thresh 18.821595988329673
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.9696274, 9.895721 , 5.520514 ], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4282067924069659
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2485605, 3.357861 , 4.277122 ], dtype=float32)}
episode index:890
target Thresh 18.822485782049988
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.012566 , 11.017406 ,  6.2273235], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.4278765711751489
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7962246, 2.81003  , 6.207592 ], dtype=float32)}
episode index:891
target Thresh 18.82337113790561
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.092883, 9.990192, 5.176339], dtype=float32)}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.4275327284006891
{'scaleFactor': 20, 'timeStep': 211, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0654235, 3.5012796, 4.6397552], dtype=float32)}
episode index:892
target Thresh 18.82425207803048
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.177294  , 6.0245485 , 0.08386438], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.4273541326431485
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4333587, 4.601476 , 4.6143703], dtype=float32)}
episode index:893
target Thresh 18.825128624448144
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.953985 , 10.06679  ,  4.2749987], dtype=float32)}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4272282434267478
{'scaleFactor': 20, 'timeStep': 116, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.968132 , 1.3116564, 0.5519465], dtype=float32)}
episode index:894
target Thresh 18.82600079907231
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.129823 , 10.904075 ,  5.4595942], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.42772155914668325
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8345962, 4.2145805, 4.259045 ], dtype=float32)}
episode index:895
target Thresh 18.826868623707398
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.848855  , 10.105401  ,  0.73976153], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.4274589070861334
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9839964, 2.840465 , 4.3353972], dtype=float32)}
episode index:896
target Thresh 18.827732120049056
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.756039  ,  8.92672   ,  0.15908104], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.42712018927905443
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0071392, 1.0592417, 2.6314301], dtype=float32)}
episode index:897
target Thresh 18.828591309684743
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.925902 , 4.9547987, 1.8068054], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42775814007050317
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.925902 , 4.9547987, 1.8068054], dtype=float32)}
episode index:898
target Thresh 18.829446214094247
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.132477 , 11.784984 ,  2.3088024], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4280264533280106
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8638906, 4.9227343, 0.9541337], dtype=float32)}
episode index:899
target Thresh 18.830296854650218
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.792408 ,  1.9562377,  2.9855018], dtype=float32)}
done in step count: 268
reward sum = 0.0676444472200646
running average episode reward sum: 0.42762602887677953
{'scaleFactor': 20, 'timeStep': 269, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.80325  , 4.4862185, 6.0907207], dtype=float32)}
episode index:900
target Thresh 18.831143252618716
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.758674 , 11.0876045,  5.9327   ], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.427342592585231
{'scaleFactor': 20, 'timeStep': 176, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.939943, 2.586372, 4.227719], dtype=float32)}
episode index:901
target Thresh 18.831985429159737
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.734175,  8.536186,  2.486309], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.4270188569004436
{'scaleFactor': 20, 'timeStep': 200, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9469848, 3.7652922, 3.9724827], dtype=float32)}
episode index:902
target Thresh 18.832823405327737
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2203288, 7.0886297, 3.7690127], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4272295670710326
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9764967, 2.4454432, 5.4087753], dtype=float32)}
episode index:903
target Thresh 18.83365720207216
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.6606455, 11.35878  ,  0.9559332], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42758348579587835
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.948773 , 4.9271455, 3.9722104], dtype=float32)}
episode index:904
target Thresh 18.83448684023797
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0752904e+01, 1.0813423e+01, 1.0460075e-02], dtype=float32)}
done in step count: 341
reward sum = 0.03247890341721044
running average episode reward sum: 0.42714690614684114
{'scaleFactor': 20, 'timeStep': 342, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.927191 , 1.5802807, 1.053773 ], dtype=float32)}
episode index:905
target Thresh 18.835312340566173
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.938497  ,  9.024299  ,  0.74763083], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4266754415705201
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.011591  , -0.34747845,  0.23630023], dtype=float32)}
episode index:906
target Thresh 18.836133723694306
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.127737 ,  5.9169817,  4.5837636], dtype=float32)}
done in step count: 361
reward sum = 0.026564720430486827
running average episode reward sum: 0.42623430516353
{'scaleFactor': 20, 'timeStep': 362, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.802486, 4.167819, 3.584305], dtype=float32)}
episode index:907
target Thresh 18.836951010156998
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.998913 ,  6.8880215,  1.3431296], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.42672165263886636
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4410996, 3.5354269, 4.5749946], dtype=float32)}
episode index:908
target Thresh 18.837764220386454
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.065873 , 10.916015 ,  2.2134414], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42712527489519725
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9597335, 2.275402 , 5.690132 ], dtype=float32)}
episode index:909
target Thresh 18.838573374712965
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6610928, 5.5437737, 3.6511989], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4277438185491586
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3043187, 4.3737154, 4.205888 ], dtype=float32)}
episode index:910
target Thresh 18.839378493365437
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.085865  ,  4.7694626 ,  0.27346846], dtype=float32)}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.4274046459238703
{'scaleFactor': 20, 'timeStep': 213, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0870266, 1.3496206, 1.1556034], dtype=float32)}
episode index:911
target Thresh 18.84017959647188
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.051388 ,  9.832824 ,  4.8193803], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.42722991135259075
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6142894, 4.398712 , 5.6019864], dtype=float32)}
episode index:912
target Thresh 18.840976704059912
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.937621, 8.258619, 4.713975], dtype=float32)}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.42684811626586683
{'scaleFactor': 20, 'timeStep': 254, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5751083, 3.7819085, 4.022171 ], dtype=float32)}
episode index:913
target Thresh 18.841769836057264
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.039251, 11.158542,  0.840518], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42689597023229986
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5963707, 2.1169765, 1.0526817], dtype=float32)}
episode index:914
target Thresh 18.84255901229227
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.909881 , 10.961326 ,  5.8494897], dtype=float32)}
done in step count: 437
reward sum = 0.01237599254632405
running average episode reward sum: 0.4264429429342824
{'scaleFactor': 20, 'timeStep': 438, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0591998, 1.0937549, 4.7899394], dtype=float32)}
episode index:915
target Thresh 18.84334425249439
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.70003  ,  9.232073 ,  5.4315557], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.42641044824376684
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5091796, 4.045816 , 5.619861 ], dtype=float32)}
episode index:916
target Thresh 18.84412557629466
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.932994 , 6.134099 , 2.3813872], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.4263527097838001
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9606123, 4.969823 , 4.9970675], dtype=float32)}
episode index:917
target Thresh 18.844903003226218
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.812551 ,  7.247946 ,  2.8028052], dtype=float32)}
done in step count: 319
reward sum = 0.0405160479665409
running average episode reward sum: 0.4259324084092715
{'scaleFactor': 20, 'timeStep': 320, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6696918, 3.6570542, 4.9882646], dtype=float32)}
episode index:918
target Thresh 18.84567655272478
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.014857  , -0.02188492,  3.9382873 ], dtype=float32)}
done in step count: 307
reward sum = 0.045709317994222766
running average episode reward sum: 0.4255186727287328
{'scaleFactor': 20, 'timeStep': 308, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.286839, 4.188485, 4.646147], dtype=float32)}
episode index:919
target Thresh 18.846446244129123
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.759319 ,  1.9880388,  3.3325908], dtype=float32)}
done in step count: 374
reward sum = 0.02331110064784238
running average episode reward sum: 0.42508149058516664
{'scaleFactor': 20, 'timeStep': 375, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4378791, 3.2659888, 4.85916  ], dtype=float32)}
episode index:920
target Thresh 18.84721209668157
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.061523  , 5.889958  , 0.61950195], dtype=float32)}
done in step count: 378
reward sum = 0.022392550271025807
running average episode reward sum: 0.42464426046539017
{'scaleFactor': 20, 'timeStep': 379, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9758816, 3.8325458, 5.7126517], dtype=float32)}
episode index:921
target Thresh 18.847974129528478
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.1276903, 5.2990875, 5.415305 ], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4251547745584415
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7595534, 2.0544424, 4.065131 ], dtype=float32)}
episode index:922
target Thresh 18.848732361720703
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.955417 ,  5.5474195,  3.8374271], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.42525790104608163
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.039305 , 4.931117 , 6.1780634], dtype=float32)}
episode index:923
target Thresh 18.849486812214096
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0328174, 7.1218076, 4.7404294], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42584777236529586
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.836089 , 3.2542624, 4.9480896], dtype=float32)}
episode index:924
target Thresh 18.85023749986995
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.096126, 9.054373, 4.371531], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4264258785681442
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7683825, 4.2562523, 4.352925 ], dtype=float32)}
episode index:925
target Thresh 18.85098444345551
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.1038423, 11.157224 ,  4.89797  ], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.426740462427788
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4749837, 1.066543 , 2.07991  ], dtype=float32)}
episode index:926
target Thresh 18.851727661644386
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.794815 , 7.0852003, 5.7175775], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.42690704921494044
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.186407 , 2.0128865, 4.407002 ], dtype=float32)}
episode index:927
target Thresh 18.852467173017086
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.841142, 8.704727, 4.420116], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4274021761788426
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8778626, 4.7785926, 5.10682  ], dtype=float32)}
episode index:928
target Thresh 18.853202996061427
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.866196 , 10.138739 ,  1.9408861], dtype=float32)}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.42715769270271625
{'scaleFactor': 20, 'timeStep': 161, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.780063, 4.039484, 4.312024], dtype=float32)}
episode index:929
target Thresh 18.853935149173022
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.367265 ,  9.100788 ,  4.6152477], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4275431985608931
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4280837, 3.258224 , 4.870512 ], dtype=float32)}
episode index:930
target Thresh 18.854663650655738
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.886037  ,  3.067509  ,  0.09102762], dtype=float32)}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.4272077389907028
{'scaleFactor': 20, 'timeStep': 216, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3776507, 3.452589 , 5.03423  ], dtype=float32)}
episode index:931
target Thresh 18.855388518722155
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.895995 , 10.890449 ,  3.7408469], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4276358086096351
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.20027  , 4.8974385, 4.7971425], dtype=float32)}
episode index:932
target Thresh 18.856109771494005
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.823047,  8.996647,  6.226209], dtype=float32)}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.4272626142292288
{'scaleFactor': 20, 'timeStep': 253, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1862941, 1.4154751, 5.4770064], dtype=float32)}
episode index:933
target Thresh 18.85682742700265
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.2844214, 7.927672 , 4.65175  ], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.42693752474304797
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6298406, 4.533443 , 5.727797 ], dtype=float32)}
episode index:934
target Thresh 18.857541503189506
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.96937   ,  7.114501  ,  0.46094322], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4267763633986161
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7595515, 3.719554 , 2.5899458], dtype=float32)}
episode index:935
target Thresh 18.85825201790653
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.077966 , 10.42846  ,  2.9979858], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4273162020572789
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.573058 , 3.2537885, 4.7074347], dtype=float32)}
episode index:936
target Thresh 18.858958988916616
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9063027, 7.7798486, 0.2067639], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.4270045871190182
{'scaleFactor': 20, 'timeStep': 200, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7345225, 4.6670713, 3.741181 ], dtype=float32)}
episode index:937
target Thresh 18.85966243389408
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.750695,  9.562986,  4.211792], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4268717511366621
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5158681, 3.4674156, 5.8474545], dtype=float32)}
episode index:938
target Thresh 18.860362370425083
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.205666 , 10.049953 ,  0.6438772], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4264171486327892
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.25930363,  5.513797  ,  4.8089027 ], dtype=float32)}
episode index:939
target Thresh 18.86105881600807
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.6170864, 5.6648216, 5.713139 ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42692562195872114
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7223992, 1.012392 , 3.3352754], dtype=float32)}
episode index:940
target Thresh 18.861751788054224
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.374133,  8.26139 ,  4.425674], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4270475767451273
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.599841 , 1.1416771, 4.164688 ], dtype=float32)}
episode index:941
target Thresh 18.862441303887877
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.174414 ,  9.196152 ,  4.3415775], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.42685945517369767
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0169665, 3.0370524, 4.7709894], dtype=float32)}
episode index:942
target Thresh 18.863127380746963
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9430182, 8.881022 , 1.7254678], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.42654887216169796
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4176936, 3.4345608, 4.9259787], dtype=float32)}
episode index:943
target Thresh 18.86381003578344
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.851035,  3.072678,  3.904524], dtype=float32)}
done in step count: 252
reward sum = 0.07944545169055386
running average episode reward sum: 0.4261811778603514
{'scaleFactor': 20, 'timeStep': 253, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.201455 , 1.7075783, 5.086113 ], dtype=float32)}
episode index:944
target Thresh 18.86448928606372
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.8714988, 10.91898  ,  2.0825589], dtype=float32)}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.4259508194305889
{'scaleFactor': 20, 'timeStep': 157, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3668765, 4.362393 , 3.7215643], dtype=float32)}
episode index:945
target Thresh 18.86516514856909
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0965579, 6.7849636, 1.2979052], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.42581702405202837
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3133653, 3.334306 , 4.5188937], dtype=float32)}
episode index:946
target Thresh 18.865837640196155
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.901592  ,  0.28165877,  3.9892666 ], dtype=float32)}
done in step count: 382
reward sum = 0.021510194444071807
running average episode reward sum: 0.4253900897018616
{'scaleFactor': 20, 'timeStep': 383, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0367547, 4.494751 , 4.23201  ], dtype=float32)}
episode index:947
target Thresh 18.866506777757234
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.939944 ,  3.881654 ,  2.2059765], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.42502268015822026
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5699193, 4.9813833, 3.0195186], dtype=float32)}
episode index:948
target Thresh 18.867172577980806
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6664164, 9.725181 , 1.0076494], dtype=float32)}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.42462754205780734
{'scaleFactor': 20, 'timeStep': 299, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3126398, 1.3894827, 4.5217867], dtype=float32)}
episode index:949
target Thresh 18.867835057511908
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.014429  , 11.033572  ,  0.72018504], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4246240754052778
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9719987, 3.5544157, 4.466041 ], dtype=float32)}
episode index:950
target Thresh 18.868494232912564
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.799309 ,  6.8546104,  5.3754277], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.4246979077756195
{'scaleFactor': 20, 'timeStep': 71, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6722815, 4.9129934, 3.452268 ], dtype=float32)}
episode index:951
target Thresh 18.869150120662198
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.349717 , 10.9190445,  2.9602482], dtype=float32)}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.4244187531552667
{'scaleFactor': 20, 'timeStep': 184, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3169744, 2.6939564, 4.69221  ], dtype=float32)}
episode index:952
target Thresh 18.86980273715803
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3328292 , 7.9369173 , 0.44799542], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42456511877677466
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6154358, 2.7937717, 5.510329 ], dtype=float32)}
episode index:953
target Thresh 18.87045209871551
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.172421 ,  8.968979 ,  4.2345357], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4241200819646397
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.87206  ,  1.5125061,  5.075461 ], dtype=float32)}
episode index:954
target Thresh 18.87109822156871
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.635794 , 10.393578 ,  2.1883903], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4242845283857428
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1859062, 1.2624966, 5.1752996], dtype=float32)}
episode index:955
target Thresh 18.871741121870738
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.041479 ,  1.9032332,  1.8894075], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4243329615585461
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.279069 , 3.81746  , 5.6575994], dtype=float32)}
episode index:956
target Thresh 18.87238081569413
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1157274, 7.2227144, 3.94957  ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4249137003656949
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4487278, 3.5688431, 5.1032577], dtype=float32)}
episode index:957
target Thresh 18.87301731903127
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0468445, 5.5137243, 2.9046483], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42510168822245054
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5840601, 2.7694516, 5.0824113], dtype=float32)}
episode index:958
target Thresh 18.87365064779477
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.76498  ,  9.193682 ,  3.2103662], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42519557292443316
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0911512, 3.734097 , 5.161838 ], dtype=float32)}
episode index:959
target Thresh 18.874280817817883
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.829499 , 11.014712 ,  5.9742756], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4247526608693036
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.156654 , 13.413508 ,  1.4266406], dtype=float32)}
episode index:960
target Thresh 18.874907844854896
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.192695 ,  2.2434907,  5.339028 ], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.4247580056691522
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.607048, 4.709928, 4.096207], dtype=float32)}
episode index:961
target Thresh 18.87553174458151
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.484644 ,  0.7340826,  4.027334 ], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4245787984523333
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7665093, 3.4297812, 4.5760484], dtype=float32)}
episode index:962
target Thresh 18.876152532595263
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.884377 , 9.822639 , 6.1539054], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.42424185875950793
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1193526, 3.955836 , 3.884647 ], dtype=float32)}
episode index:963
target Thresh 18.87677022441588
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.867704,  8.923532,  2.221299], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42477841300291197
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.818005 , 4.4543123, 3.949798 ], dtype=float32)}
episode index:964
target Thresh 18.877384835485685
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.837541 ,  3.9605381,  0.4928869], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4251203206445707
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8765206, 3.676537 , 4.624304 ], dtype=float32)}
episode index:965
target Thresh 18.877996381169993
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8362565, 3.271194 , 4.305783 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42571543418427615
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8362565, 3.271194 , 4.305783 ], dtype=float32)}
episode index:966
target Thresh 18.878604876757475
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1451817, 6.690869 , 6.233766 ], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42604013422481807
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.578728 , 4.025708 , 2.0662837], dtype=float32)}
episode index:967
target Thresh 18.879210337460556
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.979854 , 11.2469635,  0.6210294], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.42594198645012404
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4775091, 3.8049624, 4.262802 ], dtype=float32)}
episode index:968
target Thresh 18.87981277841578
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.02183   , 9.855441  , 0.26006234], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.42564927798916635
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1668906, 4.3279085, 4.239053 ], dtype=float32)}
episode index:969
target Thresh 18.880412214684203
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.725355  ,  0.83068013,  5.7397666 ], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.4253678660863691
{'scaleFactor': 20, 'timeStep': 188, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.921358 , 3.24373  , 3.7687352], dtype=float32)}
episode index:970
target Thresh 18.88100866125177
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.0840435,  9.819442 ,  1.8456584], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42492979413365406
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.0858    ,  2.8480005 ,  0.84465855], dtype=float32)}
episode index:971
target Thresh 18.881602133029666
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.847742  ,  5.0817256 ,  0.09174609], dtype=float32)}
done in step count: 211
reward sum = 0.11995712819347787
running average episode reward sum: 0.4246160362468843
{'scaleFactor': 20, 'timeStep': 212, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3155966, 4.172144 , 3.953232 ], dtype=float32)}
episode index:972
target Thresh 18.882192644854722
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.722137 ,  5.4219337,  2.484813 ], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.4245130860861614
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2535481, 1.1698011, 3.927106 ], dtype=float32)}
episode index:973
target Thresh 18.882780211489763
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.670528, 12.028223,  2.343377], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4246983971550026
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.265878 , 3.6059034, 4.5238543], dtype=float32)}
episode index:974
target Thresh 18.883364847623987
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.129371 , 10.898817 ,  0.2582074], dtype=float32)}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.4243274747427741
{'scaleFactor': 20, 'timeStep': 276, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0533218, 1.2587365, 5.5673676], dtype=float32)}
episode index:975
target Thresh 18.88394656787333
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.927234,  8.922978,  5.648287], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.42431158611539677
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.53444  , 3.5785704, 4.9473104], dtype=float32)}
episode index:976
target Thresh 18.88452538678082
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7515671, 8.919754 , 3.8832893], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4248604954540709
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2420692, 4.508875 , 4.8099637], dtype=float32)}
episode index:977
target Thresh 18.885101318816968
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.104502 , 5.1262836, 1.1945814], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.4245370052506493
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8322875, 1.8058898, 4.6818123], dtype=float32)}
episode index:978
target Thresh 18.885674378380102
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8403603, 5.9060845, 5.7125015], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.425094474091047
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9925766, 4.1879945, 4.9515743], dtype=float32)}
episode index:979
target Thresh 18.88624457979674
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.6827846, 11.500592 ,  1.9346243], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42528429323945705
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2468204, 2.8542886, 1.2359766], dtype=float32)}
episode index:980
target Thresh 18.88681193732195
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.959651,  6.06735 ,  4.071324], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.42516529240092005
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0531551, 1.2268094, 4.9614043], dtype=float32)}
episode index:981
target Thresh 18.887376465139695
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.1186905 , 10.14489   ,  0.12591332], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.42491678085379686
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1658717, 3.2216163, 5.566262 ], dtype=float32)}
episode index:982
target Thresh 18.887938177363207
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.1783   , 10.810051 ,  1.9733753], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.424972960584944
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1311773, 3.9788392, 3.8398411], dtype=float32)}
episode index:983
target Thresh 18.888497088035315
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.948987,  5.856835,  2.759518], dtype=float32)}
done in step count: 406
reward sum = 0.016900089579220106
running average episode reward sum: 0.4245582523827024
{'scaleFactor': 20, 'timeStep': 407, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9820619, 4.8488984, 5.850507 ], dtype=float32)}
episode index:984
target Thresh 18.889053211128815
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.702263 ,  4.9658957,  2.7052486], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4248342472717043
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4193555, 1.9807106, 5.4749374], dtype=float32)}
episode index:985
target Thresh 18.889606560546813
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2171383, 5.9092827, 4.105471 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4254074376902929
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5912262, 4.5319834, 4.149946 ], dtype=float32)}
episode index:986
target Thresh 18.890157150123077
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.583035 ,  7.176842 ,  2.7114835], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4259303077122895
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6837783, 4.8388605, 4.1665363], dtype=float32)}
episode index:987
target Thresh 18.890704993622375
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.6261666, 10.926879 ,  3.2771962], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42638738333504933
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1035218, 3.480852 , 5.1898146], dtype=float32)}
episode index:988
target Thresh 18.89125010474082
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.200723 , 10.956831 ,  3.3727407], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.42616286976418094
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1339226, 3.5681534, 5.6980906], dtype=float32)}
episode index:989
target Thresh 18.891792497106216
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.649782 ,  5.8965054,  4.0346527], dtype=float32)}
done in step count: 368
reward sum = 0.024760055390093627
running average episode reward sum: 0.42575741237592435
{'scaleFactor': 20, 'timeStep': 369, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9556592, 4.846221 , 4.012141 ], dtype=float32)}
episode index:990
target Thresh 18.89233218427841
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.868989 ,  9.164919 ,  1.4511058], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.42595697643497105
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3492807, 4.468874 , 4.419974 ], dtype=float32)}
episode index:991
target Thresh 18.892869179749596
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.082099, 10.127036,  5.957978], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.42566264447773616
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1474097, 1.7773538, 5.445951 ], dtype=float32)}
episode index:992
target Thresh 18.8934034969447
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.945929 ,  8.100274 ,  2.6664033], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4256846591497262
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.540466 , 4.1724825, 5.7903633], dtype=float32)}
episode index:993
target Thresh 18.89393514922168
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.088681 ,  7.7695756,  4.5625815], dtype=float32)}
done in step count: 331
reward sum = 0.0359128119792669
running average episode reward sum: 0.4252925345549873
{'scaleFactor': 20, 'timeStep': 332, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8530126, 2.769485 , 4.9956627], dtype=float32)}
episode index:994
target Thresh 18.894464149871858
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5897837, 6.414133 , 2.5844643], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.42551094567813275
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.021415 , 4.262759 , 3.5589406], dtype=float32)}
episode index:995
target Thresh 18.89499051212029
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1142259, 5.961205 , 1.837002 ], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.4253935094582096
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2922835, 3.9902453, 5.0680227], dtype=float32)}
episode index:996
target Thresh 18.895514249126055
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.481936 , 12.691202 ,  0.6756643], dtype=float32)}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.42504895958819405
{'scaleFactor': 20, 'timeStep': 250, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5283527, 2.0600095, 3.5079825], dtype=float32)}
episode index:997
target Thresh 18.89603537398261
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.033455 ,  2.93611  ,  4.2086024], dtype=float32)}
done in step count: 409
reward sum = 0.016398140018627688
running average episode reward sum: 0.4246394898291063
{'scaleFactor': 20, 'timeStep': 410, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6109028, 4.351404 , 6.0632315], dtype=float32)}
episode index:998
target Thresh 18.896553899718096
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9428556, 9.782139 , 0.1681853], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.42456994160575895
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6207613, 4.288139 , 0.4426406], dtype=float32)}
episode index:999
target Thresh 18.89706983929569
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.996636 , 10.914118 ,  2.5938518], dtype=float32)}
done in step count: 432
reward sum = 0.01301379814397157
running average episode reward sum: 0.4241583854622971
{'scaleFactor': 20, 'timeStep': 433, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4018188, 4.9820423, 5.3804107], dtype=float32)}
episode index:1000
target Thresh 18.8975832056139
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.498853 ,  9.789823 ,  3.5805893], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.42459384996697147
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.970796 , 3.3630369, 4.81232  ], dtype=float32)}
episode index:1001
target Thresh 18.89809401150692
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.937028 , 11.123939 ,  3.0920222], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.42476786409521416
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1780015, 1.6973543, 2.6037664], dtype=float32)}
episode index:1002
target Thresh 18.89860226974492
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.239186 ,  7.6913996,  0.9041526], dtype=float32)}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.42464891211299394
{'scaleFactor': 20, 'timeStep': 119, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.828619, 4.913825, 5.035614], dtype=float32)}
episode index:1003
target Thresh 18.89910799303438
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.30006  ,  6.161756 ,  1.4199789], dtype=float32)}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.42431703913550317
{'scaleFactor': 20, 'timeStep': 239, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.992466 , 3.2978897, 5.1072125], dtype=float32)}
episode index:1004
target Thresh 18.899611194018416
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.789145 ,  6.9058266,  1.2952019], dtype=float32)}
done in step count: 318
reward sum = 0.040925300976303945
running average episode reward sum: 0.42393555481892686
{'scaleFactor': 20, 'timeStep': 319, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.923009  , 1.0690475 , 0.03390771], dtype=float32)}
episode index:1005
target Thresh 18.90011188527707
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.8859901, 10.627856 ,  1.2736866], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.42433538888355576
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3646665, 1.5129852, 4.598067 ], dtype=float32)}
episode index:1006
target Thresh 18.900610079327652
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.0505085,  3.8596728,  3.0029006], dtype=float32)}
done in step count: 451
reward sum = 0.010751591703479103
running average episode reward sum: 0.42392468004822303
{'scaleFactor': 20, 'timeStep': 452, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5806932, 3.5678275, 1.147229 ], dtype=float32)}
episode index:1007
target Thresh 18.901105788625046
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.881654 ,  1.641582 ,  3.4200566], dtype=float32)}
done in step count: 302
reward sum = 0.04806498027926714
running average episode reward sum: 0.4235518033619443
{'scaleFactor': 20, 'timeStep': 303, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2001551, 4.859474 , 5.2747765], dtype=float32)}
episode index:1008
target Thresh 18.901599025562
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.778877, 10.863815,  5.928579], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42313202952313167
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.678148 , 13.296319 ,  1.3710587], dtype=float32)}
episode index:1009
target Thresh 18.902089802469472
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.53281 ,  8.943025,  1.859417], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42343088923247296
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1733918, 3.2622516, 5.4428916], dtype=float32)}
episode index:1010
target Thresh 18.902578131616902
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0167403, 7.034636 , 1.3428526], dtype=float32)}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.42311309253590135
{'scaleFactor': 20, 'timeStep': 228, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.212182 , 4.696989 , 4.8199053], dtype=float32)}
episode index:1011
target Thresh 18.90306402521255
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0996423, 5.090621 , 1.3078371], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.42310717137720305
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.96496  , 4.9249687, 3.5470161], dtype=float32)}
episode index:1012
target Thresh 18.90354749540378
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.393363 ,  8.849456 ,  4.0780435], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.42286473836178506
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2507715 , 1.6300249 , 0.44063604], dtype=float32)}
episode index:1013
target Thresh 18.90402855427737
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.874651,  9.550491,  2.093224], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.423169904664835
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.577444 , 3.1814187, 3.198109 ], dtype=float32)}
episode index:1014
target Thresh 18.904507213859823
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9427311, 10.052373 ,  4.3893867], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4236899245123573
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4323938, 4.241459 , 4.308133 ], dtype=float32)}
episode index:1015
target Thresh 18.904983486117647
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.49893832, 9.355308  , 3.582036  ], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4238744002161176
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.718817 , 3.642367 , 4.3564777], dtype=float32)}
episode index:1016
target Thresh 18.90545738295767
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.975441,  6.796143,  3.824013], dtype=float32)}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.423606237516744
{'scaleFactor': 20, 'timeStep': 189, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.90949  , 1.6219351, 2.1510797], dtype=float32)}
episode index:1017
target Thresh 18.90592891622735
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.2475944, 11.936777 ,  1.881134 ], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42402652389550444
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0001028, 3.7712898, 5.059373 ], dtype=float32)}
episode index:1018
target Thresh 18.906398097715034
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.8760605 ,  6.051962  ,  0.56734526], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.42402805834054186
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8916025, 4.794166 , 4.1572475], dtype=float32)}
episode index:1019
target Thresh 18.90686493915028
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.143736  ,  9.001162  ,  0.08882778], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.42378292282106705
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.632413 , 4.234166 , 5.3603187], dtype=float32)}
episode index:1020
target Thresh 18.907329452204163
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.168269 , 12.76645  ,  1.8115478], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42410705246296965
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0208473, 4.953105 , 3.6326246], dtype=float32)}
episode index:1021
target Thresh 18.90779164848952
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.904164 ,  3.9842105,  3.1431668], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.42443800712234747
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1297984, 3.4247172, 4.789695 ], dtype=float32)}
episode index:1022
target Thresh 18.908251539561284
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9119234, 11.849247 ,  1.8161   ], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4247461814784237
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8738074, 3.901773 , 5.3288846], dtype=float32)}
episode index:1023
target Thresh 18.908709136916762
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6590586, 8.009314 , 2.9913464], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.424893260158598
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8026903, 2.636974 , 5.9168444], dtype=float32)}
episode index:1024
target Thresh 18.90916445199591
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.134119 ,  3.2283604,  1.1693308], dtype=float32)}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.4247324720622265
{'scaleFactor': 20, 'timeStep': 135, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.132391 , 4.8531866, 4.631348 ], dtype=float32)}
episode index:1025
target Thresh 18.909617496181625
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.005497, 8.849951, 5.956637], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.4244863877134246
{'scaleFactor': 20, 'timeStep': 176, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0998325, 1.1482723, 5.7576084], dtype=float32)}
episode index:1026
target Thresh 18.910068280800036
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.2192826, 11.0187435,  2.9776583], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4243921680150063
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.054642 , 1.2065094, 2.3845184], dtype=float32)}
episode index:1027
target Thresh 18.910516817120783
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.037976 , 11.450903 ,  2.5800688], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4243179471458209
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9498613, 4.603291 , 5.9777904], dtype=float32)}
episode index:1028
target Thresh 18.9109631163573
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.842115,  4.072442,  5.183354], dtype=float32)}
done in step count: 300
reward sum = 0.04904089407128572
running average episode reward sum: 0.42395324641397003
{'scaleFactor': 20, 'timeStep': 301, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.969334  , 4.7577724 , 0.41390145], dtype=float32)}
episode index:1029
target Thresh 18.911407189667084
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.611029 , 10.858853 ,  0.2073052], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4240675491611088
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1195562, 4.8825407, 5.331926 ], dtype=float32)}
episode index:1030
target Thresh 18.911849048152
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.951575 ,  5.0533233,  1.8626764], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4245246497480124
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.050373 , 3.7661526, 4.159474 ], dtype=float32)}
episode index:1031
target Thresh 18.912288702858525
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.522137 ,  8.669289 ,  0.8997464], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.42447517264598356
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7487574, 3.5107222, 6.1495905], dtype=float32)}
episode index:1032
target Thresh 18.912726164778057
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.138918 ,  9.333157 ,  4.2939663], dtype=float32)}
done in step count: 318
reward sum = 0.040925300976303945
running average episode reward sum: 0.4241038755775715
{'scaleFactor': 20, 'timeStep': 319, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3948982, 4.1559544, 5.5934877], dtype=float32)}
episode index:1033
target Thresh 18.913161444847162
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.065407 , 6.0581822, 6.2373204], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4243089835180805
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3584433, 3.8684194, 2.8370378], dtype=float32)}
episode index:1034
target Thresh 18.913594553947863
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.428082 ,  9.699356 ,  5.3943663], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42444385908033583
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0627306, 3.6720538, 3.3989007], dtype=float32)}
episode index:1035
target Thresh 18.914025502907915
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.954123  , 8.036305  , 0.99888134], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.424807938935991
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.941729 , 4.128253 , 3.8890965], dtype=float32)}
episode index:1036
target Thresh 18.91445430250106
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.9203439, 10.915313 ,  5.9789457], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4253061763617046
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.471387 , 4.6706486, 4.6876454], dtype=float32)}
episode index:1037
target Thresh 18.91488096344731
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.767674 ,  8.852653 ,  1.4513963], dtype=float32)}
done in step count: 258
reward sum = 0.07479631572685258
running average episode reward sum: 0.42496849826860744
{'scaleFactor': 20, 'timeStep': 259, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0056409, 1.8426421, 4.722897 ], dtype=float32)}
episode index:1038
target Thresh 18.915305496413218
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.169553 ,  3.2582905,  5.9072623], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.424960945219199
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0144002, 2.8575444, 5.754222 ], dtype=float32)}
episode index:1039
target Thresh 18.915727912012123
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.878541  , 10.887172  ,  0.02043669], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4251340655287359
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.125815, 2.795957, 6.282226], dtype=float32)}
episode index:1040
target Thresh 18.916148220804434
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.113121, 9.342713, 5.541901], dtype=float32)}
done in step count: 253
reward sum = 0.07865099717364833
running average episode reward sum: 0.42480122876758786
{'scaleFactor': 20, 'timeStep': 254, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.713322 , 2.2538774, 5.838964 ], dtype=float32)}
episode index:1041
target Thresh 18.916566433297906
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7121184, 5.0855746, 5.8764873], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4252614790998731
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.704316 , 2.856915 , 3.8311205], dtype=float32)}
episode index:1042
target Thresh 18.91698255994786
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8316891, 8.964076 , 5.6794677], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42578404623400556
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.065656 , 4.6584163, 4.8883185], dtype=float32)}
episode index:1043
target Thresh 18.91739661115749
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9416271, 4.871852 , 1.3192732], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4263340615153906
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9416271, 4.871852 , 1.3192732], dtype=float32)}
episode index:1044
target Thresh 18.917808597278096
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.879394, 10.171979,  5.337017], dtype=float32)}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.42606090395691104
{'scaleFactor': 20, 'timeStep': 196, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0080906, 2.7590168, 5.017338 ], dtype=float32)}
episode index:1045
target Thresh 18.918218528609355
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.95802  ,  6.279421 ,  1.2142116], dtype=float32)}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.4259124272413805
{'scaleFactor': 20, 'timeStep': 131, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6931462, 2.1654158, 5.9291553], dtype=float32)}
episode index:1046
target Thresh 18.91862641539957
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.261396 ,  3.0998497,  1.3830948], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.425783082510787
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6787374, 2.1868072, 1.7525995], dtype=float32)}
episode index:1047
target Thresh 18.91903226784593
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.93151  , 11.20436  ,  2.6912982], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4258395789685765
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9708695, 4.8365436, 2.5769444], dtype=float32)}
episode index:1048
target Thresh 18.91943609609477
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.251714 , 8.389511 , 4.8596406], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4259876503080898
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2823913, 4.3900914, 4.0640173], dtype=float32)}
episode index:1049
target Thresh 18.919837910241814
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.511367 ,  6.3203206,  2.136805 ], dtype=float32)}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4258082264075948
{'scaleFactor': 20, 'timeStep': 144, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4279964, 3.4207578, 4.4031034], dtype=float32)}
episode index:1050
target Thresh 18.920237720332445
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.810716 , 11.29479  ,  4.7176943], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4261735162665993
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.899013 , 4.6791134, 4.590059 ], dtype=float32)}
episode index:1051
target Thresh 18.920635536361925
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.048502  , 10.0381565 ,  0.04337806], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.4258957654667812
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5651827, 1.3014348, 2.2345448], dtype=float32)}
episode index:1052
target Thresh 18.92103136827568
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.273394 , 10.911671 ,  2.8206365], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42598051508877266
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.010377 , 4.8355055, 4.5837975], dtype=float32)}
episode index:1053
target Thresh 18.921425225969532
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.475328 ,  8.012079 ,  1.7927433], dtype=float32)}
done in step count: 450
reward sum = 0.010860193639877882
running average episode reward sum: 0.42558666279138285
{'scaleFactor': 20, 'timeStep': 451, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0297623, 3.7581434, 4.9132977], dtype=float32)}
episode index:1054
target Thresh 18.921817119289937
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.972604, 4.893485, 5.206834], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42613113040959005
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.972604, 4.893485, 5.206834], dtype=float32)}
episode index:1055
target Thresh 18.922207058034257
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.739159 ,  3.6736867,  0.6730608], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.42611864796709414
{'scaleFactor': 20, 'timeStep': 89, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.234209 , 3.1003492, 3.7829545], dtype=float32)}
episode index:1056
target Thresh 18.92259505195097
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.85007  ,  9.0627365,  5.176403 ], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.42625439305177726
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3250805, 4.7137785, 6.2276206], dtype=float32)}
episode index:1057
target Thresh 18.922981110739958
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.339485 ,  8.2506895,  2.362227 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4258515061018228
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.8306675 , 11.369032  ,  0.79180664], dtype=float32)}
episode index:1058
target Thresh 18.923365244052697
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.329987,  9.801082,  4.615334], dtype=float32)}
done in step count: 201
reward sum = 0.1326398781093821
running average episode reward sum: 0.42557463015471003
{'scaleFactor': 20, 'timeStep': 202, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7668219, 3.71161  , 4.08776  ], dtype=float32)}
episode index:1059
target Thresh 18.92374746149255
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.915793,  9.250282,  4.076981], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42601780338499684
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2259684, 4.766244 , 4.6445465], dtype=float32)}
episode index:1060
target Thresh 18.924127772614966
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4299102, 8.079038 , 4.0500216], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42638716166417895
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5462852, 2.5219834, 6.247966 ], dtype=float32)}
episode index:1061
target Thresh 18.924506186927744
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.96198  ,  8.939366 ,  0.7432915], dtype=float32)}
done in step count: 312
reward sum = 0.04346910660022087
running average episode reward sum: 0.42602659852381747
{'scaleFactor': 20, 'timeStep': 313, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1980078, 4.943648 , 3.783299 ], dtype=float32)}
episode index:1062
target Thresh 18.924882713891268
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.299034 , 7.5366716, 2.9339182], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.4261670765590509
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5941333, 1.9235233, 5.511608 ], dtype=float32)}
episode index:1063
target Thresh 18.925257362918725
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.360253 ,  2.2479012,  5.7211304], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4262223625493846
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3199234, 4.6033745, 5.105709 ], dtype=float32)}
episode index:1064
target Thresh 18.92563014337636
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.291845 ,  2.9928327,  4.9311852], dtype=float32)}
done in step count: 458
reward sum = 0.010021186061657156
running average episode reward sum: 0.42583156332263555
{'scaleFactor': 20, 'timeStep': 459, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.252071 , 2.08797  , 2.6042516], dtype=float32)}
episode index:1065
target Thresh 18.92600106458371
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.569171 ,  9.174283 ,  2.3161206], dtype=float32)}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.4255308441280068
{'scaleFactor': 20, 'timeStep': 225, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.451679 , 4.7410417, 5.172757 ], dtype=float32)}
episode index:1066
target Thresh 18.926370135813816
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.173646 , 9.899049 , 1.6281683], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.4252780314805454
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3082643, 1.5107524, 3.0657744], dtype=float32)}
episode index:1067
target Thresh 18.926737366293484
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.056138 ,  3.1882555,  6.1178236], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.42505996713729954
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6439219, 3.1857677, 6.019342 ], dtype=float32)}
episode index:1068
target Thresh 18.927102765203493
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.009035, 10.884016,  5.001901], dtype=float32)}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.4248369655559884
{'scaleFactor': 20, 'timeStep': 168, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0189921, 4.811421 , 4.193735 ], dtype=float32)}
episode index:1069
target Thresh 18.927466341678837
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9402593, 4.6431594, 4.5745726], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4253745011021977
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9402593, 4.6431594, 4.5745726], dtype=float32)}
episode index:1070
target Thresh 18.927828104808945
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1951346, 9.135537 , 6.011665 ], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4257258139765552
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6068606, 1.5194435, 4.9530807], dtype=float32)}
episode index:1071
target Thresh 18.928188063637915
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.086542 , 5.5663624, 4.1798706], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42624295407545765
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2662249, 3.996277 , 4.564501 ], dtype=float32)}
episode index:1072
target Thresh 18.928546227164734
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.882057,  8.200104,  4.356167], dtype=float32)}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.42593093682348826
{'scaleFactor': 20, 'timeStep': 239, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2270846, 4.2222643, 3.388343 ], dtype=float32)}
episode index:1073
target Thresh 18.928902604343513
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.792803 , 10.733241 ,  0.6796044], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42553435308342913
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.837622 , 13.41827  ,  5.9202447], dtype=float32)}
episode index:1074
target Thresh 18.929257204083694
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.935312 ,  3.2739692,  4.0789227], dtype=float32)}
done in step count: 315
reward sum = 0.04217803066508771
running average episode reward sum: 0.425177742550947
{'scaleFactor': 20, 'timeStep': 316, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7269266, 1.7105896, 2.5186765], dtype=float32)}
episode index:1075
target Thresh 18.929610035250295
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.128729 ,  9.590677 ,  1.0402983], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.42530143065131587
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.581724 , 4.683292 , 5.2584486], dtype=float32)}
episode index:1076
target Thresh 18.92996110666411
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9100313, 8.128468 , 0.0106561], dtype=float32)}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.4249529953608934
{'scaleFactor': 20, 'timeStep': 299, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5439682, 4.0608053, 4.870119 ], dtype=float32)}
episode index:1077
target Thresh 18.930310427101944
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9435712, 4.9444747, 5.5748596], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.425486434140707
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9435712, 4.9444747, 5.5748596], dtype=float32)}
episode index:1078
target Thresh 18.930658005296827
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.59969  ,  7.228709 ,  1.0691524], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.42533557793413573
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3721691 , 3.6351457 , 0.37138134], dtype=float32)}
episode index:1079
target Thresh 18.93100384993823
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.712578, 6.224947, 4.915252], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4258584153619745
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6897259, 4.274233 , 5.027359 ], dtype=float32)}
episode index:1080
target Thresh 18.931347969672288
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.020849 ,  6.2538657,  3.3267007], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4258868624779615
{'scaleFactor': 20, 'timeStep': 79, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5010126, 1.44443  , 2.2618752], dtype=float32)}
episode index:1081
target Thresh 18.93169037310201
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.797873 ,  3.9494543,  1.8555562], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.42581174724771154
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0093758, 4.4198437, 3.970587 ], dtype=float32)}
episode index:1082
target Thresh 18.932031068787506
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.099076 , 10.151711 ,  5.9563427], dtype=float32)}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.4255304518359935
{'scaleFactor': 20, 'timeStep': 211, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.084667, 4.012511, 5.08355 ], dtype=float32)}
episode index:1083
target Thresh 18.93237006524618
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.1895485,  9.734761 ,  2.6448147], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4254965528196081
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7373493, 4.979011 , 4.097864 ], dtype=float32)}
episode index:1084
target Thresh 18.93270737095296
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.206663 ,  4.9920273,  1.7631665], dtype=float32)}
done in step count: 300
reward sum = 0.04904089407128572
running average episode reward sum: 0.4251495890788262
{'scaleFactor': 20, 'timeStep': 301, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7001824 , 1.677764  , 0.04871434], dtype=float32)}
episode index:1085
target Thresh 18.933042994340514
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.79057   , 11.749594  ,  0.35199147], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.42522301108592403
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.101876 , 2.2704592, 5.3654976], dtype=float32)}
episode index:1086
target Thresh 18.933376943799434
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.055594 , 11.157022 ,  5.3611856], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42556191750042055
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.848596 , 4.659137 , 4.2326827], dtype=float32)}
episode index:1087
target Thresh 18.93370922767848
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.05875  , 9.290924 , 5.0300126], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.42582385804249023
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.982755 , 1.0137722, 1.5332718], dtype=float32)}
episode index:1088
target Thresh 18.93403985428477
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0504029, 4.939524 , 1.6751391], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42635110886155125
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0504029, 4.939524 , 1.6751391], dtype=float32)}
episode index:1089
target Thresh 18.934368831883976
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9663143, 8.246714 , 4.8524895], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.426773158185271
{'scaleFactor': 20, 'timeStep': 13, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7177668 , 1.0546759 , 0.05376833], dtype=float32)}
episode index:1090
target Thresh 18.934696168700565
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0550356 , 6.0181904 , 0.15668362], dtype=float32)}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.4265233403975612
{'scaleFactor': 20, 'timeStep': 187, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7370253, 3.138888 , 2.54713  ], dtype=float32)}
episode index:1091
target Thresh 18.935021872917968
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.381154 ,  9.980289 ,  2.4774451], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.42652248580323066
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4888043, 4.5156345, 4.4605784], dtype=float32)}
episode index:1092
target Thresh 18.935345952678812
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.855413 ,  5.8694673,  1.9392557], dtype=float32)}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.4262836386942293
{'scaleFactor': 20, 'timeStep': 180, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.41092  , 3.3744762, 4.4930105], dtype=float32)}
episode index:1093
target Thresh 18.9356684160851
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0503697, 6.9772034, 2.159035 ], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42658385409506056
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8630967, 3.4148273, 5.337844 ], dtype=float32)}
episode index:1094
target Thresh 18.935989271198444
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.093865 , 8.058561 , 2.0052884], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4264935699885242
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1513712, 4.169638 , 4.707417 ], dtype=float32)}
episode index:1095
target Thresh 18.936308526040236
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.080121 ,  6.973079 ,  1.4246725], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.42664546129911995
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4597607, 3.7704663, 4.579847 ], dtype=float32)}
episode index:1096
target Thresh 18.93662618859186
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.3326893, 9.652373 , 3.960182 ], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4271061904573769
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7077508, 3.7961144, 4.710017 ], dtype=float32)}
episode index:1097
target Thresh 18.936942266794894
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.946116 , 11.127665 ,  5.9139433], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4273908846130517
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8972487, 3.9487488, 5.0697737], dtype=float32)}
episode index:1098
target Thresh 18.93725676855132
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.102818 ,  5.991236 ,  0.5950268], dtype=float32)}
done in step count: 328
reward sum = 0.03701210861730961
running average episode reward sum: 0.42703567189604014
{'scaleFactor': 20, 'timeStep': 329, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4331064, 4.641221 , 2.2541213], dtype=float32)}
episode index:1099
target Thresh 18.937569701723685
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.279935 , 6.820243 , 0.9443966], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.42701171642427244
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.131731 , 4.1279306, 2.6332517], dtype=float32)}
episode index:1100
target Thresh 18.937881074135348
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.984756 ,  4.9059715,  2.921851 ], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.42716244733251696
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2878277, 4.7000904, 4.19608  ], dtype=float32)}
episode index:1101
target Thresh 18.938190893570624
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.23522  , 7.268189 , 3.6314802], dtype=float32)}
done in step count: 344
reward sum = 0.031514247506815876
running average episode reward sum: 0.42680341992795645
{'scaleFactor': 20, 'timeStep': 345, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3497375, 4.9298506, 0.821175 ], dtype=float32)}
episode index:1102
target Thresh 18.93849916777502
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.909047  ,  8.799688  ,  0.68334866], dtype=float32)}
done in step count: 399
reward sum = 0.018131871994995087
running average episode reward sum: 0.42643291081831636
{'scaleFactor': 20, 'timeStep': 400, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3576647, 4.340993 , 5.5241594], dtype=float32)}
episode index:1103
target Thresh 18.93880590445541
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.544296 ,  8.693376 ,  3.0406168], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.426513258831546
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0724547 , 3.6168592 , 0.64445764], dtype=float32)}
episode index:1104
target Thresh 18.939111111280226
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.087082 ,  2.162447 ,  3.8424163], dtype=float32)}
done in step count: 218
reward sum = 0.11180788242357734
running average episode reward sum: 0.4262284575859279
{'scaleFactor': 20, 'timeStep': 219, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7644377, 3.6670794, 4.610819 ], dtype=float32)}
episode index:1105
target Thresh 18.93941479587965
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.967606 , 10.037689 ,  1.5902166], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.42653235836057635
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.456579 , 2.7963774, 4.3585505], dtype=float32)}
episode index:1106
target Thresh 18.939716965845815
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.069834 ,  1.8441539,  6.055869 ], dtype=float32)}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.42645524336794177
{'scaleFactor': 20, 'timeStep': 108, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.785403 , 4.4138603, 4.577308 ], dtype=float32)}
episode index:1107
target Thresh 18.94001762873299
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1162598, 7.153995 , 6.013521 ], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4268159955163783
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2294126, 4.853199 , 3.660716 ], dtype=float32)}
episode index:1108
target Thresh 18.940316792057754
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.329352 , 11.67944  ,  2.7308993], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.427125487987333
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8278203, 1.7618273, 4.8445525], dtype=float32)}
episode index:1109
target Thresh 18.940614463299212
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.733696 , 8.867348 , 2.2658472], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4273494551594708
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0090616, 4.0312743, 4.940216 ], dtype=float32)}
episode index:1110
target Thresh 18.940910649899163
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.148138,  4.0566  ,  4.590966], dtype=float32)}
done in step count: 222
reward sum = 0.10740220574263752
running average episode reward sum: 0.4270614738368634
{'scaleFactor': 20, 'timeStep': 223, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6396155, 2.7541423, 3.7498608], dtype=float32)}
episode index:1111
target Thresh 18.94120535926228
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0267048 , 4.960526  , 0.37859833], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4275767063244202
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0267048 , 4.960526  , 0.37859833], dtype=float32)}
episode index:1112
target Thresh 18.94149859875632
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.144027 ,  1.9166917,  2.0448735], dtype=float32)}
done in step count: 427
reward sum = 0.013684473507730199
running average episode reward sum: 0.42720483549529464
{'scaleFactor': 20, 'timeStep': 428, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4388449, 2.0878925, 2.3517098], dtype=float32)}
episode index:1113
target Thresh 18.94179037571228
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.834875 ,  6.1793094,  4.158727 ], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.4269669226178402
{'scaleFactor': 20, 'timeStep': 182, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.145224 , 3.475893 , 4.5889044], dtype=float32)}
episode index:1114
target Thresh 18.9420806974246
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.042184 , 9.976028 , 1.3490909], dtype=float32)}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.42677098139731723
{'scaleFactor': 20, 'timeStep': 157, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0207137, 4.163606 , 3.5474472], dtype=float32)}
episode index:1115
target Thresh 18.942369571151342
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.096474 ,  3.09415  ,  3.9050012], dtype=float32)}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.42648976838080827
{'scaleFactor': 20, 'timeStep': 218, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.691936 , 3.9251232, 4.190579 ], dtype=float32)}
episode index:1116
target Thresh 18.94265700411436
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.008693 , 11.129106 ,  2.0873978], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.42658804810939055
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.785481 , 4.1703115, 4.520575 ], dtype=float32)}
episode index:1117
target Thresh 18.942943003499494
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.489231 , 4.122862 , 4.2673316], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42710093894292417
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.489231 , 4.122862 , 4.2673316], dtype=float32)}
episode index:1118
target Thresh 18.943227576456742
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.6989975,  5.802541 ,  4.859835 ], dtype=float32)}
done in step count: 338
reward sum = 0.03347308759177372
running average episode reward sum: 0.42674917142607777
{'scaleFactor': 20, 'timeStep': 339, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.980514 , 3.9905128, 4.236551 ], dtype=float32)}
episode index:1119
target Thresh 18.943510730100446
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.810074, 8.796438, 3.632369], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.42660271019020646
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9520268, 3.968722 , 5.0104737], dtype=float32)}
episode index:1120
target Thresh 18.94379247150946
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.816636 ,  7.2681694,  5.400555 ], dtype=float32)}
done in step count: 375
reward sum = 0.02307798964136396
running average episode reward sum: 0.42624274166161696
{'scaleFactor': 20, 'timeStep': 376, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8708987, 4.743425 , 5.5269713], dtype=float32)}
episode index:1121
target Thresh 18.944072807727334
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.134731 ,  6.075824 ,  5.2535405], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.42616086475896836
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2267195, 3.0760453, 4.117506 ], dtype=float32)}
episode index:1122
target Thresh 18.94435174576249
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8905945, 8.444753 , 4.3441877], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4265395797245391
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1395082, 4.807904 , 1.7023526], dtype=float32)}
episode index:1123
target Thresh 18.944629292588388
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.905179 ,  4.763797 ,  4.2727575], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.4264346018694769
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4815414, 4.3147125, 3.534761 ], dtype=float32)}
episode index:1124
target Thresh 18.94490545514372
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.918814 ,  6.2387657,  5.172209 ], dtype=float32)}
done in step count: 404
reward sum = 0.01724322985330079
running average episode reward sum: 0.4260708762054625
{'scaleFactor': 20, 'timeStep': 405, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8309712, 3.7861915, 5.332181 ], dtype=float32)}
episode index:1125
target Thresh 18.94518024033256
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.9862294 , 11.145375  ,  0.25595167], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.42628065521503494
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2397637, 4.4323244, 5.1144786], dtype=float32)}
episode index:1126
target Thresh 18.945453655024554
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.01533  ,  9.094073 ,  1.5345355], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42644466283199844
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0077188, 4.081127 , 3.9444842], dtype=float32)}
episode index:1127
target Thresh 18.945725706055082
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.027454 ,  2.91415  ,  5.7910213], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.42620196342547706
{'scaleFactor': 20, 'timeStep': 188, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.330675 , 4.580908 , 3.0351682], dtype=float32)}
episode index:1128
target Thresh 18.945996400225436
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3316016, 8.025343 , 1.8941381], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.42620907535051367
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.700975 , 1.4023857, 5.1743765], dtype=float32)}
episode index:1129
target Thresh 18.94626574430298
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.013775 ,  7.002694 ,  5.0123253], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4258318991776371
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.11889915, 12.976233  ,  2.2800946 ], dtype=float32)}
episode index:1130
target Thresh 18.946533745021334
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9670959, 9.618683 , 4.5871186], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4261094133016076
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7626843, 1.3428278, 2.4897494], dtype=float32)}
episode index:1131
target Thresh 18.94680040908053
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.025704  ,  9.061505  ,  0.15917268], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.4259974618687549
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2942803, 3.1129446, 5.657383 ], dtype=float32)}
episode index:1132
target Thresh 18.94706574314718
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.833686 , 10.978574 ,  1.9556875], dtype=float32)}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4258270002633527
{'scaleFactor': 20, 'timeStep': 146, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8094664, 4.7006044, 3.428623 ], dtype=float32)}
episode index:1133
target Thresh 18.947329753854657
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.483603, 9.380023, 4.676649], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42629857787334974
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2184308, 3.7162356, 4.3720956], dtype=float32)}
episode index:1134
target Thresh 18.94759244780323
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.084309 , 10.027661 ,  5.5307317], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42645602940574107
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1329594, 4.9324064, 5.8975377], dtype=float32)}
episode index:1135
target Thresh 18.947853831560273
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.06909  ,  7.9585643,  3.1873512], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4265206253568797
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8499784, 4.155221 , 3.3187675], dtype=float32)}
episode index:1136
target Thresh 18.94811391166039
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.822189 , 10.1500635,  2.278219 ], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4265430730480931
{'scaleFactor': 20, 'timeStep': 80, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3785746, 1.0995488, 6.0707645], dtype=float32)}
episode index:1137
target Thresh 18.94837269460559
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.067824 , 10.113414 ,  4.2974997], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.4264037968124769
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.486186, 4.500149, 5.176709], dtype=float32)}
episode index:1138
target Thresh 18.948630186865476
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.54592  ,  8.982277 ,  1.9466283], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4260294300022816
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.495094 ,  9.197678 ,  1.1596334], dtype=float32)}
episode index:1139
target Thresh 18.94888639487735
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.988381 ,  8.530019 ,  3.6435468], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.42594319607897946
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7286417, 4.957577 , 6.0252895], dtype=float32)}
episode index:1140
target Thresh 18.949141325046437
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.8715935, 9.302624 , 4.181812 ], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42637860492941676
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6000446, 3.533418 , 5.1151385], dtype=float32)}
episode index:1141
target Thresh 18.949394983746
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.064681 ,  9.013691 ,  3.0789237], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.426295114984926
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7342954, 1.7940401, 1.609323 ], dtype=float32)}
episode index:1142
target Thresh 18.949647377317522
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.963873 , 8.251866 , 3.9565148], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4267541656716408
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6747273, 2.4611602, 5.0479593], dtype=float32)}
episode index:1143
target Thresh 18.949898512070853
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.06087  ,  9.949528 ,  6.1439114], dtype=float32)}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.4266793508952793
{'scaleFactor': 20, 'timeStep': 108, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3490257, 4.9079976, 3.4463065], dtype=float32)}
episode index:1144
target Thresh 18.950148394284376
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9810214, 9.737512 , 1.8905373], dtype=float32)}
done in step count: 483
reward sum = 0.0077946925652699495
running average episode reward sum: 0.4263135127657334
{'scaleFactor': 20, 'timeStep': 484, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3726283, 2.3008337, 5.195531 ], dtype=float32)}
episode index:1145
target Thresh 18.950397030205163
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.447838 ,  8.667624 ,  2.3237336], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.42603244709388716
{'scaleFactor': 20, 'timeStep': 226, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.311797 , 1.588382 , 1.7381005], dtype=float32)}
episode index:1146
target Thresh 18.95064442604912
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.942774 , 9.838499 , 0.5728799], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4261832086975247
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.889256 , 4.6366234, 4.8684387], dtype=float32)}
episode index:1147
target Thresh 18.950890588001158
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4650817e+01, 2.1298347e+00, 2.6485622e-03], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4260650077268038
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2940376, 2.1859648, 5.0882316], dtype=float32)}
episode index:1148
target Thresh 18.951135522215335
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.94399  ,  4.073712 ,  3.3041973], dtype=float32)}
done in step count: 437
reward sum = 0.01237599254632405
running average episode reward sum: 0.42570496506781297
{'scaleFactor': 20, 'timeStep': 438, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4096875, 4.8005195, 5.660821 ], dtype=float32)}
episode index:1149
target Thresh 18.951379234815025
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.854609 , 11.128648 ,  3.4865675], dtype=float32)}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.4254662241720611
{'scaleFactor': 20, 'timeStep': 189, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.986337 , 4.9444294, 4.4808354], dtype=float32)}
episode index:1150
target Thresh 18.951621731893052
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.820447 ,  5.156068 ,  5.2181587], dtype=float32)}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.42525713497985995
{'scaleFactor': 20, 'timeStep': 169, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0600278, 4.0096407, 5.4877543], dtype=float32)}
episode index:1151
target Thresh 18.95186301951186
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.8391738 , 10.305866  ,  0.17655466], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.4250533192895693
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3018997, 4.4814425, 5.545003 ], dtype=float32)}
episode index:1152
target Thresh 18.952103103703642
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.027129  , 10.091554  ,  0.45810544], dtype=float32)}
done in step count: 391
reward sum = 0.01964993362138638
running average episode reward sum: 0.42470171184319616
{'scaleFactor': 20, 'timeStep': 392, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.407063 , 1.7677212, 2.3607736], dtype=float32)}
episode index:1153
target Thresh 18.95234199047053
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.843014 , 4.8388047, 3.9608889], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4252002372228814
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.843014 , 4.8388047, 3.9608889], dtype=float32)}
episode index:1154
target Thresh 18.95257968578469
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1268704, 9.984014 , 5.500625 ], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.42538847217081377
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2666402, 3.2061353, 4.4643903], dtype=float32)}
episode index:1155
target Thresh 18.95281619558853
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.2942696, 8.8397875, 3.776968 ], dtype=float32)}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.4251538990563008
{'scaleFactor': 20, 'timeStep': 187, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5256648, 4.154702 , 5.7191973], dtype=float32)}
episode index:1156
target Thresh 18.953051525794802
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.404266 , 10.718641 ,  1.8792226], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.42547928945429797
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1247535, 4.5025883, 4.7114153], dtype=float32)}
episode index:1157
target Thresh 18.953285682286772
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.296734  ,  3.1597023 ,  0.09271543], dtype=float32)}
done in step count: 425
reward sum = 0.013962323750362412
running average episode reward sum: 0.4251239207447091
{'scaleFactor': 20, 'timeStep': 426, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9311683, 1.1772208, 2.541094 ], dtype=float32)}
episode index:1158
target Thresh 18.953518670918367
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.687769 , 10.725857 ,  1.2169908], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.4249726809998547
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0639622, 4.4920616, 5.151894 ], dtype=float32)}
episode index:1159
target Thresh 18.953750497514314
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.8804493, 9.79147  , 5.801343 ], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4248888448588529
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5913805, 3.719362 , 4.6169734], dtype=float32)}
episode index:1160
target Thresh 18.95398116787029
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.033438,  7.033424,  3.940854], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4248137936581984
{'scaleFactor': 20, 'timeStep': 109, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0563147, 4.2408185, 5.281506 ], dtype=float32)}
episode index:1161
target Thresh 18.95421068775306
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.887432, 11.075905,  5.124777], dtype=float32)}
done in step count: 372
reward sum = 0.02378441041510293
running average episode reward sum: 0.4244686737070425
{'scaleFactor': 20, 'timeStep': 373, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5569708, 4.21085  , 4.291871 ], dtype=float32)}
episode index:1162
target Thresh 18.954439062900647
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1245046, 6.1214056, 1.9690669], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.42469651584991613
{'scaleFactor': 20, 'timeStep': 38, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7074339 , 2.3316855 , 0.06841915], dtype=float32)}
episode index:1163
target Thresh 18.95466629902243
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.2031007, 9.885723 , 1.2158935], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4243316563002169
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.45648  ,  9.6990795,  5.870883 ], dtype=float32)}
episode index:1164
target Thresh 18.95489240179933
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.111608 , 6.825422 , 0.6554395], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4242118074735842
{'scaleFactor': 20, 'timeStep': 126, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6581004, 2.6934314, 0.8081839], dtype=float32)}
episode index:1165
target Thresh 18.955117376883923
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.814271 ,  5.897105 ,  0.8041498], dtype=float32)}
done in step count: 396
reward sum = 0.018686891355133923
running average episode reward sum: 0.4238640159503265
{'scaleFactor': 20, 'timeStep': 397, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7522949, 3.4021888, 4.778862 ], dtype=float32)}
episode index:1166
target Thresh 18.9553412299006
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.005003 , 9.202276 , 4.6371207], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42432394053820116
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7724996, 3.3089476, 4.63989  ], dtype=float32)}
episode index:1167
target Thresh 18.955563966445702
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.983349 ,  5.0640564,  4.217455 ], dtype=float32)}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.4240489800842315
{'scaleFactor': 20, 'timeStep': 227, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5920177, 1.1089575, 5.2607465], dtype=float32)}
episode index:1168
target Thresh 18.95578559208765
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6729164, 3.4260097, 3.794369 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42454166701315854
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6729164, 3.4260097, 3.794369 ], dtype=float32)}
episode index:1169
target Thresh 18.956006112367096
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9965649, 10.81188  ,  1.8818605], dtype=float32)}
done in step count: 361
reward sum = 0.026564720430486827
running average episode reward sum: 0.4242015157767631
{'scaleFactor': 20, 'timeStep': 362, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6549945, 3.3572621, 4.1724787], dtype=float32)}
episode index:1170
target Thresh 18.95622553279706
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.90245 ,  6.091804,  5.475135], dtype=float32)}
done in step count: 271
reward sum = 0.06563533949318147
running average episode reward sum: 0.4238953106731905
{'scaleFactor': 20, 'timeStep': 272, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.071944 , 2.8750954, 5.361325 ], dtype=float32)}
episode index:1171
target Thresh 18.956443858863064
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1841574, 6.949083 , 4.928799 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42437833515213824
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1881332, 4.734378 , 4.4838457], dtype=float32)}
episode index:1172
target Thresh 18.956661096023268
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.422749 , 10.858179 ,  1.4612074], dtype=float32)}
done in step count: 393
reward sum = 0.01925889994232079
running average episode reward sum: 0.4240329647896405
{'scaleFactor': 20, 'timeStep': 394, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3257232, 3.292382 , 0.6006301], dtype=float32)}
episode index:1173
target Thresh 18.95687724970862
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.1205015 ,  7.9099364 ,  0.74709386], dtype=float32)}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.42375286830375425
{'scaleFactor': 20, 'timeStep': 235, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7559935, 3.0136056, 6.0383368], dtype=float32)}
episode index:1174
target Thresh 18.957092325322968
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.562457 , 10.034756 ,  5.9381075], dtype=float32)}
done in step count: 288
reward sum = 0.05532686267122055
running average episode reward sum: 0.4234393142564074
{'scaleFactor': 20, 'timeStep': 289, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0805712, 4.7251754, 3.9202285], dtype=float32)}
episode index:1175
target Thresh 18.957306328243213
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.1420817, 10.938157 ,  1.4815668], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4230792468123118
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.51354927, 3.7108536 , 4.7361875 ], dtype=float32)}
episode index:1176
target Thresh 18.957519263819442
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.986316  , 10.1176    ,  0.02104205], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.42351169039862846
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8464417, 4.7717032, 4.256    ], dtype=float32)}
episode index:1177
target Thresh 18.95773113737505
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.253783 ,  9.387509 ,  4.6958523], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.42335590159488107
{'scaleFactor': 20, 'timeStep': 143, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.212081, 4.773365, 5.83655 ], dtype=float32)}
episode index:1178
target Thresh 18.957941954206895
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.099234 ,  5.895617 ,  0.3257083], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.42339197018994035
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6080463, 4.584022 , 4.232546 ], dtype=float32)}
episode index:1179
target Thresh 18.958151719585402
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.237254,  4.003329,  4.736285], dtype=float32)}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.42307911755257943
{'scaleFactor': 20, 'timeStep': 291, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5459571, 3.9495432, 5.9672995], dtype=float32)}
episode index:1180
target Thresh 18.95836043875472
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.994982 ,  8.940004 ,  1.4830303], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.4230601552624854
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.411882 , 3.0816288, 0.5464036], dtype=float32)}
episode index:1181
target Thresh 18.958568116932838
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.976959 , 9.094264 , 2.8448493], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4232625426446525
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.063376 , 3.8744388, 5.3109865], dtype=float32)}
episode index:1182
target Thresh 18.958774759311723
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.322513, 3.230722, 4.622961], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4237500637413181
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.322513, 3.230722, 4.622961], dtype=float32)}
episode index:1183
target Thresh 18.958980371057443
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9950337, 4.88929  , 3.4518895], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4242367613226177
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9950337, 4.88929  , 3.4518895], dtype=float32)}
episode index:1184
target Thresh 18.959184957310303
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.877639 ,  9.6328945,  5.5633507], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42387875561686017
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5514153, 7.1665545, 4.1860037], dtype=float32)}
episode index:1185
target Thresh 18.95938852318497
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.079777 ,  1.7682147,  2.1039681], dtype=float32)}
done in step count: 438
reward sum = 0.012252232620860809
running average episode reward sum: 0.42353168434957855
{'scaleFactor': 20, 'timeStep': 439, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0121307, 1.6186776, 5.215514 ], dtype=float32)}
episode index:1186
target Thresh 18.9595910737706
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.926066 , 10.924136 ,  5.9546294], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.423674426356362
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9018545, 3.5564723, 5.3938527], dtype=float32)}
episode index:1187
target Thresh 18.95979261413097
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6778156, 6.0450473, 1.6454241], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4236517019288078
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8271275, 3.795737 , 4.3787355], dtype=float32)}
episode index:1188
target Thresh 18.959993149304598
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.084461 ,  4.8471594,  0.5546233], dtype=float32)}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4236126648766587
{'scaleFactor': 20, 'timeStep': 98, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.94639 , 4.399357, 5.070403], dtype=float32)}
episode index:1189
target Thresh 18.96019268430488
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.828828 ,  2.3206089,  2.7425005], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.42361075021890926
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.899518, 4.505966, 4.164674], dtype=float32)}
episode index:1190
target Thresh 18.96039122412019
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.026542, 10.702486,  4.834666], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4237529464373665
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.698327 , 4.0646753, 3.8085659], dtype=float32)}
episode index:1191
target Thresh 18.960588773714043
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.258636 , 9.867525 , 2.2610984], dtype=float32)}
done in step count: 222
reward sum = 0.10740220574263752
running average episode reward sum: 0.4234875515206763
{'scaleFactor': 20, 'timeStep': 223, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4442859, 1.610277 , 4.9779124], dtype=float32)}
episode index:1192
target Thresh 18.96078533802519
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.230564,  4.24903 ,  3.676324], dtype=float32)}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.42321481574183556
{'scaleFactor': 20, 'timeStep': 232, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6383963, 3.022747 , 3.9918623], dtype=float32)}
episode index:1193
target Thresh 18.96098092196774
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0125065, 6.9349766, 2.0191202], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4234924576777332
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2469378, 2.079083 , 3.81559  ], dtype=float32)}
episode index:1194
target Thresh 18.961175530431305
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.278566 ,  3.9791498,  1.8826505], dtype=float32)}
done in step count: 482
reward sum = 0.00787342683360601
running average episode reward sum: 0.4231446593255624
{'scaleFactor': 20, 'timeStep': 483, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3964279, 4.4916744, 4.494458 ], dtype=float32)}
episode index:1195
target Thresh 18.961369168281113
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3044114, 5.9726033, 0.779763 ], dtype=float32)}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.4229669451005383
{'scaleFactor': 20, 'timeStep': 156, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8435941, 4.198428 , 6.1096706], dtype=float32)}
episode index:1196
target Thresh 18.961561840358115
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.5792646, 9.723137 , 3.8992174], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42321925369774577
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5203934 , 3.8578193 , 0.03450613], dtype=float32)}
episode index:1197
target Thresh 18.961753551479124
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.6711855,  5.8747716,  1.5093795], dtype=float32)}
done in step count: 201
reward sum = 0.1326398781093821
running average episode reward sum: 0.4229766999618623
{'scaleFactor': 20, 'timeStep': 202, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8296397, 4.442152 , 4.518169 ], dtype=float32)}
episode index:1198
target Thresh 18.961944306436926
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.840295 ,  1.9891862,  4.7147636], dtype=float32)}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.42278929175221014
{'scaleFactor': 20, 'timeStep': 162, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4187014, 1.6988295, 5.3834662], dtype=float32)}
episode index:1199
target Thresh 18.962134110000413
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.036482 , 9.942178 , 3.0564318], dtype=float32)}
done in step count: 427
reward sum = 0.013684473507730199
running average episode reward sum: 0.4224483710703397
{'scaleFactor': 20, 'timeStep': 428, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0696683, 2.4247363, 4.007935 ], dtype=float32)}
episode index:1200
target Thresh 18.962322966914673
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.089553 , 11.080466 ,  2.9504552], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.42281997593436854
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3042648, 4.950182 , 3.2016206], dtype=float32)}
episode index:1201
target Thresh 18.962510881901142
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.962216 ,  4.9054737,  5.107325 ], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.42310244077497816
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0333887, 4.509182 , 3.9089315], dtype=float32)}
episode index:1202
target Thresh 18.96269785965771
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.52674  , 4.943008 , 4.4517236], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4235819898682658
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.52674  , 4.943008 , 4.4517236], dtype=float32)}
episode index:1203
target Thresh 18.962883904858824
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1454887, 4.9645185, 2.5344114], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4240607423683752
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1454887, 4.9645185, 2.5344114], dtype=float32)}
episode index:1204
target Thresh 18.963069022155626
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.053019 , 10.977378 ,  2.5006614], dtype=float32)}
done in step count: 316
reward sum = 0.04175625035843684
running average episode reward sum: 0.42374347722977773
{'scaleFactor': 20, 'timeStep': 317, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.841588, 2.446254, 4.194166], dtype=float32)}
episode index:1205
target Thresh 18.96325321617606
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5142894, 6.051017 , 4.921013 ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42418862858365025
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7749424, 4.051979 , 4.268514 ], dtype=float32)}
episode index:1206
target Thresh 18.963436491524984
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.487822 ,  8.021916 ,  4.1788063], dtype=float32)}
done in step count: 256
reward sum = 0.0763149839065938
running average episode reward sum: 0.42390041512492854
{'scaleFactor': 20, 'timeStep': 257, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8299274, 4.940754 , 5.981484 ], dtype=float32)}
episode index:1207
target Thresh 18.963618852784293
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.088875, 9.128713, 4.609105], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4243446995577721
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.526123 , 3.5790458, 5.0741687], dtype=float32)}
episode index:1208
target Thresh 18.963800304513025
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0547292, 8.987413 , 4.676449 ], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4243352743895142
{'scaleFactor': 20, 'timeStep': 89, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.288248 , 4.8744836, 3.1267722], dtype=float32)}
episode index:1209
target Thresh 18.963980851247484
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8699343, 9.093431 , 4.3397913], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42477846508010136
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3679676, 4.190893 , 5.6172123], dtype=float32)}
episode index:1210
target Thresh 18.964160497501346
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.2744055, 11.069297 ,  3.385966 ], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.42513790346949953
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8441498, 4.399081 , 5.565209 ], dtype=float32)}
episode index:1211
target Thresh 18.964339247765786
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.875369  , 10.162411  ,  0.03160971], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.42510790843204466
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0024056, 1.1497056, 1.095921 ], dtype=float32)}
episode index:1212
target Thresh 18.964517106509557
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8985072, 10.020716 ,  2.312078 ], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.42490379847188525
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2286441, 4.929968 , 4.4208207], dtype=float32)}
episode index:1213
target Thresh 18.964694078179146
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.7956326, 8.088913 , 2.9652643], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.4248183761292929
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4798162, 3.697534 , 4.7547455], dtype=float32)}
episode index:1214
target Thresh 18.96487016719885
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.940101 ,  8.882064 ,  1.6335968], dtype=float32)}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.42458237813501987
{'scaleFactor': 20, 'timeStep': 198, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7076061, 3.3857388, 5.336962 ], dtype=float32)}
episode index:1215
target Thresh 18.965045377970903
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.563507 , 11.957871 ,  1.3259072], dtype=float32)}
done in step count: 254
reward sum = 0.07786448720191184
running average episode reward sum: 0.4242972482905025
{'scaleFactor': 20, 'timeStep': 255, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2252668, 1.6918341, 5.511282 ], dtype=float32)}
episode index:1216
target Thresh 18.965219714875584
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.972848,  5.969123,  4.627967], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.42432758906217677
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.500447 , 4.6894083, 3.5136633], dtype=float32)}
episode index:1217
target Thresh 18.965393182271324
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8664968, 10.0860195,  1.5664185], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4246507248163107
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.851249 , 3.4437351, 3.4060209], dtype=float32)}
episode index:1218
target Thresh 18.965565784494824
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.218451 , 9.984057 , 1.3364942], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.424966620750195
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5683413, 4.6627088, 4.3817363], dtype=float32)}
episode index:1219
target Thresh 18.965737525861137
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.64867  ,  5.0889635,  3.7138736], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.42476675217017507
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.473804 , 1.5649232, 5.5582223], dtype=float32)}
episode index:1220
target Thresh 18.965908410663815
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.087333 ,  7.7631445,  2.0922487], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4249094133121047
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1740551, 1.4740028, 4.9095864], dtype=float32)}
episode index:1221
target Thresh 18.966078443174986
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.978251 ,  8.098842 ,  5.7583604], dtype=float32)}
done in step count: 222
reward sum = 0.10740220574263752
running average episode reward sum: 0.42464958744666326
{'scaleFactor': 20, 'timeStep': 223, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1854756, 4.5000567, 2.862527 ], dtype=float32)}
episode index:1222
target Thresh 18.966247627645465
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.13029  ,  6.994689 ,  5.6511884], dtype=float32)}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.42446612664487327
{'scaleFactor': 20, 'timeStep': 161, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1860516, 1.1143548, 5.4697447], dtype=float32)}
episode index:1223
target Thresh 18.96641596830488
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.178361, 10.75241 ,  3.127005], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42472977531128403
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0261722, 2.2400293, 3.6327262], dtype=float32)}
episode index:1224
target Thresh 18.966583469361755
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1668887, 7.9094152, 1.7737588], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42438305712735647
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.131152, 13.001616,  5.963661], dtype=float32)}
episode index:1225
target Thresh 18.96675013500362
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.277439, 10.151332,  3.513404], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42403690455221177
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.7324936, 12.4955015,  2.8642583], dtype=float32)}
episode index:1226
target Thresh 18.96691596939713
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.763983, 10.718535,  5.658647], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42446636921834696
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7245636, 4.9917135, 4.6237154], dtype=float32)}
episode index:1227
target Thresh 18.967080976688152
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0929759, 4.97828  , 1.6888523], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.424935044813446
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0929759, 4.97828  , 1.6888523], dtype=float32)}
episode index:1228
target Thresh 18.967245161001877
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.868685  , 10.134482  ,  0.46490163], dtype=float32)}
done in step count: 349
reward sum = 0.02996973580906778
running average episode reward sum: 0.4246136735286581
{'scaleFactor': 20, 'timeStep': 350, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6904452, 4.432275 , 5.1888213], dtype=float32)}
episode index:1229
target Thresh 18.96740852644292
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.957911 ,  6.830097 ,  1.2328176], dtype=float32)}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.42431623625772386
{'scaleFactor': 20, 'timeStep': 283, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.542508 , 1.8117086, 3.2989426], dtype=float32)}
episode index:1230
target Thresh 18.967571077095425
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.206998 ,  9.852802 ,  5.9572062], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.4242571728704779
{'scaleFactor': 20, 'timeStep': 105, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9514761, 2.0472543, 5.646104 ], dtype=float32)}
episode index:1231
target Thresh 18.967732817023172
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.893376 ,  9.019063 ,  4.2865496], dtype=float32)}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4241607458031547
{'scaleFactor': 20, 'timeStep': 119, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.536386 , 2.8722162, 3.8806927], dtype=float32)}
episode index:1232
target Thresh 18.967893750269663
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.089306, 10.183433,  4.209595], dtype=float32)}
done in step count: 439
reward sum = 0.012129710294652202
running average episode reward sum: 0.42382657626908454
{'scaleFactor': 20, 'timeStep': 440, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1925614, 1.4999496, 5.5417695], dtype=float32)}
episode index:1233
target Thresh 18.968053880858236
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.784357 ,  3.1256397,  2.1074784], dtype=float32)}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.42357935664237667
{'scaleFactor': 20, 'timeStep': 213, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.818245 , 4.5209203, 4.42488  ], dtype=float32)}
episode index:1234
target Thresh 18.96821321279217
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0014753, 7.934046 , 6.12217  ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42396867058437376
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6181543, 4.3176117, 4.3699484], dtype=float32)}
episode index:1235
target Thresh 18.968371750054768
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6897383, 5.725498 , 3.0876012], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4242122075304688
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7516837, 4.51022  , 6.225441 ], dtype=float32)}
episode index:1236
target Thresh 18.968529496609467
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.045507 ,  7.9023724,  4.1550713], dtype=float32)}
done in step count: 280
reward sum = 0.05995901467146544
running average episode reward sum: 0.4239177425402837
{'scaleFactor': 20, 'timeStep': 281, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3303733, 3.917307 , 4.8267665], dtype=float32)}
episode index:1237
target Thresh 18.968686456399947
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8631954, 7.0329156, 0.7683964], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4239331950758942
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.574835 , 4.3817344, 5.877441 ], dtype=float32)}
episode index:1238
target Thresh 18.968842633350203
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7182932, 9.10369  , 0.6511234], dtype=float32)}
done in step count: 463
reward sum = 0.009530048232832523
running average episode reward sum: 0.42359872925923314
{'scaleFactor': 20, 'timeStep': 464, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6322871, 4.1145573, 4.5257463], dtype=float32)}
episode index:1239
target Thresh 18.968998031364666
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.837872 ,  5.118451 ,  6.0773005], dtype=float32)}
done in step count: 297
reward sum = 0.050542043299318794
running average episode reward sum: 0.42329787709313643
{'scaleFactor': 20, 'timeStep': 298, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4143262, 3.4278407, 5.365666 ], dtype=float32)}
episode index:1240
target Thresh 18.969152654328305
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.268659 , 10.783213 ,  0.4765768], dtype=float32)}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.4231299291242508
{'scaleFactor': 20, 'timeStep': 154, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4990805 , 3.300386  , 0.00436896], dtype=float32)}
episode index:1241
target Thresh 18.96930650610669
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.709792, 9.845584, 5.220223], dtype=float32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.42321670498079733
{'scaleFactor': 20, 'timeStep': 64, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.032804 , 4.8673635, 5.141949 ], dtype=float32)}
episode index:1242
target Thresh 18.969459590546133
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.511149 ,  7.590769 ,  3.0732288], dtype=float32)}
done in step count: 277
reward sum = 0.06179436923202584
running average episode reward sum: 0.4229259388217075
{'scaleFactor': 20, 'timeStep': 278, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9180229, 1.009509 , 0.8199878], dtype=float32)}
episode index:1243
target Thresh 18.96961191147375
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.607001 ,  6.999165 ,  5.1795926], dtype=float32)}
done in step count: 391
reward sum = 0.01964993362138638
running average episode reward sum: 0.42260176196865257
{'scaleFactor': 20, 'timeStep': 392, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1095545, 3.7063174, 5.721584 ], dtype=float32)}
episode index:1244
target Thresh 18.96976347269757
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.077415  , 10.628109  ,  0.24386722], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4226930603326996
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8088272, 4.9600587, 3.7875326], dtype=float32)}
episode index:1245
target Thresh 18.969914278006634
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.004259, 10.853477,  5.611904], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4227093965456157
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3169985, 1.4410675, 2.537536 ], dtype=float32)}
episode index:1246
target Thresh 18.970064331171077
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.615598, 10.014258,  6.275001], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.4227672387774157
{'scaleFactor': 20, 'timeStep': 71, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0733916, 3.3986733, 5.5062428], dtype=float32)}
episode index:1247
target Thresh 18.97021363594224
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.732967 ,  7.732005 ,  5.3404846], dtype=float32)}
done in step count: 363
reward sum = 0.026036082493920136
running average episode reward sum: 0.4224493452227013
{'scaleFactor': 20, 'timeStep': 364, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2436472, 1.039784 , 5.650297 ], dtype=float32)}
episode index:1248
target Thresh 18.970362196052754
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.8456   ,  6.0886416,  5.1609783], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.422590663606403
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9825616, 4.861243 , 5.712441 ], dtype=float32)}
episode index:1249
target Thresh 18.970510015216625
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.932535,  8.454102,  5.604033], dtype=float32)}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.4224826579630114
{'scaleFactor': 20, 'timeStep': 125, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.74499  , 3.3145478, 4.2986803], dtype=float32)}
episode index:1250
target Thresh 18.970657097129337
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.060614,  9.892749,  4.425466], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42281201935668616
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7512708, 3.9669533, 4.798835 ], dtype=float32)}
episode index:1251
target Thresh 18.970803445467954
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.6388197, 11.589494 ,  1.9955139], dtype=float32)}
done in step count: 387
reward sum = 0.02045598088772655
running average episode reward sum: 0.4224906487189314
{'scaleFactor': 20, 'timeStep': 388, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9055943, 3.704736 , 4.5590973], dtype=float32)}
episode index:1252
target Thresh 18.970949063891183
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.196492  ,  3.0445995 ,  0.09098876], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.42248636318917426
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.125525 , 4.515077 , 3.5071802], dtype=float32)}
episode index:1253
target Thresh 18.9710939560395
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.831099, 9.664549, 5.731889], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42278231846864356
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3195713, 3.5672538, 3.8929434], dtype=float32)}
episode index:1254
target Thresh 18.97123812553521
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.8385181, 11.248713 ,  1.6202562], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.42301161799757075
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.910224 , 4.8163433, 4.5697365], dtype=float32)}
episode index:1255
target Thresh 18.97138157598256
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.25816 ,  8.939215,  4.441068], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42304949620106447
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6266717, 1.0698171, 5.204543 ], dtype=float32)}
episode index:1256
target Thresh 18.97152431096782
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.0438848, 11.092519 ,  5.9994206], dtype=float32)}
done in step count: 361
reward sum = 0.026564720430486827
running average episode reward sum: 0.4227340747406265
{'scaleFactor': 20, 'timeStep': 362, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6755502, 2.0078645, 2.4795203], dtype=float32)}
episode index:1257
target Thresh 18.97166633405937
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7059155, 4.0345182, 4.448782 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42319295067485496
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7059155, 4.0345182, 4.448782 ], dtype=float32)}
episode index:1258
target Thresh 18.971807648807797
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.162565  ,  9.017372  ,  0.43905368], dtype=float32)}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.4230143019901163
{'scaleFactor': 20, 'timeStep': 162, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4705215, 1.0393792, 3.8779545], dtype=float32)}
episode index:1259
target Thresh 18.971948258745975
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.0575695, 10.805304 ,  4.6563406], dtype=float32)}
done in step count: 390
reward sum = 0.019848417799380184
running average episode reward sum: 0.4226943290661554
{'scaleFactor': 20, 'timeStep': 391, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2649362, 3.9916415, 5.408086 ], dtype=float32)}
episode index:1260
target Thresh 18.97208816738916
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.048094 ,  5.8114557,  4.033591 ], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4227323086954333
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7455902, 4.840735 , 3.6547964], dtype=float32)}
episode index:1261
target Thresh 18.972227378235075
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.1209385, 10.809266 ,  4.2787113], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42314336086714927
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5187535, 4.048009 , 5.2623606], dtype=float32)}
episode index:1262
target Thresh 18.972365894764003
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.718366 , 10.4709635,  3.2322004], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.42323721812376025
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1015954, 1.0884769, 6.226546 ], dtype=float32)}
episode index:1263
target Thresh 18.97250372043886
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.196141 ,  3.1566844,  3.526148 ], dtype=float32)}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4231253696090583
{'scaleFactor': 20, 'timeStep': 127, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0895804, 3.6551714, 1.9204407], dtype=float32)}
episode index:1264
target Thresh 18.972640858705294
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.650084 , 6.922369 , 1.5521712], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4227908831508693
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.423592 , 12.626997 ,  1.7171563], dtype=float32)}
episode index:1265
target Thresh 18.972777312991774
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.183329 , 10.046796 ,  3.2850099], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4228104189570407
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0515921, 1.646564 , 5.1349063], dtype=float32)}
episode index:1266
target Thresh 18.972913086709656
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9056268, 10.093566 ,  1.1226424], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.42311579973783325
{'scaleFactor': 20, 'timeStep': 22, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3624473, 4.793691 , 4.052853 ], dtype=float32)}
episode index:1267
target Thresh 18.973048183253297
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.887606, 10.484036,  6.091312], dtype=float32)}
done in step count: 422
reward sum = 0.014389712604426484
running average episode reward sum: 0.42279346055239675
{'scaleFactor': 20, 'timeStep': 423, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5366385, 4.3100467, 4.212094 ], dtype=float32)}
episode index:1268
target Thresh 18.97318260600012
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0699081, 7.889522 , 2.9593277], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4229821828380009
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.137341 , 4.4948735, 3.5210333], dtype=float32)}
episode index:1269
target Thresh 18.97331635831069
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.530071 ,  8.25943  ,  5.3490114], dtype=float32)}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.4227578510507958
{'scaleFactor': 20, 'timeStep': 198, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.761755 , 4.7959976, 3.6407306], dtype=float32)}
episode index:1270
target Thresh 18.973449443528832
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9501843 , 7.3272395 , 0.04652473], dtype=float32)}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.42254903549694606
{'scaleFactor': 20, 'timeStep': 185, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7387294, 2.2713814, 1.3758947], dtype=float32)}
episode index:1271
target Thresh 18.973581864981675
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2620385, 5.9841876, 1.8536878], dtype=float32)}
done in step count: 227
reward sum = 0.10213842899856092
running average episode reward sum: 0.42229714036605115
{'scaleFactor': 20, 'timeStep': 228, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5382854, 3.4618604, 0.6595762], dtype=float32)}
episode index:1272
target Thresh 18.97371362597977
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0216398, 3.4505672, 5.3978276], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42275095251030403
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0216398, 3.4505672, 5.3978276], dtype=float32)}
episode index:1273
target Thresh 18.973844729817145
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.982012 ,  4.1227713,  5.3539624], dtype=float32)}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.42258277473104533
{'scaleFactor': 20, 'timeStep': 157, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6040497, 4.5700684, 4.261666 ], dtype=float32)}
episode index:1274
target Thresh 18.973975179771404
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.047358, 10.972755,  5.133144], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4228373545895556
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.711814 , 4.7075205, 4.3058786], dtype=float32)}
episode index:1275
target Thresh 18.9741049791038
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.0812631, 12.781544 ,  1.8995866], dtype=float32)}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.4226038439764439
{'scaleFactor': 20, 'timeStep': 208, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.777772 , 3.4892087, 4.637676 ], dtype=float32)}
episode index:1276
target Thresh 18.974234131059326
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.23532   ,  8.848204  ,  0.16152458], dtype=float32)}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.4223757387574555
{'scaleFactor': 20, 'timeStep': 203, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6375957 , 3.410345  , 0.94363195], dtype=float32)}
episode index:1277
target Thresh 18.974362638866786
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.0167055,  6.0628977,  3.8170624], dtype=float32)}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.4221318530491941
{'scaleFactor': 20, 'timeStep': 220, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.463948 , 3.581663 , 4.2670465], dtype=float32)}
episode index:1278
target Thresh 18.974490505738885
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2938535, 5.3456187, 3.1041539], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4222339262502208
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7599845, 1.899808 , 1.2868121], dtype=float32)}
episode index:1279
target Thresh 18.974617734872297
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.729004 ,  8.789467 ,  2.3971941], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42247044688280494
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1451051 , 4.9662313 , 0.74075997], dtype=float32)}
episode index:1280
target Thresh 18.974744329447756
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.353901 ,  6.139737 ,  3.7790737], dtype=float32)}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.42238640217265516
{'scaleFactor': 20, 'timeStep': 116, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1655807, 2.4891856, 4.0405216], dtype=float32)}
episode index:1281
target Thresh 18.974870292630136
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7318852, 8.972529 , 2.0733595], dtype=float32)}
done in step count: 297
reward sum = 0.050542043299318794
running average episode reward sum: 0.42209635197072587
{'scaleFactor': 20, 'timeStep': 298, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7029414, 4.241983 , 4.0994763], dtype=float32)}
episode index:1282
target Thresh 18.974995627568525
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.120244 , 9.783345 , 4.0028443], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4225085840034065
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.557246, 4.106087, 4.353288], dtype=float32)}
episode index:1283
target Thresh 18.975120337396305
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1687164, 4.7777076, 5.4737263], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4229583436731858
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1687164, 4.7777076, 5.4737263], dtype=float32)}
episode index:1284
target Thresh 18.975244425231217
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.856009 ,  3.009447 ,  1.3229616], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.4228552542962495
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.223711, 4.822068, 3.843982], dtype=float32)}
episode index:1285
target Thresh 18.975367894175474
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.8376327, 9.322698 , 5.4991646], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4230789696718141
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8126023, 4.96799  , 4.936921 ], dtype=float32)}
episode index:1286
target Thresh 18.975490747315803
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.968608 ,  7.0154815,  5.79345  ], dtype=float32)}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.4228502372486733
{'scaleFactor': 20, 'timeStep': 205, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.368699 , 2.3332753, 1.7683507], dtype=float32)}
episode index:1287
target Thresh 18.975612987723537
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.979057 ,  9.90914  ,  2.7835975], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4228361702730752
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.954236 , 4.1122804, 4.9254174], dtype=float32)}
episode index:1288
target Thresh 18.975734618454695
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.90487 , 10.10098 ,  3.350317], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42322399690158946
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2969503, 3.5907884, 3.466392 ], dtype=float32)}
episode index:1289
target Thresh 18.975855642550048
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.796526  ,  9.688071  ,  0.35174975], dtype=float32)}
done in step count: 467
reward sum = 0.009154526307566474
running average episode reward sum: 0.4229030128158577
{'scaleFactor': 20, 'timeStep': 468, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.668642 , 4.9593587, 1.8731802], dtype=float32)}
episode index:1290
target Thresh 18.97597606303521
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.815432  ,  9.885228  ,  0.38203418], dtype=float32)}
done in step count: 421
reward sum = 0.014535063236794427
running average episode reward sum: 0.422586693722458
{'scaleFactor': 20, 'timeStep': 422, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1564255, 3.425586 , 4.492424 ], dtype=float32)}
episode index:1291
target Thresh 18.976095882920696
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.214383  , 10.930173  ,  0.14829144], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4224890156215886
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.979116 , 3.3568628, 5.194731 ], dtype=float32)}
episode index:1292
target Thresh 18.97621510520201
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.890679 , 6.013623 , 0.7320839], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4227343453646409
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.52897  , 3.7756886, 4.378933 ], dtype=float32)}
episode index:1293
target Thresh 18.976333732859715
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2393575, 3.148526 , 5.0351315], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42318045483499284
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2393575, 3.148526 , 5.0351315], dtype=float32)}
episode index:1294
target Thresh 18.976451768859505
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.719791 ,  7.9873695,  3.8282979], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4234191597885213
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.490353 , 1.5716745, 6.1155834], dtype=float32)}
episode index:1295
target Thresh 18.976569216152296
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.004079  , 11.150896  ,  0.29803854], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.42334531997189256
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3886712, 3.2698848, 4.5063457], dtype=float32)}
episode index:1296
target Thresh 18.97668607767427
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.182322 ,  3.107789 ,  6.2573147], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42357788359254483
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5703602, 4.7111263, 5.0594616], dtype=float32)}
episode index:1297
target Thresh 18.97680235634697
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.948131 ,  5.7653008,  3.0402322], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.4236484222934934
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.674036 , 3.076178 , 5.9240065], dtype=float32)}
episode index:1298
target Thresh 18.97691805507737
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.2421126, 10.681685 ,  3.1587455], dtype=float32)}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.4234285857967991
{'scaleFactor': 20, 'timeStep': 198, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6990314, 4.090102 , 5.5030146], dtype=float32)}
episode index:1299
target Thresh 18.977033176757946
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1607394, 9.975338 , 1.5726006], dtype=float32)}
done in step count: 301
reward sum = 0.04855048513057287
running average episode reward sum: 0.42314021802705587
{'scaleFactor': 20, 'timeStep': 302, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.822926 , 2.3037176, 4.539253 ], dtype=float32)}
episode index:1300
target Thresh 18.97714772426674
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.760405  , 12.23419   ,  0.59962785], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.42293962592250856
{'scaleFactor': 20, 'timeStep': 182, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0691427, 4.7322044, 4.2358   ], dtype=float32)}
episode index:1301
target Thresh 18.977261700467455
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.084225 ,  7.8257504,  4.550068 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42261478750014103
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.894336 , 7.8287263, 0.5788443], dtype=float32)}
episode index:1302
target Thresh 18.977375108209497
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.8865232, 11.136737 ,  1.4892418], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4227409747560408
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2703373 , 2.71573   , 0.39028177], dtype=float32)}
episode index:1303
target Thresh 18.977487950328065
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.2813635,  5.197503 ,  4.4677024], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42241678689196405
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.044682,  8.111421,  2.287486], dtype=float32)}
episode index:1304
target Thresh 18.97760022964422
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.855787 ,  1.9371729,  5.6919236], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.42240323531019114
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0643044, 4.1451087, 3.9155288], dtype=float32)}
episode index:1305
target Thresh 18.97771194896495
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.753239  ,  3.5821357 ,  0.27808416], dtype=float32)}
done in step count: 351
reward sum = 0.02937333806646733
running average episode reward sum: 0.4221022935818268
{'scaleFactor': 20, 'timeStep': 352, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.924221 , 3.3575563, 1.5053082], dtype=float32)}
episode index:1306
target Thresh 18.97782311108324
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.09162  ,  1.7214184,  4.6125383], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.4219034164559119
{'scaleFactor': 20, 'timeStep': 182, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6448326, 4.382962 , 4.278712 ], dtype=float32)}
episode index:1307
target Thresh 18.977933718778154
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.017987 ,  7.9547825,  2.2381105], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.42176619415437416
{'scaleFactor': 20, 'timeStep': 142, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6767201, 3.4633913, 5.0353656], dtype=float32)}
episode index:1308
target Thresh 18.978043774814886
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.229968 ,  1.8611673,  3.40025  ], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.42166148947837634
{'scaleFactor': 20, 'timeStep': 126, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3912086, 4.7385116, 2.7752733], dtype=float32)}
episode index:1309
target Thresh 18.97815328194485
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.9400167, 11.22459  ,  0.738888 ], dtype=float32)}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.42162184684339266
{'scaleFactor': 20, 'timeStep': 100, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1031725, 4.224122 , 5.348172 ], dtype=float32)}
episode index:1310
target Thresh 18.97826224290572
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.881127 , 5.17814  , 2.1421168], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42203296367265014
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0992565, 4.982217 , 3.583509 ], dtype=float32)}
episode index:1311
target Thresh 18.978370660421533
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.191399,  9.099408,  2.584533], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4222923461045667
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0910203, 2.846439 , 5.383927 ], dtype=float32)}
episode index:1312
target Thresh 18.97847853720273
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1559665, 6.7097306, 1.5672202], dtype=float32)}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.4222129071995511
{'scaleFactor': 20, 'timeStep': 115, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0350927, 2.2556295, 1.2013822], dtype=float32)}
episode index:1313
target Thresh 18.978585875946234
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0580175, 7.1532536, 5.5596457], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4225461229129771
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.221824 , 1.9102726, 3.222827 ], dtype=float32)}
episode index:1314
target Thresh 18.978692679335524
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.16074  ,  2.1886759,  4.433425 ], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42265362030274095
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3506675, 3.5220098, 5.090251 ], dtype=float32)}
episode index:1315
target Thresh 18.978798950040684
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.849509 , 10.179264 ,  4.9028296], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4223324549377692
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.424906, 13.040738,  2.036774], dtype=float32)}
episode index:1316
target Thresh 18.978904690718494
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.904479 ,  5.7071476,  3.4218836], dtype=float32)}
done in step count: 305
reward sum = 0.04663740229999262
running average episode reward sum: 0.4220471891422964
{'scaleFactor': 20, 'timeStep': 306, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6495082, 4.5630217, 5.7939596], dtype=float32)}
episode index:1317
target Thresh 18.97900990401247
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.220618 , 12.58981  ,  2.3264263], dtype=float32)}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.42185125650236144
{'scaleFactor': 20, 'timeStep': 181, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.585306 , 2.0345836, 5.048673 ], dtype=float32)}
episode index:1318
target Thresh 18.979114592552957
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.097082 ,  6.970669 ,  4.1252155], dtype=float32)}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.4215755377119705
{'scaleFactor': 20, 'timeStep': 284, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6393304, 2.9205036, 3.1023183], dtype=float32)}
episode index:1319
target Thresh 18.97921875895717
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.902949 , 10.087559 ,  3.1111922], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.4214454328019299
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3907628, 1.5587869, 1.5728047], dtype=float32)}
episode index:1320
target Thresh 18.979322405829272
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.871457  ,  0.10805821,  4.7615404 ], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.4212491606272207
{'scaleFactor': 20, 'timeStep': 182, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5390043, 3.6552143, 5.423848 ], dtype=float32)}
episode index:1321
target Thresh 18.979425535760445
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.894339, 11.13172 ,  6.041128], dtype=float32)}
done in step count: 341
reward sum = 0.03247890341721044
running average episode reward sum: 0.4209550832768349
{'scaleFactor': 20, 'timeStep': 342, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.670988, 4.939854, 4.69495 ], dtype=float32)}
episode index:1322
target Thresh 18.97952815132894
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.003792 , 10.085382 ,  1.3224038], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.42123076208071275
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2543278, 3.8890533, 5.1572514], dtype=float32)}
episode index:1323
target Thresh 18.97963025510015
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.691007  , 9.793454  , 0.98641133], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.4210669499203393
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7296557, 1.9779902, 3.8665848], dtype=float32)}
episode index:1324
target Thresh 18.979731849626678
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.808581 ,  1.8480481,  4.0989056], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.4209940280938812
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7211604, 3.872398 , 3.9659486], dtype=float32)}
episode index:1325
target Thresh 18.97983293744839
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.82877  ,  7.9887214,  5.759719 ], dtype=float32)}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.42086306629735026
{'scaleFactor': 20, 'timeStep': 140, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.671454 , 4.2673244, 3.6818447], dtype=float32)}
episode index:1326
target Thresh 18.97993352109249
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0877147, 8.160403 , 3.1115508], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4212005815546763
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4497943, 3.1623633, 5.158224 ], dtype=float32)}
episode index:1327
target Thresh 18.98003360307357
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.62438  , 9.871375 , 4.2578816], dtype=float32)}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.42103122231707724
{'scaleFactor': 20, 'timeStep': 163, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7338291, 4.1215534, 6.054802 ], dtype=float32)}
episode index:1328
target Thresh 18.98013318589369
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.781175 ,  8.998389 ,  4.7666254], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4210314503079257
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3090655, 1.6249124, 5.194581 ], dtype=float32)}
episode index:1329
target Thresh 18.980232272042418
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.733501 , 8.815074 , 4.0715084], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.42086852700825533
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9515609, 1.071209 , 0.3234555], dtype=float32)}
episode index:1330
target Thresh 18.98033086399692
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9715395,  3.133902 ,  3.1425307], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.42068173617668747
{'scaleFactor': 20, 'timeStep': 176, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4281361, 4.575936 , 4.747261 ], dtype=float32)}
episode index:1331
target Thresh 18.980428964221996
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.00271  , 7.0854053, 2.250031 ], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.4205088981313334
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9995546, 3.1405725, 4.6613846], dtype=float32)}
episode index:1332
target Thresh 18.98052657517016
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.770235 ,  8.708768 ,  3.9240224], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.42026853577284146
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5216304, 4.7053246, 0.5591483], dtype=float32)}
episode index:1333
target Thresh 18.980623699281686
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.877419 ,  5.5142207,  4.9156413], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4205024449436672
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.808256 , 3.981664 , 5.4933043], dtype=float32)}
episode index:1334
target Thresh 18.980720338984693
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.910138,  9.880062,  4.181377], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4207642731840128
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1827929, 4.453598 , 3.7192874], dtype=float32)}
episode index:1335
target Thresh 18.980816496695162
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.994286 ,  5.195289 ,  5.6656885], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42044932986576133
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.308952 , 13.487379 ,  2.6545243], dtype=float32)}
episode index:1336
target Thresh 18.980912174817053
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1144066, 7.0571938, 2.706454 ], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.42067167930684773
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0073419, 3.4804971, 2.6887975], dtype=float32)}
episode index:1337
target Thresh 18.98100737574232
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.2213545, 10.76048  ,  6.030491 ], dtype=float32)}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.4204525026688596
{'scaleFactor': 20, 'timeStep': 206, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.513881 , 4.4965796, 2.42589  ], dtype=float32)}
episode index:1338
target Thresh 18.98110210185099
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.249125 ,  7.3390837,  0.5887412], dtype=float32)}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.4202405889289699
{'scaleFactor': 20, 'timeStep': 199, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1314688, 1.6631925, 0.4204912], dtype=float32)}
episode index:1339
target Thresh 18.981196355511223
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.158199, 11.26851 ,  5.972029], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4204789917531933
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2628708, 4.0542164, 4.4757156], dtype=float32)}
episode index:1340
target Thresh 18.981290139079363
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.046556 ,  3.1579716,  0.5025942], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4204553563514938
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1022515, 3.5641623, 2.6671836], dtype=float32)}
episode index:1341
target Thresh 18.98138345490001
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.954887 ,  9.059152 ,  1.7049588], dtype=float32)}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.420312118723911
{'scaleFactor': 20, 'timeStep': 148, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.356269 , 1.7733722, 5.918456 ], dtype=float32)}
episode index:1342
target Thresh 18.981476305306053
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.907798  , 11.003056  ,  0.10279404], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.42043626218125546
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9921471, 1.010623 , 0.8521492], dtype=float32)}
episode index:1343
target Thresh 18.98156869261877
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0777073, 6.875037 , 3.4636402], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42084538624213247
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5879235, 3.9947176, 4.6892037], dtype=float32)}
episode index:1344
target Thresh 18.98166061914784
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.858543 ,  6.9882   ,  5.6685934], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.4206460065737561
{'scaleFactor': 20, 'timeStep': 188, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2738833, 4.7872424, 5.8071213], dtype=float32)}
episode index:1345
target Thresh 18.98175208719143
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9762915, 7.087206 , 3.2502167], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4206974510510447
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.439622 , 3.9537315, 4.876025 ], dtype=float32)}
episode index:1346
target Thresh 18.981843099036254
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7701697, 6.595229 , 1.0449264], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42105653391961023
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4679475, 4.4127483, 4.3536673], dtype=float32)}
episode index:1347
target Thresh 18.981933656957608
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.860739 , 10.995927 ,  3.3837407], dtype=float32)}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.42088405640569904
{'scaleFactor': 20, 'timeStep': 167, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2268031, 3.9447272, 5.126172 ], dtype=float32)}
episode index:1348
target Thresh 18.982023763219445
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.607611 ,  7.156776 ,  1.8617222], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42112593041453966
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3712032, 2.5128953, 5.6312284], dtype=float32)}
episode index:1349
target Thresh 18.982113420074423
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.410202 , 11.032496 ,  6.2716455], dtype=float32)}
done in step count: 367
reward sum = 0.025010156959690534
running average episode reward sum: 0.4208325113230916
{'scaleFactor': 20, 'timeStep': 368, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2495475, 3.9260027, 4.395704 ], dtype=float32)}
episode index:1350
target Thresh 18.982202629763975
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.737911  , 11.210849  ,  0.18453711], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.4207685175004171
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9903529, 4.373456 , 3.9463692], dtype=float32)}
episode index:1351
target Thresh 18.98229139451834
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.009397  , 10.04248   ,  0.77562344], dtype=float32)}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.42054252772320794
{'scaleFactor': 20, 'timeStep': 216, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.909616 , 1.1153331, 3.919761 ], dtype=float32)}
episode index:1352
target Thresh 18.98237971655665
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0457497, 8.707928 , 1.4026449], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.4204715025954476
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7316536, 4.6797056, 4.9355106], dtype=float32)}
episode index:1353
target Thresh 18.982467598086956
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9356577 , 11.180849  ,  0.27155298], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4205816426987576
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.70315  , 1.1908594, 0.7038207], dtype=float32)}
episode index:1354
target Thresh 18.9825550413063
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.05343  , 10.758022 ,  2.5688648], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.42049442704781315
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7445618, 4.3254013, 0.5677675], dtype=float32)}
episode index:1355
target Thresh 18.982642048400766
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.386851, 12.063877,  2.26442 ], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.42041186808290665
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8114233, 3.469069 , 4.9269605], dtype=float32)}
episode index:1356
target Thresh 18.98272862154554
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.9367976, 11.001669 ,  0.9403135], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.4201758282937973
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5335464, 3.8859205, 3.7744195], dtype=float32)}
episode index:1357
target Thresh 18.98281476290495
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.809592 ,  7.251464 ,  1.1853721], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4202734038820658
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.086194 , 4.7648807, 3.6320114], dtype=float32)}
episode index:1358
target Thresh 18.98290047463254
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9084215 , 5.855768  , 0.60186774], dtype=float32)}
done in step count: 412
reward sum = 0.015911098861934425
running average episode reward sum: 0.41997585987542846
{'scaleFactor': 20, 'timeStep': 413, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1576583, 1.8312415, 5.477275 ], dtype=float32)}
episode index:1359
target Thresh 18.98298575887111
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.41388 , 11.188294,  2.572926], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4202684562561063
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1703396, 3.431656 , 5.0389676], dtype=float32)}
episode index:1360
target Thresh 18.98307061775276
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.0830607 , 10.130945  ,  0.15199363], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.420476523294125
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.780994 , 2.3474054, 1.8625176], dtype=float32)}
episode index:1361
target Thresh 18.98315505339897
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.009861  , 6.9208994 , 0.53684264], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4206443838705378
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3143628, 3.5333219, 3.3726594], dtype=float32)}
episode index:1362
target Thresh 18.983239067920643
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.0500126 , 8.419132  , 0.10519617], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4209180228285519
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.304891 , 3.7870445, 4.665238 ], dtype=float32)}
episode index:1363
target Thresh 18.98332266341814
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.911787  , 10.753234  ,  0.01113194], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4211148931093734
{'scaleFactor': 20, 'timeStep': 38, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.336559 , 4.5824924, 5.5934763], dtype=float32)}
episode index:1364
target Thresh 18.98340584198135
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.94392  , 12.327387 ,  1.9110358], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4208063840301724
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.970765 ,  4.899739 ,  3.2074702], dtype=float32)}
episode index:1365
target Thresh 18.98348860568975
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.526163 , 11.2442   ,  2.8373954], dtype=float32)}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.42056199177180353
{'scaleFactor': 20, 'timeStep': 244, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0619054, 4.461732 , 4.966758 ], dtype=float32)}
episode index:1366
target Thresh 18.98357095661243
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.983045  , 10.224374  ,  0.36517423], dtype=float32)}
done in step count: 373
reward sum = 0.0235465663109519
running average episode reward sum: 0.42027156351616285
{'scaleFactor': 20, 'timeStep': 374, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.201022 , 1.5166887, 3.6877694], dtype=float32)}
episode index:1367
target Thresh 18.98365289680817
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.174395 ,  3.9599404,  3.8070984], dtype=float32)}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.4200353508452373
{'scaleFactor': 20, 'timeStep': 233, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0283345, 4.3008857, 2.5430684], dtype=float32)}
episode index:1368
target Thresh 18.983734428325477
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.730328 ,  7.2578096,  4.637693 ], dtype=float32)}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.4198335316971792
{'scaleFactor': 20, 'timeStep': 194, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0951684, 3.218226 , 4.087451 ], dtype=float32)}
episode index:1369
target Thresh 18.983815553202646
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.158002 ,  2.951726 ,  4.1136045], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.42001538441752406
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9213035, 3.3929222, 3.959425 ], dtype=float32)}
episode index:1370
target Thresh 18.983896273467803
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.058748 , 4.9614296, 4.6610374], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42043842206565135
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.058748 , 4.9614296, 4.6610374], dtype=float32)}
episode index:1371
target Thresh 18.98397659113896
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.653016, 7.886194, 4.730292], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4206498760053063
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1957316, 1.2139943, 2.0975008], dtype=float32)}
episode index:1372
target Thresh 18.984056508224057
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.168947 ,  7.9488745,  1.4102911], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.420969911313854
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6954298, 4.5199733, 3.7618556], dtype=float32)}
episode index:1373
target Thresh 18.984136026721032
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7762797, 6.8997555, 2.7474093], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.42122962852497864
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1176062, 2.6550198, 5.6708994], dtype=float32)}
episode index:1374
target Thresh 18.984215148617846
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.922137, 12.69093 ,  2.598426], dtype=float32)}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.4210122926088114
{'scaleFactor': 20, 'timeStep': 210, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.111633 , 4.7498646, 4.933534 ], dtype=float32)}
episode index:1375
target Thresh 18.984293875892558
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9916856 , 9.067326  , 0.63034886], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4213769237147846
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.303681 , 4.758644 , 3.4280248], dtype=float32)}
episode index:1376
target Thresh 18.984372210513346
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.947684 ,  3.070583 ,  3.5641015], dtype=float32)}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.4211597964962517
{'scaleFactor': 20, 'timeStep': 210, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.683759 , 3.3520155, 4.339302 ], dtype=float32)}
episode index:1377
target Thresh 18.98445015443858
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.238723 ,  9.942479 ,  3.1776447], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.42118552215027755
{'scaleFactor': 20, 'timeStep': 79, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.69136  , 3.71246  , 3.8826425], dtype=float32)}
episode index:1378
target Thresh 18.98452770961687
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0327888, 4.9785137, 1.6042191], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42160525708707935
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0327888, 4.9785137, 1.6042191], dtype=float32)}
episode index:1379
target Thresh 18.984604877987092
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.036327, 10.548559,  3.308307], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.421910574432222
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2246373, 4.612058 , 4.824761 ], dtype=float32)}
episode index:1380
target Thresh 18.98468166147846
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8454149, 9.04029  , 1.6011469], dtype=float32)}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4218330209193681
{'scaleFactor': 20, 'timeStep': 116, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9561033, 4.992445 , 0.7732537], dtype=float32)}
episode index:1381
target Thresh 18.98475806201057
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1128016, 8.8990345, 2.6944036], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4220962952463492
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3667262, 3.363786 , 4.657398 ], dtype=float32)}
episode index:1382
target Thresh 18.98483408149344
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.800057 ,  8.688487 ,  5.4352407], dtype=float32)}
done in step count: 269
reward sum = 0.06696800274786396
running average episode reward sum: 0.4218395141237906
{'scaleFactor': 20, 'timeStep': 270, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1612353, 3.3772717, 4.525352 ], dtype=float32)}
episode index:1383
target Thresh 18.984909721827552
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.948947 , 10.844914 ,  5.5916576], dtype=float32)}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.42170467099887493
{'scaleFactor': 20, 'timeStep': 145, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0310655, 1.5380584, 4.1572347], dtype=float32)}
episode index:1384
target Thresh 18.984984984903924
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.894818 , 6.189975 , 2.7815118], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.42159765618300726
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.849469, 2.588663, 3.855997], dtype=float32)}
episode index:1385
target Thresh 18.98505987260414
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.054082 ,  3.916792 ,  2.4303334], dtype=float32)}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.4215396247294222
{'scaleFactor': 20, 'timeStep': 108, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7364873, 4.942624 , 5.5857673], dtype=float32)}
episode index:1386
target Thresh 18.98513438680039
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.30261   , 8.90735   , 0.72851545], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.42135989171245175
{'scaleFactor': 20, 'timeStep': 176, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.545586 , 4.1527004, 4.8039412], dtype=float32)}
episode index:1387
target Thresh 18.985208529355535
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.457216 ,  8.947722 ,  1.1104705], dtype=float32)}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.42120206760252116
{'scaleFactor': 20, 'timeStep': 160, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7673514, 4.9495907, 4.91218  ], dtype=float32)}
episode index:1388
target Thresh 18.985282302123146
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.035358  ,  7.258827  ,  0.04376836], dtype=float32)}
done in step count: 301
reward sum = 0.04855048513057287
running average episode reward sum: 0.4209337799261555
{'scaleFactor': 20, 'timeStep': 302, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4038286, 4.2705154, 5.5752373], dtype=float32)}
episode index:1389
target Thresh 18.98535570694754
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.977253,  8.722329,  4.757473], dtype=float32)}
done in step count: 239
reward sum = 0.0905339582851764
running average episode reward sum: 0.42069608221274474
{'scaleFactor': 20, 'timeStep': 240, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9165266, 2.1302862, 0.5739784], dtype=float32)}
episode index:1390
target Thresh 18.98542874566385
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.049678 , 10.389823 ,  3.3435445], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42107731439656015
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.90412  , 4.9596834, 4.1578894], dtype=float32)}
episode index:1391
target Thresh 18.985501420098043
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.255778,  5.895263,  5.727358], dtype=float32)}
done in step count: 321
reward sum = 0.03970977861200674
running average episode reward sum: 0.42080334346568044
{'scaleFactor': 20, 'timeStep': 322, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0508723, 1.2477046, 5.3800354], dtype=float32)}
episode index:1392
target Thresh 18.985573732066978
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.8912325,  8.75284  ,  3.4882336], dtype=float32)}
done in step count: 236
reward sum = 0.09330521652106866
running average episode reward sum: 0.42056824071841226
{'scaleFactor': 20, 'timeStep': 237, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0293565 , 1.1827426 , 0.30531266], dtype=float32)}
episode index:1393
target Thresh 18.98564568337847
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.911198 , 9.143413 , 1.5869842], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.42078141309422273
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1905259, 4.9808416, 5.5796766], dtype=float32)}
episode index:1394
target Thresh 18.98571727583129
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.915854 ,  6.010165 ,  4.5850816], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.42075292972863876
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7716722, 1.4958748, 3.9506896], dtype=float32)}
episode index:1395
target Thresh 18.98578851121527
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6621275, 5.361456 , 3.925002 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.421160699836283
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6435039, 3.7047038, 3.9373965], dtype=float32)}
episode index:1396
target Thresh 18.985859391311283
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0126233, 8.904961 , 0.4425766], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4214748713858929
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7367058, 3.9901867, 3.390177 ], dtype=float32)}
episode index:1397
target Thresh 18.985929917891347
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.397287  ,  0.90291536,  3.866717  ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42117338721465836
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6714052 , 0.24122666, 3.9139378 ], dtype=float32)}
episode index:1398
target Thresh 18.986000092718626
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.843651 , 9.372484 , 5.5245824], dtype=float32)}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.42103222587567624
{'scaleFactor': 20, 'timeStep': 150, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5837141, 4.647525 , 4.5609155], dtype=float32)}
episode index:1399
target Thresh 18.986069917547493
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.310198, 10.970087,  5.909473], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4212493316685921
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7910688, 4.9968414, 4.742878 ], dtype=float32)}
episode index:1400
target Thresh 18.98613939412357
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.333834 ,  5.7067885,  5.497693 ], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.4210113557067945
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4424975, 3.6736474, 4.2633076], dtype=float32)}
episode index:1401
target Thresh 18.986208524183784
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.931566 , 9.101163 , 1.9673266], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4211694157969356
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6458063, 4.6885138, 4.9082766], dtype=float32)}
episode index:1402
target Thresh 18.98627730945638
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9142936, 5.021623 , 3.65082  ], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4212438470668777
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3625593, 4.4195867, 4.7591486], dtype=float32)}
episode index:1403
target Thresh 18.986345751661
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0830507, 6.842999 , 1.4235379], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.42126256456452504
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.337913  , 1.071728  , 0.31263512], dtype=float32)}
episode index:1404
target Thresh 18.986413852508697
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.377487 , 9.413251 , 4.5968065], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.42156269312596234
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7159305, 2.550207 , 5.024603 ], dtype=float32)}
episode index:1405
target Thresh 18.986481613702
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.903718 ,  9.127252 ,  0.2247259], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4215338769275118
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6491613, 4.9751787, 4.377638 ], dtype=float32)}
episode index:1406
target Thresh 18.986549036934946
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.047099 , 10.306122 ,  3.7271206], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4218901035213288
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7137   , 4.146015 , 4.3528323], dtype=float32)}
episode index:1407
target Thresh 18.98661612389311
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1775823, 5.9142222, 1.260376 ], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42219519419432133
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.098481 , 4.996361 , 2.6826086], dtype=float32)}
episode index:1408
target Thresh 18.986682876253674
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.029823 ,  7.3563595,  4.7190995], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.42226485021167826
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.258478 , 2.4517899, 1.875673 ], dtype=float32)}
episode index:1409
target Thresh 18.986749295685453
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.672393 ,  6.323056 ,  2.4152064], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.42228276394469394
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7560792, 3.6637905, 5.7896557], dtype=float32)}
episode index:1410
target Thresh 18.986815383848935
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.918971 , 10.564565 ,  5.7696385], dtype=float32)}
done in step count: 466
reward sum = 0.009246996270269165
running average episode reward sum: 0.42199003838291194
{'scaleFactor': 20, 'timeStep': 467, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5088172, 1.7215581, 1.7441053], dtype=float32)}
episode index:1411
target Thresh 18.986881142396324
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.996716 ,  6.0719576,  0.7979903], dtype=float32)}
done in step count: 338
reward sum = 0.03347308759177372
running average episode reward sum: 0.42171488473504287
{'scaleFactor': 20, 'timeStep': 339, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7068787, 4.708438 , 2.1634219], dtype=float32)}
episode index:1412
target Thresh 18.98694657297159
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.12822  ,  6.6821647,  0.5659576], dtype=float32)}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.42153353138113603
{'scaleFactor': 20, 'timeStep': 180, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8951955, 3.6086466, 4.8635798], dtype=float32)}
episode index:1413
target Thresh 18.9870116772105
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7977371, 8.126646 , 4.2316504], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4217855029709649
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.216028 , 1.0334944, 5.4897594], dtype=float32)}
episode index:1414
target Thresh 18.987076456740667
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.061008 ,  8.768688 ,  5.7510514], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4217763401946056
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8703957, 3.139399 , 4.441143 ], dtype=float32)}
episode index:1415
target Thresh 18.987140913181577
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.261927  ,  2.8788018 ,  0.33129853], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4217851360890952
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1543736, 1.5272532, 5.342761 ], dtype=float32)}
episode index:1416
target Thresh 18.987205048144645
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.708848  , 10.418147  ,  0.20318323], dtype=float32)}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.4215625129751899
{'scaleFactor': 20, 'timeStep': 224, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5833526, 1.9585012, 1.5442345], dtype=float32)}
episode index:1417
target Thresh 18.987268863233254
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8375094, 9.931396 , 3.5340028], dtype=float32)}
done in step count: 267
reward sum = 0.06832772446471172
running average episode reward sum: 0.42131340522588767
{'scaleFactor': 20, 'timeStep': 268, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9647784, 3.923709 , 4.9992576], dtype=float32)}
episode index:1418
target Thresh 18.987332360042778
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.722346 ,  2.9695458,  4.3912053], dtype=float32)}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.42109935066360193
{'scaleFactor': 20, 'timeStep': 214, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1652796, 4.8852654, 5.6809964], dtype=float32)}
episode index:1419
target Thresh 18.987395540160644
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.8549833, 11.038276 ,  1.0630534], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.42099931033819443
{'scaleFactor': 20, 'timeStep': 128, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.429739 , 3.2515364, 5.3706803], dtype=float32)}
episode index:1420
target Thresh 18.987458405166358
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.8212597 , 10.067771  ,  0.46056828], dtype=float32)}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.42082923037567266
{'scaleFactor': 20, 'timeStep': 172, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2710638, 1.6270941, 5.332413 ], dtype=float32)}
episode index:1421
target Thresh 18.98752095663155
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.139272 ,  5.8099036,  1.3436296], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4208576359572777
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8975534, 1.0719092, 3.5724978], dtype=float32)}
episode index:1422
target Thresh 18.987583196120003
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8711286, 9.818399 , 3.6304128], dtype=float32)}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.4207054826373824
{'scaleFactor': 20, 'timeStep': 159, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.912586 , 2.6017118, 1.821574 ], dtype=float32)}
episode index:1423
target Thresh 18.987645125187715
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9697793, 10.927189 ,  3.2314591], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4210201177006771
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3899677, 3.0564163, 4.80016  ], dtype=float32)}
episode index:1424
target Thresh 18.987706745382912
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.950456 , 10.855831 ,  3.3986812], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4210203381248554
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.894656 , 3.6022437, 1.2475475], dtype=float32)}
episode index:1425
target Thresh 18.987768058246104
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.918314 , 10.869708 ,  0.9402434], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4210296024927846
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.829212 , 1.031996 , 4.3070035], dtype=float32)}
episode index:1426
target Thresh 18.987829065310116
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.9468493, 10.001477 ,  1.4056084], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.4209859575700019
{'scaleFactor': 20, 'timeStep': 103, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7579677, 4.9950805, 4.497916 ], dtype=float32)}
episode index:1427
target Thresh 18.98788976810013
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.790231 ,  4.0200644,  4.285773 ], dtype=float32)}
done in step count: 484
reward sum = 0.00771674563961725
running average episode reward sum: 0.42069655335996664
{'scaleFactor': 20, 'timeStep': 485, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.639035 , 4.3656864, 3.2779882], dtype=float32)}
episode index:1428
target Thresh 18.987950168133708
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.00554  ,  3.8797581,  4.1188912], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.42060956248105774
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1389756, 1.8265916, 1.3006638], dtype=float32)}
episode index:1429
target Thresh 18.988010266920867
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.014782 ,  2.6518204,  5.4705067], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.42076934784181813
{'scaleFactor': 20, 'timeStep': 44, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2804716, 1.8746959, 5.544794 ], dtype=float32)}
episode index:1430
target Thresh 18.988070065964074
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.801834 ,  8.052286 ,  5.3742933], dtype=float32)}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.4205373163845669
{'scaleFactor': 20, 'timeStep': 242, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0216117, 4.7379613, 6.056735 ], dtype=float32)}
episode index:1431
target Thresh 18.98812956675831
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.8559246, 10.907265 ,  2.6367857], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4205151422237356
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.739069 , 1.0859007, 2.5340672], dtype=float32)}
episode index:1432
target Thresh 18.98818877079109
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.972448 , 9.038877 , 5.1468377], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.42040874416001833
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.206392 , 1.790918 , 4.5483665], dtype=float32)}
episode index:1433
target Thresh 18.988247679542532
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.9483595,  3.918437 ,  3.035408 ], dtype=float32)}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.42017266922619173
{'scaleFactor': 20, 'timeStep': 250, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.699533 , 3.8906708, 5.6013656], dtype=float32)}
episode index:1434
target Thresh 18.988306294485348
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.106686 ,  4.047685 ,  1.1794639], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4204614086632816
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6561174, 4.0125484, 4.959589 ], dtype=float32)}
episode index:1435
target Thresh 18.988364617084915
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.915549  ,  9.825901  ,  0.18303746], dtype=float32)}
done in step count: 307
reward sum = 0.045709317994222766
running average episode reward sum: 0.42020043924081013
{'scaleFactor': 20, 'timeStep': 308, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3961453, 2.1571722, 2.711537 ], dtype=float32)}
episode index:1436
target Thresh 18.988422648799308
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.043291 , 10.992045 ,  2.1913352], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4202012282337913
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4934995, 4.669911 , 3.6703937], dtype=float32)}
episode index:1437
target Thresh 18.988480391079314
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6836748, 4.017146 , 4.652596 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42060442626700845
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6836748, 4.017146 , 4.652596 ], dtype=float32)}
episode index:1438
target Thresh 18.988537845368498
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.867061 , 10.070156 ,  2.1184747], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.42078646252047847
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9555674, 1.0168813, 1.7120122], dtype=float32)}
episode index:1439
target Thresh 18.988595013103218
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.0229   , 11.094742 ,  6.0826006], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.42075886575352295
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.247677, 4.132661, 5.17883 ], dtype=float32)}
episode index:1440
target Thresh 18.988651895712675
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.788744,  4.521472,  3.26709 ], dtype=float32)}
done in step count: 247
reward sum = 0.08353972967320515
running average episode reward sum: 0.4205248483100252
{'scaleFactor': 20, 'timeStep': 248, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7549719, 4.985527 , 3.2308335], dtype=float32)}
episode index:1441
target Thresh 18.98870849461893
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0522816, 8.80621  , 6.050232 ], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4207835788477046
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.916095 , 2.203849 , 3.6946325], dtype=float32)}
episode index:1442
target Thresh 18.988764811236965
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.934522 , 6.6207457, 2.0392194], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4209555734282465
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.502257 , 1.5365273, 4.038465 ], dtype=float32)}
episode index:1443
target Thresh 18.988820846974694
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.132547 ,  6.902003 ,  3.1901784], dtype=float32)}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.42076161833093073
{'scaleFactor': 20, 'timeStep': 196, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8615057, 4.9480658, 4.71488  ], dtype=float32)}
episode index:1444
target Thresh 18.988876603233017
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.081493  , 10.505393  ,  0.28932604], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.420759098788787
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5117493, 3.1100073, 5.562326 ], dtype=float32)}
episode index:1445
target Thresh 18.988932081405842
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4354293, 3.3013334, 4.599843 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4211596803248943
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4354293, 3.3013334, 4.599843 ], dtype=float32)}
episode index:1446
target Thresh 18.98898728288012
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.166366 ,  1.4708325,  6.2355714], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.42112671874930996
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0568355, 1.9068736, 5.1136837], dtype=float32)}
episode index:1447
target Thresh 18.989042209035905
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5971954, 9.146818 , 4.251912 ], dtype=float32)}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.42101370624103274
{'scaleFactor': 20, 'timeStep': 136, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7703495, 3.3470705, 3.5643072], dtype=float32)}
episode index:1448
target Thresh 18.989096861246338
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.04112  , 9.219349 , 1.6368012], dtype=float32)}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.42085197923652945
{'scaleFactor': 20, 'timeStep': 168, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.548626 , 1.0681193, 2.2062962], dtype=float32)}
episode index:1449
target Thresh 18.989151240877735
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.061685 , 9.763757 , 0.6864101], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.42114894874815584
{'scaleFactor': 20, 'timeStep': 17, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6317444, 4.8364944, 1.7949722], dtype=float32)}
episode index:1450
target Thresh 18.989205349289595
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.146427 , 11.3575945,  3.4489577], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42151410457251964
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3407106, 4.9344034, 4.1427317], dtype=float32)}
episode index:1451
target Thresh 18.98925918783462
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.2252445,  3.8787036,  3.9420109], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.42142792859650496
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5694942, 4.6940126, 4.683816 ], dtype=float32)}
episode index:1452
target Thresh 18.98931275785878
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0025432, 9.250906 , 2.0777779], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42155847870726637
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9237866, 3.6362054, 4.606842 ], dtype=float32)}
episode index:1453
target Thresh 18.989366060701336
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.0752177, 9.856808 , 2.4844131], dtype=float32)}
done in step count: 330
reward sum = 0.036275567655825146
running average episode reward sum: 0.421293497337905
{'scaleFactor': 20, 'timeStep': 331, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5280485, 3.8556376, 3.7911677], dtype=float32)}
episode index:1454
target Thresh 18.98941909769485
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.772193 , 10.9286785,  3.187133 ], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42160705577478547
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.072705 , 3.7145226, 4.5859594], dtype=float32)}
episode index:1455
target Thresh 18.989471870165257
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.116274 , 10.263219 ,  2.7435267], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42131749048922584
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.049688 ,  6.9406166,  4.586807 ], dtype=float32)}
episode index:1456
target Thresh 18.98952437943187
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0020895, 5.03962  , 2.3450985], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4216810269061173
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2586539, 3.5874422, 4.272502 ], dtype=float32)}
episode index:1457
target Thresh 18.989576626807423
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4706173, 9.128075 , 4.9292455], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42204406464479627
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.95985  , 4.6281557, 5.5139675], dtype=float32)}
episode index:1458
target Thresh 18.989628613598104
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.956112 ,  1.925279 ,  4.5199466], dtype=float32)}
done in step count: 316
reward sum = 0.04175625035843684
running average episode reward sum: 0.42178341501197486
{'scaleFactor': 20, 'timeStep': 317, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8192334, 1.5566444, 6.0357637], dtype=float32)}
episode index:1459
target Thresh 18.989680341103583
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.673648, 9.121911, 4.848718], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.42207188061359946
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.997317  , 2.1139922 , 0.02086418], dtype=float32)}
episode index:1460
target Thresh 18.98973181061705
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.919534 ,  8.5931   ,  5.2422843], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.4219044957033634
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0587654, 4.994766 , 3.728078 ], dtype=float32)}
episode index:1461
target Thresh 18.989783023425254
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.212559 ,  7.5527987,  3.372937 ], dtype=float32)}
done in step count: 406
reward sum = 0.016900089579220106
running average episode reward sum: 0.4216274749057408
{'scaleFactor': 20, 'timeStep': 407, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0211797, 1.0000213, 2.1888804], dtype=float32)}
episode index:1462
target Thresh 18.98983398080851
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.82294  , 11.036619 ,  3.4756851], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.42162439452640216
{'scaleFactor': 20, 'timeStep': 88, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.826635, 4.727149, 5.763194], dtype=float32)}
episode index:1463
target Thresh 18.989884684040756
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3750873, 9.747869 , 3.737778 ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42195414704039286
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2645837, 4.742267 , 4.287364 ], dtype=float32)}
episode index:1464
target Thresh 18.989935134389572
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.6355529, 10.879136 ,  6.1504984], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.4219110031159161
{'scaleFactor': 20, 'timeStep': 103, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5840608, 1.8460709, 5.119499 ], dtype=float32)}
episode index:1465
target Thresh 18.989985333116227
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.979855 , 10.005148 ,  6.0209856], dtype=float32)}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.4217263113913849
{'scaleFactor': 20, 'timeStep': 189, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2853258, 1.2001019, 5.2159376], dtype=float32)}
episode index:1466
target Thresh 18.99003528147569
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.9588678, 11.1205015,  6.0542984], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.4215699780590758
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0326211, 4.200606 , 4.2201824], dtype=float32)}
episode index:1467
target Thresh 18.99008498071667
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.8315115,  2.9608946,  5.9740434], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.42158159081367447
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.38511  , 4.070822 , 5.4658575], dtype=float32)}
episode index:1468
target Thresh 18.99013443208165
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9291604, 6.088398 , 5.181391 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4219685332297305
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5600653, 4.183802 , 5.2073984], dtype=float32)}
episode index:1469
target Thresh 18.99018363680692
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.20373929, 3.9355717 , 3.196283  ], dtype=float32)}
done in step count: 484
reward sum = 0.00771674563961725
running average episode reward sum: 0.4216867292925944
{'scaleFactor': 20, 'timeStep': 485, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1934662, 1.4624228, 5.2617   ], dtype=float32)}
episode index:1470
target Thresh 18.990232596122596
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.6430764, 8.430385 , 4.0956635], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4220596812101385
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6077938, 4.6513743, 4.0319204], dtype=float32)}
episode index:1471
target Thresh 18.99028131125267
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.795609 ,  4.8032084,  5.3058386], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4218972469962111
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9644734, 1.2425526, 5.9246273], dtype=float32)}
episode index:1472
target Thresh 18.990329783415014
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.48106  , 3.3347373, 3.661083 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42228971322364073
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.48106  , 3.3347373, 3.661083 ], dtype=float32)}
episode index:1473
target Thresh 18.99037801382144
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2022705, 6.9645185, 1.3867599], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4225581102551018
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9868727, 3.1823118, 4.1367273], dtype=float32)}
episode index:1474
target Thresh 18.99042600367772
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.90673 ,  5.846681,  1.909339], dtype=float32)}
done in step count: 425
reward sum = 0.013962323750362412
running average episode reward sum: 0.4222810961625562
{'scaleFactor': 20, 'timeStep': 426, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4729676, 1.7152295, 1.1400616], dtype=float32)}
episode index:1475
target Thresh 18.990473754183583
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.937694 , 10.133896 ,  3.3804395], dtype=float32)}
done in step count: 246
reward sum = 0.08438356532646984
running average episode reward sum: 0.42205216829613607
{'scaleFactor': 20, 'timeStep': 247, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2103183, 3.9257474, 4.7359786], dtype=float32)}
episode index:1476
target Thresh 18.99052126653281
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.146393 ,  7.1051035,  2.831399 ], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4221760368803212
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9030042, 4.7519345, 4.7963204], dtype=float32)}
episode index:1477
target Thresh 18.990568541913202
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.83151  ,  9.891626 ,  1.5833086], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.42201294548400564
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.65955  , 3.4001327, 4.889873 ], dtype=float32)}
episode index:1478
target Thresh 18.990615581506656
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.0842    ,  8.005522  ,  0.70723677], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.42183293656162746
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9364758, 1.9875828, 2.2992327], dtype=float32)}
episode index:1479
target Thresh 18.990662386489156
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.141467 , 10.049727 ,  0.6093873], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42197347081476366
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2039084, 4.4561834, 5.742257 ], dtype=float32)}
episode index:1480
target Thresh 18.99070895803083
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.28585  ,  9.8976555,  3.4832568], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.4217405959812155
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.205342 , 3.5880203, 4.33719  ], dtype=float32)}
episode index:1481
target Thresh 18.990755297295973
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.762128 ,  4.9405127,  0.945428 ], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.42164430818945015
{'scaleFactor': 20, 'timeStep': 128, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.333453 , 4.3818007, 5.460471 ], dtype=float32)}
episode index:1482
target Thresh 18.990801405443065
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0374947, 6.9322653, 4.0468316], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.42198848960530827
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0457678, 4.9661236, 5.826694 ], dtype=float32)}
episode index:1483
target Thresh 18.990847283624817
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.254648,  6.75208 ,  5.027378], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.42197413392023164
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6403766, 2.3663673, 3.8747401], dtype=float32)}
episode index:1484
target Thresh 18.99089293298818
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.124063, 8.133218, 4.495178], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4223368422542921
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0242722, 4.5333295, 3.6117537], dtype=float32)}
episode index:1485
target Thresh 18.990938354674398
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.068318 , 10.825881 ,  3.1260748], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.42259208703712164
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3018703, 1.1497496, 2.5944333], dtype=float32)}
episode index:1486
target Thresh 18.990983549819006
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.921202 , 10.893427 ,  3.0529933], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4228103654549391
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7820745, 1.408717 , 4.8683324], dtype=float32)}
episode index:1487
target Thresh 18.991028519551893
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.260483 ,  6.9123597,  0.6704917], dtype=float32)}
done in step count: 201
reward sum = 0.1326398781093821
running average episode reward sum: 0.42261535840699177
{'scaleFactor': 20, 'timeStep': 202, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9635715, 4.345266 , 4.3373065], dtype=float32)}
episode index:1488
target Thresh 18.9910732649973
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3524804, 6.9888835, 4.0820756], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.42289764708058275
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5168742, 1.0283184, 5.554985 ], dtype=float32)}
episode index:1489
target Thresh 18.99111778727387
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.1449203, 11.953405 ,  1.6955003], dtype=float32)}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.4228427936674508
{'scaleFactor': 20, 'timeStep': 108, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4039717, 4.2619023, 4.187949 ], dtype=float32)}
episode index:1490
target Thresh 18.991162087494654
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.3899064, 9.070509 , 5.3549495], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4231843245556062
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.853581 , 2.9493964, 4.7457786], dtype=float32)}
episode index:1491
target Thresh 18.991206166767167
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.2068224, 9.719571 , 3.8092008], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.42355102339973777
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8776574, 3.9335651, 4.4242043], dtype=float32)}
episode index:1492
target Thresh 18.99125002619339
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.229848 , 10.851984 ,  6.2714434], dtype=float32)}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.4233337118070513
{'scaleFactor': 20, 'timeStep': 231, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9990053, 1.0042179, 4.0373816], dtype=float32)}
episode index:1493
target Thresh 18.99129366686982
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.374511 , 10.182515 ,  2.4099185], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4234761828741576
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9799814, 1.1467593, 4.7174506], dtype=float32)}
episode index:1494
target Thresh 18.99133708988746
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.966329 , 11.081514 ,  1.2032734], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4236099950561088
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2753973, 4.5581865, 4.1692705], dtype=float32)}
episode index:1495
target Thresh 18.991380296331897
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.120434 , 4.8615975, 5.5451074], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4239952824925686
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.120434 , 4.8615975, 5.5451074], dtype=float32)}
episode index:1496
target Thresh 18.991423287283293
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.746023 ,  8.867881 ,  4.8643866], dtype=float32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.4238756231849817
{'scaleFactor': 20, 'timeStep': 141, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4606974, 1.4407212, 4.2719774], dtype=float32)}
episode index:1497
target Thresh 18.99146606381643
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.8085804, 9.091417 , 4.5585957], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.42401735206540814
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1317284, 4.1680145, 0.8892008], dtype=float32)}
episode index:1498
target Thresh 18.991508627000712
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.160882 , 7.077978 , 2.2543683], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.42425862010326126
{'scaleFactor': 20, 'timeStep': 25, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.320118 , 4.793781 , 6.2097306], dtype=float32)}
episode index:1499
target Thresh 18.99155097790023
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9801672, 5.3171206, 5.653571 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4246291810231924
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.985883 , 4.9173713, 6.1256547], dtype=float32)}
episode index:1500
target Thresh 18.991593117573753
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.3399014, 10.241515 ,  2.5082667], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4249488032043954
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.272047 , 4.8684087, 4.5155444], dtype=float32)}
episode index:1501
target Thresh 18.99163504707478
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.918636  , 10.827162  ,  0.01435917], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.4247325296165506
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.946847, 4.068705, 5.662342], dtype=float32)}
episode index:1502
target Thresh 18.991676767451544
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.551    ,  9.954996 ,  1.6546626], dtype=float32)}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.42449317263483516
{'scaleFactor': 20, 'timeStep': 273, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4096684, 3.7810063, 4.593932 ], dtype=float32)}
episode index:1503
target Thresh 18.991718279747065
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7390503, 7.9182696, 2.4832718], dtype=float32)}
done in step count: 295
reward sum = 0.05156825150425344
running average episode reward sum: 0.4242452172351473
{'scaleFactor': 20, 'timeStep': 296, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6165063, 3.6328282, 5.4686637], dtype=float32)}
episode index:1504
target Thresh 18.991759584999148
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.047915 , 10.729984 ,  1.1074736], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4243268859561809
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8118186, 1.917893 , 4.2468724], dtype=float32)}
episode index:1505
target Thresh 18.991800684240424
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2006176, 6.356067 , 3.062675 ], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.424664029689216
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6366178, 4.119728 , 4.0392356], dtype=float32)}
episode index:1506
target Thresh 18.99184157849838
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.506122 ,  8.020676 ,  2.5375135], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4247918505991384
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2523665, 4.754792 , 2.545935 ], dtype=float32)}
episode index:1507
target Thresh 18.991882268795376
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.1952515, 10.970686 ,  1.3291264], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4249073440711988
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.523188 , 4.0469346, 4.494157 ], dtype=float32)}
episode index:1508
target Thresh 18.991922756148668
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.88212  , 10.967189 ,  4.7968526], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.42491642966280824
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3017373, 2.7965517, 2.56901  ], dtype=float32)}
episode index:1509
target Thresh 18.99196304157044
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.125183 , 10.974925 ,  5.5150747], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4248313105619714
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9966316, 4.900723 , 5.2074585], dtype=float32)}
episode index:1510
target Thresh 18.992003126067836
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2033033, 5.985752 , 1.3188908], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.42512509911406077
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8761649, 3.9620578, 4.5555034], dtype=float32)}
episode index:1511
target Thresh 18.992043010642966
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1567414, 9.020122 , 1.6543946], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42533809315851684
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0060542, 4.5854864, 5.809423 ], dtype=float32)}
episode index:1512
target Thresh 18.992082696292947
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.641403 , 6.570333 , 5.2114553], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.42512313465296697
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5773335, 4.6060743, 3.5835729], dtype=float32)}
episode index:1513
target Thresh 18.992122184009926
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.878251 ,  2.0209262,  3.7339203], dtype=float32)}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.4249264967421517
{'scaleFactor': 20, 'timeStep': 206, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.408236 , 2.049316 , 4.4122987], dtype=float32)}
episode index:1514
target Thresh 18.992161474781092
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7888598, 6.929899 , 1.3057983], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42506174237545935
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.326064 , 3.2690744, 5.221828 ], dtype=float32)}
episode index:1515
target Thresh 18.992200569588718
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.859173,  8.478388,  5.075064], dtype=float32)}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.42491888664944305
{'scaleFactor': 20, 'timeStep': 157, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8029613, 3.1887693, 3.5738046], dtype=float32)}
episode index:1516
target Thresh 18.99223946941018
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.6727054, 10.938742 ,  5.79375  ], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4252594016545528
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6303267, 4.096689 , 4.0592365], dtype=float32)}
episode index:1517
target Thresh 18.992278175217972
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.043738 ,  6.285363 ,  2.9184499], dtype=float32)}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.425164936103753
{'scaleFactor': 20, 'timeStep': 127, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9423857, 3.3876402, 5.3336706], dtype=float32)}
episode index:1518
target Thresh 18.992316687979745
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.923142 ,  1.8154665,  2.445621 ], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.42504951287818005
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3142862, 1.1257012, 6.16835  ], dtype=float32)}
episode index:1519
target Thresh 18.992355008658315
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.111637  ,  9.929972  ,  0.90343386], dtype=float32)}
done in step count: 328
reward sum = 0.03701210861730961
running average episode reward sum: 0.4247942251122189
{'scaleFactor': 20, 'timeStep': 329, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3096066, 4.9353466, 4.80452  ], dtype=float32)}
episode index:1520
target Thresh 18.992393138211703
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.052979 ,  9.837159 ,  4.8956265], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.4249087298994339
{'scaleFactor': 20, 'timeStep': 52, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9208682, 3.6476   , 4.582363 ], dtype=float32)}
episode index:1521
target Thresh 18.99243107759315
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.8800216, 9.006175 , 4.2737613], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42526069263274574
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.895502, 4.64952 , 3.065793], dtype=float32)}
episode index:1522
target Thresh 18.992468827751143
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.886084  ,  4.739009  ,  0.73599344], dtype=float32)}
done in step count: 452
reward sum = 0.010644075786444312
running average episode reward sum: 0.42498845585215067
{'scaleFactor': 20, 'timeStep': 453, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.114756 , 3.0093424, 3.4947095], dtype=float32)}
episode index:1523
target Thresh 18.992506389629437
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9223052, 10.723402 ,  2.532383 ], dtype=float32)}
done in step count: 374
reward sum = 0.02331110064784238
running average episode reward sum: 0.4247248880337751
{'scaleFactor': 20, 'timeStep': 375, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0689553, 2.5183642, 1.6213093], dtype=float32)}
episode index:1524
target Thresh 18.99254376416708
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.13835 , 11.211641,  5.412885], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4244463799104743
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.03853668, 1.5444661 , 5.110329  ], dtype=float32)}
episode index:1525
target Thresh 18.992580952298447
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1935878, 9.978979 , 3.7914915], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42416823680437304
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.788411 , 12.46292  ,  5.6279035], dtype=float32)}
episode index:1526
target Thresh 18.992617954953225
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.056347, 10.187551,  3.618566], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4242866636742703
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9330087, 3.4302404, 4.0875897], dtype=float32)}
episode index:1527
target Thresh 18.992654773056497
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.838602 ,  3.934277 ,  6.0095415], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.42416605229724796
{'scaleFactor': 20, 'timeStep': 143, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6226149, 4.040346 , 3.7242007], dtype=float32)}
episode index:1528
target Thresh 18.992691407528714
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.300768 ,  3.6520813,  3.4129505], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4242574448009465
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.043151 , 3.1449218, 4.89822  ], dtype=float32)}
episode index:1529
target Thresh 18.992727859285736
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.127085 , 11.121567 ,  2.2107368], dtype=float32)}
done in step count: 307
reward sum = 0.045709317994222766
running average episode reward sum: 0.4240100277246023
{'scaleFactor': 20, 'timeStep': 308, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2815514, 2.061092 , 5.698317 ], dtype=float32)}
episode index:1530
target Thresh 18.99276412923886
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.2655177, 9.377885 , 5.075588 ], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.42409046313587995
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8381262, 1.7572598, 6.2569284], dtype=float32)}
episode index:1531
target Thresh 18.992800218294843
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0798807, 9.195664 , 5.6160574], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42442818486320705
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6424713, 3.121869 , 5.241643 ], dtype=float32)}
episode index:1532
target Thresh 18.992836127355904
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.2044554, 11.743761 ,  2.8437338], dtype=float32)}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.42423698153278633
{'scaleFactor': 20, 'timeStep': 203, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.240915 , 3.6331694, 4.9997153], dtype=float32)}
episode index:1533
target Thresh 18.992871857319777
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.100554 , 10.9458685,  3.5788624], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42458662887859283
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.292539, 4.678684, 4.419435], dtype=float32)}
episode index:1534
target Thresh 18.992907409079713
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.03953  ,  4.8774347,  1.1663766], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4246135306025611
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8390248, 4.514852 , 5.039196 ], dtype=float32)}
episode index:1535
target Thresh 18.992942783524505
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.246638 ,  5.6318483,  2.69601  ], dtype=float32)}
done in step count: 360
reward sum = 0.026833050939885684
running average episode reward sum: 0.424354558936114
{'scaleFactor': 20, 'timeStep': 361, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8682532, 4.4470663, 4.8681703], dtype=float32)}
episode index:1536
target Thresh 18.99297798153852
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.692929, 11.032888,  3.790896], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.42438773101358285
{'scaleFactor': 20, 'timeStep': 75, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2911906, 3.1761117, 2.4411395], dtype=float32)}
episode index:1537
target Thresh 18.993013004001703
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.069334 , 11.058869 ,  3.0792143], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.4242043238333283
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6232297, 3.1068563, 5.187162 ], dtype=float32)}
episode index:1538
target Thresh 18.99304785178963
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.554749 ,  7.7953253,  1.9289224], dtype=float32)}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.4241193455342327
{'scaleFactor': 20, 'timeStep': 123, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5348723, 3.1611316, 4.0469255], dtype=float32)}
episode index:1539
target Thresh 18.99308252577348
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.738989 , 10.789296 ,  6.2759323], dtype=float32)}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.42400455289810257
{'scaleFactor': 20, 'timeStep': 140, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5570395, 3.5064113, 4.659536 ], dtype=float32)}
episode index:1540
target Thresh 18.99311702682012
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.096699 ,  9.588737 ,  1.6786892], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.4240505192230228
{'scaleFactor': 20, 'timeStep': 71, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7485986, 3.3049872, 4.5474963], dtype=float32)}
episode index:1541
target Thresh 18.993151355792072
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.156769 ,  4.991379 ,  5.5524836], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.4239375403236943
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5529966, 3.7824826, 4.9331293], dtype=float32)}
episode index:1542
target Thresh 18.99318551354756
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.960464 , 11.1489   ,  4.1612234], dtype=float32)}
done in step count: 259
reward sum = 0.07404835256958406
running average episode reward sum: 0.4237107812908011
{'scaleFactor': 20, 'timeStep': 260, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1689281 , 2.7517962 , 0.18718462], dtype=float32)}
episode index:1543
target Thresh 18.993219500940533
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.241865 ,  7.057585 ,  1.6143472], dtype=float32)}
done in step count: 303
reward sum = 0.047584330476474465
running average episode reward sum: 0.42346717607654316
{'scaleFactor': 20, 'timeStep': 304, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2702546, 3.4210734, 4.2800198], dtype=float32)}
episode index:1544
target Thresh 18.993253318820678
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.013273 , 9.7825775, 1.3489342], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.4235231751510888
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.789811, 3.687142, 4.264659], dtype=float32)}
episode index:1545
target Thresh 18.993286968033438
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.832211 ,  4.7564955,  5.166149 ], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.42345699009249477
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7215242, 2.8588562, 3.5880504], dtype=float32)}
episode index:1546
target Thresh 18.993320449420054
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0032382 , 8.94957   , 0.19383395], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4237014462007343
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9436882, 1.65782  , 3.5085564], dtype=float32)}
episode index:1547
target Thresh 18.993353763817552
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.308757 ,  5.5260487,  3.5115323], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.42361920145990645
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0023146, 4.8448143, 4.2499695], dtype=float32)}
episode index:1548
target Thresh 18.993386912058803
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2270827, 9.28148  , 3.4153523], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.42367495778320513
{'scaleFactor': 20, 'timeStep': 68, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.820776 , 4.9033213, 4.517733 ], dtype=float32)}
episode index:1549
target Thresh 18.99341989497251
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.878806 ,  8.893383 ,  1.0209827], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.423807956927347
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1712519, 1.2089398, 3.877213 ], dtype=float32)}
episode index:1550
target Thresh 18.99345271338325
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.172335 ,  0.9495754,  4.82614  ], dtype=float32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.4236925844851211
{'scaleFactor': 20, 'timeStep': 141, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.18611 , 3.79077 , 4.385119], dtype=float32)}
episode index:1551
target Thresh 18.993485368111482
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.342194, 8.267048, 4.396439], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.42353169353408443
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0055643, 2.4936323, 4.3884053], dtype=float32)}
episode index:1552
target Thresh 18.99351785997358
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.007287 , 10.916259 ,  6.0056996], dtype=float32)}
done in step count: 238
reward sum = 0.09144844271229938
running average episode reward sum: 0.4233178601465623
{'scaleFactor': 20, 'timeStep': 239, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0159488, 4.0174108, 5.4965563], dtype=float32)}
episode index:1553
target Thresh 18.993550189781836
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.865308 ,  7.993917 ,  3.7706158], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4233606995370756
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2716188, 4.605253 , 5.370188 ], dtype=float32)}
episode index:1554
target Thresh 18.9935823583445
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.986859 , 9.761409 , 1.2504654], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42367003804220216
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1735783, 4.3964415, 4.7963896], dtype=float32)}
episode index:1555
target Thresh 18.993614366465792
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.065731 ,  7.0222225,  5.357151 ], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4235599420428751
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2329319, 2.0591304, 2.3066373], dtype=float32)}
episode index:1556
target Thresh 18.99364621494591
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.008177, 10.904447,  6.140242], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4235426767020782
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7724524, 1.7827271, 5.8670177], dtype=float32)}
episode index:1557
target Thresh 18.993677904581073
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.133341 ,  7.1305146,  5.792789 ], dtype=float32)}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.42334938406221484
{'scaleFactor': 20, 'timeStep': 210, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8058333, 3.3174157, 4.225759 ], dtype=float32)}
episode index:1558
target Thresh 18.993709436163524
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8546095, 6.9820895, 3.4593556], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42337968377839413
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1924775, 4.718257 , 0.9826131], dtype=float32)}
episode index:1559
target Thresh 18.993740810481547
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.798231 ,  8.289788 ,  1.7302479], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.4232766920498504
{'scaleFactor': 20, 'timeStep': 134, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.259926, 3.929895, 2.540092], dtype=float32)}
episode index:1560
target Thresh 18.99377202831951
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.132635 ,  5.3993435,  3.4503133], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4231825056152888
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0499419, 3.9124756, 4.7542963], dtype=float32)}
episode index:1561
target Thresh 18.99380309045785
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0805593, 7.9967513, 3.585027 ], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4235265603556119
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1873934, 4.4729166, 4.9215374], dtype=float32)}
episode index:1562
target Thresh 18.993833997673136
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.681881,  6.667001,  2.366569], dtype=float32)}
done in step count: 404
reward sum = 0.01724322985330079
running average episode reward sum: 0.4232666222042989
{'scaleFactor': 20, 'timeStep': 405, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2192428, 4.5562334, 4.875526 ], dtype=float32)}
episode index:1563
target Thresh 18.993864750738037
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.353681 ,  9.02931  ,  1.4571732], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.42320978731599046
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3508296, 2.4296474, 1.252039 ], dtype=float32)}
episode index:1564
target Thresh 18.99389535042139
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.750331,  4.051437,  2.92035 ], dtype=float32)}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.4230881609106435
{'scaleFactor': 20, 'timeStep': 146, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0222958, 4.586568 , 4.8437634], dtype=float32)}
episode index:1565
target Thresh 18.993925797488185
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.947576, 9.791136, 2.908216], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.42293135016086575
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9752264, 1.4827261, 3.3734703], dtype=float32)}
episode index:1566
target Thresh 18.993956092699598
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.7971992, 10.989153 ,  5.396144 ], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4232158520514899
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.832929 , 4.8570013, 3.8467364], dtype=float32)}
episode index:1567
target Thresh 18.99398623681302
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.333002 ,  8.730302 ,  3.0573554], dtype=float32)}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.4230871759897472
{'scaleFactor': 20, 'timeStep': 151, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4879167, 3.3374228, 4.6330895], dtype=float32)}
episode index:1568
target Thresh 18.994016230582044
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.893921 , 10.871738 ,  1.4467576], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4228175219578863
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.0461145, 11.423549 ,  4.2724547], dtype=float32)}
episode index:1569
target Thresh 18.994046074756522
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.861183 ,  2.788156 ,  3.6619356], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.42263885314630933
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.193901 , 4.2881427, 5.0483637], dtype=float32)}
episode index:1570
target Thresh 18.99407577008256
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.935452 ,  6.1789722,  3.95754  ], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4226300570427296
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.416873 , 2.125851 , 4.1040993], dtype=float32)}
episode index:1571
target Thresh 18.994105317302537
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2719946, 9.624354 , 1.0563009], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42291942788621323
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2443233, 4.6870003, 5.6524625], dtype=float32)}
episode index:1572
target Thresh 18.994134717155145
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0582712, 6.792252 , 5.76647  ], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.42292385864631343
{'scaleFactor': 20, 'timeStep': 85, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.231956  , 4.344167  , 0.80912364], dtype=float32)}
episode index:1573
target Thresh 18.994163970375375
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.119597 , 10.145578 ,  2.7227368], dtype=float32)}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.422686954430443
{'scaleFactor': 20, 'timeStep': 299, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.907861 , 4.20072  , 1.9101062], dtype=float32)}
episode index:1574
target Thresh 18.994193077694558
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.8317127, 11.026637 ,  3.9161766], dtype=float32)}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.4224994791182197
{'scaleFactor': 20, 'timeStep': 206, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1298438, 3.7830808, 3.3601153], dtype=float32)}
episode index:1575
target Thresh 18.994222039840384
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.864518  ,  2.0080752 ,  0.67214364], dtype=float32)}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.42237191078580893
{'scaleFactor': 20, 'timeStep': 151, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.796439 , 2.6168094, 4.026218 ], dtype=float32)}
episode index:1576
target Thresh 18.994250857536905
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.8803   ,  2.147684 ,  3.4962869], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4224085433449628
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.215191 , 4.4013453, 4.0008693], dtype=float32)}
episode index:1577
target Thresh 18.994279531504564
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.695171 , 11.174886 ,  0.5571329], dtype=float32)}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.4222292448819908
{'scaleFactor': 20, 'timeStep': 197, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2976179, 3.229249 , 5.282199 ], dtype=float32)}
episode index:1578
target Thresh 18.994308062460213
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.592205 ,  6.4556546,  3.7398198], dtype=float32)}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.42205472575390346
{'scaleFactor': 20, 'timeStep': 192, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.868477 , 2.3446527, 1.0694717], dtype=float32)}
episode index:1579
target Thresh 18.994336451117125
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.098674  , 5.9406133 , 0.34695518], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4222418623405138
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9187388, 4.1567907, 4.143851 ], dtype=float32)}
episode index:1580
target Thresh 18.99436469818502
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.9083605, 10.109404 ,  5.629836 ], dtype=float32)}
done in step count: 284
reward sum = 0.05759639025694116
running average episode reward sum: 0.4220112200431807
{'scaleFactor': 20, 'timeStep': 285, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4541857, 4.333053 , 5.8894167], dtype=float32)}
episode index:1581
target Thresh 18.99439280437008
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.8375437, 11.0845585,  1.1654794], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.42213859942045506
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2053717, 4.3137517, 5.594659 ], dtype=float32)}
episode index:1582
target Thresh 18.994420770374955
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.9169393, 10.202749 ,  3.4233704], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42247268119586856
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5366044, 4.9543   , 4.294331 ], dtype=float32)}
episode index:1583
target Thresh 18.9944485968988
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.01602  , 10.830602 ,  6.1835227], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.42242352684116635
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8997855, 4.706619 , 5.512266 ], dtype=float32)}
episode index:1584
target Thresh 18.994476284637276
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.842683 ,  3.1358354,  4.331247 ], dtype=float32)}
done in step count: 375
reward sum = 0.02307798964136396
running average episode reward sum: 0.422171573820851
{'scaleFactor': 20, 'timeStep': 376, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0109084, 3.54562  , 4.8089385], dtype=float32)}
episode index:1585
target Thresh 18.994503834282582
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.07318  ,  6.9032707,  6.0513473], dtype=float32)}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.422088545397452
{'scaleFactor': 20, 'timeStep': 124, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5016365, 3.7959304, 3.7307532], dtype=float32)}
episode index:1586
target Thresh 18.994531246523454
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.954377 , 10.144688 ,  3.1070638], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4221924825345283
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4139948, 3.1727235, 4.9844694], dtype=float32)}
episode index:1587
target Thresh 18.99455852204521
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.363871 , 11.218898 ,  2.6038775], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4223232326281483
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9921399, 2.1024714, 5.736226 ], dtype=float32)}
episode index:1588
target Thresh 18.994585661529733
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.851707 , 7.1297407, 1.6757579], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.4221470112657531
{'scaleFactor': 20, 'timeStep': 195, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7767448, 4.2943063, 5.199439 ], dtype=float32)}
episode index:1589
target Thresh 18.994612665655513
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0284   , 9.056709 , 1.2241598], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4221491767450757
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0603068, 1.021591 , 5.8308187], dtype=float32)}
episode index:1590
target Thresh 18.99463953509765
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.999475 ,  7.2963586,  4.779414 ], dtype=float32)}
done in step count: 398
reward sum = 0.018315022217166757
running average episode reward sum: 0.42189535263789285
{'scaleFactor': 20, 'timeStep': 399, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0292478, 3.9192953, 4.8434396], dtype=float32)}
episode index:1591
target Thresh 18.994666270527887
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.11269 , 12.716284,  2.132189], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.42181651547379817
{'scaleFactor': 20, 'timeStep': 122, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6770353, 4.788368 , 3.9518652], dtype=float32)}
episode index:1592
target Thresh 18.99469287261461
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.068537, 9.019647, 1.682822], dtype=float32)}
done in step count: 328
reward sum = 0.03701210861730961
running average episode reward sum: 0.42157495589636157
{'scaleFactor': 20, 'timeStep': 329, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4218156, 3.2701502, 5.395832 ], dtype=float32)}
episode index:1593
target Thresh 18.994719342022872
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.846806 ,  7.8773313,  5.3218   ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42131047976342784
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.3313174, 4.802147 , 5.515381 ], dtype=float32)}
episode index:1594
target Thresh 18.99474567941441
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.348528 , 9.281021 , 4.0008416], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.42149632305674123
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3597925, 1.5399698, 5.269579 ], dtype=float32)}
episode index:1595
target Thresh 18.99477188544766
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.073505 ,  4.995038 ,  1.5595781], dtype=float32)}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.42138876712528867
{'scaleFactor': 20, 'timeStep': 139, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.454255 , 4.4889607, 2.2182555], dtype=float32)}
episode index:1596
target Thresh 18.994797960777774
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.071415 , 8.695005 , 1.9695843], dtype=float32)}
done in step count: 408
reward sum = 0.016563777796593626
running average episode reward sum: 0.4211352762114948
{'scaleFactor': 20, 'timeStep': 409, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.410996 , 3.064765 , 5.7905917], dtype=float32)}
episode index:1597
target Thresh 18.99482390605663
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.035324 ,  3.9770844,  5.9102707], dtype=float32)}
done in step count: 424
reward sum = 0.014103357323598397
running average episode reward sum: 0.4208805628705137
{'scaleFactor': 20, 'timeStep': 425, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6283565, 4.850002 , 0.5720765], dtype=float32)}
episode index:1598
target Thresh 18.994849721932876
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.029179 ,  6.764603 ,  4.6207337], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.42090870559240195
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.123595 , 4.963024 , 3.5730789], dtype=float32)}
episode index:1599
target Thresh 18.9948754090519
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.100557  ,  8.867442  ,  0.30160874], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.42094875725782804
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.491537 , 2.1400504, 3.9930558], dtype=float32)}
episode index:1600
target Thresh 18.99490096805588
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.9467974, 11.090144 ,  0.4362542], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.42080599433192933
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2412589, 3.418842 , 4.9021325], dtype=float32)}
episode index:1601
target Thresh 18.9949263995838
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.070229, 10.702391,  2.784841], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4210856072023645
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3501635, 4.6796074, 4.729347 ], dtype=float32)}
episode index:1602
target Thresh 18.994951704271443
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.940972 ,  2.3856568,  5.3250237], dtype=float32)}
done in step count: 408
reward sum = 0.016563777796593626
running average episode reward sum: 0.42083325422082624
{'scaleFactor': 20, 'timeStep': 409, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0897186, 3.6734886, 4.215084 ], dtype=float32)}
episode index:1603
target Thresh 18.99497688275143
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.520699, 11.283672,  2.961526], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.4208037224416701
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1050875 , 2.0690699 , 0.22149342], dtype=float32)}
episode index:1604
target Thresh 18.995001935653224
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.819991 , 9.549286 , 5.8413277], dtype=float32)}
done in step count: 269
reward sum = 0.06696800274786396
running average episode reward sum: 0.4205832640493375
{'scaleFactor': 20, 'timeStep': 270, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8606672, 4.129225 , 3.877349 ], dtype=float32)}
episode index:1605
target Thresh 18.99502686360315
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.5097294, 8.695353 , 4.320494 ], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.42084100408507896
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5678434, 2.129561 , 2.9627018], dtype=float32)}
episode index:1606
target Thresh 18.99505166722441
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.1316333, 11.341384 ,  6.101239 ], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.42070756695566536
{'scaleFactor': 20, 'timeStep': 158, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9404645, 4.51068  , 3.9535205], dtype=float32)}
episode index:1607
target Thresh 18.99507634713709
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.126498, 10.767026,  1.766284], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.42080374057694725
{'scaleFactor': 20, 'timeStep': 56, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0331774, 4.340221 , 3.2200713], dtype=float32)}
episode index:1608
target Thresh 18.99510090395819
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.193915 ,  6.121264 ,  5.5597644], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4207563872163323
{'scaleFactor': 20, 'timeStep': 107, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1919072, 4.2921815, 3.7633216], dtype=float32)}
episode index:1609
target Thresh 18.995125338301637
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.6538343, 9.436794 , 4.0997176], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.42074897341956596
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5902461, 3.7045014, 1.1090295], dtype=float32)}
episode index:1610
target Thresh 18.995149650778288
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.3081717, 10.997812 ,  2.7318137], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.42077698819408504
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1282957, 4.032203 , 5.0465593], dtype=float32)}
episode index:1611
target Thresh 18.995173841995953
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.098049, 9.913377, 3.018009], dtype=float32)}
done in step count: 280
reward sum = 0.05995901467146544
running average episode reward sum: 0.42055315570430674
{'scaleFactor': 20, 'timeStep': 281, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.453343 , 4.783736 , 5.5629616], dtype=float32)}
episode index:1612
target Thresh 18.995197912559416
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.287744, 12.738233,  2.044292], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4208256325790352
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.370805 , 3.2868495, 4.9838066], dtype=float32)}
episode index:1613
target Thresh 18.995221863070444
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.075876 , 10.859273 ,  1.2221408], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.4206126587312972
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0496811, 4.234752 , 4.2422194], dtype=float32)}
episode index:1614
target Thresh 18.995245694127803
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.214561 ,  8.155224 ,  1.0349698], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.4205721312736958
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3036942, 3.272851 , 5.0440364], dtype=float32)}
episode index:1615
target Thresh 18.995269406327264
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.827802,  9.97194 ,  5.280774], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.4207176059576689
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2322072, 3.7186265, 5.409632 ], dtype=float32)}
episode index:1616
target Thresh 18.995293000261636
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.262442  , 10.85636   ,  0.78011346], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4204574219094576
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.798626 , 13.44023  ,  2.7293572], dtype=float32)}
episode index:1617
target Thresh 18.99531647652077
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.1263757 , 11.333466  ,  0.37522215], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.4202803652054703
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6635604, 2.334275 , 1.6345371], dtype=float32)}
episode index:1618
target Thresh 18.995339835691574
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.159766 ,  4.004749 ,  3.6730134], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4202732866441467
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1193252, 3.6110406, 3.7836263], dtype=float32)}
episode index:1619
target Thresh 18.995363078358032
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1624856, 7.2198243, 4.909889 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42061885868942805
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.621133 , 3.2579215, 4.865988 ], dtype=float32)}
episode index:1620
target Thresh 18.9953862051012
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.94465  ,  6.1352534,  6.085548 ], dtype=float32)}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4205332583420197
{'scaleFactor': 20, 'timeStep': 127, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.731723 , 3.9603887, 4.7824626], dtype=float32)}
episode index:1621
target Thresh 18.99540921649926
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.9036508, 10.770702 ,  2.8340333], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4206823019811331
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0534844, 1.3723307, 5.549444 ], dtype=float32)}
episode index:1622
target Thresh 18.995432113127492
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.038148  , 11.233278  ,  0.09852713], dtype=float32)}
done in step count: 262
reward sum = 0.07184904244991483
running average episode reward sum: 0.42046737082923463
{'scaleFactor': 20, 'timeStep': 263, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8085303, 2.6010447, 3.8791964], dtype=float32)}
episode index:1623
target Thresh 18.995454895558314
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.132599 , 12.054373 ,  2.4114008], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42071209962650563
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2752862, 4.718615 , 5.6492615], dtype=float32)}
episode index:1624
target Thresh 18.995477564361288
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.056946  ,  6.073399  ,  0.44804862], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4206674110202693
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.47726  , 4.997859 , 4.4598637], dtype=float32)}
episode index:1625
target Thresh 18.995500120103138
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.047279 ,  9.922344 ,  4.6932883], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42094837880131397
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9414015, 1.902658 , 4.6527047], dtype=float32)}
episode index:1626
target Thresh 18.995522563347752
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.823906 ,  8.017594 ,  6.1471167], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.42109670926362663
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1302714, 3.9431157, 4.7116756], dtype=float32)}
episode index:1627
target Thresh 18.99554489465622
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.971526 ,  4.799319 ,  3.8749378], dtype=float32)}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.4209203474488812
{'scaleFactor': 20, 'timeStep': 201, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0648488, 2.3632154, 4.5943913], dtype=float32)}
episode index:1628
target Thresh 18.99556711458682
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9052663, 10.7164955,  3.5940356], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4211691186437164
{'scaleFactor': 20, 'timeStep': 20, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2456613, 4.0736246, 3.9056067], dtype=float32)}
episode index:1629
target Thresh 18.995589223695053
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.923956 , 8.867264 , 3.6836154], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4212672764937007
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7786272, 2.7838628, 5.673503 ], dtype=float32)}
episode index:1630
target Thresh 18.995611222533654
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.849212 ,  7.182765 ,  3.0460052], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.42112694420455316
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9270071, 2.8516264, 5.619299 ], dtype=float32)}
episode index:1631
target Thresh 18.995633111652584
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.251232 ,  3.9796562,  5.3965635], dtype=float32)}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.42091957740808983
{'scaleFactor': 20, 'timeStep': 249, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5328383, 1.55437  , 5.1802087], dtype=float32)}
episode index:1632
target Thresh 18.99565489159908
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8461144, 6.050382 , 0.6450635], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.42110577505570146
{'scaleFactor': 20, 'timeStep': 33, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6670442, 3.943253 , 4.390301 ], dtype=float32)}
episode index:1633
target Thresh 18.995676562917637
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.102742 , 11.208766 ,  1.8190644], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.4209743807852374
{'scaleFactor': 20, 'timeStep': 158, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1334307, 4.9234767, 2.514943 ], dtype=float32)}
episode index:1634
target Thresh 18.995698126150046
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.981412  , 11.905823  ,  0.81182796], dtype=float32)}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.42078325705173436
{'scaleFactor': 20, 'timeStep': 222, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2443268, 4.9597287, 4.637414 ], dtype=float32)}
episode index:1635
target Thresh 18.995719581835385
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.935746 , 9.449311 , 3.4138358], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4211132159471795
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7768786, 3.7553298, 4.8741436], dtype=float32)}
episode index:1636
target Thresh 18.995740930510046
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.779786 ,  7.971114 ,  3.5219398], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.42129441162014897
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.003394, 4.221947, 3.72316 ], dtype=float32)}
episode index:1637
target Thresh 18.995762172707746
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9952247, 7.993451 , 1.8883232], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.42121998303632247
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0464876, 3.6790388, 5.084962 ], dtype=float32)}
episode index:1638
target Thresh 18.995783308959542
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.173938 , 11.064939 ,  3.6245298], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.421520347444161
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8623953, 4.961947 , 5.2848606], dtype=float32)}
episode index:1639
target Thresh 18.995804339793846
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9309709, 11.01384  ,  3.4106174], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4216397192694647
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6261387, 4.060741 , 5.4396772], dtype=float32)}
episode index:1640
target Thresh 18.995825265736425
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2950032, 5.787872 , 1.5047133], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4216610294635381
{'scaleFactor': 20, 'timeStep': 79, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.965502 , 4.4696636, 4.317831 ], dtype=float32)}
episode index:1641
target Thresh 18.995846087310426
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.910045 , 11.222464 ,  0.7361112], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4215940380213856
{'scaleFactor': 20, 'timeStep': 117, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0816557, 4.2628727, 3.3720698], dtype=float32)}
episode index:1642
target Thresh 18.9958668050364
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.126737 , 9.556343 , 3.7502646], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.42169116058748224
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.682471 , 4.7191067, 3.8185084], dtype=float32)}
episode index:1643
target Thresh 18.995887419432282
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4326875 , 7.2465467 , 0.99788785], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.4214613655922861
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0495539, 4.7214746, 3.986011 ], dtype=float32)}
episode index:1644
target Thresh 18.995907931013434
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9126654, 7.298855 , 4.278485 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42180096354633334
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7374349, 4.2407312, 5.5684896], dtype=float32)}
episode index:1645
target Thresh 18.99592834029265
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.92279  ,  2.9550524,  1.9591148], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4217809045879663
{'scaleFactor': 20, 'timeStep': 95, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7133229, 4.75548  , 4.101613 ], dtype=float32)}
episode index:1646
target Thresh 18.99594864778016
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0039086, 6.8281636, 2.7786808], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.42198768164307204
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6057848, 2.860161 , 5.2550254], dtype=float32)}
episode index:1647
target Thresh 18.995968853983655
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9557402, 6.5465364, 4.0976286], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4221416509194174
{'scaleFactor': 20, 'timeStep': 40, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.159566 , 2.9469838, 5.9240947], dtype=float32)}
episode index:1648
target Thresh 18.99598895940829
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9111831, 9.019407 , 6.1067286], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4224124842498295
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.693082 , 2.9391255, 3.9989295], dtype=float32)}
episode index:1649
target Thresh 18.9960089645567
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.095421 ,  2.7471957,  2.7620182], dtype=float32)}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.42233430863605703
{'scaleFactor': 20, 'timeStep': 123, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1406958, 2.3430595, 5.945841 ], dtype=float32)}
episode index:1650
target Thresh 18.996028869929013
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5729132, 9.685365 , 4.097118 ], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4226545119923647
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1956977, 4.614492 , 4.889251 ], dtype=float32)}
episode index:1651
target Thresh 18.996048676022873
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.605606 , 10.349861 ,  2.2181726], dtype=float32)}
done in step count: 389
reward sum = 0.02004890686806079
running average episode reward sum: 0.4224108039989481
{'scaleFactor': 20, 'timeStep': 390, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8160343, 4.0457816, 5.6380095], dtype=float32)}
episode index:1652
target Thresh 18.996068383333423
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1846166, 7.8153777, 2.6741083], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4224486627807238
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3648422, 3.386013 , 4.1501994], dtype=float32)}
episode index:1653
target Thresh 18.996087992353356
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.034008, 10.798759,  2.95503 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4221932524646532
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8868814, 9.842341 , 4.0222445], dtype=float32)}
episode index:1654
target Thresh 18.99610750357289
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.7747188, 10.045385 ,  0.566833 ], dtype=float32)}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.42207744736849245
{'scaleFactor': 20, 'timeStep': 147, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6218352, 4.921728 , 6.1146674], dtype=float32)}
episode index:1655
target Thresh 18.996126917479813
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.951726 , 4.9372916, 2.1535995], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42242643441718297
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.951726 , 4.9372916, 2.1535995], dtype=float32)}
episode index:1656
target Thresh 18.996146234559472
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.831756  , 9.97835   , 0.10991117], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.42238157423617834
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1203218, 4.332984 , 3.9910827], dtype=float32)}
episode index:1657
target Thresh 18.996165455294793
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.066029 , 9.63283  , 3.6876156], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4225959528762042
{'scaleFactor': 20, 'timeStep': 26, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.202039, 4.08928 , 5.701211], dtype=float32)}
episode index:1658
target Thresh 18.996184580166297
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.1711848, 10.071786 ,  0.5124576], dtype=float32)}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4224948762082236
{'scaleFactor': 20, 'timeStep': 137, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.881717 , 3.8336139, 4.4599295], dtype=float32)}
episode index:1659
target Thresh 18.996203609652106
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.999658 ,  9.949338 ,  3.8989415], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.422240361222556
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.49789783, 11.889935  ,  3.7870598 ], dtype=float32)}
episode index:1660
target Thresh 18.996222544227958
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.350037 ,  4.6080217,  4.792495 ], dtype=float32)}
done in step count: 329
reward sum = 0.03664198753113651
running average episode reward sum: 0.4220082128940241
{'scaleFactor': 20, 'timeStep': 330, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0409184, 1.7486678, 2.7980196], dtype=float32)}
episode index:1661
target Thresh 18.99624138436722
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9538608, 9.8298025, 2.9019089], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4220155673548531
{'scaleFactor': 20, 'timeStep': 84, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.574526 , 4.6691256, 4.621739 ], dtype=float32)}
episode index:1662
target Thresh 18.996260130540897
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2899767, 5.952196 , 2.2912683], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4222156297239745
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5818498, 4.736068 , 4.093681 ], dtype=float32)}
episode index:1663
target Thresh 18.99627878321764
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.577858 , 9.983601 , 6.0928144], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.4222051227185384
{'scaleFactor': 20, 'timeStep': 91, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.625136 , 3.0387979, 5.2766075], dtype=float32)}
episode index:1664
target Thresh 18.99629734286377
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.081536, 4.91025 , 4.885314], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4225521466688576
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.081536, 4.91025 , 4.885314], dtype=float32)}
episode index:1665
target Thresh 18.99631580994328
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.0479732, 11.918731 ,  2.711442 ], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.422799422548078
{'scaleFactor': 20, 'timeStep': 19, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9658017, 4.51497  , 5.2517095], dtype=float32)}
episode index:1666
target Thresh 18.996334184917846
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.647883,  7.942048,  4.479755], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4228674902161399
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0091448, 4.6207933, 4.894888 ], dtype=float32)}
episode index:1667
target Thresh 18.996352468246844
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.093929, 6.879283, 3.292563], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42318986942464343
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3537924, 4.1713476, 4.766249 ], dtype=float32)}
episode index:1668
target Thresh 18.996370660387356
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.309706 , 6.068096 , 4.3261533], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4235235483524896
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9290515, 4.7837496, 3.8765955], dtype=float32)}
episode index:1669
target Thresh 18.996388761794194
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.916946, 10.114549,  6.059922], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4236110200016661
{'scaleFactor': 20, 'timeStep': 57, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3547616, 2.8249748, 3.0449479], dtype=float32)}
episode index:1670
target Thresh 18.996406772919883
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.179975 ,  6.823053 ,  5.6439295], dtype=float32)}
done in step count: 259
reward sum = 0.07404835256958406
running average episode reward sum: 0.42340182630481865
{'scaleFactor': 20, 'timeStep': 260, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8269129, 4.6936426, 5.252011 ], dtype=float32)}
episode index:1671
target Thresh 18.99642469421471
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9835846, 7.960151 , 5.764137 ], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42359100007699774
{'scaleFactor': 20, 'timeStep': 31, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9970984, 1.8309551, 1.0537709], dtype=float32)}
episode index:1672
target Thresh 18.996442526126707
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.968537 , 5.0946345, 6.2538977], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4238169651633468
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3591657, 1.3496974, 5.6046534], dtype=float32)}
episode index:1673
target Thresh 18.996460269101675
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8387163, 7.709973 , 5.46742  ], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.423865393433134
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1294627, 4.5242834, 3.9821784], dtype=float32)}
episode index:1674
target Thresh 18.996477923583182
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0776694, 6.0894694, 0.8782626], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.42399598818456774
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5436022, 3.3576398, 5.6659713], dtype=float32)}
episode index:1675
target Thresh 18.996495490012595
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.923351 ,  3.8219707,  6.049339 ], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4240566090075636
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7465515, 4.345343 , 4.6737022], dtype=float32)}
episode index:1676
target Thresh 18.996512968829077
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.977442 , 11.802051 ,  2.8770978], dtype=float32)}
done in step count: 264
reward sum = 0.07041924650516153
running average episode reward sum: 0.42384573401501596
{'scaleFactor': 20, 'timeStep': 265, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9574566, 1.4958248, 3.1697626], dtype=float32)}
episode index:1677
target Thresh 18.9965303604696
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.030836 ,  6.8910832,  0.6491575], dtype=float32)}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.423652802036617
{'scaleFactor': 20, 'timeStep': 230, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.187027 , 1.0391634, 3.6984563], dtype=float32)}
episode index:1678
target Thresh 18.99654766536895
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.804695 , 5.129189 , 1.9140455], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4236367359284487
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3791022, 2.8950775, 4.5452957], dtype=float32)}
episode index:1679
target Thresh 18.996564883959756
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.427746, 13.217069,  1.337321], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.42346512656474544
{'scaleFactor': 20, 'timeStep': 200, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4890256 , 1.1638391 , 0.72598493], dtype=float32)}
episode index:1680
target Thresh 18.996582016672484
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8301725, 9.032951 , 1.1119895], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4235589405371151
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.529588 , 1.004343 , 6.1228695], dtype=float32)}
episode index:1681
target Thresh 18.99659906393545
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.562057 ,  8.127825 ,  6.1413484], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4236164801222002
{'scaleFactor': 20, 'timeStep': 66, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2936053, 4.836261 , 4.642395 ], dtype=float32)}
episode index:1682
target Thresh 18.996616026174834
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0801945, 7.779665 , 6.18884  ], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4239075679221773
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0693784, 1.6866373, 4.125653 ], dtype=float32)}
episode index:1683
target Thresh 18.9966329038147
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.869286 ,  7.9389763,  1.7989252], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.424018737560901
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0057096, 1.7080288, 5.6174097], dtype=float32)}
episode index:1684
target Thresh 18.996649697276982
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.052949,  8.109571,  3.932561], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.4239778723247848
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.008038 , 3.5065093, 4.7466674], dtype=float32)}
episode index:1685
target Thresh 18.996666406981525
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8756614, 6.1643796, 4.875091 ], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42431359126172147
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8097773, 4.1951337, 4.976012 ], dtype=float32)}
episode index:1686
target Thresh 18.996683033346066
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9025321, 10.756601 ,  2.6537762], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4245770365619629
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2223623, 2.053408 , 5.5604873], dtype=float32)}
episode index:1687
target Thresh 18.99669957678627
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.7656674, 7.7889433, 4.3816133], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.424894583347175
{'scaleFactor': 20, 'timeStep': 5, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.058757 , 4.5249443, 3.9738874], dtype=float32)}
episode index:1688
target Thresh 18.99671603771572
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.669964 , 11.0414095,  2.559982 ], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.42494801291145956
{'scaleFactor': 20, 'timeStep': 67, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.160545 , 4.2224903, 4.9858775], dtype=float32)}
episode index:1689
target Thresh 18.996732416545946
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.941728 , 11.307995 ,  1.5187558], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4248755019190084
{'scaleFactor': 20, 'timeStep': 120, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7533293, 1.5745778, 2.1005645], dtype=float32)}
episode index:1690
target Thresh 18.99674871368641
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.873797  ,  8.982391  ,  0.81233734], dtype=float32)}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.42470035398238554
{'scaleFactor': 20, 'timeStep': 205, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.968371 , 1.5432173, 4.5934577], dtype=float32)}
episode index:1691
target Thresh 18.99676492954455
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.679787 ,  8.017552 ,  4.9747043], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.42474180688168683
{'scaleFactor': 20, 'timeStep': 71, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8651638, 3.8574252, 4.537876 ], dtype=float32)}
episode index:1692
target Thresh 18.996781064525763
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7658585, 5.9090614, 3.5715642], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4248411717012497
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2176266, 4.865693 , 4.652256 ], dtype=float32)}
episode index:1693
target Thresh 18.996797119033417
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.048718 , 11.280644 ,  2.0998356], dtype=float32)}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.4247762177469874
{'scaleFactor': 20, 'timeStep': 116, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3789034, 2.1999688, 4.2044253], dtype=float32)}
episode index:1694
target Thresh 18.99681309346888
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.768145 ,  9.955303 ,  5.8392816], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.42479499859064335
{'scaleFactor': 20, 'timeStep': 79, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4218411, 3.5860095, 4.3660116], dtype=float32)}
episode index:1695
target Thresh 18.996828988231517
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.166868  ,  3.241082  ,  0.04559708], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42454452984147434
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.074205 , 13.485404 ,  2.8347692], dtype=float32)}
episode index:1696
target Thresh 18.99684480371869
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.047945 , 10.804849 ,  0.9658693], dtype=float32)}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4244743557083493
{'scaleFactor': 20, 'timeStep': 119, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1902128, 2.1058645, 5.1274743], dtype=float32)}
episode index:1697
target Thresh 18.996860540325795
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.937287  ,  7.879996  ,  0.31712312], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.42464283560915256
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.771399 , 1.7737355, 4.2840304], dtype=float32)}
episode index:1698
target Thresh 18.996876198446245
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.852415 ,  3.9782383,  5.6333146], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42439289868413244
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.4842   , 13.110505 ,  2.1416125], dtype=float32)}
episode index:1699
target Thresh 18.99689177847149
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.107877  , -0.04565835,  4.694942  ], dtype=float32)}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.42427352155975284
{'scaleFactor': 20, 'timeStep': 151, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9830751, 3.5558703, 4.3075347], dtype=float32)}
episode index:1700
target Thresh 18.99690728079104
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.892038, 11.19395 ,  2.971723], dtype=float32)}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.4241184802132221
{'scaleFactor': 20, 'timeStep': 183, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0433633, 4.4100003, 5.9967356], dtype=float32)}
episode index:1701
target Thresh 18.996922705792443
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.002973, 10.070522,  2.029607], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.42413223152553153
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.134349 , 4.0041504, 4.7481594], dtype=float32)}
episode index:1702
target Thresh 18.996938053861335
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.006928 , 9.335825 , 3.8144162], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.4240453961973892
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.30841  , 4.0890408, 5.162739 ], dtype=float32)}
episode index:1703
target Thresh 18.996953325381412
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8550764, 11.139188 ,  1.0052934], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4241376033675305
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8852596, 4.6678686, 4.4731646], dtype=float32)}
episode index:1704
target Thresh 18.996968520734466
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.342634 , 13.044839 ,  1.9767396], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4242331454077475
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2719055, 4.20631  , 3.716024 ], dtype=float32)}
episode index:1705
target Thresh 18.99698364030038
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0987644, 4.8391013, 2.593996 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42457064063318256
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0987644, 4.8391013, 2.593996 ], dtype=float32)}
episode index:1706
target Thresh 18.996998684457143
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8462133, 7.660102 , 4.1726694], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4248624824924648
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1537652, 4.983534 , 4.720791 ], dtype=float32)}
episode index:1707
target Thresh 18.99701365358086
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.56492  ,  9.8783455,  1.1219857], dtype=float32)}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.42465064792732404
{'scaleFactor': 20, 'timeStep': 276, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.434219 , 4.340384 , 4.5521345], dtype=float32)}
episode index:1708
target Thresh 18.997028548045765
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.039212 , 10.806942 ,  4.3096747], dtype=float32)}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.4245158778576666
{'scaleFactor': 20, 'timeStep': 164, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.031702, 3.303217, 3.505021], dtype=float32)}
episode index:1709
target Thresh 18.997043368224215
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.175174 , 11.226495 ,  1.9900635], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.42445735718632494
{'scaleFactor': 20, 'timeStep': 113, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4089315, 4.9092717, 4.408494 ], dtype=float32)}
episode index:1710
target Thresh 18.997058114486716
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.4466505,  7.2254205,  2.4568572], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.4242967400316886
{'scaleFactor': 20, 'timeStep': 190, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9462624, 4.23215  , 4.5978374], dtype=float32)}
episode index:1711
target Thresh 18.997072787201926
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.938874 ,  1.8226922,  3.8789163], dtype=float32)}
done in step count: 411
reward sum = 0.016071817032256998
running average episode reward sum: 0.4240582908944226
{'scaleFactor': 20, 'timeStep': 412, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5600791, 4.4147153, 4.1177135], dtype=float32)}
episode index:1712
target Thresh 18.997087386736666
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.98167  , 10.33621  ,  6.2021694], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.42397363461753446
{'scaleFactor': 20, 'timeStep': 128, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6853768, 3.331816 , 5.5703177], dtype=float32)}
episode index:1713
target Thresh 18.99710191345592
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.069493 ,  1.9486684,  4.1527243], dtype=float32)}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.42382184601490347
{'scaleFactor': 20, 'timeStep': 181, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.528035 , 4.5542183, 4.9772363], dtype=float32)}
episode index:1714
target Thresh 18.99711636772286
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.796004  , 11.981135  ,  0.71083343], dtype=float32)}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.423649763504743
{'scaleFactor': 20, 'timeStep': 205, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1949184, 3.9753175, 5.010748 ], dtype=float32)}
episode index:1715
target Thresh 18.99713074989884
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.8126287, 9.585623 , 4.2939606], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.4236584859629628
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.506405 , 4.898955 , 5.7760897], dtype=float32)}
episode index:1716
target Thresh 18.997145060343424
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.554551 , 6.3197265, 5.929625 ], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4236144758456241
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6175523, 4.068244 , 4.63707  ], dtype=float32)}
episode index:1717
target Thresh 18.997159299414363
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.438956, 11.958595,  2.494926], dtype=float32)}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.42347123256908925
{'scaleFactor': 20, 'timeStep': 173, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.946432 , 2.7141   , 4.9223123], dtype=float32)}
episode index:1718
target Thresh 18.997173467467643
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.030954 , 10.505247 ,  3.3237133], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.42356638413940245
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.918347 , 2.186097 , 3.8278608], dtype=float32)}
episode index:1719
target Thresh 18.99718756485746
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.263701 , 10.799744 ,  4.4680696], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4237250160195828
{'scaleFactor': 20, 'timeStep': 37, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4877315, 2.7235703, 5.806492 ], dtype=float32)}
episode index:1720
target Thresh 18.997201591936253
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.962772 ,  4.0269256,  4.342776 ], dtype=float32)}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.42362993202502036
{'scaleFactor': 20, 'timeStep': 135, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7328033, 2.9767127, 4.0322385], dtype=float32)}
episode index:1721
target Thresh 18.997215549054697
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.146706 , 9.687202 , 2.4145527], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.42374239439953676
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8555509 , 4.887879  , 0.12288683], dtype=float32)}
episode index:1722
target Thresh 18.997229436561724
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.145215 , 11.752579 ,  1.6923445], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4239130201326759
{'scaleFactor': 20, 'timeStep': 34, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.722475 , 4.442177 , 3.9016435], dtype=float32)}
episode index:1723
target Thresh 18.99724325480452
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.1406984, 11.048874 ,  2.6167412], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.42398450715254715
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2741933, 1.8498538, 2.6914194], dtype=float32)}
episode index:1724
target Thresh 18.997257004128542
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.862729 ,  4.0836177,  6.059194 ], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.42388501506903226
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.829882 , 4.9719963, 3.959427 ], dtype=float32)}
episode index:1725
target Thresh 18.997270684877527
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.83358 ,  7.836933,  4.753783], dtype=float32)}
done in step count: 222
reward sum = 0.10740220574263752
running average episode reward sum: 0.4237016530705813
{'scaleFactor': 20, 'timeStep': 223, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8576775, 3.8254411, 3.9814186], dtype=float32)}
episode index:1726
target Thresh 18.99728429739349
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9786923, 5.11699  , 5.503656 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42402382929926075
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8840194, 4.9756036, 6.2243943], dtype=float32)}
episode index:1727
target Thresh 18.997297842016746
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.913209 ,  1.6581819,  3.9866095], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.423933564766632
{'scaleFactor': 20, 'timeStep': 132, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.243997 , 4.135792 , 5.0334816], dtype=float32)}
episode index:1728
target Thresh 18.997311319085913
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.7621965, 10.710901 ,  1.9258077], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.42416142675207485
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3538262, 3.3408954, 5.102019 ], dtype=float32)}
episode index:1729
target Thresh 18.997324728937915
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.975818 , 10.911404 ,  1.2131038], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42435250065985036
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4492596, 3.6761782, 3.5161061], dtype=float32)}
episode index:1730
target Thresh 18.997338071908004
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.633225  ,  6.79838   ,  0.29010704], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4244568643608773
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5315475, 3.2238784, 4.686242 ], dtype=float32)}
episode index:1731
target Thresh 18.99735134832975
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.213188 , 10.305937 ,  6.2426953], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.42438464930715414
{'scaleFactor': 20, 'timeStep': 121, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2093267, 3.4005938, 3.9886622], dtype=float32)}
episode index:1732
target Thresh 18.997364558535068
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.75491  ,  6.8455853,  5.212783 ], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42450319459388003
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.089541 , 3.7502608, 4.9252863], dtype=float32)}
episode index:1733
target Thresh 18.99737770285421
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.96203  ,  8.9630785,  3.5072038], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.42464032195627344
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1033865, 4.5807834, 5.2012978], dtype=float32)}
episode index:1734
target Thresh 18.997390781615785
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.99925  ,  7.640229 ,  1.3380718], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.42444620362038515
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1423753, 4.8809896, 2.6728816], dtype=float32)}
episode index:1735
target Thresh 18.99740379514677
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1266546, 7.1150675, 4.8110566], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4247606349547052
{'scaleFactor': 20, 'timeStep': 4, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5044122, 3.1557927, 5.3231115], dtype=float32)}
episode index:1736
target Thresh 18.997416743772497
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.376048 , 12.866403 ,  1.1450067], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.42462037376769957
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6833904, 1.0944828, 6.2593923], dtype=float32)}
episode index:1737
target Thresh 18.99742962781668
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.2687495, 10.051462 ,  0.2517454], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.42458040854384305
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3637686, 2.9405797, 2.6403174], dtype=float32)}
episode index:1738
target Thresh 18.997442447601426
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.844066  , 7.6551414 , 0.33264977], dtype=float32)}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.4245049872172078
{'scaleFactor': 20, 'timeStep': 123, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0354302, 4.148717 , 4.3662453], dtype=float32)}
episode index:1739
target Thresh 18.99745520344723
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.842135 ,  1.9676741,  4.0450287], dtype=float32)}
done in step count: 309
reward sum = 0.04479970256613774
running average episode reward sum: 0.42428676578924746
{'scaleFactor': 20, 'timeStep': 310, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1581132, 4.052593 , 5.3988137], dtype=float32)}
episode index:1740
target Thresh 18.997467895672987
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.655259  , 7.4831285 , 0.11329716], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.4242778820492321
{'scaleFactor': 20, 'timeStep': 90, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.948679 , 2.5890992, 2.6777253], dtype=float32)}
episode index:1741
target Thresh 18.997480524596003
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.6423979, 10.1799345,  0.5014976], dtype=float32)}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.42419290718450764
{'scaleFactor': 20, 'timeStep': 129, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4443675, 4.1189256, 4.6841764], dtype=float32)}
episode index:1742
target Thresh 18.997493090532007
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2341194, 5.8492627, 3.236285 ], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4244789380435113
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9837024, 3.1356   , 5.5908422], dtype=float32)}
episode index:1743
target Thresh 18.99750559379514
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.167818 ,  1.9144654,  1.5014931], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4246726672730432
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.80327 , 3.566091, 4.858419], dtype=float32)}
episode index:1744
target Thresh 18.99751803469799
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.905508  ,  5.085196  ,  0.84802955], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4247760101956016
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7087994, 4.4625382, 4.952407 ], dtype=float32)}
episode index:1745
target Thresh 18.997530413551576
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0205612, 7.0612955, 1.0212227], dtype=float32)}
done in step count: 337
reward sum = 0.03381119958765022
running average episode reward sum: 0.42455208991461196
{'scaleFactor': 20, 'timeStep': 338, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6292315, 3.6117482, 5.067279 ], dtype=float32)}
episode index:1746
target Thresh 18.997542730665373
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.955666 ,  3.1356666,  4.7758675], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.42448568601118897
{'scaleFactor': 20, 'timeStep': 118, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.463023 , 3.6220536, 4.2526073], dtype=float32)}
episode index:1747
target Thresh 18.997554986347307
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.841017 , 11.089948 ,  1.2434621], dtype=float32)}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.4243234427199378
{'scaleFactor': 20, 'timeStep': 196, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.211465 , 3.2244787, 3.7042496], dtype=float32)}
episode index:1748
target Thresh 18.997567180903776
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.81006  ,  5.2475266,  5.4233346], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4242880216651297
{'scaleFactor': 20, 'timeStep': 102, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.997473 , 4.5129046, 3.998637 ], dtype=float32)}
episode index:1749
target Thresh 18.997579314639637
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.982833, 10.347602,  2.897428], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42454701195160616
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4029918, 3.8766642, 4.424255 ], dtype=float32)}
episode index:1750
target Thresh 18.99759138785824
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.040824 , 11.486941 ,  2.5100799], dtype=float32)}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.4245199928967643
{'scaleFactor': 20, 'timeStep': 98, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.383987 , 4.7762113, 5.8572583], dtype=float32)}
episode index:1751
target Thresh 18.997603400861415
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0184772, 9.937198 , 0.8617984], dtype=float32)}
done in step count: 330
reward sum = 0.036275567655825146
running average episode reward sum: 0.4242983921974259
{'scaleFactor': 20, 'timeStep': 331, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0168927, 1.7649648, 1.8904456], dtype=float32)}
episode index:1752
target Thresh 18.997615353949485
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.101123,  7.310205,  5.479868], dtype=float32)}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.42418395425206434
{'scaleFactor': 20, 'timeStep': 150, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0630581, 4.576576 , 0.8098277], dtype=float32)}
episode index:1753
target Thresh 18.997627247421278
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.868635 , 10.079638 ,  1.1979679], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4241508005388495
{'scaleFactor': 20, 'timeStep': 101, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1166716, 4.0269566, 4.5102615], dtype=float32)}
episode index:1754
target Thresh 18.997639081574135
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.250008 ,  4.076606 ,  5.2214036], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.42419680343813626
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.773506 , 3.3948565, 4.4604363], dtype=float32)}
episode index:1755
target Thresh 18.99765085670391
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.074532, 7.298709, 2.534961], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.4241327170360924
{'scaleFactor': 20, 'timeStep': 117, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8082385, 3.0964792, 4.253275 ], dtype=float32)}
episode index:1756
target Thresh 18.99766257310498
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.005043 , 10.510214 ,  3.6355662], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4244218078903161
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.407337 , 2.9973958, 4.408914 ], dtype=float32)}
episode index:1757
target Thresh 18.997674231070253
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.893237 ,  6.0111556,  4.4531417], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.42443240298345364
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8202808, 3.4448156, 4.5487914], dtype=float32)}
episode index:1758
target Thresh 18.997685830891182
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9157252 , 8.221908  , 0.81490296], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42471569592912983
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0701265, 3.7365236, 3.9831085], dtype=float32)}
episode index:1759
target Thresh 18.997697372857765
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9833555,  4.035245 ,  3.6372535], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.42457729323435534
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.314712 , 3.7976305, 4.358292 ], dtype=float32)}
episode index:1760
target Thresh 18.997708857258548
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.910829 ,  5.831878 ,  5.0409713], dtype=float32)}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.42441782000546224
{'scaleFactor': 20, 'timeStep': 194, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4282962, 4.897004 , 6.0383964], dtype=float32)}
episode index:1761
target Thresh 18.997720284380645
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.457937 ,  3.0923119,  5.664625 ], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.4244020765244274
{'scaleFactor': 20, 'timeStep': 93, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6062895, 4.2189107, 4.5001497], dtype=float32)}
episode index:1762
target Thresh 18.99773165450973
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8968039, 7.1397753, 1.0796652], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.4243886236466209
{'scaleFactor': 20, 'timeStep': 92, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3225017, 4.5303974, 5.7495522], dtype=float32)}
episode index:1763
target Thresh 18.997742967930062
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.845256 ,  1.9789933,  3.5590062], dtype=float32)}
done in step count: 483
reward sum = 0.0077946925652699495
running average episode reward sum: 0.4241524592865975
{'scaleFactor': 20, 'timeStep': 484, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0861278, 4.8821607, 5.338842 ], dtype=float32)}
episode index:1764
target Thresh 18.997754224924474
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.122498 , 12.582431 ,  2.2252038], dtype=float32)}
done in step count: 293
reward sum = 0.05261529589251448
running average episode reward sum: 0.4239419566444479
{'scaleFactor': 20, 'timeStep': 294, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6172011, 2.1564374, 5.5806055], dtype=float32)}
episode index:1765
target Thresh 18.997765425774393
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0137255, 9.0053005, 1.2611722], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.42410022716446777
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4660761, 3.2497792, 5.4462337], dtype=float32)}
episode index:1766
target Thresh 18.997776570759836
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1124845, 6.2753816, 0.7417687], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.42414594627121516
{'scaleFactor': 20, 'timeStep': 69, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2568293, 4.631428 , 5.014178 ], dtype=float32)}
episode index:1767
target Thresh 18.997787660159435
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.117868, 8.104313, 4.384512], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.42446039992151424
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3825917, 4.765348 , 5.092768 ], dtype=float32)}
episode index:1768
target Thresh 18.997798694250424
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.2404175, 10.958003 ,  3.8131151], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4247420756108904
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.82673  , 3.3539877, 5.3661695], dtype=float32)}
episode index:1769
target Thresh 18.99780967330865
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.763506 ,  1.8329902,  1.1847954], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4249158390538528
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8210816, 4.9012966, 3.8884194], dtype=float32)}
episode index:1770
target Thresh 18.9978205976086
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.17684  , 6.8527184, 3.5482194], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.42483033555976374
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.210084 , 1.7435875, 1.1095091], dtype=float32)}
episode index:1771
target Thresh 18.997831467423374
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.351732 , 6.3877754, 4.0592484], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.4247792895785731
{'scaleFactor': 20, 'timeStep': 110, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.039912 , 4.7379646, 2.4662874], dtype=float32)}
episode index:1772
target Thresh 18.99784228302472
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.321635,  8.322704,  5.51593 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42453970735094837
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.96141  ,  2.9126167,  2.8383565], dtype=float32)}
episode index:1773
target Thresh 18.997853044683033
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.830621 , 9.985184 , 0.3052414], dtype=float32)}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.424407757944192
{'scaleFactor': 20, 'timeStep': 166, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2402507, 4.4680424, 2.306926 ], dtype=float32)}
episode index:1774
target Thresh 18.99786375266735
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.935463  , 9.907923  , 0.40165108], dtype=float32)}
done in step count: 171
reward sum = 0.17931568359471053
running average episode reward sum: 0.42426967790230496
{'scaleFactor': 20, 'timeStep': 172, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2177835, 4.105446 , 4.34071  ], dtype=float32)}
episode index:1775
target Thresh 18.99787440724537
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.851203 , 10.931815 ,  3.9819052], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4240307873179005
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.05889528, 11.157984  ,  4.115065  ], dtype=float32)}
episode index:1776
target Thresh 18.99788500868346
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.894997, 10.954473,  4.522777], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4243114366747435
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4552565, 4.2661624, 4.945247 ], dtype=float32)}
episode index:1777
target Thresh 18.99789555724666
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8537571, 8.24972  , 5.384996 ], dtype=float32)}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.42414517621603426
{'scaleFactor': 20, 'timeStep': 205, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9635357, 1.1864659, 2.5808015], dtype=float32)}
episode index:1778
target Thresh 18.99790605319868
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.9123654, 9.709285 , 1.4173346], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.42430617006148463
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.712211 , 4.7652993, 5.018439 ], dtype=float32)}
episode index:1779
target Thresh 18.99791649680192
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.673906 ,  4.851142 ,  2.6681595], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.42425376945376525
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7765894, 4.850218 , 4.787567 ], dtype=float32)}
episode index:1780
target Thresh 18.997926888317473
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.112316 ,  6.853876 ,  2.6945457], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42435525867200435
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2215154, 3.1139004, 4.8201485], dtype=float32)}
episode index:1781
target Thresh 18.997937228005128
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.115064 , 10.908606 ,  5.6370893], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.42432670032283615
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5315609, 3.4227135, 4.804779 ], dtype=float32)}
episode index:1782
target Thresh 18.99794751612337
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.964078 ,  5.159763 ,  3.0890043], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4244049832673844
{'scaleFactor': 20, 'timeStep': 58, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3214766, 3.9602747, 3.815021 ], dtype=float32)}
episode index:1783
target Thresh 18.997957752929413
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.92469  ,  7.2311535,  0.290497 ], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4244800175472501
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6277275, 4.849801 , 5.881495 ], dtype=float32)}
episode index:1784
target Thresh 18.99796793867917
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5155716, 9.882306 , 5.645914 ], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42474887024050595
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0003042, 1.8717341, 5.05787  ], dtype=float32)}
episode index:1785
target Thresh 18.99797807362729
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.947056, 10.247147,  5.498906], dtype=float32)}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.42456653874290146
{'scaleFactor': 20, 'timeStep': 231, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6011248, 2.9610786, 4.612659 ], dtype=float32)}
episode index:1786
target Thresh 18.997988158027145
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0575912 , 8.921878  , 0.16336623], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42432895254326913
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.808968,  4.544825,  1.331263], dtype=float32)}
episode index:1787
target Thresh 18.997998192130844
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0394337, 7.831685 , 2.7570872], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4242963481745499
{'scaleFactor': 20, 'timeStep': 101, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4253616, 4.1426682, 5.106809 ], dtype=float32)}
episode index:1788
target Thresh 18.998008176189245
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6269941, 8.984732 , 0.6476762], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.4242075107802364
{'scaleFactor': 20, 'timeStep': 133, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0703797, 1.6667453, 5.6424036], dtype=float32)}
episode index:1789
target Thresh 18.998018110451945
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.9086075 ,  0.08165228,  4.5466776 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4239705233440463
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.0238037, 12.365573 ,  3.8290966], dtype=float32)}
episode index:1790
target Thresh 18.998027995167305
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 0.3430618, 10.791984 ,  2.9916503], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.42407501620623994
{'scaleFactor': 20, 'timeStep': 50, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.909154, 4.546156, 5.209606], dtype=float32)}
episode index:1791
target Thresh 18.99803783058244
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.307268 , 12.841742 ,  1.9684764], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.42411729411566684
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4121165, 4.9559417, 1.7236339], dtype=float32)}
episode index:1792
target Thresh 18.998047616943236
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.047876 ,  2.1019924,  0.8409243], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.42426143092598184
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7170327, 3.0714805, 4.5806475], dtype=float32)}
episode index:1793
target Thresh 18.998057354494357
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.338545 ,  4.3286586,  5.194023 ], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.42423311590342233
{'scaleFactor': 20, 'timeStep': 99, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.852665 , 3.7519588, 5.453257 ], dtype=float32)}
episode index:1794
target Thresh 18.99806704347924
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9879532, 9.127646 , 5.3777795], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42441723076208543
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.152542 , 4.383751 , 3.6840959], dtype=float32)}
episode index:1795
target Thresh 18.998076684140102
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.333115 ,  6.734268 ,  3.0192542], dtype=float32)}
done in step count: 389
reward sum = 0.02004890686806079
running average episode reward sum: 0.4241920813612536
{'scaleFactor': 20, 'timeStep': 390, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.126009, 3.925539, 4.482275], dtype=float32)}
episode index:1796
target Thresh 18.998086276717977
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.9623384, 9.75075  , 4.3491607], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4244747042140893
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.8443933, 1.7928789, 4.3307447], dtype=float32)}
episode index:1797
target Thresh 18.998095821452665
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.298804  , 10.679631  ,  0.13589495], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.42451661874450375
{'scaleFactor': 20, 'timeStep': 70, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6829183, 3.4132135, 4.139626 ], dtype=float32)}
episode index:1798
target Thresh 18.99810531858279
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.031223 ,  4.260328 ,  4.7184577], dtype=float32)}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.42437543520484006
{'scaleFactor': 20, 'timeStep': 177, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5519397, 4.915091 , 4.0916896], dtype=float32)}
episode index:1799
target Thresh 18.99811476834578
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0584445 , 5.811766  , 0.85072446], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42466799887967077
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.512168 , 4.9148273, 5.820098 ], dtype=float32)}
episode index:1800
target Thresh 18.99812417097788
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.9927225, 10.158209 ,  3.7641428], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4247360103419201
{'scaleFactor': 20, 'timeStep': 61, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8396568, 3.38221  , 5.9172354], dtype=float32)}
episode index:1801
target Thresh 18.998133526714163
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.931824 ,  6.868211 ,  5.7370667], dtype=float32)}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.4245894022291393
{'scaleFactor': 20, 'timeStep': 183, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3657851, 4.9892592, 6.06386  ], dtype=float32)}
episode index:1802
target Thresh 18.99814283578851
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.772575 ,  8.661793 ,  6.1707916], dtype=float32)}
done in step count: 228
reward sum = 0.10111704470857531
running average episode reward sum: 0.4244099943769371
{'scaleFactor': 20, 'timeStep': 229, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5730999, 4.808327 , 4.3073993], dtype=float32)}
episode index:1803
target Thresh 18.998152098433657
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.220394  , 10.274763  ,  0.56680214], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42417473384790333
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.593224 , 12.885124 ,  2.3629773], dtype=float32)}
episode index:1804
target Thresh 18.99816131488117
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.838168  ,  8.044115  ,  0.46489257], dtype=float32)}
done in step count: 425
reward sum = 0.013962323750362412
running average episode reward sum: 0.42394746935477445
{'scaleFactor': 20, 'timeStep': 426, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8290884, 3.9029818, 5.4266753], dtype=float32)}
episode index:1805
target Thresh 18.998170485361456
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.01441   ,  5.1141825 ,  0.94281614], dtype=float32)}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.4237585196665251
{'scaleFactor': 20, 'timeStep': 249, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.4854963, 3.2484796, 4.5817556], dtype=float32)}
episode index:1806
target Thresh 18.998179610103787
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8099968, 7.1373515, 3.2576237], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.42373488303035356
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7928169, 1.9883435, 2.9879153], dtype=float32)}
episode index:1807
target Thresh 18.99818868933627
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.127244 ,  5.1583233,  4.178304 ], dtype=float32)}
done in step count: 245
reward sum = 0.08523592457219176
running average episode reward sum: 0.42354766015510015
{'scaleFactor': 20, 'timeStep': 246, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2307074, 4.9216886, 4.9976263], dtype=float32)}
episode index:1808
target Thresh 18.998197723285895
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.807916 , 10.49378  ,  2.3723829], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4236796305148729
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.629335 , 4.496934 , 5.5432734], dtype=float32)}
episode index:1809
target Thresh 18.998206712178508
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.009037, 10.889697,  2.647275], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.42379703706489996
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.94075  , 3.8058476, 4.4068885], dtype=float32)}
episode index:1810
target Thresh 18.99821565623883
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.905454 ,  9.2151165,  4.6665173], dtype=float32)}
done in step count: 444
reward sum = 0.01153523379838384
running average episode reward sum: 0.42356939388253306
{'scaleFactor': 20, 'timeStep': 445, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2394204, 2.9224334, 4.890811 ], dtype=float32)}
episode index:1811
target Thresh 18.998224555690467
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.0073743, 10.706152 ,  1.9588217], dtype=float32)}
done in step count: 265
reward sum = 0.06971505404010991
running average episode reward sum: 0.4233741100305229
{'scaleFactor': 20, 'timeStep': 266, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2449346, 4.4260416, 4.9465175], dtype=float32)}
episode index:1812
target Thresh 18.9982334107559
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.992451 , 9.850948 , 2.0613396], dtype=float32)}
done in step count: 295
reward sum = 0.05156825150425344
running average episode reward sum: 0.4231690323369066
{'scaleFactor': 20, 'timeStep': 296, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0111046, 1.160943 , 2.8619301], dtype=float32)}
episode index:1813
target Thresh 18.998242221656515
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.203472, 10.383859,  3.372986], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.4232435070371332
{'scaleFactor': 20, 'timeStep': 59, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.774095 , 1.2051096, 4.589546 ], dtype=float32)}
episode index:1814
target Thresh 18.998250988612575
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7827823, 9.100475 , 3.0370681], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42351871430291327
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.958656 , 3.3023136, 5.0432487], dtype=float32)}
episode index:1815
target Thresh 18.998259711843264
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2208922, 4.872611 , 2.7760935], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42383615994481694
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2208922, 4.872611 , 2.7760935], dtype=float32)}
episode index:1816
target Thresh 18.998268391566658
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.0900955, 12.621483 ,  0.9940466], dtype=float32)}
done in step count: 218
reward sum = 0.11180788242357734
running average episode reward sum: 0.4236644327695163
{'scaleFactor': 20, 'timeStep': 219, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1453104, 4.9749866, 4.1344304], dtype=float32)}
episode index:1817
target Thresh 18.998277027999748
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.895138  , 8.954034  , 0.29470825], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.42379568557931524
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.519368 , 4.913578 , 1.4972706], dtype=float32)}
episode index:1818
target Thresh 18.998285621358452
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.100505  , 11.561579  ,  0.54521406], dtype=float32)}
done in step count: 288
reward sum = 0.05532686267122055
running average episode reward sum: 0.42359311888172974
{'scaleFactor': 20, 'timeStep': 289, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8544111, 1.435693 , 0.6959543], dtype=float32)}
episode index:1819
target Thresh 18.998294171857594
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.141774 ,  9.997867 ,  3.9220412], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4234845227480221
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.433953, 4.746167, 4.789368], dtype=float32)}
episode index:1820
target Thresh 18.99830267971095
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([5.5661035, 8.082555 , 4.7964272], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4237536236402438
{'scaleFactor': 20, 'timeStep': 10, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.47176  , 2.9808362, 3.8206148], dtype=float32)}
episode index:1821
target Thresh 18.998311145131208
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.564463 ,  9.526828 ,  0.7276572], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.42388821098103935
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.099737 , 4.1421185, 4.7674093], dtype=float32)}
episode index:1822
target Thresh 18.998319568330007
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2696426, 6.9392977, 1.5509431], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.42401534812288966
{'scaleFactor': 20, 'timeStep': 43, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0517542, 3.661405 , 4.071201 ], dtype=float32)}
episode index:1823
target Thresh 18.99832794951793
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1582756, 4.460193 , 4.999146 ], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4243311291820328
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1582756, 4.460193 , 4.999146 ], dtype=float32)}
episode index:1824
target Thresh 18.998336288904497
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.204661 , 5.193509 , 0.4688108], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.4243924645771151
{'scaleFactor': 20, 'timeStep': 63, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2869067, 3.5035248, 4.3290715], dtype=float32)}
episode index:1825
target Thresh 18.998344586698202
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.110558 ,  2.0536206,  6.0367436], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.4243098231129558
{'scaleFactor': 20, 'timeStep': 130, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.778776, 4.219451, 4.567587], dtype=float32)}
episode index:1826
target Thresh 18.998352843106492
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.2978   ,  6.8100553,  3.1757255], dtype=float32)}
done in step count: 314
reward sum = 0.04260407137887648
running average episode reward sum: 0.42410089823515934
{'scaleFactor': 20, 'timeStep': 315, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4197087, 2.932495 , 5.308721 ], dtype=float32)}
episode index:1827
target Thresh 18.99836105833577
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.003955 ,  9.187506 ,  2.3616264], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.42430303903680516
{'scaleFactor': 20, 'timeStep': 24, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.371673 , 3.7715495, 4.092298 ], dtype=float32)}
episode index:1828
target Thresh 18.998369232591422
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1420636, 5.212636 , 6.0040245], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4245460367261065
{'scaleFactor': 20, 'timeStep': 15, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.811988 , 3.814651 , 4.1761036], dtype=float32)}
episode index:1829
target Thresh 18.998377366077804
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.599804 ,  4.7211447,  1.6078113], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42431404435631087
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([0.24061996, 2.5166473 , 3.4025455 ], dtype=float32)}
episode index:1830
target Thresh 18.998385458998253
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.160347 ,  3.1862192,  5.2652044], dtype=float32)}
done in step count: 385
reward sum = 0.02087132015888843
running average episode reward sum: 0.4240937042557115
{'scaleFactor': 20, 'timeStep': 386, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2989695, 4.718175 , 5.108306 ], dtype=float32)}
episode index:1831
target Thresh 18.998393511555093
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9303973, 5.8959846, 2.0104768], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.4240375401565352
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0555704, 1.2154291, 4.0902433], dtype=float32)}
episode index:1832
target Thresh 18.99840152394964
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.184868 , 10.14131  ,  5.3694034], dtype=float32)}
done in step count: 320
reward sum = 0.040110887486875496
running average episode reward sum: 0.42382808753642087
{'scaleFactor': 20, 'timeStep': 321, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1960859, 4.757025 , 5.5719576], dtype=float32)}
episode index:1833
target Thresh 18.9984094963822
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.920846, 10.948487,  5.895715], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42407546645433936
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8956456, 4.077117 , 4.2221904], dtype=float32)}
episode index:1834
target Thresh 18.99841742905209
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.030198  ,  8.104969  ,  0.09264909], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.424184158513433
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8633385, 3.627226 , 4.58598  ], dtype=float32)}
episode index:1835
target Thresh 18.99842532215762
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.596525 , 6.3510666, 4.5630846], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4244923370763342
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6111903, 4.4417644, 4.564231 ], dtype=float32)}
episode index:1836
target Thresh 18.998433175896125
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.844788 ,  7.950651 ,  3.6181254], dtype=float32)}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.4243452111180966
{'scaleFactor': 20, 'timeStep': 187, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1881261, 1.1598777, 3.2411451], dtype=float32)}
episode index:1837
target Thresh 18.998440990463944
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.0570135 ,  8.717929  ,  0.76120484], dtype=float32)}
done in step count: 380
reward sum = 0.021946938520632394
running average episode reward sum: 0.42412627843442
{'scaleFactor': 20, 'timeStep': 381, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2053466, 3.8389735, 5.0039005], dtype=float32)}
episode index:1838
target Thresh 18.998448766056445
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2437077, 7.2577715, 1.2252207], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.42426680497959457
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2968419, 2.7732983, 5.9444957], dtype=float32)}
episode index:1839
target Thresh 18.998456502868017
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.67653  ,  6.7790613,  4.8087783], dtype=float32)}
done in step count: 259
reward sum = 0.07404835256958406
running average episode reward sum: 0.42407646886415434
{'scaleFactor': 20, 'timeStep': 260, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8881698, 4.9693375, 4.3408136], dtype=float32)}
episode index:1840
target Thresh 18.998464201092077
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.152929, 10.017265,  6.144355], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.42410692241532616
{'scaleFactor': 20, 'timeStep': 74, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.968441 , 1.5094862, 1.5357018], dtype=float32)}
episode index:1841
target Thresh 18.998471860921086
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7104163, 7.0524096, 5.757178 ], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.42438779821716416
{'scaleFactor': 20, 'timeStep': 7, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8454223, 3.412682 , 4.756593 ], dtype=float32)}
episode index:1842
target Thresh 18.99847948254654
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.152957 ,  8.354199 ,  3.1611516], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.42438614136634356
{'scaleFactor': 20, 'timeStep': 87, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0727746, 3.717082 , 4.3694654], dtype=float32)}
episode index:1843
target Thresh 18.998487066158972
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.02157  , 10.881688 ,  1.6829091], dtype=float32)}
done in step count: 280
reward sum = 0.05995901467146544
running average episode reward sum: 0.4241885127726912
{'scaleFactor': 20, 'timeStep': 281, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5056335, 4.5947237, 4.559509 ], dtype=float32)}
episode index:1844
target Thresh 18.998494611947986
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.9968   , 10.246658 ,  1.6290449], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.4240150839055135
{'scaleFactor': 20, 'timeStep': 226, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6766521, 4.132289 , 4.773393 ], dtype=float32)}
episode index:1845
target Thresh 18.99850212010222
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.014626,  4.921201,  4.634159], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4239918076510168
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5043247, 4.6498427, 4.223952 ], dtype=float32)}
episode index:1846
target Thresh 18.998509590809377
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.3256683, 11.082745 ,  0.5157659], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4241469573097181
{'scaleFactor': 20, 'timeStep': 35, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7701054, 3.1012363, 4.45599  ], dtype=float32)}
episode index:1847
target Thresh 18.998517024256227
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.994301 ,  4.170386 ,  4.6078773], dtype=float32)}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.4240258155724604
{'scaleFactor': 20, 'timeStep': 161, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2827601, 2.658319 , 3.7143555], dtype=float32)}
episode index:1848
target Thresh 18.998524420628605
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.896329 ,  5.3031087,  1.3995757], dtype=float32)}
done in step count: 211
reward sum = 0.11995712819347787
running average episode reward sum: 0.42386136522774487
{'scaleFactor': 20, 'timeStep': 212, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4533246, 2.9309514, 4.621578 ], dtype=float32)}
episode index:1849
target Thresh 18.998531780111424
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.743462  , 11.107874  ,  0.27487695], dtype=float32)}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.42381666506357535
{'scaleFactor': 20, 'timeStep': 108, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0241365, 2.6396437, 1.7631327], dtype=float32)}
episode index:1850
target Thresh 18.998539102888667
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.992361 ,  6.5645695,  2.4098969], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.42377017005322165
{'scaleFactor': 20, 'timeStep': 109, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2402434, 4.814158 , 4.409993 ], dtype=float32)}
episode index:1851
target Thresh 18.998546389143407
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8215835, 5.9572916, 4.6569915], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.424075909702221
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.177859 , 4.0421987, 4.7595778], dtype=float32)}
episode index:1852
target Thresh 18.9985536390578
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.9933014, 10.091325 ,  4.1607685], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.42436026703638063
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.734189 , 4.5026865, 4.4382615], dtype=float32)}
episode index:1853
target Thresh 18.998560852813092
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2049112, 8.931822 , 3.9305606], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4245860399200632
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8343875, 3.7940214, 5.4300632], dtype=float32)}
episode index:1854
target Thresh 18.998568030589635
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0245955, 9.139383 , 5.270671 ], dtype=float32)}
done in step count: 337
reward sum = 0.03381119958765022
running average episode reward sum: 0.4243753796287789
{'scaleFactor': 20, 'timeStep': 338, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3558395, 4.809409 , 4.4909105], dtype=float32)}
episode index:1855
target Thresh 18.998575172566863
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.252448 ,  1.9640063,  2.0975149], dtype=float32)}
done in step count: 301
reward sum = 0.04855048513057287
running average episode reward sum: 0.4241728877675191
{'scaleFactor': 20, 'timeStep': 302, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4568462, 1.3010247, 3.2737026], dtype=float32)}
episode index:1856
target Thresh 18.998582278923333
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.022779, 11.249787,  6.085622], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.424011044012198
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.484541 , 2.0814893, 4.504864 ], dtype=float32)}
episode index:1857
target Thresh 18.998589349836703
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.159045 , 10.8328905,  1.5240381], dtype=float32)}
done in step count: 323
reward sum = 0.038919554017627804
running average episode reward sum: 0.4238037827151073
{'scaleFactor': 20, 'timeStep': 324, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0681084, 2.5774179, 4.596116 ], dtype=float32)}
episode index:1858
target Thresh 18.998596385483744
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.118049 ,  6.987437 ,  2.0373375], dtype=float32)}
done in step count: 235
reward sum = 0.0942476934556249
running average episode reward sum: 0.42362650671227814
{'scaleFactor': 20, 'timeStep': 236, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0055295, 4.603671 , 3.1786587], dtype=float32)}
episode index:1859
target Thresh 18.998603386040347
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9891593, 6.190453 , 5.7741375], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.42344597902543823
{'scaleFactor': 20, 'timeStep': 243, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1552649, 4.81401  , 3.957203 ], dtype=float32)}
episode index:1860
target Thresh 18.99861035168153
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.150807 , 5.137143 , 5.0610948], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4237504142865745
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8992853, 3.069989 , 4.766082 ], dtype=float32)}
episode index:1861
target Thresh 18.998617282581435
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.992427, 9.896444, 3.311512], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.42400854084979805
{'scaleFactor': 20, 'timeStep': 11, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3911538, 3.7715347, 3.7179787], dtype=float32)}
episode index:1862
target Thresh 18.99862417891333
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([6.2191834, 9.448465 , 5.094    ], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42426153586504706
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.602668 , 4.886942 , 4.6698785], dtype=float32)}
episode index:1863
target Thresh 18.998631040849627
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.744382  , 11.054689  ,  0.52642924], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.4241002523340767
{'scaleFactor': 20, 'timeStep': 209, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2516043, 4.3955884, 5.566674 ], dtype=float32)}
episode index:1864
target Thresh 18.998637868561875
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.804025 ,  5.128288 ,  4.4794974], dtype=float32)}
done in step count: 244
reward sum = 0.0860968935072644
running average episode reward sum: 0.4239190172891294
{'scaleFactor': 20, 'timeStep': 245, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6630657, 4.3049855, 5.4751897], dtype=float32)}
episode index:1865
target Thresh 18.998644662220766
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.878352 , 10.714197 ,  3.1981626], dtype=float32)}
done in step count: 327
reward sum = 0.03738596830031274
running average episode reward sum: 0.42371187203243654
{'scaleFactor': 20, 'timeStep': 328, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0832163, 3.8363502, 3.3009481], dtype=float32)}
episode index:1866
target Thresh 18.99865142199614
target distance 4.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([-0.31256127,  6.025423  ,  3.6903918 ], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4238432377991946
{'scaleFactor': 20, 'timeStep': 41, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1386743, 2.7600603, 4.5991316], dtype=float32)}
episode index:1867
target Thresh 18.998658148056993
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.944797, 10.17543 ,  5.384676], dtype=float32)}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.4237235556734228
{'scaleFactor': 20, 'timeStep': 161, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.4049416, 3.8661547, 4.0619698], dtype=float32)}
episode index:1868
target Thresh 18.99866484057148
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2105265 , 5.840335  , 0.28138107], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42349684430067086
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.893373 ,  2.7122364,  2.3777046], dtype=float32)}
episode index:1869
target Thresh 18.99867149970691
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9426985,  7.9656987,  0.8734386], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4232703753999753
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.632342  , -0.36830735,  3.0915387 ], dtype=float32)}
episode index:1870
target Thresh 18.998678125629766
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.066923,  9.043459,  1.449065], dtype=float32)}
done in step count: 325
reward sum = 0.03814505489267701
running average episode reward sum: 0.4230645361052092
{'scaleFactor': 20, 'timeStep': 326, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8386137, 4.096488 , 4.782045 ], dtype=float32)}
episode index:1871
target Thresh 18.99868471850569
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.177921 ,  3.3950582,  4.2713594], dtype=float32)}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.42290009476044876
{'scaleFactor': 20, 'timeStep': 216, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7097092, 2.7851126, 5.0143805], dtype=float32)}
episode index:1872
target Thresh 18.99869127849951
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.94318 ,  9.344823,  0.384688], dtype=float32)}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4228011585404957
{'scaleFactor': 20, 'timeStep': 144, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5978928, 1.4867929, 5.6427493], dtype=float32)}
episode index:1873
target Thresh 18.998697805775226
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.354717 ,  9.981717 ,  1.1576505], dtype=float32)}
done in step count: 426
reward sum = 0.013822700512858787
running average episode reward sum: 0.42258292030248734
{'scaleFactor': 20, 'timeStep': 427, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9226732, 4.4928026, 5.49222  ], dtype=float32)}
episode index:1874
target Thresh 18.998704300496016
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.4083014, 9.399599 , 3.9215221], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4228647374382727
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0485063, 3.760078 , 3.9596655], dtype=float32)}
episode index:1875
target Thresh 18.998710762824253
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.08282  , 10.698301 ,  2.7626226], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4229491199951383
{'scaleFactor': 20, 'timeStep': 55, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.924917 , 3.5722747, 0.5508482], dtype=float32)}
episode index:1876
target Thresh 18.998717192921493
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.343244,  9.973253,  6.131246], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4229598279661724
{'scaleFactor': 20, 'timeStep': 82, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9725726, 4.1420746, 5.2086306], dtype=float32)}
episode index:1877
target Thresh 18.998723590948487
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.145479 ,  9.776028 ,  4.6827383], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4231614630894806
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7341759, 4.844942 , 4.102572 ], dtype=float32)}
episode index:1878
target Thresh 18.99872995706519
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.063634 ,  9.440549 ,  3.6614578], dtype=float32)}
done in step count: 303
reward sum = 0.047584330476474465
running average episode reward sum: 0.4229615816990533
{'scaleFactor': 20, 'timeStep': 304, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2613038, 1.0456551, 2.8933916], dtype=float32)}
episode index:1879
target Thresh 18.998736291430756
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.091162 , 11.005074 ,  5.7468543], dtype=float32)}
done in step count: 241
reward sum = 0.08873233251530138
running average episode reward sum: 0.42278380018353
{'scaleFactor': 20, 'timeStep': 242, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8719752, 2.5879197, 3.9893541], dtype=float32)}
episode index:1880
target Thresh 18.998742594203538
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.625739 , 11.322886 ,  6.0742593], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42255903473951967
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.5988693, 13.357924 ,  1.175925 ], dtype=float32)}
episode index:1881
target Thresh 18.99874886554111
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.20149  ,  5.125926 ,  4.3673015], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.4223578387531995
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7858043, 4.158585 , 3.9013958], dtype=float32)}
episode index:1882
target Thresh 18.99875510560026
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.8601876, 10.8254175,  3.1337752], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.42259028724809494
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.616188 , 1.8189473, 5.8179665], dtype=float32)}
episode index:1883
target Thresh 18.99876131453698
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.079249,  9.745213,  5.042439], dtype=float32)}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.42245292933008005
{'scaleFactor': 20, 'timeStep': 181, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6703446, 4.92264  , 1.1213863], dtype=float32)}
episode index:1884
target Thresh 18.998767492506502
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0416994, 8.234517 , 1.0268066], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.42248870510921754
{'scaleFactor': 20, 'timeStep': 72, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.1750267 , 4.954299  , 0.98887706], dtype=float32)}
episode index:1885
target Thresh 18.99877363966327
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9037857, 9.966628 , 2.4735491], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.4223472899682724
{'scaleFactor': 20, 'timeStep': 186, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.053131 , 1.2907002, 6.1716   ], dtype=float32)}
episode index:1886
target Thresh 18.998779756160967
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.384744 , 10.742658 ,  0.9094055], dtype=float32)}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.4222444193642274
{'scaleFactor': 20, 'timeStep': 148, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.317333 , 2.4934616, 5.351166 ], dtype=float32)}
episode index:1887
target Thresh 18.998785842152504
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([8.032051  , 9.967747  , 0.34143305], dtype=float32)}
done in step count: 206
reward sum = 0.12613920430197118
running average episode reward sum: 0.42208758397489354
{'scaleFactor': 20, 'timeStep': 207, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.955244 , 3.7304738, 4.6287546], dtype=float32)}
episode index:1888
target Thresh 18.99879189779003
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.961344 ,  9.882377 ,  5.0515046], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.42217490488434967
{'scaleFactor': 20, 'timeStep': 54, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1508961, 2.573165 , 4.868023 ], dtype=float32)}
episode index:1889
target Thresh 18.99879792322494
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.194521 ,  6.6955175,  4.6930833], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4219515319188024
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 1.9142938, 11.219608 ,  1.5159286], dtype=float32)}
episode index:1890
target Thresh 18.99880391860787
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0834055, 8.954583 , 1.8304373], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.42210039292519097
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0155433, 4.435848 , 4.8389993], dtype=float32)}
episode index:1891
target Thresh 18.998809884088697
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.885587 ,  8.762473 ,  4.1411467], dtype=float32)}
done in step count: 370
reward sum = 0.024267330287830763
running average episode reward sum: 0.42189012175043544
{'scaleFactor': 20, 'timeStep': 371, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1205268, 2.098648 , 2.032061 ], dtype=float32)}
episode index:1892
target Thresh 18.99881581981657
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.087268 ,  2.2418978,  4.642388 ], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.4218005657764993
{'scaleFactor': 20, 'timeStep': 138, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.533083 , 4.122237 , 3.4162827], dtype=float32)}
episode index:1893
target Thresh 18.998821725939873
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8556159, 9.982704 , 5.386031 ], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4220699769603064
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5357475, 3.5820408, 4.256742 ], dtype=float32)}
episode index:1894
target Thresh 18.998827602606266
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.826136,  2.065003,  2.164396], dtype=float32)}
done in step count: 226
reward sum = 0.10317013030157669
running average episode reward sum: 0.4219016920808031
{'scaleFactor': 20, 'timeStep': 227, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8949451, 2.1884732, 2.4313116], dtype=float32)}
episode index:1895
target Thresh 18.99883344996266
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.155763,  6.015561,  6.194621], dtype=float32)}
done in step count: 415
reward sum = 0.015438523314636111
running average episode reward sum: 0.4216873127723821
{'scaleFactor': 20, 'timeStep': 416, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0740645, 3.808463 , 4.1013284], dtype=float32)}
episode index:1896
target Thresh 18.998839268155248
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.054431 ,  9.863963 ,  3.1619134], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.42171308996205703
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4993929, 4.7637143, 5.4414754], dtype=float32)}
episode index:1897
target Thresh 18.998845057329472
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7166793, 6.2638   , 1.7167416], dtype=float32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.42189661475438744
{'scaleFactor': 20, 'timeStep': 27, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.89429  , 2.1097455, 1.6992478], dtype=float32)}
episode index:1898
target Thresh 18.99885081763007
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.119771 ,  4.8981123,  2.6829677], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.4219124899705602
{'scaleFactor': 20, 'timeStep': 80, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.45158  , 3.5493114, 5.0762205], dtype=float32)}
episode index:1899
target Thresh 18.998856549201054
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.022396 ,  3.820617 ,  2.7080328], dtype=float32)}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.4218387784998075
{'scaleFactor': 20, 'timeStep': 127, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.3993826, 2.8405755, 4.3450637], dtype=float32)}
episode index:1900
target Thresh 18.99886225218571
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.026836 , 4.8319736, 0.4588285], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4221429138083294
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.026836 , 4.8319736, 0.4588285], dtype=float32)}
episode index:1901
target Thresh 18.998867926726604
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4714292, 9.630147 , 4.0855556], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4223217780567725
{'scaleFactor': 20, 'timeStep': 28, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8640811, 1.7943007, 4.908582 ], dtype=float32)}
episode index:1902
target Thresh 18.99887357296561
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.995176 ,  5.8470697,  1.2365896], dtype=float32)}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.4221738866405074
{'scaleFactor': 20, 'timeStep': 196, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.081363 , 2.1743379, 5.6948857], dtype=float32)}
episode index:1903
target Thresh 18.998879191043883
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9595609, 7.371943 , 5.0582995], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4224669150613895
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5371304, 3.901721 , 5.707512 ], dtype=float32)}
episode index:1904
target Thresh 18.99888478110187
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.065777 ,  6.080717 ,  5.4984207], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42224514765190846
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.561157 , 10.829004 ,  5.1496654], dtype=float32)}
episode index:1905
target Thresh 18.99889034327933
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3229322, 7.9366007, 3.441558 ], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4224933601947242
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.066674 , 2.6941102, 5.216826 ], dtype=float32)}
episode index:1906
target Thresh 18.998895877715313
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.057386 ,  5.1982327,  3.7074993], dtype=float32)}
done in step count: 421
reward sum = 0.014535063236794427
running average episode reward sum: 0.42227943345274316
{'scaleFactor': 20, 'timeStep': 422, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.904736 , 2.1712108, 5.9291434], dtype=float32)}
episode index:1907
target Thresh 18.998901384548184
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.158367 ,  4.044422 ,  2.6749136], dtype=float32)}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.42217535024547165
{'scaleFactor': 20, 'timeStep': 150, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7687863, 4.7848887, 5.3667555], dtype=float32)}
episode index:1908
target Thresh 18.998906863915607
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.144083 , 10.059894 ,  4.1509466], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.42244244820129223
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5615273, 3.0539136, 4.126988 ], dtype=float32)}
episode index:1909
target Thresh 18.998912315954573
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.052401, 11.156522,  2.807902], dtype=float32)}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.4222847133155099
{'scaleFactor': 20, 'timeStep': 211, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6392703, 4.169311 , 3.9055495], dtype=float32)}
episode index:1910
target Thresh 18.998917740801385
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.768474, 8.955303, 4.380585], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.42253225572312014
{'scaleFactor': 20, 'timeStep': 12, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0356089, 3.859908 , 4.7897544], dtype=float32)}
episode index:1911
target Thresh 18.998923138591657
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.11591  ,  3.9205778,  5.375209 ], dtype=float32)}
done in step count: 365
reward sum = 0.025517964452291125
running average episode reward sum: 0.4223246122653425
{'scaleFactor': 20, 'timeStep': 366, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3892847, 3.2832673, 4.9909096], dtype=float32)}
episode index:1912
target Thresh 18.998928509460335
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.881779 ,  4.063424 ,  6.1380963], dtype=float32)}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.42219034043230513
{'scaleFactor': 20, 'timeStep': 180, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0708972, 3.31451  , 5.584123 ], dtype=float32)}
episode index:1913
target Thresh 18.998933853541697
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.928433 ,  8.347103 ,  6.0904317], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4221462255213681
{'scaleFactor': 20, 'timeStep': 109, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9886706, 3.4357748, 4.4985766], dtype=float32)}
episode index:1914
target Thresh 18.998939170969344
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.118728 ,  7.776388 ,  2.2652087], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.42223542668109665
{'scaleFactor': 20, 'timeStep': 53, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.395598 , 1.2579283, 3.8357244], dtype=float32)}
episode index:1915
target Thresh 18.998944461876206
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.0143833, 11.071366 ,  0.8078236], dtype=float32)}
done in step count: 240
reward sum = 0.08962861870232462
running average episode reward sum: 0.4220618323136756
{'scaleFactor': 20, 'timeStep': 241, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.310358, 4.687096, 4.235216], dtype=float32)}
episode index:1916
target Thresh 18.99894972639456
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.2332516, 5.8887744, 0.2005668], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4223377468768401
{'scaleFactor': 20, 'timeStep': 6, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.4225855, 4.3261085, 4.8878937], dtype=float32)}
episode index:1917
target Thresh 18.998954964656022
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.022345 ,  2.8742223,  5.2841196], dtype=float32)}
done in step count: 428
reward sum = 0.013547628772652897
running average episode reward sum: 0.42212461334289636
{'scaleFactor': 20, 'timeStep': 429, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.215271 , 2.1073976, 3.5226493], dtype=float32)}
episode index:1918
target Thresh 18.99896017679154
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.0290785, 10.131631 ,  1.8572402], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4221926481859498
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2739744, 4.316387 , 4.283135 ], dtype=float32)}
episode index:1919
target Thresh 18.998965362931433
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.7371082, 11.142856 ,  3.0697906], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4223282533665875
{'scaleFactor': 20, 'timeStep': 39, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4525317, 4.898659 , 4.963312 ], dtype=float32)}
episode index:1920
target Thresh 18.99897052320534
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.798755 ,  6.7535152,  2.460824 ], dtype=float32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.422353374859674
{'scaleFactor': 20, 'timeStep': 76, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5458114 , 4.795726  , 0.90495706], dtype=float32)}
episode index:1921
target Thresh 18.998975657742275
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.056456 ,  8.967927 ,  6.2597737], dtype=float32)}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.42225237438375085
{'scaleFactor': 20, 'timeStep': 148, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.70709  , 3.1165576, 5.318863 ], dtype=float32)}
episode index:1922
target Thresh 18.998980766670595
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.166459, 10.29354 ,  1.81562 ], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4224800426002134
{'scaleFactor': 20, 'timeStep': 16, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0290077, 3.458784 , 4.433657 ], dtype=float32)}
episode index:1923
target Thresh 18.998985850118036
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.313211 ,  7.0014215,  4.2512283], dtype=float32)}
done in step count: 311
reward sum = 0.043908188485071595
running average episode reward sum: 0.42228327968227414
{'scaleFactor': 20, 'timeStep': 312, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5499194, 4.082157 , 3.897449 ], dtype=float32)}
episode index:1924
target Thresh 18.998990908211674
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.2690535, 11.222409 ,  3.1949785], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.42228499752316057
{'scaleFactor': 20, 'timeStep': 86, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0867195, 4.922485 , 2.5762231], dtype=float32)}
episode index:1925
target Thresh 18.998995941077965
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.481878 ,  1.7784114,  4.272054 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42206574259194396
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.952905 ,  7.042743 ,  4.1638885], dtype=float32)}
episode index:1926
target Thresh 18.99900094884273
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6335385, 5.931941 , 2.7655034], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.42217685818274414
{'scaleFactor': 20, 'timeStep': 46, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0191075, 1.6678154, 3.9748259], dtype=float32)}
episode index:1927
target Thresh 18.999005931631164
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.015836 , 10.9398365,  1.9786069], dtype=float32)}
done in step count: 235
reward sum = 0.0942476934556249
running average episode reward sum: 0.4220067704417031
{'scaleFactor': 20, 'timeStep': 236, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.736722, 4.537391, 5.508533], dtype=float32)}
episode index:1928
target Thresh 18.99901088956784
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1565588, 5.819844 , 6.1259656], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42230122001638337
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.864933 , 4.8730097, 5.284052 ], dtype=float32)}
episode index:1929
target Thresh 18.9990158227767
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.865189 , 10.792134 ,  1.9157609], dtype=float32)}
done in step count: 205
reward sum = 0.1274133376787588
running average episode reward sum: 0.42214842836750377
{'scaleFactor': 20, 'timeStep': 206, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5486906, 2.9860249, 5.588077 ], dtype=float32)}
episode index:1930
target Thresh 18.99902073138108
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 9.620043 , 11.349653 ,  2.5911162], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.42192981188466194
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.280764  ,  0.9389717 ,  0.41024563], dtype=float32)}
episode index:1931
target Thresh 18.99902561550369
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.056541,  4.745424,  2.325189], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.4218014785599164
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2730745, 4.5619226, 4.8135424], dtype=float32)}
episode index:1932
target Thresh 18.99903047526664
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2219043, 5.922398 , 2.234463 ], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4217001576478492
{'scaleFactor': 20, 'timeStep': 149, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9078975, 3.201538 , 0.6408716], dtype=float32)}
episode index:1933
target Thresh 18.99903531079142
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.107977 ,  9.0146885,  4.189266 ], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4218684471704364
{'scaleFactor': 20, 'timeStep': 30, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3310277, 4.3108053, 4.002747 ], dtype=float32)}
episode index:1934
target Thresh 18.99904012219892
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.957841 ,  7.4024754,  3.774471 ], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.42188878490699855
{'scaleFactor': 20, 'timeStep': 78, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.3559065, 3.2528663, 4.5894947], dtype=float32)}
episode index:1935
target Thresh 18.999044909609427
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.96161 , 10.083407,  5.725447], dtype=float32)}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.4217644244566984
{'scaleFactor': 20, 'timeStep': 171, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0758783, 1.0466733, 5.2275963], dtype=float32)}
episode index:1936
target Thresh 18.999049673142622
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7246614, 5.85384  , 4.526082 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4220526720434528
{'scaleFactor': 20, 'timeStep': 3, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1956804, 3.9881833, 4.2933044], dtype=float32)}
episode index:1937
target Thresh 18.999054412917598
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.079025 ,  8.944454 ,  4.7617636], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4219292994151069
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3075774, 1.1721983, 3.267437 ], dtype=float32)}
episode index:1938
target Thresh 18.999059129052846
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.96756  ,  3.1385233,  3.7374012], dtype=float32)}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.42178658028504024
{'scaleFactor': 20, 'timeStep': 193, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.5629187, 4.7589836, 3.892721 ], dtype=float32)}
episode index:1939
target Thresh 18.999063821666272
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.5108824, 9.128481 , 5.0440264], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.42175784098658053
{'scaleFactor': 20, 'timeStep': 101, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2666986, 1.0961252, 5.0718884], dtype=float32)}
episode index:1940
target Thresh 18.999068490875192
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.074175, 10.901566,  4.492944], dtype=float32)}
done in step count: 312
reward sum = 0.04346910660022087
running average episode reward sum: 0.4215629472542846
{'scaleFactor': 20, 'timeStep': 313, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.734793 , 2.9467762, 4.7024007], dtype=float32)}
episode index:1941
target Thresh 18.999073136796333
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.977641 ,  9.257505 ,  0.1616377], dtype=float32)}
done in step count: 276
reward sum = 0.06241855477982408
running average episode reward sum: 0.42137801193375196
{'scaleFactor': 20, 'timeStep': 277, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.748635 , 3.6446922, 4.8332295], dtype=float32)}
episode index:1942
target Thresh 18.999077759545848
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.06766  , 10.749033 ,  1.1329988], dtype=float32)}
done in step count: 177
reward sum = 0.1688221565806905
running average episode reward sum: 0.4212480295069104
{'scaleFactor': 20, 'timeStep': 178, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1816707, 2.0651171, 2.0558171], dtype=float32)}
episode index:1943
target Thresh 18.999082359239303
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.98079   ,  2.8563097 ,  0.09294575], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.42119656502391556
{'scaleFactor': 20, 'timeStep': 114, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7354155, 4.580945 , 4.5648394], dtype=float32)}
episode index:1944
target Thresh 18.999086935991688
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 7.888761 , 10.01451  ,  5.0051217], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.4212101005759669
{'scaleFactor': 20, 'timeStep': 81, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.1147513, 4.729275 , 4.1095   ], dtype=float32)}
episode index:1945
target Thresh 18.99909148991743
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.918161,  2.806336,  4.123719], dtype=float32)}
done in step count: 345
reward sum = 0.031199105031747717
running average episode reward sum: 0.42100968382594417
{'scaleFactor': 20, 'timeStep': 346, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2962012, 3.5015852, 4.809206 ], dtype=float32)}
episode index:1946
target Thresh 18.99909602113037
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 4.77568  , 12.967685 ,  1.8230964], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4210716639965353
{'scaleFactor': 20, 'timeStep': 62, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2167883, 3.2869582, 3.3744867], dtype=float32)}
episode index:1947
target Thresh 18.999100529743792
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 2.024784, 11.243828,  5.569042], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4212166208913007
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6937056, 4.498388 , 3.284575 ], dtype=float32)}
episode index:1948
target Thresh 18.99910501587041
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.071614 , 8.85677  , 4.8985543], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.4211827287383062
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.323923, 2.485733, 4.645253], dtype=float32)}
episode index:1949
target Thresh 18.99910947962238
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.938434 ,  6.9569817,  5.610575 ], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.42128649420812825
{'scaleFactor': 20, 'timeStep': 48, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2972229, 3.8274589, 4.830803 ], dtype=float32)}
episode index:1950
target Thresh 18.99911392111129
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.0379305,  7.2580833,  0.5757317], dtype=float32)}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.42111007152648894
{'scaleFactor': 20, 'timeStep': 256, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9334793, 3.961553 , 4.629257 ], dtype=float32)}
episode index:1951
target Thresh 18.999118340448184
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.157131 ,  7.8590236,  1.2454417], dtype=float32)}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4210160564052091
{'scaleFactor': 20, 'timeStep': 144, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.5041986, 4.440654 , 5.702675 ], dtype=float32)}
episode index:1952
target Thresh 18.999122737743544
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.9951725,  9.081893 ,  3.9743538], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.4208941621204697
{'scaleFactor': 20, 'timeStep': 170, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.4133635, 1.8111027, 6.14848  ], dtype=float32)}
episode index:1953
target Thresh 18.999127113107303
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.7618546, 6.67078  , 3.9649034], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42106500404732905
{'scaleFactor': 20, 'timeStep': 29, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.574427 , 3.7151396, 5.2744827], dtype=float32)}
episode index:1954
target Thresh 18.999131466648844
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0194485, 3.2229733, 4.5989857], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4213611344800414
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0194485, 3.2229733, 4.5989857], dtype=float32)}
episode index:1955
target Thresh 18.999135798477003
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.2074676, 8.16439  , 5.342106 ], dtype=float32)}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.42124914004888026
{'scaleFactor': 20, 'timeStep': 160, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.326582, 4.824444, 5.085153], dtype=float32)}
episode index:1956
target Thresh 18.99914010870008
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.2075822 , 7.760149  , 0.48443064], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4213933396170717
{'scaleFactor': 20, 'timeStep': 36, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5468174, 3.89247  , 4.4957323], dtype=float32)}
episode index:1957
target Thresh 18.99914439742583
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.019856 , 11.028083 ,  4.6118965], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4213454996874602
{'scaleFactor': 20, 'timeStep': 112, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7564824, 4.8074694, 3.2942274], dtype=float32)}
episode index:1958
target Thresh 18.99914866476147
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 8.019   , 10.167612,  2.552413], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4214584481828136
{'scaleFactor': 20, 'timeStep': 45, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8338227, 4.7762356, 4.373477 ], dtype=float32)}
episode index:1959
target Thresh 18.999152910813688
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.054941,  9.93457 ,  0.261295], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.4214210168901144
{'scaleFactor': 20, 'timeStep': 106, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2582948, 3.7000973, 4.4085183], dtype=float32)}
episode index:1960
target Thresh 18.99915713568863
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.333342 ,  3.7286263,  1.4986894], dtype=float32)}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.42128716767660584
{'scaleFactor': 20, 'timeStep': 184, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2165097, 1.1142641, 4.809818 ], dtype=float32)}
episode index:1961
target Thresh 18.999161339491923
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.310993 ,  2.7541342,  5.6670265], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.42126665796734386
{'scaleFactor': 20, 'timeStep': 97, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3197422, 4.826236 , 5.5217476], dtype=float32)}
episode index:1962
target Thresh 18.999165522328656
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.8611176, 9.97955  , 3.5270493], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.42152212309034975
{'scaleFactor': 20, 'timeStep': 9, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.6562194, 4.9434624, 5.59031  ], dtype=float32)}
episode index:1963
target Thresh 18.999169684303403
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.001217 , 11.185646 ,  4.2426095], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.4215308274583332
{'scaleFactor': 20, 'timeStep': 83, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.6366193, 4.509052 , 4.4670696], dtype=float32)}
episode index:1964
target Thresh 18.99917382552022
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9860773 , 9.939079  , 0.62179583], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.42163682888517534
{'scaleFactor': 20, 'timeStep': 47, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9667794, 3.7121978, 5.8786263], dtype=float32)}
episode index:1965
target Thresh 18.99917794608263
target distance 10.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([11.349222 , 11.8325405,  2.9999306], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.4214223645775023
{'scaleFactor': 20, 'timeStep': 500, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([17.55887  ,  8.85245  ,  5.6199236], dtype=float32)}
episode index:1966
target Thresh 18.999182046093644
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([7.5566945, 8.976316 , 5.3680673], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4216239327386715
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9776158, 1.0065423, 4.255283 ], dtype=float32)}
episode index:1967
target Thresh 18.999186125655772
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.337545 ,  4.013885 ,  3.9629004], dtype=float32)}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.421459048946472
{'scaleFactor': 20, 'timeStep': 233, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.3832757 , 3.9459531 , 0.76014084], dtype=float32)}
episode index:1968
target Thresh 18.999190184870997
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([9.120507 , 9.602382 , 3.4207003], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4216603937350199
{'scaleFactor': 20, 'timeStep': 21, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.2387543, 4.1837564, 4.661981 ], dtype=float32)}
episode index:1969
target Thresh 18.999194223840803
target distance 6.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0799816, 9.27247  , 3.6981497], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4218742428718975
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5141957, 3.258347 , 5.5145135], dtype=float32)}
episode index:1970
target Thresh 18.999198242666164
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.509403 , 9.5319605, 4.123274 ], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4221330917328996
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9831083, 2.272183 , 5.608594 ], dtype=float32)}
episode index:1971
target Thresh 18.99920224144755
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9960816, 5.7847233, 0.6080041], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4223640186757323
{'scaleFactor': 20, 'timeStep': 14, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.9187262, 3.376132 , 4.555347 ], dtype=float32)}
episode index:1972
target Thresh 18.99920622028493
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8029615, 6.2942615, 0.6818056], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4225771860222646
{'scaleFactor': 20, 'timeStep': 18, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.7338595, 4.9093227, 4.105164 ], dtype=float32)}
episode index:1973
target Thresh 18.999210179277775
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.8674254, 11.918612 ,  1.4500487], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.4224605741311155
{'scaleFactor': 20, 'timeStep': 165, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9603533, 3.944299 , 4.4944725], dtype=float32)}
episode index:1974
target Thresh 18.99921411852506
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.0800357, 5.095253 , 4.9126062], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42274793586573267
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.7609763, 3.2954361, 5.1728153], dtype=float32)}
episode index:1975
target Thresh 18.999218038125267
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.202063 ,  8.779387 ,  3.5954163], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.42284017176212524
{'scaleFactor': 20, 'timeStep': 51, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2471657, 4.3862467, 4.2392697], dtype=float32)}
episode index:1976
target Thresh 18.99922193817639
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 3.046251 , 12.372225 ,  0.7844877], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.42303177035482986
{'scaleFactor': 20, 'timeStep': 23, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.5829556, 4.9682517, 4.2406406], dtype=float32)}
episode index:1977
target Thresh 18.999225818775923
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.997662,  9.775069,  6.279482], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.42308362309354103
{'scaleFactor': 20, 'timeStep': 65, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.03292 , 4.736182, 5.204236], dtype=float32)}
episode index:1978
target Thresh 18.99922968002089
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.195468, 10.050652,  0.898203], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.42310524873885497
{'scaleFactor': 20, 'timeStep': 77, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.1040068, 4.185669 , 5.706157 ], dtype=float32)}
episode index:1979
target Thresh 18.999233522007813
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.207986 , 6.2173834, 2.2780795], dtype=float32)}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.4229759702975803
{'scaleFactor': 20, 'timeStep': 179, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8775954, 4.980774 , 3.8568256], dtype=float32)}
episode index:1980
target Thresh 18.999237344832746
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([12.089904 ,  7.647792 ,  2.4470215], dtype=float32)}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.4228625418706703
{'scaleFactor': 20, 'timeStep': 162, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.470395 , 4.3079534, 3.970102 ], dtype=float32)}
episode index:1981
target Thresh 18.999241148591263
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.802372 ,  2.7337434,  3.9144042], dtype=float32)}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.42284732919987666
{'scaleFactor': 20, 'timeStep': 94, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7418418, 3.735818 , 4.2676554], dtype=float32)}
episode index:1982
target Thresh 18.999244933378453
target distance 3.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.956421  , 6.163667  , 0.36364216], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4229453840721623
{'scaleFactor': 20, 'timeStep': 49, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.0534644, 2.1916206, 2.5051486], dtype=float32)}
episode index:1983
target Thresh 18.999248699288938
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.151673 , 9.984383 , 6.0501475], dtype=float32)}
done in step count: 429
reward sum = 0.013412152484926368
running average episode reward sum: 0.4227389661126929
{'scaleFactor': 20, 'timeStep': 430, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.2656393, 4.6969213, 4.6576834], dtype=float32)}
episode index:1984
target Thresh 18.999252446416868
target distance 2.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8818207, 5.0351143, 2.3312783], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.42302473993329104
{'scaleFactor': 20, 'timeStep': 2, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.8891124, 4.8346786, 2.4046426], dtype=float32)}
episode index:1985
target Thresh 18.99925617485592
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.9661727, 11.390898 ,  4.2092357], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4232810544388166
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.6093667, 3.4901485, 4.733427 ], dtype=float32)}
episode index:1986
target Thresh 18.999259884699303
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 5.021679 , 10.974852 ,  5.0194917], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.42343657648975547
{'scaleFactor': 20, 'timeStep': 32, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.9121127, 1.2907367, 3.796363 ], dtype=float32)}
episode index:1987
target Thresh 18.999263576039763
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.901528 , 10.078026 ,  4.2336345], dtype=float32)}
done in step count: 309
reward sum = 0.04479970256613774
running average episode reward sum: 0.42324611528556855
{'scaleFactor': 20, 'timeStep': 310, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.3504918, 4.408559 , 4.634861 ], dtype=float32)}
episode index:1988
target Thresh 18.999267248969588
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.969045  ,  4.177502  ,  0.43596834], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.4232771586515759
{'scaleFactor': 20, 'timeStep': 73, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.538414 , 4.9217386, 3.7388291], dtype=float32)}
episode index:1989
target Thresh 18.999270903580598
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([16.028055  ,  6.759699  ,  0.41527233], dtype=float32)}
done in step count: 297
reward sum = 0.050542043299318794
running average episode reward sum: 0.42308985457350945
{'scaleFactor': 20, 'timeStep': 298, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.0893106, 1.0082514, 5.6239204], dtype=float32)}
episode index:1990
target Thresh 18.999274539964162
target distance 7.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.997207  , 10.1209345 ,  0.25755376], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.42320999128190245
{'scaleFactor': 20, 'timeStep': 42, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.9134402, 4.4470463, 3.85564  ], dtype=float32)}
episode index:1991
target Thresh 18.999278158211183
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.01352  , 10.9506235,  6.1090617], dtype=float32)}
done in step count: 277
reward sum = 0.06179436923202584
running average episode reward sum: 0.4230285577366967
{'scaleFactor': 20, 'timeStep': 278, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.074728 , 3.2748518, 3.5543723], dtype=float32)}
episode index:1992
target Thresh 18.999281758412124
target distance 5.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.9862418, 8.13884  , 1.2520162], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4232839700749658
{'scaleFactor': 20, 'timeStep': 8, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.8947544, 4.5338864, 5.349531 ], dtype=float32)}
episode index:1993
target Thresh 18.99928534065699
target distance 8.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([ 6.778703 , 10.502272 ,  1.5117778], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.42324980600507117
{'scaleFactor': 20, 'timeStep': 104, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.7073908, 3.3832512, 4.3763657], dtype=float32)}
episode index:1994
target Thresh 18.999288905035332
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.786656 ,  6.2667933,  1.4941938], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.42320358208643255
{'scaleFactor': 20, 'timeStep': 111, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.82975  , 2.6876202, 3.5169032], dtype=float32)}
episode index:1995
target Thresh 18.999292451636265
target distance 9.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([10.528443,  9.987252,  3.508762], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4232684517733443
{'scaleFactor': 20, 'timeStep': 60, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([2.0862598, 4.865014 , 4.4918466], dtype=float32)}
episode index:1996
target Thresh 18.99929598054845
target distance 11.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([13.522119 ,  1.8766634,  4.505541 ], dtype=float32)}
done in step count: 209
reward sum = 0.12239274379499834
running average episode reward sum: 0.4231177879235805
{'scaleFactor': 20, 'timeStep': 210, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([3.5576234, 1.0216818, 3.2947478], dtype=float32)}
episode index:1997
target Thresh 18.999299491860114
target distance 13.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([18.046976  , 10.756448  ,  0.35350722], dtype=float32)}
done in step count: 440
reward sum = 0.01200841319170568
running average episode reward sum: 0.4229120274757667
{'scaleFactor': 20, 'timeStep': 441, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([4.6705675, 3.3666391, 4.32482  ], dtype=float32)}
episode index:1998
target Thresh 18.99930298565904
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([15.003351 ,  2.0404081,  0.7683089], dtype=float32)}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.4227875041145864
{'scaleFactor': 20, 'timeStep': 175, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.1100857, 1.3728771, 4.578414 ], dtype=float32)}
episode index:1999
target Thresh 18.999306462032568
target distance 12.0
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([14.9937525,  2.031996 ,  6.076949 ], dtype=float32)}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.4227035652428773
{'scaleFactor': 20, 'timeStep': 137, 'currentTarget': array([3., 3.], dtype=float32), 'previousTarget': array([3., 3.], dtype=float32), 'currentState': array([1.775282 , 4.210729 , 4.1627965], dtype=float32)}

Process finished with exit code 0
