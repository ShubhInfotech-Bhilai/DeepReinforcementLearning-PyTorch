/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/TimedMaze/StackedDQNHERFreeSpaceExample/StackedDQN_CNNTimedMazeGPU.py
episode index:0
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.076733 , 14.451325 ,  2.9214873], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 1.0
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.076733 , 14.451325 ,  2.9214873], dtype=float32)}
episode index:1
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.993158,  4.165285,  5.985014], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.9008152947695229
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.92133  , 13.756759 ,  2.6392074], dtype=float32)}
episode index:2
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.050549 , 15.239733 ,  4.9582863], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.6005435298463486
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 0.04374123, 29.664156  ,  2.0910726 ], dtype=float32)}
episode index:3
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       , 18.       ,  5.5233703], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.45040764738476147
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.400747 , 21.801702 ,  3.9403481], dtype=float32)}
episode index:4
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.783908  , 18.904253  ,  0.32565817], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.36032611790780916
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([29.281843, 11.288858,  4.353551], dtype=float32)}
episode index:5
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      , 18.      ,  4.962494], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.31764047372815357
{'scaleFactor': 30, 'timeStep': 226, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.732552, 15.043964,  3.498169], dtype=float32)}
episode index:6
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        , 13.        ,  0.67058134], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2722632631955602
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.354347, 12.823767,  2.822931], dtype=float32)}
episode index:7
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 22.       ,  1.1120882], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2382303552961152
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.326057 , 15.454667 ,  4.0619664], dtype=float32)}
episode index:8
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 29.       ,  2.7117295], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.21176031581876906
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.526733 , 20.69652  ,  0.6033579], dtype=float32)}
episode index:9
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 21.       ,  1.5403705], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.19058428423689217
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.895323 ,  7.937569 ,  3.0405433], dtype=float32)}
episode index:10
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 15.       ,  0.7755562], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.1732584402153565
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.913393 , 22.458233 ,  3.0749803], dtype=float32)}
episode index:11
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 21.       ,  4.6823697], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.15882023686407679
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.337545, 19.009655,  6.123588], dtype=float32)}
episode index:12
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.3598485, 21.533436 ,  5.767441 ], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.14660329556684012
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.814388,  9.479523,  3.474286], dtype=float32)}
episode index:13
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 14.       ,  0.9672789], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.1361316315977801
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.890038 , 11.625899 ,  2.3949246], dtype=float32)}
episode index:14
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.      , 20.      ,  5.176662], dtype=float32)}
done in step count: 323
reward sum = 0.038919554017627804
running average episode reward sum: 0.12965082642576994
{'scaleFactor': 30, 'timeStep': 324, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.594614 , 15.697704 ,  4.7127724], dtype=float32)}
episode index:15
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 14.       ,  3.5109856], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.13444812084399768
{'scaleFactor': 30, 'timeStep': 158, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.930619 , 13.165904 ,  3.4328015], dtype=float32)}
episode index:16
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.        , 29.        ,  0.49808967], dtype=float32)}
done in step count: 495
reward sum = 0.00690909756953347
running average episode reward sum: 0.1269458253572645
{'scaleFactor': 30, 'timeStep': 496, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.9762945, 16.019995 ,  5.0980434], dtype=float32)}
episode index:17
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.      , 19.      ,  5.151372], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.11989327950408314
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.777649 , 18.243074 ,  2.5332274], dtype=float32)}
episode index:18
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.025255 , 26.00016  ,  4.6859574], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.11358310689860508
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.0067635, 17.172935 ,  4.4232035], dtype=float32)}
episode index:19
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.     , 13.     ,  3.75324], dtype=float32)}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.12144166452927455
{'scaleFactor': 30, 'timeStep': 131, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.976333 , 13.31365  ,  1.9624236], dtype=float32)}
episode index:20
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 13.       ,  4.5816092], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.13922247381836655
{'scaleFactor': 30, 'timeStep': 71, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.905166 , 13.066358 ,  1.8841022], dtype=float32)}
episode index:21
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.      , 20.      ,  3.171982], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.14824665232202727
{'scaleFactor': 30, 'timeStep': 109, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.183984, 16.44601 ,  4.808174], dtype=float32)}
episode index:22
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.023872, 14.308088,  4.198209], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.18192048023967478
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.128086 , 14.630654 ,  2.6793637], dtype=float32)}
episode index:23
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([8.      , 8.      , 3.885828], dtype=float32)}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.1848554878584111
{'scaleFactor': 30, 'timeStep': 138, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.284725 , 14.973716 ,  0.7882936], dtype=float32)}
episode index:24
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.      , 21.      ,  4.088337], dtype=float32)}
done in step count: 485
reward sum = 0.007639578183221077
running average episode reward sum: 0.17776685147140353
{'scaleFactor': 30, 'timeStep': 486, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.458591, 16.723518,  3.803715], dtype=float32)}
episode index:25
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        , 18.        ,  0.07277649], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.20400883236255293
{'scaleFactor': 30, 'timeStep': 16, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.117167, 14.25274 ,  4.484171], dtype=float32)}
episode index:26
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.000698, 22.94716 ,  2.588195], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.21631473580124302
{'scaleFactor': 30, 'timeStep': 63, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.105356, 15.154102,  4.678753], dtype=float32)}
episode index:27
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 25.       ,  2.2071102], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.21938936793937797
{'scaleFactor': 30, 'timeStep': 120, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.632425, 15.360781,  5.384405], dtype=float32)}
episode index:28
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.984154, 19.258896,  5.039195], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.24562076904491667
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.309675, 15.485162,  4.392335], dtype=float32)}
episode index:29
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.999355, 27.05078 ,  5.927156], dtype=float32)}
done in step count: 329
reward sum = 0.03664198753113651
running average episode reward sum: 0.23865480966112398
{'scaleFactor': 30, 'timeStep': 330, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.142521 , 14.2200165,  1.6977515], dtype=float32)}
episode index:30
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 29.       ,  1.9095186], dtype=float32)}
done in step count: 352
reward sum = 0.029079604685802656
running average episode reward sum: 0.23189431917804912
{'scaleFactor': 30, 'timeStep': 353, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.799377 , 15.182098 ,  5.2087564], dtype=float32)}
episode index:31
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.255713 ,  7.8563504,  1.4342619], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.2429925211392815
{'scaleFactor': 30, 'timeStep': 54, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.55789 , 14.363835,  4.045495], dtype=float32)}
episode index:32
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.0550816, 24.533854 ,  3.4691   ], dtype=float32)}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.2385725244286987
{'scaleFactor': 30, 'timeStep': 233, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.789454 , 14.988435 ,  2.2105827], dtype=float32)}
episode index:33
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.7177906, 23.975697 ,  6.205162 ], dtype=float32)}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.2373289652991201
{'scaleFactor': 30, 'timeStep': 163, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.754073 , 16.57828  ,  4.0343885], dtype=float32)}
episode index:34
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([5.7324467, 4.9993143, 0.3267269], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.25004969757658485
{'scaleFactor': 30, 'timeStep': 39, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.6793585, 14.140963 ,  4.12178  ], dtype=float32)}
episode index:35
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.000246 ,  9.031386 ,  3.0809262], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.2653713890199866
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.112853, 14.564411,  5.979891], dtype=float32)}
episode index:36
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.033734 , 19.248905 ,  3.4361572], dtype=float32)}
done in step count: 434
reward sum = 0.012754823560906535
running average episode reward sum: 0.25854391427784923
{'scaleFactor': 30, 'timeStep': 435, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.85652 , 13.104542,  0.661229], dtype=float32)}
episode index:37
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       ,  6.       ,  5.8095984], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.2517401270600111
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 0.09154618, 27.409964  ,  2.8793194 ], dtype=float32)}
episode index:38
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([28.196075  ,  6.6029353 ,  0.91801155], dtype=float32)}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.2518874214113941
{'scaleFactor': 30, 'timeStep': 136, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.625443 , 14.751617 ,  3.0136993], dtype=float32)}
episode index:39
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.      , 10.      ,  4.353732], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.25689132713277113
{'scaleFactor': 30, 'timeStep': 80, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.228103, 14.486649,  2.520188], dtype=float32)}
episode index:40
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 20.       ,  1.9694091], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.26761137325269313
{'scaleFactor': 30, 'timeStep': 37, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.611256 , 13.911797 ,  0.6525147], dtype=float32)}
episode index:41
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.      , 24.      ,  3.645608], dtype=float32)}
done in step count: 270
reward sum = 0.06629832272038531
running average episode reward sum: 0.26281820538287626
{'scaleFactor': 30, 'timeStep': 271, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.472911 , 16.6773   ,  5.7956276], dtype=float32)}
episode index:42
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 29.       ,  2.0245519], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.2684476863922761
{'scaleFactor': 30, 'timeStep': 69, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.609227 , 15.443951 ,  5.0428233], dtype=float32)}
episode index:43
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.515186, 21.6601  ,  3.653011], dtype=float32)}
done in step count: 258
reward sum = 0.07479631572685258
running average episode reward sum: 0.26404651887715286
{'scaleFactor': 30, 'timeStep': 259, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.008063 , 15.264242 ,  1.1373978], dtype=float32)}
episode index:44
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.        , 23.        ,  0.87107265], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.2744522266722027
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.077279 , 15.291829 ,  4.1490083], dtype=float32)}
episode index:45
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 14.       ,  2.6508818], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.28591262586496013
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.168022, 14.081057,  3.43292 ], dtype=float32)}
episode index:46
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.9424806, 18.476202 ,  5.854027 ], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.29244568587637587
{'scaleFactor': 30, 'timeStep': 53, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.602394  , 16.016233  ,  0.24985477], dtype=float32)}
episode index:47
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.      , 15.      ,  2.004625], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.30445193851997177
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.633224 , 13.096059 ,  6.1434755], dtype=float32)}
episode index:48
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.        , 21.        ,  0.56859607], dtype=float32)}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.2995647354093244
{'scaleFactor': 30, 'timeStep': 273, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.285545 , 13.920175 ,  1.3586605], dtype=float32)}
episode index:49
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 21.       ,  2.6510322], dtype=float32)}
done in step count: 360
reward sum = 0.026833050939885684
running average episode reward sum: 0.2941101017199356
{'scaleFactor': 30, 'timeStep': 361, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.116575 , 16.545792 ,  1.0662345], dtype=float32)}
episode index:50
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       ,  2.       ,  4.8515573], dtype=float32)}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.29483408184936855
{'scaleFactor': 30, 'timeStep': 111, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.379557 , 15.634976 ,  3.8574219], dtype=float32)}
episode index:51
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.        , 19.        ,  0.75777876], dtype=float32)}
done in step count: 247
reward sum = 0.08353972967320515
running average episode reward sum: 0.2907707289229039
{'scaleFactor': 30, 'timeStep': 248, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.3497505, 16.158693 ,  1.057826 ], dtype=float32)}
episode index:52
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.8797  , 17.003622,  5.119994], dtype=float32)}
done in step count: 390
reward sum = 0.019848417799380184
running average episode reward sum: 0.2856589872035921
{'scaleFactor': 30, 'timeStep': 391, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.768639, 13.045579,  0.665438], dtype=float32)}
episode index:53
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.        , 29.        ,  0.44904634], dtype=float32)}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.28362358561632517
{'scaleFactor': 30, 'timeStep': 174, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.5989895, 14.250721 ,  1.8407412], dtype=float32)}
episode index:54
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.11759  , 27.341389 ,  4.5780582], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.2920517403202392
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.560123 , 13.443319 ,  2.0398269], dtype=float32)}
episode index:55
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.      , 9.      , 3.427467], dtype=float32)}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.291165399351031
{'scaleFactor': 30, 'timeStep': 142, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.657043 , 13.815582 ,  2.4047306], dtype=float32)}
episode index:56
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 26.       ,  4.1172214], dtype=float32)}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.29294690161430775
{'scaleFactor': 30, 'timeStep': 94, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.494122 , 14.536797 ,  1.7086891], dtype=float32)}
episode index:57
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.      , 14.      ,  4.221156], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.29530797250930063
{'scaleFactor': 30, 'timeStep': 85, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.316675 , 13.524932 ,  2.7463408], dtype=float32)}
episode index:58
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.212804 , 27.102264 ,  3.7288983], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.30594249321978567
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.332607 , 16.327633 ,  5.6871233], dtype=float32)}
episode index:59
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 20.       ,  5.1803293], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3126860054539941
{'scaleFactor': 30, 'timeStep': 35, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.105435 , 14.220644 ,  5.4314637], dtype=float32)}
episode index:60
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.998506  , 11.922714  ,  0.06276625], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3228397651663383
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.773955 , 14.153238 ,  1.4932101], dtype=float32)}
episode index:61
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.486725 , 26.69233  ,  3.3430638], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.32470714801542816
{'scaleFactor': 30, 'timeStep': 83, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.240252 , 13.753941 ,  1.4677788], dtype=float32)}
episode index:62
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.        , 11.        ,  0.51114625], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.3293513225063307
{'scaleFactor': 30, 'timeStep': 49, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.467352 , 16.314957 ,  3.3256574], dtype=float32)}
episode index:63
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.85122  , 22.362837 ,  4.0571113], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.33698500399212605
{'scaleFactor': 30, 'timeStep': 21, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.941824 , 14.035748 ,  4.4030023], dtype=float32)}
episode index:64
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.        , 10.        ,  0.79103124], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.33737557343625485
{'scaleFactor': 30, 'timeStep': 102, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.735612, 15.526426,  3.922518], dtype=float32)}
episode index:65
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.       , 8.       , 3.8132937], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.3343143223979333
{'scaleFactor': 30, 'timeStep': 200, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.182103  , 14.531481  ,  0.51421803], dtype=float32)}
episode index:66
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.      ,  4.      ,  2.719317], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.33693628394795766
{'scaleFactor': 30, 'timeStep': 68, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.827396, 14.050953,  2.013881], dtype=float32)}
episode index:67
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.      , 19.      ,  1.803731], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.33418194750171737
{'scaleFactor': 30, 'timeStep': 190, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.16063  , 16.167337 ,  0.5151541], dtype=float32)}
episode index:68
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.       , 11.       ,  5.1851625], dtype=float32)}
done in step count: 266
reward sum = 0.06901790349970881
running average episode reward sum: 0.330338990342268
{'scaleFactor': 30, 'timeStep': 267, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.265182, 13.581205,  5.893823], dtype=float32)}
episode index:69
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 26.       ,  3.2298388], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3357706222984112
{'scaleFactor': 30, 'timeStep': 35, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.376523  , 15.009821  ,  0.23238438], dtype=float32)}
episode index:70
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.        ,  8.        ,  0.20519751], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.33599370094995507
{'scaleFactor': 30, 'timeStep': 105, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.107694 , 15.591113 ,  1.1064662], dtype=float32)}
episode index:71
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.        , 22.        ,  0.39697993], dtype=float32)}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.333999140004733
{'scaleFactor': 30, 'timeStep': 165, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.208363 , 14.908893 ,  1.9241825], dtype=float32)}
episode index:72
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.299286  ,  5.5204787 ,  0.60404396], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.3294238093197367
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.638187 , 20.15241  ,  0.5144612], dtype=float32)}
episode index:73
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 15.       ,  5.2124896], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3316591451343376
{'scaleFactor': 30, 'timeStep': 71, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.39384  , 13.682621 ,  3.1043904], dtype=float32)}
episode index:74
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       ,  3.       ,  3.3275263], dtype=float32)}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.3313509494743418
{'scaleFactor': 30, 'timeStep': 118, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.245692 , 15.572586 ,  5.0310197], dtype=float32)}
episode index:75
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 23.       ,  2.1959755], dtype=float32)}
done in step count: 251
reward sum = 0.08024793100055946
running average episode reward sum: 0.3280469623891604
{'scaleFactor': 30, 'timeStep': 252, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.230991 , 14.210887 ,  2.4576542], dtype=float32)}
episode index:76
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.9424586, 12.476293 ,  0.5034379], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.33310778797629176
{'scaleFactor': 30, 'timeStep': 34, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.196873  , 16.16079   ,  0.97590995], dtype=float32)}
episode index:77
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       ,  2.       ,  5.1094794], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.33890997198694467
{'scaleFactor': 30, 'timeStep': 25, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.133614 , 13.903856 ,  1.7467889], dtype=float32)}
episode index:78
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.789735 , 10.892664 ,  5.8597183], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.3467794155060973
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.978447 , 13.982628 ,  1.0336738], dtype=float32)}
episode index:79
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       ,  4.       ,  4.0899158], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.346003519978186
{'scaleFactor': 30, 'timeStep': 126, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.181705 , 15.758277 ,  1.1630245], dtype=float32)}
episode index:80
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.401077,  8.798565,  4.538315], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.34958601338665124
{'scaleFactor': 30, 'timeStep': 46, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.189074, 15.625818,  4.475712], dtype=float32)}
episode index:81
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       ,  3.       ,  3.5550632], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.34532276932096034
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.056604 , 11.552191 ,  3.9857082], dtype=float32)}
episode index:82
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.750947, 15.033468,  6.12385 ], dtype=float32)}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.3443274659225178
{'scaleFactor': 30, 'timeStep': 134, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.602479 , 13.943083 ,  1.0186814], dtype=float32)}
episode index:83
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.        ,  3.        ,  0.33408087], dtype=float32)}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.3440521517396877
{'scaleFactor': 30, 'timeStep': 114, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.38022  , 15.591619 ,  3.5295045], dtype=float32)}
episode index:84
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       ,  3.       ,  2.4654152], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.35086029930072565
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.515578 , 14.219883 ,  1.3314576], dtype=float32)}
episode index:85
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 21.       ,  2.3996325], dtype=float32)}
done in step count: 409
reward sum = 0.016398140018627688
running average episode reward sum: 0.3469712044253524
{'scaleFactor': 30, 'timeStep': 410, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.799347 , 15.471686 ,  4.4102993], dtype=float32)}
episode index:86
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 27.       ,  1.4113817], dtype=float32)}
done in step count: 230
reward sum = 0.09910481551887466
running average episode reward sum: 0.3441221654724044
{'scaleFactor': 30, 'timeStep': 231, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.095321 , 15.113626 ,  2.3342195], dtype=float32)}
episode index:87
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.873749 ,  6.9960113,  1.521405 ], dtype=float32)}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.34139591646510287
{'scaleFactor': 30, 'timeStep': 226, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.374562 , 16.172613 ,  0.9361525], dtype=float32)}
episode index:88
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.123446 , 29.654652 ,  1.0060908], dtype=float32)}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.3400987506119443
{'scaleFactor': 30, 'timeStep': 149, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.1312475, 16.624262 ,  4.987842 ], dtype=float32)}
episode index:89
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.442709 ,  9.745072 ,  3.7488663], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.3439804210036902
{'scaleFactor': 30, 'timeStep': 38, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.31415  , 16.339626 ,  5.4875107], dtype=float32)}
episode index:90
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.43413  , 13.952314 ,  0.7093851], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.3455838259707288
{'scaleFactor': 30, 'timeStep': 72, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.391901  , 13.077333  ,  0.12553005], dtype=float32)}
episode index:91
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([2.5000000e+01, 2.4000000e+01, 1.3074096e-02], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.34182748003626434
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.078563 , 31.376305 ,  2.4610195], dtype=float32)}
episode index:92
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       ,  5.       ,  3.8144174], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.34446413919649255
{'scaleFactor': 30, 'timeStep': 54, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.632843 , 13.780301 ,  5.9683156], dtype=float32)}
episode index:93
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       , 23.       ,  2.4148097], dtype=float32)}
done in step count: 369
reward sum = 0.02451245483619269
running average episode reward sum: 0.3410603978735106
{'scaleFactor': 30, 'timeStep': 370, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.019591, 15.999522,  5.053405], dtype=float32)}
episode index:94
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.       , 21.       ,  1.8917066], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.340590147236939
{'scaleFactor': 30, 'timeStep': 122, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.477001 , 15.842112 ,  5.0318713], dtype=float32)}
episode index:95
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.      , 11.      ,  5.609823], dtype=float32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.34156557619063554
{'scaleFactor': 30, 'timeStep': 84, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.220461 , 15.80325  ,  4.6506834], dtype=float32)}
episode index:96
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.922682, 12.001495,  4.368745], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.3435728199949299
{'scaleFactor': 30, 'timeStep': 63, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.4685   , 14.366653 ,  2.0168407], dtype=float32)}
episode index:97
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.99983 , 19.026114,  5.789484], dtype=float32)}
done in step count: 269
reward sum = 0.06696800274786396
running average episode reward sum: 0.34075032185975573
{'scaleFactor': 30, 'timeStep': 270, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.295299 , 13.946299 ,  1.6767576], dtype=float32)}
episode index:98
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.920172, 13.44059 ,  5.889293], dtype=float32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.34072006003186833
{'scaleFactor': 30, 'timeStep': 109, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.716506 , 13.117738 ,  0.8340652], dtype=float32)}
episode index:99
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.777839 ,  5.9161277,  1.1262233], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.3467276609255597
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.545483  , 13.381226  ,  0.54727674], dtype=float32)}
episode index:100
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.      ,  6.      ,  5.128074], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.34814511253029873
{'scaleFactor': 30, 'timeStep': 72, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.666767, 13.098892,  1.600701], dtype=float32)}
episode index:101
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.711393, 10.979067,  2.709882], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.35434074868196247
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.3317585, 13.063561 ,  2.776599 ], dtype=float32)}
episode index:102
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.004898 , 19.860111 ,  2.9091485], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.35551549910258134
{'scaleFactor': 30, 'timeStep': 75, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.635585, 13.126461,  2.262749], dtype=float32)}
episode index:103
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.8728666, 21.004045 ,  4.6880603], dtype=float32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.35730559118781474
{'scaleFactor': 30, 'timeStep': 62, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.970132 , 13.427392 ,  2.3301153], dtype=float32)}
episode index:104
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.      , 12.      ,  3.329321], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3626907255043872
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.227482 , 13.735157 ,  1.5345535], dtype=float32)}
episode index:105
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 16.       ,  2.5474882], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.3685153413015156
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.373467 , 16.77007  ,  3.0855668], dtype=float32)}
episode index:106
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.00593  , 13.153902 ,  2.2660427], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.366401249212549
{'scaleFactor': 30, 'timeStep': 195, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.539624, 13.570248,  5.95336 ], dtype=float32)}
episode index:107
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 23.       ,  1.2280476], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.3712988140740876
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.871526, 16.802258,  5.50096 ], dtype=float32)}
episode index:108
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 16.       ,  1.8029114], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.3711181754730228
{'scaleFactor': 30, 'timeStep': 105, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.396215, 15.574607,  3.799709], dtype=float32)}
episode index:109
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.0556  , 25.237017,  4.520878], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.3763897379678135
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.88432  , 16.522524 ,  5.5701876], dtype=float32)}
episode index:110
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.999306  , 16.052675  ,  0.26735145], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.38191775834648184
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.928253  , 16.581032  ,  0.88224113], dtype=float32)}
episode index:111
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       ,  5.       ,  3.2792659], dtype=float32)}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.38003034470847386
{'scaleFactor': 30, 'timeStep': 177, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.792376, 13.766682,  6.08825 ], dtype=float32)}
episode index:112
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.234564, 11.847731,  2.125856], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3854283062597263
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.180576 , 13.547468 ,  1.9137626], dtype=float32)}
episode index:113
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       ,  7.       ,  4.0305333], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.3853233586649416
{'scaleFactor': 30, 'timeStep': 99, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.4076  , 16.77213 ,  5.726578], dtype=float32)}
episode index:114
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 21.       ,  3.0304856], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3881514444789186
{'scaleFactor': 30, 'timeStep': 35, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.429912 , 13.758528 ,  1.4202654], dtype=float32)}
episode index:115
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.777874, 18.012373,  4.716816], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.3930863114230659
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.776744, 13.031615,  3.570667], dtype=float32)}
episode index:116
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.        , 11.        ,  0.18344212], dtype=float32)}
done in step count: 298
reward sum = 0.050036622866325604
running average episode reward sum: 0.39015426280292276
{'scaleFactor': 30, 'timeStep': 299, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.413153  , 14.164666  ,  0.16177398], dtype=float32)}
episode index:117
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 19.       ,  1.2441738], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.390916866139944
{'scaleFactor': 30, 'timeStep': 74, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.705217 , 14.17636  ,  1.7858496], dtype=float32)}
episode index:118
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 20.       ,  4.6919503], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.390833927080823
{'scaleFactor': 30, 'timeStep': 97, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.437517 , 13.530088 ,  3.9775229], dtype=float32)}
episode index:119
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.      , 15.      ,  6.181807], dtype=float32)}
done in step count: 326
reward sum = 0.03776360434375024
running average episode reward sum: 0.38789167439134736
{'scaleFactor': 30, 'timeStep': 327, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.376511  , 14.039552  ,  0.14020443], dtype=float32)}
episode index:120
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.      ,  9.      ,  5.721591], dtype=float32)}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.3846859580740635
{'scaleFactor': 30, 'timeStep': 500, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.782652 ,  8.649274 ,  0.5801348], dtype=float32)}
episode index:121
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.469941 , 19.055996 ,  4.6661143], dtype=float32)}
done in step count: 317
reward sum = 0.04133868785485247
running average episode reward sum: 0.3818716361870208
{'scaleFactor': 30, 'timeStep': 318, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.0050745, 13.44456  ,  2.3953297], dtype=float32)}
episode index:122
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 18.       ,  0.7813273], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.38604616153719723
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.971426 , 16.34041  ,  4.6199923], dtype=float32)}
episode index:123
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([6.9991918, 7.0568566, 6.2027574], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.3880634141543478
{'scaleFactor': 30, 'timeStep': 46, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.334352 , 14.668823 ,  2.5822134], dtype=float32)}
episode index:124
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 24.       ,  3.7448483], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.3891216310223155
{'scaleFactor': 30, 'timeStep': 66, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.530857 , 15.431077 ,  5.3974686], dtype=float32)}
episode index:125
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 22.       ,  1.7223911], dtype=float32)}
done in step count: 182
reward sum = 0.16054819111089647
running average episode reward sum: 0.38730755610238365
{'scaleFactor': 30, 'timeStep': 183, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.797693, 15.410422,  4.015209], dtype=float32)}
episode index:126
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.      , 10.      ,  5.189464], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.39130779782015
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.079266, 13.389307,  0.843596], dtype=float32)}
episode index:127
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.       , 4.       , 4.1997404], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.3893624828979777
{'scaleFactor': 30, 'timeStep': 195, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.800465 , 13.265892 ,  1.0570145], dtype=float32)}
episode index:128
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      , 14.      ,  5.660623], dtype=float32)}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.3873122141333341
{'scaleFactor': 30, 'timeStep': 208, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.353484 , 13.936724 ,  1.9337606], dtype=float32)}
episode index:129
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.       , 20.       ,  2.9640658], dtype=float32)}
done in step count: 263
reward sum = 0.07113055202541568
running average episode reward sum: 0.3848800475017347
{'scaleFactor': 30, 'timeStep': 264, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.909924 , 15.549691 ,  4.4430447], dtype=float32)}
episode index:130
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 28.       ,  5.4688153], dtype=float32)}
done in step count: 369
reward sum = 0.02451245483619269
running average episode reward sum: 0.3821291498477993
{'scaleFactor': 30, 'timeStep': 370, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.433451, 13.004666,  2.201865], dtype=float32)}
episode index:131
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.764812, 13.427006,  4.728363], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.38680998962167956
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.764812, 13.427006,  4.728363], dtype=float32)}
episode index:132
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 20.       ,  0.2218389], dtype=float32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.3865188852973996
{'scaleFactor': 30, 'timeStep': 106, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.3462925, 13.201594 ,  2.7286758], dtype=float32)}
episode index:133
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.981585 , 22.729216 ,  5.7970715], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.38973819912053265
{'scaleFactor': 30, 'timeStep': 21, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.174856, 15.440023,  5.451391], dtype=float32)}
episode index:134
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.      , 21.      ,  4.548752], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.3910283249822501
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.423307 , 14.28756  ,  4.8908377], dtype=float32)}
episode index:135
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.       , 22.       ,  5.7351084], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.3942278860032305
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.831352 , 13.372214 ,  3.2312493], dtype=float32)}
episode index:136
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.108986,  5.651201,  3.275422], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.3935576418402071
{'scaleFactor': 30, 'timeStep': 120, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.369497 , 14.271889 ,  1.8405219], dtype=float32)}
episode index:137
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 10.       ,  2.8273606], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.39712885365090217
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.359697 , 14.607973 ,  0.8334661], dtype=float32)}
episode index:138
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.294338, 19.475307,  5.461777], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4013228906749964
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.170753 , 16.704306 ,  4.6387653], dtype=float32)}
episode index:139
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       , 28.       ,  3.6085765], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4047876191110045
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.21164 , 13.954136,  4.701499], dtype=float32)}
episode index:140
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.1144085, 19.206755 ,  4.8065767], dtype=float32)}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.4027590371095899
{'scaleFactor': 30, 'timeStep': 213, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.660973 , 13.834623 ,  4.2083454], dtype=float32)}
episode index:141
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.417501  , 10.410918  ,  0.42313623], dtype=float32)}
done in step count: 321
reward sum = 0.03970977861200674
running average episode reward sum: 0.4002023521905928
{'scaleFactor': 30, 'timeStep': 322, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.653456 , 16.309982 ,  4.2563753], dtype=float32)}
episode index:142
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([7.       , 3.       , 1.2291105], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4028430445486946
{'scaleFactor': 30, 'timeStep': 26, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.270979 , 16.801237 ,  3.8386624], dtype=float32)}
episode index:143
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      , 16.      ,  5.497442], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.40318471542173473
{'scaleFactor': 30, 'timeStep': 80, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.881128 , 16.137928 ,  3.5629358], dtype=float32)}
episode index:144
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.      ,  8.      ,  2.670395], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4057684164146824
{'scaleFactor': 30, 'timeStep': 26, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.11623  , 13.764325 ,  1.0432475], dtype=float32)}
episode index:145
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.022034 , 13.703942 ,  3.2827497], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.40963506424745855
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.083054 , 13.180428 ,  2.8877044], dtype=float32)}
episode index:146
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.370153 , 10.965448 ,  0.7827951], dtype=float32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.40851418149091073
{'scaleFactor': 30, 'timeStep': 141, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.464344, 13.388105,  2.162383], dtype=float32)}
episode index:147
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 25.       ,  2.0878997], dtype=float32)}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.40688323387958625
{'scaleFactor': 30, 'timeStep': 179, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.036071 , 16.03675  ,  0.4223649], dtype=float32)}
episode index:148
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.180973 , 13.824608 ,  2.7348824], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4107967692226763
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.344119 , 14.615788 ,  3.2289104], dtype=float32)}
episode index:149
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.990324  , 13.196496  ,  0.44855148], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4138497628463183
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.518742 , 16.799337 ,  0.2698179], dtype=float32)}
episode index:150
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.78304   , 11.905962  ,  0.34447187], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.41747059892018373
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.114072 , 16.419897 ,  0.9537867], dtype=float32)}
episode index:151
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.917515, 17.318266,  4.237155], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.42026976072586625
{'scaleFactor': 30, 'timeStep': 18, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.736726 , 13.576326 ,  2.4339018], dtype=float32)}
episode index:152
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.      , 8.      , 4.728536], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.42003856019101404
{'scaleFactor': 30, 'timeStep': 96, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.04933  , 16.438011 ,  2.6065214], dtype=float32)}
episode index:153
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.     ,  6.     ,  4.22429], dtype=float32)}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.41973612980311314
{'scaleFactor': 30, 'timeStep': 99, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.868092 , 15.705296 ,  5.2615843], dtype=float32)}
episode index:154
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.      , 20.      ,  5.902195], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.420487949773462
{'scaleFactor': 30, 'timeStep': 63, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.039034 , 16.11598  ,  4.2208514], dtype=float32)}
episode index:155
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 23.       ,  3.7447703], dtype=float32)}
done in step count: 362
reward sum = 0.026299073226181958
running average episode reward sum: 0.417961098000723
{'scaleFactor': 30, 'timeStep': 363, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.934463 , 14.874874 ,  1.8129994], dtype=float32)}
episode index:156
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.290466 , 25.472029 ,  4.7138395], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.41935106225590224
{'scaleFactor': 30, 'timeStep': 46, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.164962  , 15.633092  ,  0.02401862], dtype=float32)}
episode index:157
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([0.25399196, 5.024574  , 4.0227833 ], dtype=float32)}
done in step count: 413
reward sum = 0.015751987873315082
running average episode reward sum: 0.4167966377344935
{'scaleFactor': 30, 'timeStep': 414, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.045628, 16.945726,  5.971492], dtype=float32)}
episode index:158
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.       , 3.       , 2.4067826], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.41975002285387486
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.57778   , 16.572786  ,  0.69568604], dtype=float32)}
episode index:159
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.549051 , 5.3764987, 2.868705 ], dtype=float32)}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.41960582150117576
{'scaleFactor': 30, 'timeStep': 93, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.610899 , 14.787652 ,  2.7237837], dtype=float32)}
episode index:160
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 15.       ,  3.2258136], dtype=float32)}
done in step count: 295
reward sum = 0.05156825150425344
running average episode reward sum: 0.4173198738614433
{'scaleFactor': 30, 'timeStep': 296, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.368376 , 13.338227 ,  4.0895147], dtype=float32)}
episode index:161
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 27.       ,  1.8364156], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.41871056354183384
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.817455 , 16.294855 ,  4.5879083], dtype=float32)}
episode index:162
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       ,  6.       ,  4.5444016], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.4192392465187985
{'scaleFactor': 30, 'timeStep': 69, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.043795 , 15.984546 ,  0.6399466], dtype=float32)}
episode index:163
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.882579 , 21.324799 ,  5.4926605], dtype=float32)}
done in step count: 242
reward sum = 0.08784500919014836
running average episode reward sum: 0.41721854994972135
{'scaleFactor': 30, 'timeStep': 243, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.459032, 14.141176,  2.479332], dtype=float32)}
episode index:164
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.669497, 25.027496,  4.860697], dtype=float32)}
done in step count: 482
reward sum = 0.00787342683360601
running average episode reward sum: 0.4147376704156843
{'scaleFactor': 30, 'timeStep': 483, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.745091, 15.444409,  4.084342], dtype=float32)}
episode index:165
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 12.       ,  3.5179052], dtype=float32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.4148030466384131
{'scaleFactor': 30, 'timeStep': 86, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.510446 , 14.297849 ,  2.3762732], dtype=float32)}
episode index:166
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.141297 ,  2.7383933,  2.481231 ], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4164476336996745
{'scaleFactor': 30, 'timeStep': 38, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.415535 , 15.832482 ,  0.1436991], dtype=float32)}
episode index:167
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.053104 ,  1.0007051,  4.9826074], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4176802394210527
{'scaleFactor': 30, 'timeStep': 48, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.054351 , 15.907993 ,  1.3468544], dtype=float32)}
episode index:168
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.      ,  7.      ,  5.195153], dtype=float32)}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.41604239429373446
{'scaleFactor': 30, 'timeStep': 196, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.448456, 16.96746 ,  3.58575 ], dtype=float32)}
episode index:169
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.9633515 , 11.381118  ,  0.45297343], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4172999309814374
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.866879, 16.907131,  6.227186], dtype=float32)}
episode index:170
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       , 24.       ,  2.6109207], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.41931772503620734
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.782696, 13.183619,  5.860526], dtype=float32)}
episode index:171
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 12.       ,  2.5970585], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4224088432040201
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.109531 , 16.233578 ,  2.7162306], dtype=float32)}
episode index:172
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 28.       ,  2.9429662], dtype=float32)}
done in step count: 216
reward sum = 0.1140780353265762
running average episode reward sum: 0.42062658419894816
{'scaleFactor': 30, 'timeStep': 217, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.62432  , 14.699126 ,  5.7895484], dtype=float32)}
episode index:173
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.593689 , 15.791632 ,  5.8985243], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4239563164736668
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.593689 , 15.791632 ,  5.8985243], dtype=float32)}
episode index:174
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       ,  9.       ,  4.8494005], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.4233876834073226
{'scaleFactor': 30, 'timeStep': 113, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.775572  , 13.162058  ,  0.43200454], dtype=float32)}
episode index:175
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.        , 28.        ,  0.22692466], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4243175078307894
{'scaleFactor': 30, 'timeStep': 54, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.804407  , 16.539253  ,  0.45566672], dtype=float32)}
episode index:176
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        ,  7.        ,  0.72485155], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4271861396956267
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.579541 , 14.182968 ,  2.8049536], dtype=float32)}
episode index:177
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 15.       ,  4.4337482], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.42694855508437873
{'scaleFactor': 30, 'timeStep': 96, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.237148 , 15.36122  ,  5.1574244], dtype=float32)}
episode index:178
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.      , 13.      ,  1.213057], dtype=float32)}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.4254077974300143
{'scaleFactor': 30, 'timeStep': 189, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.985259 , 15.241731 ,  2.0399718], dtype=float32)}
episode index:179
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.2171803, 27.093603 ,  3.160783 ], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4256067650410592
{'scaleFactor': 30, 'timeStep': 78, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.861048 , 15.64377  ,  0.9270727], dtype=float32)}
episode index:180
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.      , 11.      ,  5.000729], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.42738336907028873
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.872198  , 13.110255  ,  0.92977595], dtype=float32)}
episode index:181
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.      , 12.      ,  5.723409], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4302603288550674
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.367428 , 13.902548 ,  0.5510873], dtype=float32)}
episode index:182
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.432394, 26.0473  ,  5.22493 ], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4330538797870124
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.473373, 16.608633,  4.332459], dtype=float32)}
episode index:183
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([8.376034  , 0.03566849, 4.9616966 ], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.4337040406423133
{'scaleFactor': 30, 'timeStep': 60, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.200119, 13.711122,  2.084897], dtype=float32)}
episode index:184
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.797223, 22.16576 ,  5.278607], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4361030513577547
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.278576 , 16.513765 ,  5.9243784], dtype=float32)}
episode index:185
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.      , 10.      ,  4.335237], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.43811178693228964
{'scaleFactor': 30, 'timeStep': 22, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.123565 , 15.383784 ,  0.5207364], dtype=float32)}
episode index:186
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 25.       ,  0.4538253], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.43920536883150046
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.485111 , 13.0707   ,  3.9252934], dtype=float32)}
episode index:187
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.638052, 25.033024,  4.800998], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.44153683507707203
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.514938 , 14.109773 ,  2.3877282], dtype=float32)}
episode index:188
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([5.999998 , 4.0025425, 0.2854439], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.44408290840697073
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.345283 , 14.551697 ,  1.3253924], dtype=float32)}
episode index:189
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 12.       ,  1.6364044], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.44414883914032305
{'scaleFactor': 30, 'timeStep': 79, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.363923, 15.134206,  0.180484], dtype=float32)}
episode index:190
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.      , 28.      ,  0.831542], dtype=float32)}
done in step count: 254
reward sum = 0.07786448720191184
running average episode reward sum: 0.44223112002022663
{'scaleFactor': 30, 'timeStep': 255, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.459187, 16.120382,  5.764064], dtype=float32)}
episode index:191
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.      , 14.      ,  4.245083], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.44362863099549776
{'scaleFactor': 30, 'timeStep': 35, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.171492 , 13.071723 ,  2.6542084], dtype=float32)}
episode index:192
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.904442 , 25.002285 ,  4.9263062], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4446932631062382
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.724686 , 15.050504 ,  5.3058386], dtype=float32)}
episode index:193
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.183252, 14.836319,  2.348123], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.44697002397536134
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.815934 , 16.691038 ,  2.5443406], dtype=float32)}
episode index:194
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 16.       ,  1.5816334], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.44883032061251976
{'scaleFactor': 30, 'timeStep': 22, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.150649 , 15.730419 ,  3.9507833], dtype=float32)}
episode index:195
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.850752 , 12.005576 ,  4.9278164], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.44835241497013484
{'scaleFactor': 30, 'timeStep': 104, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.525949, 14.00913 ,  2.04014 ], dtype=float32)}
episode index:196
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.        , 24.        ,  0.08889778], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.447825814809614
{'scaleFactor': 30, 'timeStep': 107, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.734317 , 16.970936 ,  5.4794135], dtype=float32)}
episode index:197
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 29.       ,  6.0863304], dtype=float32)}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.44650685249600836
{'scaleFactor': 30, 'timeStep': 168, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.538353, 13.938263,  5.652738], dtype=float32)}
episode index:198
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.        , 21.        ,  0.95897365], dtype=float32)}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.4452494889860939
{'scaleFactor': 30, 'timeStep': 163, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.296385 , 13.526545 ,  3.4505837], dtype=float32)}
episode index:199
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       , 10.       ,  4.0239325], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4475451519162074
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.413181 , 13.049977 ,  2.6355345], dtype=float32)}
episode index:200
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 10.       ,  5.8324194], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.44842067551309794
{'scaleFactor': 30, 'timeStep': 48, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.18739  , 13.770781 ,  1.0681176], dtype=float32)}
episode index:201
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 18.       ,  2.8899808], dtype=float32)}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.44774364781971204
{'scaleFactor': 30, 'timeStep': 117, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.776182 , 16.69963  ,  5.4537745], dtype=float32)}
episode index:202
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.      , 29.      ,  5.551948], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4483158721676562
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.5307255, 15.991099 ,  4.8309994], dtype=float32)}
episode index:203
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.      , 24.      ,  6.038977], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.450596271066264
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.52062  , 16.20567  ,  4.8462234], dtype=float32)}
episode index:204
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 18.       ,  4.3166513], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4523481325158006
{'scaleFactor': 30, 'timeStep': 22, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.550347 , 13.212575 ,  5.0212994], dtype=float32)}
episode index:205
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.7819967, 9.840783 , 1.4331446], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.45291732217580716
{'scaleFactor': 30, 'timeStep': 57, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.960679 , 16.551126 ,  5.9607525], dtype=float32)}
episode index:206
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       , 11.       ,  2.6325366], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4550983113199279
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.085821 , 16.423286 ,  0.3109886], dtype=float32)}
episode index:207
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.      , 22.      ,  4.068358], dtype=float32)}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.4546701095408572
{'scaleFactor': 30, 'timeStep': 101, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.721409 , 13.591889 ,  5.7146015], dtype=float32)}
episode index:208
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.       , 22.       ,  3.1774242], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.45576046593066355
{'scaleFactor': 30, 'timeStep': 39, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.940447, 16.146225,  3.64119 ], dtype=float32)}
episode index:209
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.6391325, 26.032825 ,  4.297407 ], dtype=float32)}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.4549459292989613
{'scaleFactor': 30, 'timeStep': 126, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.02529  , 14.055889 ,  2.1870954], dtype=float32)}
episode index:210
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.489511 ,  5.3108873,  2.6846898], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4552558562816691
{'scaleFactor': 30, 'timeStep': 66, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.271799 , 13.625264 ,  1.5846478], dtype=float32)}
episode index:211
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 25.       ,  1.7036157], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.4549058150638525
{'scaleFactor': 30, 'timeStep': 97, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.148003 , 13.034893 ,  3.9015384], dtype=float32)}
episode index:212
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.131699 , 12.198319 ,  4.0768065], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4571902016100363
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.112028, 13.849601,  4.138409], dtype=float32)}
episode index:213
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.68904   ,  7.0710473 ,  0.20913199], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4592798832614324
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.574807  , 14.296335  ,  0.30949038], dtype=float32)}
episode index:214
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 19.       ,  2.0394912], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.46083492698414036
{'scaleFactor': 30, 'timeStep': 24, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.398504, 13.444484,  4.603433], dtype=float32)}
episode index:215
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.        , 23.        ,  0.44422454], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4613384745558673
{'scaleFactor': 30, 'timeStep': 57, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.189938 , 16.657042 ,  5.2286797], dtype=float32)}
episode index:216
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.        , 26.        ,  0.21163142], dtype=float32)}
done in step count: 325
reward sum = 0.03814505489267701
running average episode reward sum: 0.45938827446525354
{'scaleFactor': 30, 'timeStep': 326, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.927734, 13.693494,  3.148918], dtype=float32)}
episode index:217
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.      , 11.      ,  5.034556], dtype=float32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4598938383552164
{'scaleFactor': 30, 'timeStep': 57, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.361645 , 15.837596 ,  2.0671177], dtype=float32)}
episode index:218
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.949666  , 19.554129  ,  0.02698868], dtype=float32)}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.45863772294696675
{'scaleFactor': 30, 'timeStep': 169, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.760248, 16.20716 ,  4.570792], dtype=float32)}
episode index:219
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      , 23.      ,  2.868442], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4598483711879253
{'scaleFactor': 30, 'timeStep': 33, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.435315 , 16.053608 ,  5.8843374], dtype=float32)}
episode index:220
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.472152 , 14.709388 ,  3.0145154], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.46128716298978606
{'scaleFactor': 30, 'timeStep': 26, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.716137, 15.909023,  4.893294], dtype=float32)}
episode index:221
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.      ,  3.      ,  4.225538], dtype=float32)}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.46013905656693754
{'scaleFactor': 30, 'timeStep': 158, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.57063   , 13.549242  ,  0.77796173], dtype=float32)}
episode index:222
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       ,  3.       ,  1.0557988], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.46174339684061594
{'scaleFactor': 30, 'timeStep': 21, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.94827  , 15.489579 ,  1.4678402], dtype=float32)}
episode index:223
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.      , 19.      ,  4.978468], dtype=float32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4626685234554779
{'scaleFactor': 30, 'timeStep': 41, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.55028  , 13.690202 ,  2.5470676], dtype=float32)}
episode index:224
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.007391,  7.171783,  3.85173 ], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.46370738876478496
{'scaleFactor': 30, 'timeStep': 37, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.743063 , 13.418234 ,  1.8979145], dtype=float32)}
episode index:225
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.96329  ,  5.752733 ,  0.6056291], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.462865715146455
{'scaleFactor': 30, 'timeStep': 130, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.003328 , 13.196166 ,  1.5326933], dtype=float32)}
episode index:226
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.        , 29.        ,  0.14022195], dtype=float32)}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.4614349446528031
{'scaleFactor': 30, 'timeStep': 198, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.508671, 16.981037,  4.840796], dtype=float32)}
episode index:227
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.853813 , 11.638981 ,  1.2595565], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4616033748512522
{'scaleFactor': 30, 'timeStep': 70, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.631054 , 13.532652 ,  5.3878922], dtype=float32)}
episode index:228
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.      , 9.      , 5.094026], dtype=float32)}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.46097623812185434
{'scaleFactor': 30, 'timeStep': 115, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.299018 , 13.816961 ,  5.7743454], dtype=float32)}
episode index:229
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.       , 8.       , 3.9123242], dtype=float32)}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.4602867955024942
{'scaleFactor': 30, 'timeStep': 120, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.472733, 16.921957,  5.372409], dtype=float32)}
episode index:230
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.      , 24.      ,  6.104081], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4623698836146089
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.87019 , 15.291134,  4.654554], dtype=float32)}
episode index:231
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.826641, 24.992472,  2.035655], dtype=float32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.4614323638534896
{'scaleFactor': 30, 'timeStep': 141, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.885229 , 15.548657 ,  5.4689245], dtype=float32)}
episode index:232
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 20.       ,  3.7995527], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.46306974938795503
{'scaleFactor': 30, 'timeStep': 18, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.016403 , 13.26122  ,  0.9813336], dtype=float32)}
episode index:233
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.      , 22.      ,  4.316639], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4645165905851819
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.89533  , 13.333297 ,  3.6233106], dtype=float32)}
episode index:234
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.641045 , 15.032476 ,  4.2365704], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4667952433912024
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.641045 , 15.032476 ,  4.2365704], dtype=float32)}
episode index:235
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.13889  , 16.267694 ,  3.7945676], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4690545855802227
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.13889  , 16.267694 ,  3.7945676], dtype=float32)}
episode index:236
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.575064 ,  6.767452 ,  5.8146634], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.46984194690931097
{'scaleFactor': 30, 'timeStep': 43, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.931368 , 15.535117 ,  3.3913007], dtype=float32)}
episode index:237
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 13.       ,  3.7887642], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4705141388601258
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.99027  , 14.416922 ,  1.1107372], dtype=float32)}
episode index:238
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.915056, 16.22161 ,  5.88591 ], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.47264629727493696
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.485835, 14.438313,  5.570843], dtype=float32)}
episode index:239
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 11.       ,  5.6477113], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4744832595674732
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.733711 , 14.487606 ,  2.7589955], dtype=float32)}
episode index:240
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       ,  5.       ,  3.7957664], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.4744090956179978
{'scaleFactor': 30, 'timeStep': 79, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.268293 , 13.238636 ,  1.8681831], dtype=float32)}
episode index:241
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 10.       ,  4.8980513], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4741551310540139
{'scaleFactor': 30, 'timeStep': 89, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.918507 , 15.874448 ,  2.4036908], dtype=float32)}
episode index:242
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       ,  3.       ,  2.5410151], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.47603953523859405
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.816441 , 13.948425 ,  3.0942519], dtype=float32)}
episode index:243
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.        ,  6.        ,  0.24743193], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.47708979685505226
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.536067 , 13.925214 ,  2.4544208], dtype=float32)}
episode index:244
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.5776691, 27.085241 ,  4.5728254], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4788711333882302
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.180936, 16.860003,  5.551728], dtype=float32)}
episode index:245
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.       , 29.       ,  0.6376351], dtype=float32)}
done in step count: 364
reward sum = 0.025775721668980935
running average episode reward sum: 0.47702928212107876
{'scaleFactor': 30, 'timeStep': 365, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.52013  , 13.071861 ,  1.0311813], dtype=float32)}
episode index:246
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 10.       ,  3.7512355], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.47653588751615567
{'scaleFactor': 30, 'timeStep': 104, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.265604 , 13.586082 ,  3.7224624], dtype=float32)}
episode index:247
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.9862857, 18.766186 ,  6.2278333], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4782979091289278
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.5829315, 15.131867 ,  5.2103176], dtype=float32)}
episode index:248
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 25.       ,  1.1193666], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4795008145516997
{'scaleFactor': 30, 'timeStep': 26, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.888119, 16.806168,  4.607082], dtype=float32)}
episode index:249
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 17.       ,  2.8877783], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4815032112934929
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.38663 , 16.41308 ,  4.121584], dtype=float32)}
episode index:250
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.      , 12.      ,  4.683689], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.4808905401625937
{'scaleFactor': 30, 'timeStep': 112, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.476679 , 16.942732 ,  6.2021856], dtype=float32)}
episode index:251
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 20.       ,  1.8429428], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.4800352850418997
{'scaleFactor': 30, 'timeStep': 133, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.412397 , 16.694191 ,  4.9561777], dtype=float32)}
episode index:252
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.031176, 27.648243,  3.224733], dtype=float32)}
done in step count: 207
reward sum = 0.12487781225895148
running average episode reward sum: 0.47863150056449677
{'scaleFactor': 30, 'timeStep': 208, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.535591, 15.4317  ,  5.160494], dtype=float32)}
episode index:253
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 23.       ,  3.5368207], dtype=float32)}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.47780242661312816
{'scaleFactor': 30, 'timeStep': 132, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.331    , 14.010775 ,  0.6288397], dtype=float32)}
episode index:254
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 29.       ,  1.3123744], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.47844873710517355
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.434816 , 14.711456 ,  3.3056693], dtype=float32)}
episode index:255
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 15.       ,  3.5224764], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.4771643334664956
{'scaleFactor': 30, 'timeStep': 190, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.89046  , 13.012053 ,  1.0673963], dtype=float32)}
episode index:256
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 11.       ,  4.7753773], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.4771572350561424
{'scaleFactor': 30, 'timeStep': 75, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.16334 , 13.525509,  2.748394], dtype=float32)}
episode index:257
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.       , 11.       ,  1.7161884], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.47906863724584725
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.542324 , 15.411209 ,  2.4540253], dtype=float32)}
episode index:258
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 22.       ,  3.1270282], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.47779671743255675
{'scaleFactor': 30, 'timeStep': 190, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.629005, 16.320574,  5.082324], dtype=float32)}
episode index:259
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.511799 , 24.690624 ,  4.6481366], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.479543904472843
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.037666, 15.467476,  4.704957], dtype=float32)}
episode index:260
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.        , 28.        ,  0.30835122], dtype=float32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.47991099583492763
{'scaleFactor': 30, 'timeStep': 56, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.392122, 16.12775 ,  3.762576], dtype=float32)}
episode index:261
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.       , 8.       , 2.9153538], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.48000631985382897
{'scaleFactor': 30, 'timeStep': 69, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.409687  , 14.896637  ,  0.39113468], dtype=float32)}
episode index:262
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 20.       ,  1.1100849], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.48008172179316494
{'scaleFactor': 30, 'timeStep': 70, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.603969, 15.281432,  5.112314], dtype=float32)}
episode index:263
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       ,  5.       ,  3.4649327], dtype=float32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.4808228101540252
{'scaleFactor': 30, 'timeStep': 40, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.426718  , 16.911081  ,  0.84197474], dtype=float32)}
episode index:264
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      , 13.      ,  4.062379], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.48259702615306665
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.095934 , 14.784002 ,  1.8311971], dtype=float32)}
episode index:265
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 27.       ,  3.2548845], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.48260602744675496
{'scaleFactor': 30, 'timeStep': 73, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.030893  , 14.349699  ,  0.68857545], dtype=float32)}
episode index:266
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.      , 24.      ,  1.561857], dtype=float32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.48265184254845334
{'scaleFactor': 30, 'timeStep': 71, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.017747 , 13.8643675,  5.2021546], dtype=float32)}
episode index:267
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       ,  4.       ,  0.1743811], dtype=float32)}
done in step count: 216
reward sum = 0.1140780353265762
running average episode reward sum: 0.4812765671483717
{'scaleFactor': 30, 'timeStep': 217, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.397398 , 16.0639   ,  3.7766743], dtype=float32)}
episode index:268
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.21655  ,  9.094843 ,  3.0766525], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4827495948652884
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.055117 , 14.339391 ,  1.8227516], dtype=float32)}
episode index:269
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.552125 , 4.0777206, 4.4012446], dtype=float32)}
done in step count: 251
reward sum = 0.08024793100055946
running average episode reward sum: 0.48125884796208573
{'scaleFactor': 30, 'timeStep': 252, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.357405 , 14.1538725,  1.8767829], dtype=float32)}
episode index:270
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.       ,  6.       ,  5.6442304], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4820787330065045
{'scaleFactor': 30, 'timeStep': 36, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.104305 , 15.592599 ,  3.5306804], dtype=float32)}
episode index:271
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.        ,  5.        ,  0.45061558], dtype=float32)}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.4811539428789751
{'scaleFactor': 30, 'timeStep': 147, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.420225  , 13.089879  ,  0.28158435], dtype=float32)}
episode index:272
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.379988 ,  9.901469 ,  1.8019036], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4829456830149496
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.321615, 13.18241 ,  3.261978], dtype=float32)}
episode index:273
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.       ,  7.       ,  3.9466648], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.48419832148509795
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.055172 , 13.015369 ,  1.1061615], dtype=float32)}
episode index:274
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.941702 ,  3.7644255,  2.061236 ], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.48550284829200274
{'scaleFactor': 30, 'timeStep': 18, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.229813 , 14.320342 ,  1.7447369], dtype=float32)}
episode index:275
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.      , 19.      ,  3.343643], dtype=float32)}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.48516664604586435
{'scaleFactor': 30, 'timeStep': 94, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.66412  , 16.533943 ,  5.2954574], dtype=float32)}
episode index:276
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.365578 , 24.033695 ,  4.9388685], dtype=float32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.48501459310572104
{'scaleFactor': 30, 'timeStep': 82, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.266924 , 13.089015 ,  3.4789073], dtype=float32)}
episode index:277
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.9674165, 30.750458 ,  1.2832495], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.48558148882147284
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.287545 , 13.04398  ,  5.4462986], dtype=float32)}
episode index:278
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.879284, 26.003647,  4.70694 ], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.48721553419989405
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.265892, 15.926958,  3.704725], dtype=float32)}
episode index:279
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       ,  9.       ,  2.8967433], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4883384451118197
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.142552 , 13.121039 ,  1.1784252], dtype=float32)}
episode index:280
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.312694, 19.491081,  4.765192], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.48972343649220096
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.957138  , 15.547731  ,  0.32295504], dtype=float32)}
episode index:281
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.       , 5.       , 5.1332927], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4902886109314783
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.824831 , 14.794042 ,  2.5372763], dtype=float32)}
episode index:282
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 28.       ,  6.1950407], dtype=float32)}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.48933083233937574
{'scaleFactor': 30, 'timeStep': 152, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.349846 , 16.322395 ,  5.3822637], dtype=float32)}
episode index:283
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([6.      , 4.      , 3.396204], dtype=float32)}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.48782984013125164
{'scaleFactor': 30, 'timeStep': 276, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.300609 , 14.557405 ,  1.8763766], dtype=float32)}
episode index:284
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 22.       ,  2.9807863], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4887135963882938
{'scaleFactor': 30, 'timeStep': 31, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.093274 , 16.026608 ,  5.9196806], dtype=float32)}
episode index:285
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.      , 26.      ,  0.687459], dtype=float32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.48893726730009135
{'scaleFactor': 30, 'timeStep': 60, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.587004, 16.45324 ,  5.953868], dtype=float32)}
episode index:286
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.1943917, 12.99053  ,  1.0539606], dtype=float32)}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.4877656382581953
{'scaleFactor': 30, 'timeStep': 188, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.720756  , 16.910328  ,  0.42800856], dtype=float32)}
episode index:287
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.750707 , 13.438185 ,  3.9549055], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.489544229792021
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.750707 , 13.438185 ,  3.9549055], dtype=float32)}
episode index:288
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 28.       ,  1.9827905], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.4888758642474092
{'scaleFactor': 30, 'timeStep': 122, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.666066 , 13.069036 ,  3.4423764], dtype=float32)}
episode index:289
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.8167067, 12.836408 ,  1.4029051], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.488966420292845
{'scaleFactor': 30, 'timeStep': 67, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.40278   , 13.24728   ,  0.89571214], dtype=float32)}
episode index:290
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.        , 26.        ,  0.05921988], dtype=float32)}
done in step count: 321
reward sum = 0.03970977861200674
running average episode reward sum: 0.48742258303620983
{'scaleFactor': 30, 'timeStep': 322, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.941273 , 15.878987 ,  3.8859556], dtype=float32)}
episode index:291
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 19.       ,  1.6226525], dtype=float32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4873488097215988
{'scaleFactor': 30, 'timeStep': 77, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.620227 , 16.583271 ,  5.4814467], dtype=float32)}
episode index:292
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.        , 11.        ,  0.11880063], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4884490795458296
{'scaleFactor': 30, 'timeStep': 22, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.125992 , 16.323761 ,  2.3566344], dtype=float32)}
episode index:293
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       ,  8.       ,  4.7674074], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.48816432748165445
{'scaleFactor': 30, 'timeStep': 91, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.6471  , 14.538083,  3.364205], dtype=float32)}
episode index:294
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 13.       ,  1.2185345], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4891997510618646
{'scaleFactor': 30, 'timeStep': 24, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.974165 , 14.9238205,  1.732157 ], dtype=float32)}
episode index:295
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.6686096, 21.11507  ,  5.4918427], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4898997965584447
{'scaleFactor': 30, 'timeStep': 37, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.54717 , 15.727123,  4.843721], dtype=float32)}
episode index:296
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.      , 19.      ,  5.965077], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.4904802081558371
{'scaleFactor': 30, 'timeStep': 42, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.867846 , 15.035001 ,  3.3416882], dtype=float32)}
episode index:297
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.498039 ,  5.320649 ,  3.2330878], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.491869140594941
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.019316 , 14.048757 ,  1.6097233], dtype=float32)}
episode index:298
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.189039, 23.171793,  4.019872], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.49340466203074396
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.285803, 16.309921,  5.062946], dtype=float32)}
episode index:299
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.      ,  5.      ,  4.553247], dtype=float32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4935296649671585
{'scaleFactor': 30, 'timeStep': 64, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.86296  , 15.171534 ,  1.7602063], dtype=float32)}
episode index:300
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 14.       ,  5.5825224], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.49463477778731946
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.887194 , 16.322662 ,  1.6887181], dtype=float32)}
episode index:301
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.485756 ,  4.338854 ,  0.5158024], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.49539751142364574
{'scaleFactor': 30, 'timeStep': 33, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.684715 , 13.127731 ,  2.0483985], dtype=float32)}
episode index:302
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 13.       ,  1.4733732], dtype=float32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.4964081816484491
{'scaleFactor': 30, 'timeStep': 23, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.147764, 16.487518,  3.730153], dtype=float32)}
episode index:303
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 19.       ,  2.1065407], dtype=float32)}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.4959672732149361
{'scaleFactor': 30, 'timeStep': 102, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.321613 , 16.760971 ,  1.2310477], dtype=float32)}
episode index:304
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([7.      , 9.      , 2.510448], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.49724733091494006
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.595689 , 15.919986 ,  6.1003656], dtype=float32)}
episode index:305
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 15.       ,  1.0858122], dtype=float32)}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.495942972864119
{'scaleFactor': 30, 'timeStep': 232, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.7221   , 13.661859 ,  3.7993073], dtype=float32)}
episode index:306
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.      , 22.      ,  2.043856], dtype=float32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4969650735004614
{'scaleFactor': 30, 'timeStep': 22, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.3060465, 14.293247 ,  5.349345 ], dtype=float32)}
episode index:307
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.       , 21.       ,  3.8558593], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.49772915887758457
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.040077, 15.174929,  5.109966], dtype=float32)}
episode index:308
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.        , 28.        ,  0.07483605], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.49895825876147254
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.202629, 15.719759,  5.381923], dtype=float32)}
episode index:309
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      , 18.      ,  0.288462], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.498654302999914
{'scaleFactor': 30, 'timeStep': 91, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.769229 , 16.907787 ,  4.7807975], dtype=float32)}
episode index:310
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 10.       ,  1.9711185], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.49947766307773944
{'scaleFactor': 30, 'timeStep': 29, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.505095  , 16.755781  ,  0.48497102], dtype=float32)}
episode index:311
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 26.       ,  1.1214832], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.499875251961757
{'scaleFactor': 30, 'timeStep': 48, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.963354 , 16.833797 ,  4.5072427], dtype=float32)}
episode index:312
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 26.       ,  5.6803546], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5010537521560293
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.866072 , 15.19706  ,  4.2100153], dtype=float32)}
episode index:313
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 10.       ,  2.3305879], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5012539159722597
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.735137  , 14.089231  ,  0.06737345], dtype=float32)}
episode index:314
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 20.       ,  4.2500615], dtype=float32)}
done in step count: 336
reward sum = 0.03415272685621234
running average episode reward sum: 0.499771055054431
{'scaleFactor': 30, 'timeStep': 337, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.890776 , 13.597357 ,  1.7474859], dtype=float32)}
episode index:315
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.9788685, 26.710037 ,  6.2099113], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4995228372287991
{'scaleFactor': 30, 'timeStep': 87, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.010906, 13.036502,  3.980628], dtype=float32)}
episode index:316
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.285194 , 10.029292 ,  2.5783255], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5010079355340711
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.551442 , 13.2895565,  2.7403786], dtype=float32)}
episode index:317
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 22.       ,  3.3539004], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5017820366623652
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.265491, 15.111985,  5.344422], dtype=float32)}
episode index:318
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 10.       ,  3.8654864], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5029323933272762
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.361574 , 14.720034 ,  1.7937704], dtype=float32)}
episode index:319
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 23.       ,  1.9898885], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5023848632151217
{'scaleFactor': 30, 'timeStep': 112, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.664177, 15.660398,  4.688195], dtype=float32)}
episode index:320
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([6.382502 , 7.4452295, 0.843076 ], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5016890913315389
{'scaleFactor': 30, 'timeStep': 128, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.52178 , 16.927147,  5.585643], dtype=float32)}
episode index:321
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.508205 , 12.313513 ,  1.2157184], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5030844359233665
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.8685665 , 14.006885  ,  0.41079932], dtype=float32)}
episode index:322
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.658959, 16.029291,  4.461258], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5046228742022416
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.658959, 16.029291,  4.461258], dtype=float32)}
episode index:323
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.       ,  9.       ,  4.0451117], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5058287858690826
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.812078 , 14.109782 ,  1.5881956], dtype=float32)}
episode index:324
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 29.       ,  2.2439098], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5060074824985696
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.180521 , 15.363388 ,  4.8842015], dtype=float32)}
episode index:325
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.      , 25.      ,  6.236058], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5070935281186394
{'scaleFactor': 30, 'timeStep': 16, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.419212, 16.807545,  3.094573], dtype=float32)}
episode index:326
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.      , 24.      ,  2.718952], dtype=float32)}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5063543009676579
{'scaleFactor': 30, 'timeStep': 133, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.288006, 15.973457,  4.893517], dtype=float32)}
episode index:327
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.936302 , 24.306322 ,  4.4318776], dtype=float32)}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.506121175091305
{'scaleFactor': 30, 'timeStep': 85, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.574373, 16.289803,  3.465816], dtype=float32)}
episode index:328
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 13.       ,  5.0391455], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5072233776374379
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.89349  , 15.428546 ,  1.9458625], dtype=float32)}
episode index:329
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      , 23.      ,  4.437237], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5084268888415936
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.964989, 15.776183,  5.745707], dtype=float32)}
episode index:330
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       ,  6.       ,  4.0640078], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5077953284260973
{'scaleFactor': 30, 'timeStep': 121, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.022553, 14.144011,  2.027332], dtype=float32)}
episode index:331
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([1.7000000e+01, 2.9000000e+01, 3.8025826e-03], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5074135567082614
{'scaleFactor': 30, 'timeStep': 97, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.384203, 16.597225,  4.645142], dtype=float32)}
episode index:332
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 25.       ,  1.0828563], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5076704722929258
{'scaleFactor': 30, 'timeStep': 53, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.843645 , 13.024363 ,  5.0264425], dtype=float32)}
episode index:333
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 24.       ,  4.1307983], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5068690411770312
{'scaleFactor': 30, 'timeStep': 143, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.48803  , 15.933831 ,  5.5168037], dtype=float32)}
episode index:334
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 10.       ,  1.2442923], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5077975125096289
{'scaleFactor': 30, 'timeStep': 21, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.205043 , 15.825698 ,  2.7851093], dtype=float32)}
episode index:335
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.        , 23.        ,  0.75585335], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5077442171539579
{'scaleFactor': 30, 'timeStep': 72, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.171297 , 16.609404 ,  4.6659365], dtype=float32)}
episode index:336
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 15.       ,  3.0445251], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5072297739484266
{'scaleFactor': 30, 'timeStep': 110, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.474093  , 14.176981  ,  0.88038623], dtype=float32)}
episode index:337
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 29.       ,  1.1346369], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5081733800131816
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.042351, 13.693581,  5.240358], dtype=float32)}
episode index:338
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.      , 14.      ,  3.803264], dtype=float32)}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5079682004314611
{'scaleFactor': 30, 'timeStep': 83, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.798674 , 14.062531 ,  1.0651186], dtype=float32)}
episode index:339
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       ,  8.       ,  5.4240427], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5090551204978361
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.499554 , 13.346946 ,  1.7032033], dtype=float32)}
episode index:340
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.900295 , 13.670523 ,  2.6177006], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5101879156115043
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.536913, 16.27826 ,  2.294029], dtype=float32)}
episode index:341
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       ,  6.       ,  5.4359217], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5108590046693311
{'scaleFactor': 30, 'timeStep': 31, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.526819 , 14.8483   ,  1.9916664], dtype=float32)}
episode index:342
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.481322 , 19.656242 ,  5.2726517], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5121984798743767
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.235562 , 14.5687685,  4.9938   ], dtype=float32)}
episode index:343
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       , 13.       ,  6.2671194], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5133385484648838
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.679735 , 15.243465 ,  3.3688667], dtype=float32)}
episode index:344
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.121384 , 21.686152 ,  2.7665884], dtype=float32)}
done in step count: 260
reward sum = 0.07330786904388821
running average episode reward sum: 0.5120630972201853
{'scaleFactor': 30, 'timeStep': 261, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.345715 , 13.363186 ,  3.5208194], dtype=float32)}
episode index:345
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.774308, 26.15597 ,  4.900634], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5132233693307733
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.184484 , 15.966542 ,  4.6514225], dtype=float32)}
episode index:346
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([4.600354 , 5.907767 , 1.5822576], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5135054842304911
{'scaleFactor': 30, 'timeStep': 50, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.665415, 14.578896,  1.525247], dtype=float32)}
episode index:347
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.902668, 18.002369,  4.742386], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5138951311964047
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.80545  , 14.30992  ,  1.8556081], dtype=float32)}
episode index:348
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.     , 13.     ,  4.08905], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5142273045488599
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.393574 , 14.191384 ,  2.7425487], dtype=float32)}
episode index:349
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.403008, 25.041025,  4.428965], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5147082396644642
{'scaleFactor': 30, 'timeStep': 39, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.268192 , 15.941211 ,  5.9377055], dtype=float32)}
episode index:350
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.618336 , 5.175155 , 1.0749314], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5154137509883464
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.334108, 13.281923,  6.219228], dtype=float32)}
episode index:351
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.        , 24.        ,  0.96189034], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5160935962616853
{'scaleFactor': 30, 'timeStep': 29, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.23799 , 14.36508 ,  3.865107], dtype=float32)}
episode index:352
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.441677  , 11.386207  ,  0.92518044], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.517408061994655
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.152257 , 14.918284 ,  1.3958087], dtype=float32)}
episode index:353
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.924589  , 23.54402   ,  0.03277554], dtype=float32)}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5167588403770624
{'scaleFactor': 30, 'timeStep': 125, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.262019 , 13.074413 ,  4.7989116], dtype=float32)}
episode index:354
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.738466  ,  4.9888062 ,  0.99702895], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5161465067177252
{'scaleFactor': 30, 'timeStep': 121, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.422282, 16.04193 ,  4.80474 ], dtype=float32)}
episode index:355
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 16.       ,  4.7510824], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5154804830712852
{'scaleFactor': 30, 'timeStep': 128, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.003731 , 14.125191 ,  1.3811698], dtype=float32)}
episode index:356
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 15.       ,  5.0770698], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5158547747948066
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.11399  , 14.776013 ,  4.8852563], dtype=float32)}
episode index:357
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 28.       ,  4.1310167], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5169147845139795
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.590977 , 14.859586 ,  3.5498486], dtype=float32)}
episode index:358
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.610976 , 17.561037 ,  4.4531536], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5182325706295394
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.098293 , 15.627865 ,  5.3677397], dtype=float32)}
episode index:359
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.        , 24.        ,  0.11299789], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5182530259542506
{'scaleFactor': 30, 'timeStep': 65, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.895551 , 14.454077 ,  6.2547126], dtype=float32)}
episode index:360
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.      , 22.      ,  5.043023], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5193735014901888
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.021803 , 13.927255 ,  2.6702218], dtype=float32)}
episode index:361
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.740532 , 20.142149 ,  5.2305417], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5197318692439961
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.64656 , 13.405006,  4.562132], dtype=float32)}
episode index:362
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.7169   ,  4.465838 ,  4.2753816], dtype=float32)}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.5187082139335376
{'scaleFactor': 30, 'timeStep': 191, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.459471 , 15.776456 ,  1.2832308], dtype=float32)}
episode index:363
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 29.       ,  1.2795887], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5185890706040656
{'scaleFactor': 30, 'timeStep': 75, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.620372 , 13.169031 ,  6.2439814], dtype=float32)}
episode index:364
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.      , 15.      ,  2.676282], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5189288583615468
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.540951 , 13.445274 ,  0.0714423], dtype=float32)}
episode index:365
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.        , 23.        ,  0.03351644], dtype=float32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5197911121951221
{'scaleFactor': 30, 'timeStep': 19, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.665735 , 15.184517 ,  3.4392042], dtype=float32)}
episode index:366
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        ,  7.        ,  0.84898955], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.519631523244776
{'scaleFactor': 30, 'timeStep': 78, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.856817 , 16.697863 ,  2.0477376], dtype=float32)}
episode index:367
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([5.      , 6.      , 4.637632], dtype=float32)}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.5184442210956773
{'scaleFactor': 30, 'timeStep': 249, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.27753  , 15.763166 ,  3.3763907], dtype=float32)}
episode index:368
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       , 16.       ,  1.6099696], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5196424644260413
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.967502  , 16.35908   ,  0.04075943], dtype=float32)}
episode index:369
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.99283  , 11.999987 ,  1.6583154], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5209137010086737
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.818016 , 13.992332 ,  1.1523793], dtype=float32)}
episode index:370
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       ,  6.       ,  6.0466166], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5218046553754829
{'scaleFactor': 30, 'timeStep': 17, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.14556 , 13.785864,  1.48226 ], dtype=float32)}
episode index:371
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.    , 12.    ,  2.3431], dtype=float32)}
done in step count: 193
reward sum = 0.1437449371536248
running average episode reward sum: 0.5207883658103704
{'scaleFactor': 30, 'timeStep': 194, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.862005, 16.108847,  4.460497], dtype=float32)}
episode index:372
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.859848 ,  6.80573  ,  0.8409021], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.520434466486681
{'scaleFactor': 30, 'timeStep': 95, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.97665  , 15.810274 ,  2.1581826], dtype=float32)}
episode index:373
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      , 28.      ,  3.143313], dtype=float32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5209428054192629
{'scaleFactor': 30, 'timeStep': 35, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.675121 , 14.171469 ,  5.4492574], dtype=float32)}
episode index:374
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.8781965, 15.655765 ,  2.3828876], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5213737701915058
{'scaleFactor': 30, 'timeStep': 39, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.173083, 16.491274,  4.475498], dtype=float32)}
episode index:375
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.       , 10.       ,  1.2944319], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5225419144463156
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.469744 , 15.709944 ,  1.8026439], dtype=float32)}
episode index:376
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 23.       ,  2.2443204], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5223670280624896
{'scaleFactor': 30, 'timeStep': 79, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.550775, 15.909794,  4.311859], dtype=float32)}
episode index:377
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      , 11.      ,  6.114846], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5229224152095582
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.916082 , 16.34918  ,  1.5197091], dtype=float32)}
episode index:378
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.60674  , 19.565138 ,  4.6509104], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5239289050771023
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.494606 , 13.021463 ,  5.7005143], dtype=float32)}
episode index:379
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       ,  9.       ,  2.7184112], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5239194619654528
{'scaleFactor': 30, 'timeStep': 66, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.953263 , 15.929931 ,  3.6758478], dtype=float32)}
episode index:380
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.997751, 14.905187,  6.111279], dtype=float32)}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5251690171833913
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.997751, 14.905187,  6.111279], dtype=float32)}
episode index:381
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.       , 24.       ,  1.2560382], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5260456908416581
{'scaleFactor': 30, 'timeStep': 16, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.386056  , 14.586936  ,  0.39432973], dtype=float32)}
episode index:382
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 27.       ,  2.8652575], dtype=float32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5266035359658007
{'scaleFactor': 30, 'timeStep': 31, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.303055, 16.988186,  4.718253], dtype=float32)}
episode index:383
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.       ,  3.       ,  3.8095098], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5260770828249091
{'scaleFactor': 30, 'timeStep': 113, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.579938 , 16.384064 ,  1.1728851], dtype=float32)}
episode index:384
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       , 24.       ,  3.9232507], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5256424626037584
{'scaleFactor': 30, 'timeStep': 103, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.908009  , 13.4318495 ,  0.12383669], dtype=float32)}
episode index:385
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([8.       , 7.       , 1.7413808], dtype=float32)}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5254517921054753
{'scaleFactor': 30, 'timeStep': 80, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.1473675, 13.144636 ,  6.2159038], dtype=float32)}
episode index:386
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.        , 19.        ,  0.91539514], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5263388567583526
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.810733, 15.838121,  4.379879], dtype=float32)}
episode index:387
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.128613 ,  5.99586  ,  2.4203756], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5273605212884288
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.640504 , 13.109686 ,  2.8092315], dtype=float32)}
episode index:388
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.248758 , 12.853547 ,  1.9754373], dtype=float32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.527383420270225
{'scaleFactor': 30, 'timeStep': 63, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.467842, 16.24283 ,  3.189005], dtype=float32)}
episode index:389
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.839205 , 18.371334 ,  3.2886145], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5282587084561192
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.777178, 16.368502,  4.843256], dtype=float32)}
episode index:390
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 18.       ,  3.1557586], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5283498759292554
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.861673, 16.791124,  4.328125], dtype=float32)}
episode index:391
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.976245 , 18.281883 ,  4.4928946], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.529527554817191
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.540772, 16.329868,  4.837538], dtype=float32)}
episode index:392
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.000023 , 10.990372 ,  3.0208654], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5301199598032723
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.333796 , 13.543837 ,  2.4157436], dtype=float32)}
episode index:393
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.748827, 23.145477,  4.84064 ], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5309794162828807
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.829145 , 14.944835 ,  3.0054584], dtype=float32)}
episode index:394
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.       , 18.       ,  1.1547644], dtype=float32)}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5305433881345238
{'scaleFactor': 30, 'timeStep': 103, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.118214 , 15.3482485,  4.222283 ], dtype=float32)}
episode index:395
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.       ,  6.       ,  5.2956395], dtype=float32)}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5301372928050168
{'scaleFactor': 30, 'timeStep': 100, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.738817 , 15.674634 ,  1.0963514], dtype=float32)}
episode index:396
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.      , 22.      ,  6.078681], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.531057194471147
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.745482 , 14.873001 ,  3.0878956], dtype=float32)}
episode index:397
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.061857, 13.493558,  2.003558], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5321608170981039
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.526669 , 15.286184 ,  2.3816092], dtype=float32)}
episode index:398
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       ,  4.       ,  3.3808186], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5329826154378111
{'scaleFactor': 30, 'timeStep': 16, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.073615 , 13.999424 ,  1.2701368], dtype=float32)}
episode index:399
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 20.       ,  1.8533999], dtype=float32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5329123736211843
{'scaleFactor': 30, 'timeStep': 69, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.311314 , 13.490848 ,  0.2900191], dtype=float32)}
episode index:400
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       , 23.       ,  0.7041475], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.533062134401185
{'scaleFactor': 30, 'timeStep': 53, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.10014 , 14.489141,  5.216134], dtype=float32)}
episode index:401
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 19.       ,  3.7643795], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5338971684269755
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.1985855, 14.074009 ,  5.369081 ], dtype=float32)}
episode index:402
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.      ,  9.      ,  3.675669], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5343894915069444
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.389744 , 13.012917 ,  6.1566973], dtype=float32)}
episode index:403
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       ,  8.       ,  3.7940671], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5342672189296356
{'scaleFactor': 30, 'timeStep': 73, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.755825 , 13.02897  ,  1.9097273], dtype=float32)}
episode index:404
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 24.       ,  3.5959482], dtype=float32)}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5336798593456098
{'scaleFactor': 30, 'timeStep': 122, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.23347   , 16.678616  ,  0.06561017], dtype=float32)}
episode index:405
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.      , 20.      ,  5.878258], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5339166666654562
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.13877  , 13.497236 ,  3.6880667], dtype=float32)}
episode index:406
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.        ,  5.        ,  0.23713686], dtype=float32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5341837303888451
{'scaleFactor': 30, 'timeStep': 45, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.178875 , 13.40239  ,  1.7917054], dtype=float32)}
episode index:407
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.886931 ,  6.7925825,  1.6095262], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5350037354927181
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.484463 , 15.20486  ,  5.7141366], dtype=float32)}
episode index:408
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 23.       ,  2.6719048], dtype=float32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5350334492015153
{'scaleFactor': 30, 'timeStep': 61, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.957314 , 15.415877 ,  5.0940704], dtype=float32)}
episode index:409
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([6.       , 6.       , 1.5790141], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5359790863849944
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.901073  , 13.817343  ,  0.06101659], dtype=float32)}
episode index:410
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       ,  4.       ,  3.1931243], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.536147035243273
{'scaleFactor': 30, 'timeStep': 51, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.453746 , 15.595924 ,  1.1149408], dtype=float32)}
episode index:411
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.920263 ,  7.9984097,  1.928941 ], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5365531048057882
{'scaleFactor': 30, 'timeStep': 36, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.448218 , 14.7580185,  1.2890775], dtype=float32)}
episode index:412
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.451862 ,  7.375534 ,  1.2542334], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5374881934005149
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.534737 , 14.508274 ,  1.4771547], dtype=float32)}
episode index:413
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.      ,  7.      ,  6.236305], dtype=float32)}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5367696047197991
{'scaleFactor': 30, 'timeStep': 143, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.533838 , 16.80312  ,  1.1131186], dtype=float32)}
episode index:414
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 28.       ,  2.4283204], dtype=float32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5372407704184367
{'scaleFactor': 30, 'timeStep': 32, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.23014  , 16.136028 ,  5.3911605], dtype=float32)}
episode index:415
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       , 21.       ,  1.7009795], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5377818808605729
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.152437 , 15.415246 ,  5.2145557], dtype=float32)}
episode index:416
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.526592 , 11.35243  ,  2.7592106], dtype=float32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.538080442395641
{'scaleFactor': 30, 'timeStep': 42, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.08325  , 13.848653 ,  3.2645872], dtype=float32)}
episode index:417
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 23.       ,  2.9936452], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.538871507875003
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.822731 , 16.27751  ,  5.6630793], dtype=float32)}
episode index:418
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.029375 , 21.658478 ,  3.5713053], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5394605451851038
{'scaleFactor': 30, 'timeStep': 25, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.188829 , 16.62246  ,  3.5944078], dtype=float32)}
episode index:419
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 29.       ,  0.0382455], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5397372086979348
{'scaleFactor': 30, 'timeStep': 43, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.433599, 13.119644,  5.737775], dtype=float32)}
episode index:420
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        ,  8.        ,  0.37041682], dtype=float32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5400125578947904
{'scaleFactor': 30, 'timeStep': 43, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.105086 , 13.130954 ,  3.2169309], dtype=float32)}
episode index:421
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.        , 24.        ,  0.63478255], dtype=float32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5399536113533899
{'scaleFactor': 30, 'timeStep': 67, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.08512  , 16.275326 ,  5.6284266], dtype=float32)}
episode index:422
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.       , 28.       ,  3.1253421], dtype=float32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.540323492220284
{'scaleFactor': 30, 'timeStep': 37, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.060101  , 15.846706  ,  0.62863284], dtype=float32)}
episode index:423
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.917168 , 25.001717 ,  5.3507767], dtype=float32)}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.539886787792182
{'scaleFactor': 30, 'timeStep': 104, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.850007 , 16.53641  ,  3.8198938], dtype=float32)}
episode index:424
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 22.       ,  4.5566444], dtype=float32)}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5393798671852909
{'scaleFactor': 30, 'timeStep': 113, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.522297 , 14.9949875,  2.9682813], dtype=float32)}
episode index:425
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.       , 6.       , 2.8727028], dtype=float32)}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5385180598214556
{'scaleFactor': 30, 'timeStep': 176, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.1946125, 13.8598585,  1.7834492], dtype=float32)}
episode index:426
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.      , 21.      ,  0.971354], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5383049571374798
{'scaleFactor': 30, 'timeStep': 81, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.316459 , 15.7579   ,  5.3681026], dtype=float32)}
episode index:427
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.      , 15.      ,  4.258908], dtype=float32)}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5374747037757313
{'scaleFactor': 30, 'timeStep': 170, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.343508 , 16.5912   ,  5.3639317], dtype=float32)}
episode index:428
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.616102 , 11.09726  ,  6.2278395], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5384164414112214
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.282682 , 14.008995 ,  1.0323044], dtype=float32)}
episode index:429
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 28.       ,  2.9771519], dtype=float32)}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5377571235490937
{'scaleFactor': 30, 'timeStep': 137, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.012354 , 16.866693 ,  3.0288312], dtype=float32)}
episode index:430
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      ,  6.      ,  4.641125], dtype=float32)}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5374390899746214
{'scaleFactor': 30, 'timeStep': 92, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.39272  , 16.257898 ,  1.5638378], dtype=float32)}
episode index:431
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.6425295, 16.141094 ,  0.8284339], dtype=float32)}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5368280947455649
{'scaleFactor': 30, 'timeStep': 130, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.473301  , 16.201048  ,  0.67979956], dtype=float32)}
episode index:432
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 25.       ,  6.1809015], dtype=float32)}
done in step count: 199
reward sum = 0.13533300490703204
running average episode reward sum: 0.5359008543533281
{'scaleFactor': 30, 'timeStep': 200, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.956536, 13.545917,  4.851669], dtype=float32)}
episode index:433
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 24.       ,  1.6545849], dtype=float32)}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5355709238786841
{'scaleFactor': 30, 'timeStep': 94, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.057631 , 16.87619  ,  4.7320414], dtype=float32)}
episode index:434
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 22.       ,  1.5998204], dtype=float32)}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.5345941856711454
{'scaleFactor': 30, 'timeStep': 220, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.131576 , 15.24445  ,  6.2453613], dtype=float32)}
episode index:435
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 14.       ,  1.4525198], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5355058167771909
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.942366 , 16.859798 ,  2.5783367], dtype=float32)}
episode index:436
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.7133465, 4.979351 , 1.834223 ], dtype=float32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5363919469319981
{'scaleFactor': 30, 'timeStep': 9, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.84206  , 15.373036 ,  2.3251019], dtype=float32)}
episode index:437
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([2.      , 7.      , 5.202766], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5361006871774103
{'scaleFactor': 30, 'timeStep': 90, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.567624  , 13.671041  ,  0.79635334], dtype=float32)}
episode index:438
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.      , 12.      ,  4.628148], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5369395969446801
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.354401  , 13.87567   ,  0.14616036], dtype=float32)}
episode index:439
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 25.       ,  2.2102263], dtype=float32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5372705401221021
{'scaleFactor': 30, 'timeStep': 39, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.287075  , 13.283287  ,  0.21295613], dtype=float32)}
episode index:440
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.       , 7.       , 2.9363682], dtype=float32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5376473590674025
{'scaleFactor': 30, 'timeStep': 36, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.082822 , 15.709845 ,  6.0549335], dtype=float32)}
episode index:441
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.      , 15.      ,  2.256774], dtype=float32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5376200946521494
{'scaleFactor': 30, 'timeStep': 65, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.423028, 16.835474,  3.51088 ], dtype=float32)}
episode index:442
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([1.500000e+01, 4.000000e+00, 1.386803e-02], dtype=float32)}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5369819223407367
{'scaleFactor': 30, 'timeStep': 137, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.236773 , 16.59057  ,  0.6245377], dtype=float32)}
episode index:443
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       ,  8.       ,  2.0677767], dtype=float32)}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5366052280058471
{'scaleFactor': 30, 'timeStep': 100, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.371661 , 13.582287 ,  1.0190079], dtype=float32)}
episode index:444
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.979163 ,  1.7120535,  6.266122 ], dtype=float32)}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.5357356463824712
{'scaleFactor': 30, 'timeStep': 190, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.943304 , 16.229637 ,  2.2248814], dtype=float32)}
episode index:445
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.        , 21.        ,  0.12012833], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5363868413991822
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.710009 , 13.746038 ,  4.2651663], dtype=float32)}
episode index:446
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 28.       ,  2.3470306], dtype=float32)}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.5355496670112891
{'scaleFactor': 30, 'timeStep': 182, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.968172 , 16.760435 ,  3.5875578], dtype=float32)}
episode index:447
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([7.      , 9.      , 5.436659], dtype=float32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5362740167604633
{'scaleFactor': 30, 'timeStep': 16, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.916774  , 15.9542    ,  0.21561706], dtype=float32)}
episode index:448
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.1963005, 3.990343 , 1.4091346], dtype=float32)}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.5352959735821327
{'scaleFactor': 30, 'timeStep': 233, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.68762   , 15.480329  ,  0.38311356], dtype=float32)}
episode index:449
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.4599314, 25.633032 ,  5.366325 ], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5361364653019138
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.865416 , 16.556156 ,  5.9066997], dtype=float32)}
episode index:450
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.      , 16.      ,  4.092872], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5370776172857233
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.895761 , 13.523596 ,  2.0606198], dtype=float32)}
episode index:451
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 23.       ,  1.1089802], dtype=float32)}
done in step count: 194
reward sum = 0.14230748778208857
running average episode reward sum: 0.5362042320434587
{'scaleFactor': 30, 'timeStep': 195, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.149238, 14.266844,  5.701814], dtype=float32)}
episode index:452
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       , 23.       ,  1.7558541], dtype=float32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5363034863085241
{'scaleFactor': 30, 'timeStep': 55, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.339841, 15.983383,  6.031831], dtype=float32)}
episode index:453
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 27.       ,  1.4855616], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5359699898164205
{'scaleFactor': 30, 'timeStep': 96, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.835343 , 13.054244 ,  4.8473716], dtype=float32)}
episode index:454
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.964801 , 10.99969  ,  1.2276361], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5369460997289119
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.836817  , 14.873149  ,  0.93660873], dtype=float32)}
episode index:455
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 11.       ,  1.3342016], dtype=float32)}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.5361351081396267
{'scaleFactor': 30, 'timeStep': 179, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.223656 , 13.278641 ,  1.2140467], dtype=float32)}
episode index:456
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.9847136, 17.753195 ,  5.772378 ], dtype=float32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5359610920337281
{'scaleFactor': 30, 'timeStep': 79, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.635011 , 16.01212  ,  1.8181674], dtype=float32)}
episode index:457
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       , 11.       ,  2.9279647], dtype=float32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5360726110073171
{'scaleFactor': 30, 'timeStep': 54, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.969255 , 15.913164 ,  3.4192991], dtype=float32)}
episode index:458
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 10.       ,  2.5054734], dtype=float32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5367046284644593
{'scaleFactor': 30, 'timeStep': 20, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.205682 , 16.761524 ,  1.0836005], dtype=float32)}
episode index:459
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.935615 , 18.496616 ,  5.5406017], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5376261314677975
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.264415 , 13.628433 ,  5.3838606], dtype=float32)}
episode index:460
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.        , 18.        ,  0.45191345], dtype=float32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.537566174016131
{'scaleFactor': 30, 'timeStep': 68, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.671813, 16.38466 ,  5.836775], dtype=float32)}
episode index:461
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.       ,  6.       ,  0.0714113], dtype=float32)}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5369655664130176
{'scaleFactor': 30, 'timeStep': 135, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.484023 , 16.366297 ,  3.1605268], dtype=float32)}
episode index:462
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.      , 11.      ,  5.120648], dtype=float32)}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5366887945080706
{'scaleFactor': 30, 'timeStep': 90, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.267876 , 13.751638 ,  0.3777882], dtype=float32)}
episode index:463
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 21.       ,  1.8548416], dtype=float32)}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5362899160857645
{'scaleFactor': 30, 'timeStep': 105, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.575274, 13.220431,  2.928956], dtype=float32)}
episode index:464
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.781628, 15.091264,  5.518619], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5371612929316038
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.684502  , 15.358828  ,  0.28605676], dtype=float32)}
episode index:465
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.       , 18.       ,  2.8525116], dtype=float32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5376281555802561
{'scaleFactor': 30, 'timeStep': 29, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.92324  , 15.547491 ,  4.5165777], dtype=float32)}
episode index:466
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.      ,  3.      ,  4.093586], dtype=float32)}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5371179890614811
{'scaleFactor': 30, 'timeStep': 121, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.188955  , 16.998802  ,  0.16799524], dtype=float32)}
episode index:467
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 3.       , 16.       ,  3.4750752], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.537302620270519
{'scaleFactor': 30, 'timeStep': 48, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.362414, 16.498316,  0.533707], dtype=float32)}
episode index:468
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.000801 , 26.731588 ,  1.3379668], dtype=float32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5376270263805373
{'scaleFactor': 30, 'timeStep': 38, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.135231 , 16.016172 ,  5.5445786], dtype=float32)}
episode index:469
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 16.       ,  2.0996795], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5384862883444106
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.847635 , 15.599224 ,  3.4968324], dtype=float32)}
episode index:470
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([5.        , 2.        , 0.22268504], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5387211425695146
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.240593 , 13.143555 ,  1.0539435], dtype=float32)}
episode index:471
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 27.       ,  5.6633654], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5395152021138242
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.060818 , 15.804117 ,  3.6370394], dtype=float32)}
episode index:472
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 2.      , 18.      ,  1.749479], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5397061290252183
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.522202  , 15.885493  ,  0.29072967], dtype=float32)}
episode index:473
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.225303, 11.580707,  0.683216], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5404564077704367
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.568227 , 16.035938 ,  2.6449862], dtype=float32)}
episode index:474
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.     , 23.     ,  2.45869], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5411846782208486
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.516796 , 15.467909 ,  4.7131696], dtype=float32)}
episode index:475
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([7.      , 7.      , 2.228054], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5418186246812753
{'scaleFactor': 30, 'timeStep': 18, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.19845  , 14.0205345,  0.6498645], dtype=float32)}
episode index:476
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 13.       ,  3.5415828], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5418649277541706
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.222334 , 13.010204 ,  1.4829359], dtype=float32)}
episode index:477
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.       , 19.       ,  1.0105289], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5420892744081752
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.128038 , 16.48262  ,  3.5763836], dtype=float32)}
episode index:478
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.      ,  4.      ,  3.993283], dtype=float32)}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5412429502548317
{'scaleFactor': 30, 'timeStep': 199, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.041833 , 16.284128 ,  3.5801857], dtype=float32)}
episode index:479
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       ,  2.       ,  0.8857895], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5412901632552433
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.292616 , 14.9962015,  3.240128 ], dtype=float32)}
episode index:480
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.365482, 26.847458,  3.055819], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5420262299725062
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.2282505, 16.454062 ,  4.9488173], dtype=float32)}
episode index:481
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.      ,  7.      ,  5.789689], dtype=float32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5425154314858394
{'scaleFactor': 30, 'timeStep': 26, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.55577  , 15.35549  ,  2.2067664], dtype=float32)}
episode index:482
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.       , 16.       ,  0.9638497], dtype=float32)}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5419699380222768
{'scaleFactor': 30, 'timeStep': 128, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.486835 , 15.847091 ,  5.3685856], dtype=float32)}
episode index:483
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       ,  5.       ,  2.8068223], dtype=float32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5418828865592125
{'scaleFactor': 30, 'timeStep': 70, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.306892, 14.406534,  1.32991 ], dtype=float32)}
episode index:484
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.       , 6.       , 2.1097336], dtype=float32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5417165753857257
{'scaleFactor': 30, 'timeStep': 78, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.13762  , 13.228906 ,  6.2229896], dtype=float32)}
episode index:485
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 28.       ,  5.6173644], dtype=float32)}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5414602056419963
{'scaleFactor': 30, 'timeStep': 88, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.488306, 16.149754,  5.360258], dtype=float32)}
episode index:486
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.      , 27.      ,  5.939073], dtype=float32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5419779758226978
{'scaleFactor': 30, 'timeStep': 24, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.06778 , 15.846901,  3.146205], dtype=float32)}
episode index:487
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.        , 19.        ,  0.47642052], dtype=float32)}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5415525636937372
{'scaleFactor': 30, 'timeStep': 110, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.788763  , 13.223542  ,  0.06613249], dtype=float32)}
episode index:488
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([23.      , 16.      ,  2.129723], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5422577422377503
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.508818, 16.777273,  3.406234], dtype=float32)}
episode index:489
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([2.       , 7.       , 1.7911075], dtype=float32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5428713860155996
{'scaleFactor': 30, 'timeStep': 18, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.020404, 16.819542,  5.989944], dtype=float32)}
episode index:490
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.       , 17.       ,  4.6178236], dtype=float32)}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5423878577873159
{'scaleFactor': 30, 'timeStep': 119, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.890179 , 16.695301 ,  3.1710777], dtype=float32)}
episode index:491
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.       , 23.       ,  5.4734774], dtype=float32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5430870387099355
{'scaleFactor': 30, 'timeStep': 13, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.901053, 16.0164  ,  4.123708], dtype=float32)}
episode index:492
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 11.       ,  5.8899317], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5437476041745583
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.495853, 14.834925,  2.379409], dtype=float32)}
episode index:493
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 28.       ,  1.1216877], dtype=float32)}
done in step count: 479
reward sum = 0.008114433626754238
running average episode reward sum: 0.5426633265013845
{'scaleFactor': 30, 'timeStep': 480, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.35259  , 15.907846 ,  2.1247096], dtype=float32)}
episode index:494
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 25.       ,  4.4760823], dtype=float32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5428394079250246
{'scaleFactor': 30, 'timeStep': 47, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.8363495, 16.94488  ,  5.84255  ], dtype=float32)}
episode index:495
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([9.019798 , 3.999902 , 1.2188897], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5435867422789734
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.974628  , 14.1607685 ,  0.34272268], dtype=float32)}
episode index:496
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.       , 18.       ,  5.1676097], dtype=float32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5437226185309934
{'scaleFactor': 30, 'timeStep': 50, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.030947 , 16.160646 ,  4.5615735], dtype=float32)}
episode index:497
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.989357  , 24.206055  ,  0.41467547], dtype=float32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5435294470354769
{'scaleFactor': 30, 'timeStep': 81, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.748634 , 16.50819  ,  2.7170029], dtype=float32)}
episode index:498
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([8.902912, 8.615571, 6.049095], dtype=float32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.543421953700745
{'scaleFactor': 30, 'timeStep': 72, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.955181 , 13.71623  ,  1.1741121], dtype=float32)}
episode index:499
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.      , 25.      ,  5.592883], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.542990555308219
{'scaleFactor': 30, 'timeStep': 112, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.15523 , 16.924877,  4.177094], dtype=float32)}
episode index:500
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.441725 , 26.079498 ,  4.4758406], dtype=float32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5435392906022092
{'scaleFactor': 30, 'timeStep': 21, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.55843  , 16.985493 ,  3.2358189], dtype=float32)}
episode index:501
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.77939  , 26.415667 ,  4.2265325], dtype=float32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5436377510719288
{'scaleFactor': 30, 'timeStep': 53, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.426505 , 16.90121  ,  5.7336717], dtype=float32)}
episode index:502
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.       , 23.       ,  0.8497894], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5440725521917601
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.106717 , 15.584069 ,  5.6656237], dtype=float32)}
episode index:503
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.      , 11.      ,  4.943681], dtype=float32)}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5439361781636132
{'scaleFactor': 30, 'timeStep': 75, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.463514 , 16.447157 ,  1.8240362], dtype=float32)}
episode index:504
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 17.       ,  3.1068978], dtype=float32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.544632023858851
{'scaleFactor': 30, 'timeStep': 12, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.734879 , 15.57243  ,  6.2369075], dtype=float32)}
episode index:505
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.       , 24.       ,  2.2075968], dtype=float32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5452899072563612
{'scaleFactor': 30, 'timeStep': 14, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.695321, 16.917871,  5.227656], dtype=float32)}
episode index:506
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.0038   , 21.123226 ,  2.6447062], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5460161939234761
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.051898, 15.384796,  4.405686], dtype=float32)}
episode index:507
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       , 21.       ,  3.5175745], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5468322959236267
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.629852, 16.268703,  4.604784], dtype=float32)}
episode index:508
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.866714 ,  7.7178993,  0.2698247], dtype=float32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5468547592686646
{'scaleFactor': 30, 'timeStep': 59, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.383409 , 14.426905 ,  0.7674809], dtype=float32)}
episode index:509
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([7.       , 9.       , 1.9842092], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5476285345434339
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.767633 , 14.801433 ,  0.6138513], dtype=float32)}
episode index:510
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.        , 24.        ,  0.09313196], dtype=float32)}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.5468617071750254
{'scaleFactor': 30, 'timeStep': 186, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.455734, 13.605872,  4.016314], dtype=float32)}
episode index:511
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.       , 22.       ,  2.6076624], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5470361676806677
{'scaleFactor': 30, 'timeStep': 46, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.771382, 15.740232,  5.726628], dtype=float32)}
episode index:512
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.        , 11.        ,  0.44452262], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5474558685513624
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.95837  , 16.879293 ,  1.8618786], dtype=float32)}
episode index:513
target distance 2.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.      , 17.      ,  2.257902], dtype=float32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5482224527553501
{'scaleFactor': 30, 'timeStep': 7, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.894735 , 16.359705 ,  6.0115395], dtype=float32)}
episode index:514
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.        , 29.        ,  0.13291618], dtype=float32)}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5475167869518417
{'scaleFactor': 30, 'timeStep': 169, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.813751 , 15.604933 ,  4.7378364], dtype=float32)}
episode index:515
target distance 4.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([19.       , 13.       ,  0.1537766], dtype=float32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5482987118800358
{'scaleFactor': 30, 'timeStep': 6, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.531475 , 14.417189 ,  4.1045403], dtype=float32)}
episode index:516
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([5.       , 4.       , 2.8027382], dtype=float32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5484687056405461
{'scaleFactor': 30, 'timeStep': 46, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.415865  , 16.422077  ,  0.09622543], dtype=float32)}
episode index:517
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.       ,  7.       ,  5.4539256], dtype=float32)}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.5476485518345534
{'scaleFactor': 30, 'timeStep': 209, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.402925 , 14.94668  ,  1.0283781], dtype=float32)}
episode index:518
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.830793 , 24.377361 ,  3.9191623], dtype=float32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.548107183027179
{'scaleFactor': 30, 'timeStep': 25, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.648515 , 13.194646 ,  2.9563437], dtype=float32)}
episode index:519
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 19.       ,  2.7551165], dtype=float32)}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5477859136715585
{'scaleFactor': 30, 'timeStep': 97, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.362915, 15.264391,  4.039529], dtype=float32)}
episode index:520
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.0798163, 14.683448 ,  1.6200585], dtype=float32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5484019595431466
{'scaleFactor': 30, 'timeStep': 15, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.508171, 15.699425,  4.650615], dtype=float32)}
episode index:521
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       , 25.       ,  2.8729658], dtype=float32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.548431659219218
{'scaleFactor': 30, 'timeStep': 58, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.826631, 16.225422,  4.107202], dtype=float32)}
episode index:522
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 11.       ,  0.7064862], dtype=float32)}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.549238288933904
{'scaleFactor': 30, 'timeStep': 4, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.238352 , 15.014703 ,  6.2562633], dtype=float32)}
episode index:523
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 8.       , 16.       ,  4.1833014], dtype=float32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5498150436708524
{'scaleFactor': 30, 'timeStep': 17, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.52194   , 14.09791   ,  0.11307114], dtype=float32)}
episode index:524
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([21.      , 29.      ,  4.895155], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5501909618625871
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.814765 , 14.453346 ,  5.1908717], dtype=float32)}
episode index:525
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 21.       ,  0.6896073], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5505654507075852
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.165262, 16.496408,  4.167952], dtype=float32)}
episode index:526
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.       , 24.       ,  3.2857542], dtype=float32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5505080979029224
{'scaleFactor': 30, 'timeStep': 66, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.558956, 16.093206,  4.849066], dtype=float32)}
episode index:527
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.991301 , 21.273    ,  4.2650056], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5506345790450425
{'scaleFactor': 30, 'timeStep': 49, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.879878, 13.052187,  5.412868], dtype=float32)}
episode index:528
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 4.8744063, 17.302431 ,  5.8102975], dtype=float32)}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5502451227204725
{'scaleFactor': 30, 'timeStep': 107, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.976325, 15.418779,  5.917441], dtype=float32)}
episode index:529
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.527293 , 26.646805 ,  4.2756853], dtype=float32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5509655382396924
{'scaleFactor': 30, 'timeStep': 8, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.608452, 15.580858,  4.88902 ], dtype=float32)}
episode index:530
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 23.       ,  4.5246744], dtype=float32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5516483098201894
{'scaleFactor': 30, 'timeStep': 10, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.807182, 16.334282,  5.592106], dtype=float32)}
episode index:531
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([25.       ,  2.       ,  3.4376073], dtype=float32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5517834171229545
{'scaleFactor': 30, 'timeStep': 48, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.113974 , 16.76461  ,  2.7038136], dtype=float32)}
episode index:532
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.       , 23.       ,  5.9543242], dtype=float32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5521500000070232
{'scaleFactor': 30, 'timeStep': 30, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.283953 , 15.333773 ,  4.9194565], dtype=float32)}
episode index:533
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.       , 3.       , 1.9351628], dtype=float32)}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5518739362854339
{'scaleFactor': 30, 'timeStep': 91, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.979221  , 13.595346  ,  0.42236018], dtype=float32)}
episode index:534
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([26.      ,  2.      ,  4.273219], dtype=float32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5522673358705958
{'scaleFactor': 30, 'timeStep': 28, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.785236 , 13.003636 ,  2.4203603], dtype=float32)}
episode index:535
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([17.821321 , 20.38423  ,  4.6814203], dtype=float32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5530291430984492
{'scaleFactor': 30, 'timeStep': 5, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.607956, 13.077713,  4.044474], dtype=float32)}
episode index:536
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.      , 10.      ,  4.953246], dtype=float32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.553125934390887
{'scaleFactor': 30, 'timeStep': 51, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.047671 , 14.426711 ,  1.0380065], dtype=float32)}
episode index:537
target distance 8.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 7.       , 17.       ,  0.6752666], dtype=float32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5528809683830131
{'scaleFactor': 30, 'timeStep': 87, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.182088, 16.007494,  6.096018], dtype=float32)}
episode index:538
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 11.       ,  4.4837995], dtype=float32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5527550136555385
{'scaleFactor': 30, 'timeStep': 73, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.47049   , 15.031882  ,  0.48121086], dtype=float32)}
episode index:539
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([10.       , 18.       ,  5.5626965], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.55354639325988
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.029165 , 15.387934 ,  4.9456735], dtype=float32)}
episode index:540
target distance 3.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([12.       , 12.       ,  5.4631057], dtype=float32)}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.5527197422255461
{'scaleFactor': 30, 'timeStep': 224, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.484907  , 14.514079  ,  0.79310274], dtype=float32)}
episode index:541
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.      , 27.      ,  6.064161], dtype=float32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5528388758025142
{'scaleFactor': 30, 'timeStep': 49, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.666449 , 15.692887 ,  2.3786128], dtype=float32)}
episode index:542
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([24.      , 27.      ,  2.790604], dtype=float32)}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5527049947357903
{'scaleFactor': 30, 'timeStep': 74, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.638454, 14.486462,  4.596062], dtype=float32)}
episode index:543
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([3.0000000e+00, 2.5000000e+01, 2.2194967e-02], dtype=float32)}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5522914244466396
{'scaleFactor': 30, 'timeStep': 112, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.267029 , 13.047533 ,  3.8346934], dtype=float32)}
episode index:544
target distance 5.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.007866 , 13.822785 ,  2.9366226], dtype=float32)}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.553094559447655
{'scaleFactor': 30, 'timeStep': 2, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.049732, 14.229861,  2.489142], dtype=float32)}
episode index:545
target distance 14.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 6.        , 29.        ,  0.99111545], dtype=float32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5532703984017222
{'scaleFactor': 30, 'timeStep': 44, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.4681835, 13.074345 ,  5.9260626], dtype=float32)}
episode index:546
target distance 13.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([2.       , 6.       , 1.5974951], dtype=float32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5535710567823375
{'scaleFactor': 30, 'timeStep': 34, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.935554 , 13.790609 ,  2.0058007], dtype=float32)}
episode index:547
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([15.        , 27.        ,  0.17170209], dtype=float32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5536538760335854
{'scaleFactor': 30, 'timeStep': 52, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.425512 , 14.186476 ,  2.7034152], dtype=float32)}
episode index:548
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([27.       , 27.       ,  1.4066272], dtype=float32)}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.553353566456246
{'scaleFactor': 30, 'timeStep': 95, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.848639 , 14.70866  ,  4.6127687], dtype=float32)}
episode index:549
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.       , 21.       ,  3.5754163], dtype=float32)}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5526834773607775
{'scaleFactor': 30, 'timeStep': 169, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.882008 , 16.076046 ,  2.3178215], dtype=float32)}
episode index:550
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([20.42716  , 14.764615 ,  3.8618357], dtype=float32)}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5534591879281808
{'scaleFactor': 30, 'timeStep': 3, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([16.944399, 13.159659,  3.251095], dtype=float32)}
episode index:551
target distance 7.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([11.      ,  8.      ,  5.007769], dtype=float32)}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5530326116163891
{'scaleFactor': 30, 'timeStep': 115, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.429398 , 16.933887 ,  5.5139837], dtype=float32)}
episode index:552
target distance 6.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 9.       , 10.       ,  3.1300912], dtype=float32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5527285672534181
{'scaleFactor': 30, 'timeStep': 96, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([13.631581, 15.694895,  5.946363], dtype=float32)}
episode index:553
target distance 12.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([ 5.833515, 26.20111 ,  5.393601], dtype=float32)}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5524762587766681
{'scaleFactor': 30, 'timeStep': 89, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.10716  , 14.193042 ,  5.7663345], dtype=float32)}
episode index:554
target distance 10.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.147399 ,  6.809163 ,  2.1557462], dtype=float32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5531103233104198
{'scaleFactor': 30, 'timeStep': 11, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.597799  , 14.16755   ,  0.18276346], dtype=float32)}
episode index:555
target distance 11.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([18.       ,  4.       ,  0.4241828], dtype=float32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5534194420382029
{'scaleFactor': 30, 'timeStep': 33, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([14.899349 , 14.078924 ,  2.3131087], dtype=float32)}
episode index:556
target distance 9.0
at step 0:
{'scaleFactor': 30, 'timeStep': 1, 'currentTarget': array([15., 15.], dtype=float32), 'previousTarget': array([15., 15.], dtype=float32), 'currentState': array([22.      ,  6.      ,  4.761801], dtype=float32)}
Traceback (most recent call last):
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/TimedMaze/StackedDQNHERFreeSpaceExample/StackedDQN_CNNTimedMazeGPU.py", line 167, in <module>
    agent.train()
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/StackedDQN/StackedDQN.py", line 107, in train
    self.update_net(state, action, nextState, reward, info)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/StackedDQN/StackedDQN.py", line 173, in update_net
    info=info)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DQN/DQN.py", line 256, in update_net_on_transitions
    QNext[nonFinalMask] = self.targetNet(nonFinalNextState).gather(1, batchAction)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/TimedMaze/StackedDQNHERFreeSpaceExample/StackedDQN_CNNTimedMazeGPU.py", line 56, in forward
    xout = self.layer2(xout)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 74, in forward
    self.num_batches_tracked += 1
KeyboardInterrupt

Process finished with exit code 1
