/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Env/CustomEnv/DynamicMaze/StochAgent/FreeSpace/DQNHER_FixedTargetStochMove/DQNHER_CNNDynMazeStochAgentCPU.py
episode index:0
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.909644 , 7.9549327, 2.8745103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.312647747457552}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([8.96909539, 7.72588699]), 'previousTarget': array([10.14255433,  7.91933897]), 'currentState': array([2.5865436 , 0.02763835, 3.4439569 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:1
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.81359978, 18.79658712]), 'previousTarget': array([ 9.92623989, 18.90289239]), 'currentState': array([ 1.7445276, 24.70337  ,  3.0151308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([7.84402385, 8.3918674 ]), 'previousTarget': array([7.77345   , 8.29744821]), 'currentState': array([0.4973282, 1.6076163, 3.3595228], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:2
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.25911  , 20.954597 ,  4.9215617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.555128266357307}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.911828 , 21.405697 ,  1.9018728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.599085625383973}
episode index:3
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.98861961, 11.52519748]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([3.4301183, 4.977751 , 1.9891598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 1.0, 'currentTarget': array([14.60510746, 12.06010437]), 'previousTarget': array([14.60453014, 12.07971928]), 'currentState': array([13.273844 ,  2.1491137,  4.092081 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:4
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.530558 , 17.021961 ,  5.9862595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5359285892374532}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.19602
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.61372  , 16.99607  ,  5.9363666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5667861092706845}
episode index:5
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.79936481, 14.67877462]), 'previousTarget': array([10.974587 , 14.7124705]), 'currentState': array([ 0.828476  , 13.916294  ,  0.70421803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.16335
{'scaleFactor': 1.0, 'currentTarget': array([ 7.53586845, 23.6643508 ]), 'previousTarget': array([ 7.66406463, 23.58227733]), 'currentState': array([ 1.0090492, 31.240671 ,  5.1295185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:6
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.27979278, 20.47257762]), 'previousTarget': array([20.49208627, 21.40743398]), 'currentState': array([25.440113 , 28.349789 ,  3.8317773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.1400142857142857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.601849, 14.36695 ,  2.992887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.677754359034821}
episode index:7
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.36461538,  8.29957825]), 'previousTarget': array([10.54700196,  8.32050294]), 'currentState': array([4.675311  , 0.07572169, 0.2571381 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.13765860204463104
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.723743 , 13.389042 ,  6.0467486], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0552413016769844}
episode index:8
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.2276912 , 18.80612098]), 'previousTarget': array([11.07106781, 18.92893219]), 'currentState': array([ 4.188242 , 25.908667 ,  1.9708104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 490
reward sum = 0.0072651628376763635
running average episode reward sum: 0.12317044213274718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.519947, 15.311278,  6.113097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.572140751762517}
episode index:9
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.59252827, 13.33128845]), 'previousTarget': array([19.58258088, 13.36336397]), 'currentState': array([28.991316 ,  9.916205 ,  4.6286783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.11085339791947246
{'scaleFactor': 1.0, 'currentTarget': array([ 8.35545207, 18.61547434]), 'previousTarget': array([ 8.36538706, 18.597172  ]), 'currentState': array([-0.4284031, 23.395002 ,  2.2818372], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:10
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.54930538, 7.55689083]), 'previousTarget': array([8.54930538, 7.55689083]), 'currentState': array([2.       , 0.       , 4.4052896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.1515272834311444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.606891 , 13.279019 ,  1.5269909], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7653071450954578}
episode index:11
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.20492662, 12.21104692]), 'previousTarget': array([15., 14.]), 'currentState': array([15.937731 ,  2.2379332,  4.92849  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.13890000981188236
{'scaleFactor': 1.0, 'currentTarget': array([19.20991748, 10.6734961 ]), 'previousTarget': array([18.96512153, 10.95483686]), 'currentState': array([26.183762 ,  3.5065234,  4.668678 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:12
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.99670984,  9.43743761]), 'previousTarget': array([18.30790021, 10.22192192]), 'currentState': array([22.739529  ,  0.63370717,  4.289278  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.1282153936725068
{'scaleFactor': 1.0, 'currentTarget': array([18.57665293, 22.05141487]), 'previousTarget': array([18.57579554, 21.9013731 ]), 'currentState': array([23.100262 , 30.969765 ,  5.4799094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:13
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.994427 , 18.301657 ,  4.5636888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.734529226796998}
done in step count: 498
reward sum = 0.0067038904626207565
running average episode reward sum: 0.11953600058608638
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.291706 , 15.882076 ,  2.9588513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9290591879144764}
episode index:14
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.48124031, 11.70356123]), 'previousTarget': array([15., 12.]), 'currentState': array([12.926675 ,  1.8251339,  3.4590292], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.11156693388034729
{'scaleFactor': 1.0, 'currentTarget': array([12.71266862, 21.29679626]), 'previousTarget': array([12.73081868, 21.24667551]), 'currentState': array([ 9.298419, 30.695887,  0.631253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:15
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.01494345, 19.9555379 ]), 'previousTarget': array([15.47942816, 18.11628302]), 'currentState': array([18.021393 , 29.752178 ,  1.3110356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.10459400051282558
{'scaleFactor': 1.0, 'currentTarget': array([13.74387968, 15.18102182]), 'previousTarget': array([11.85161122, 15.44989268]), 'currentState': array([ 3.8461308, 16.607405 ,  5.9218755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:16
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.74502909, 13.8475184 ]), 'previousTarget': array([10.61523948, 13.74721128]), 'currentState': array([ 1.092819 , 11.233166 ,  3.0468168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.09844141224736525
{'scaleFactor': 1.0, 'currentTarget': array([10.26350069, 14.2870505 ]), 'previousTarget': array([10.26350069, 14.2870505 ]), 'currentState': array([ 0.37489635, 12.7985935 ,  1.9642667 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:17
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.880606  ,  7.236605  ,  0.12128548], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.764312868918309}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.11568467681265325
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.261134 , 13.470505 ,  5.2610927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6986107366580512}
episode index:18
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.28824 , 22.0478  ,  2.775806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.811184518428938}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.10959600961198729
{'scaleFactor': 1.0, 'currentTarget': array([18.98121274, 14.13312927]), 'previousTarget': array([18.98121274, 14.13312927]), 'currentState': array([28.752268, 12.005576,  5.710945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
episode index:19
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.748302 , 16.449936 ,  5.9729524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5603161440439783}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.1536162091313879
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.555123 , 16.426195 ,  0.4370758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.030197326508653}
episode index:20
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.8637247 , 12.683037  ,  0.13365433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.5029823061202405}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.17946368574653962
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.701532, 13.590797,  1.725852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.440463407767587}
episode index:21
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.69704711, 11.03084205]), 'previousTarget': array([8.06404996, 9.91363664]), 'currentState': array([0.23510133, 5.7021017 , 2.0175817 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.17130624548533327
{'scaleFactor': 1.0, 'currentTarget': array([16.95534577,  9.86536478]), 'previousTarget': array([16.86800447,  9.85530681]), 'currentState': array([20.514177 ,  0.5200602,  1.468988 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:22
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.30627272, 15.81642337]), 'previousTarget': array([13.89352217, 15.86059386]), 'currentState': array([ 7.831049 , 23.436888 ,  0.7752226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 483
reward sum = 0.0077946925652699495
running average episode reward sum: 0.16419704753228706
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.756734 , 15.635548 ,  4.2060637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9882140390349283}
episode index:23
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.11628302, 14.52057184]), 'previousTarget': array([18.11628302, 14.52057184]), 'currentState': array([28.      , 13.      ,  5.601653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.17947656817490484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.753159 , 13.607854 ,  2.9158576], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4138604899243365}
episode index:24
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.41017235, 12.6323935 ]), 'previousTarget': array([15.2875295, 10.974587 ]), 'currentState': array([17.11718  ,  2.7791643,  0.9931981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.17782073797140754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.680481  , 16.416084  ,  0.39306343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4516842404366752}
episode index:25
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.51845801, 12.7510476 ]), 'previousTarget': array([13.36875492, 12.43661488]), 'currentState': array([8.017194 , 4.4002337, 5.1230946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 196
reward sum = 0.139475568775225
running average episode reward sum: 0.17634592377155436
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.740347, 16.567665,  4.952856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.342302548397845}
episode index:26
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.83368343, 12.74016872]), 'previousTarget': array([10.804711  , 12.74099823]), 'currentState': array([2.0434835, 7.9723196, 3.2774754], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.17323578453393598
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.88458 , 16.261581,  3.955646], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5408013674068382}
episode index:27
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.39702254, 15.21246892]), 'previousTarget': array([16.10647783, 15.86059386]), 'currentState': array([24.213871, 19.930857,  4.91723 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 284
reward sum = 0.05759639025694116
running average episode reward sum: 0.16910580616690044
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.691824 , 15.378931 ,  3.7769463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7888023377245199}
episode index:28
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96361754, 14.31511525]), 'previousTarget': array([12.43661488, 13.36875492]), 'currentState': array([5.6207533 , 8.801803  , 0.06156692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.16848674164228827
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.746486 , 15.242152 ,  4.3292265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2766893786832187}
episode index:29
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.427372, 15.433024,  4.210535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.5824170943244}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.19393936185111166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.168833  , 15.377814  ,  0.49282902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8697371557104359}
episode index:30
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.21666824, 18.07328519]), 'previousTarget': array([13.24695048, 17.19131191]), 'currentState': array([ 8.197747  , 26.722588  ,  0.91606706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 442
reward sum = 0.011769445769190735
running average episode reward sum: 0.18806291294524324
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.896587 , 14.17763  ,  5.6314564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.376158111091283}
episode index:31
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.72857037, 11.6565953 ]), 'previousTarget': array([ 8.82352941, 11.70588235]), 'currentState': array([-0.09575219,  6.9522004 ,  2.4174266 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.19689177946525593
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.308198 , 14.529744 ,  1.7238905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.755942669154701}
episode index:32
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.46037165, 12.66480513]), 'previousTarget': array([12.43294146, 12.68964732]), 'currentState': array([5.0992374, 5.896223 , 4.086693 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.20147363810244337
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.519019 , 13.745061 ,  0.1570515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3439548701543136}
episode index:33
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.14014819, 16.46413144]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([25.393545, 22.110497,  3.068506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.20819173737954486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.658685 , 14.38681  ,  3.7842035], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8999262075669341}
episode index:34
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.66663211, 14.89095048]), 'previousTarget': array([12., 15.]), 'currentState': array([ 3.6999092, 14.075822 ,  5.455937 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 353
reward sum = 0.02878880863894463
running average episode reward sum: 0.2030659394155277
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.248699  , 14.073453  ,  0.37881008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1928714710766348}
episode index:35
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.45069315, 11.13573565]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([6.943938 , 2.7885418, 4.5450315], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 254
reward sum = 0.07786448720191184
running average episode reward sum: 0.1995881212984828
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.763974 , 14.316863 ,  1.8986368], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0248573761322848}
episode index:36
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.46992164, 7.52576854]), 'previousTarget': array([8.54930538, 7.55689083]), 'currentState': array([ 1.8905121 , -0.00492666,  0.63970023], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.20480765932711312
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.139885, 15.404994,  5.292258], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.950693457590692}
episode index:37
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.057984 , 19.450264 ,  3.1987574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.548872711595921}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.19941798408166278
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.580282, 10.509226,  2.2599  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.966628301144093}
episode index:38
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.40642768, 16.37595154]), 'previousTarget': array([11.33345606, 16.41020921]), 'currentState': array([ 2.067591 , 19.951721 ,  1.5767003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.19430470243854323
{'scaleFactor': 1.0, 'currentTarget': array([ 9.52289945, 15.52512366]), 'previousTarget': array([ 9.52289945, 15.52512366]), 'currentState': array([-0.43145376, 16.47951   ,  1.6935157 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:39
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.15459451, 12.55456615]), 'previousTarget': array([19.58258088, 13.36336397]), 'currentState': array([29.189405 ,  8.268287 ,  4.5477877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.20326417180663925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.96521  , 13.744029 ,  3.8161318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5840117765448836}
episode index:40
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.50074133, 12.02926325]), 'previousTarget': array([15., 12.]), 'currentState': array([17.162874 ,  2.168365 ,  6.1430783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.20714485097868457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.44241   , 13.23151   ,  0.41862327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2821265337224697}
episode index:41
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.72627411, 18.07902317]), 'previousTarget': array([14.76696499, 18.02945514]), 'currentState': array([13.840764 , 28.03974  ,  1.4446503], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.2094129363284545
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.041544, 15.670512,  4.711631], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1697113272359485}
episode index:42
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.96429119, 20.09342743]), 'previousTarget': array([16.03883865, 20.19419324]), 'currentState': array([17.824455 , 29.918894 ,  1.8342564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.20454286804174626
{'scaleFactor': 1.0, 'currentTarget': array([8.04367378, 8.85453747]), 'previousTarget': array([8.04367378, 8.85453747]), 'currentState': array([0.54932  , 2.2337625, 3.4384356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:43
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.163201 , 16.03714  ,  2.6825376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.9745041420075764}
done in step count: 288
reward sum = 0.05532686267122055
running average episode reward sum: 0.20115159519241613
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.761215 , 16.074814 ,  4.2788844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6400648013568024}
episode index:44
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.85535989, 10.10782174]), 'previousTarget': array([12.93919299, 10.1914503 ]), 'currentState': array([8.840397 , 0.9492153, 3.818361 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.20576645250864134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.786594 , 15.748941 ,  5.5401945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0861141692282632}
episode index:45
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.34860296, 17.63138719]), 'previousTarget': array([14.09529089, 19.22197586]), 'currentState': array([11.945646, 27.338385,  5.081827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.2183732283412191
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.950396, 13.912117,  2.863325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4445557027738873}
episode index:46
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.37082241, 13.37483679]), 'previousTarget': array([15.1695452 , 12.96545758]), 'currentState': array([17.595402 ,  3.625414 ,  0.8127472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.21858295667726427
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.445053  , 13.627271  ,  0.02523141], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0741855257098027}
episode index:47
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.03405983, 19.66562628]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 1.1568505 , 25.825949  ,  0.37789172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 372
reward sum = 0.02378441041510293
running average episode reward sum: 0.21452465363013593
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.750625  , 16.86065   ,  0.28653783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.87728611784626}
episode index:48
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.0093975 , 13.620184  ,  0.49691552], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.125476514984525}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.22994861988258214
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.648993  , 15.076368  ,  0.42169136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.353164174546666}
episode index:49
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.6477014 , 15.89361398]), 'previousTarget': array([19.58258088, 16.63663603]), 'currentState': array([27.122608, 19.091448,  3.886089], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.23231150977477935
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.2384405, 13.522159 ,  1.3076569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6625245825689412}
episode index:50
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.11496  ,  6.793104 ,  3.5271485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.180742486670683}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.24621677721843074
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.956729, 13.308911,  2.510334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.987006649759444}
episode index:51
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.10647783, 15.86059386]), 'currentState': array([22.561611 , 20.992188 ,  3.5994449], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.64801920630157}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.24340695216156646
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.562553, 15.955336,  4.175738], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8314582406081041}
episode index:52
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.93919299, 10.1914503 ]), 'previousTarget': array([12.93919299, 10.1914503 ]), 'currentState': array([9.       , 1.       , 4.9958687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.25118529684859625
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.21237   , 14.733468  ,  0.79392016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8073905456339483}
episode index:53
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.31968724, 12.41144608]), 'previousTarget': array([17.31035268, 12.43294146]), 'currentState': array([23.993406,  4.9642  ,  4.999639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.2505128737163274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.061132 , 13.86743  ,  1.3661069], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4711178474648334}
episode index:54
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.32908146, 13.25111125]), 'previousTarget': array([14.41743063, 13.39793423]), 'currentState': array([10.747342 ,  3.9145625,  1.9144223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.2617534726081938
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.252326, 13.067153,  1.716938], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0724174307323464}
episode index:55
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([7.033119 , 9.447559 , 4.7946877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.710859218032303}
done in step count: 223
reward sum = 0.10632818368521114
running average episode reward sum: 0.2589780210202834
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.806234 , 13.159407 ,  0.7545517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.193823201581931}
episode index:56
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.4954694 , 21.09990399]), 'previousTarget': array([16.57464375, 21.298575  ]), 'currentState': array([18.876583 , 30.812283 ,  1.5253426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.25828081485091825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.140155, 16.44212 ,  4.827809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4489141847704603}
episode index:57
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.64947329, 18.63928893]), 'previousTarget': array([12.54700196, 18.67949706]), 'currentState': array([ 7.2239704, 27.039522 ,  4.698603 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.26684009885699955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.478997, 16.18364 ,  4.237784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.927291351382765}
episode index:58
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.460729 , 18.378805 ,  5.7654123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.410072608815823}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.27764589506296233
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.416315, 14.030444,  3.981382], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.716387764412509}
episode index:59
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.1319203 , 12.45829564]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([7.2097044, 4.400544 , 3.9684036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.28213774085175897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.929695, 16.458874,  5.95038 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.41909815991132}
episode index:60
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.141631 , 11.74583  ,  1.4599186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.8475100016946975}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.27751253198533665
{'scaleFactor': 1.0, 'currentTarget': array([7.11620196, 7.1622692 ]), 'previousTarget': array([7.11620196, 7.1622692 ]), 'currentState': array([0.02444494, 0.11195131, 4.469832  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:61
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.93663623, 21.0761907 ]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([22.288095 , 30.07979  ,  2.0076857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.27816537927297874
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.76644  , 13.162941 ,  3.5883605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5485474557622467}
episode index:62
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.353006 , 11.002245 ,  2.4052942], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.638823599684476}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.27637644619983204
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.705118, 13.784443,  2.304043], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7760343434087362}
episode index:63
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.87075  ,  7.9470706,  3.2088678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.715958529546455}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2863317712198915
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.6389475 , 13.7437935 ,  0.07304704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3070630126955822}
episode index:64
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.72301235, 13.84147114]), 'previousTarget': array([12.48683298, 14.16227766]), 'currentState': array([ 1.07085 , 11.226942,  2.937478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.2846034336392203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.327677 , 16.752674 ,  5.3454566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1987705938364503}
episode index:65
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.870014 ,  8.243996 ,  1.6105094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.811792660246775}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.2948457454022624
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.351269 , 15.8647   ,  1.8893775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6042549432971802}
episode index:66
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.646335 , 23.052862 ,  5.7998557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.060624558158251}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.2967336331149864
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.869748 , 15.436754 ,  2.1388137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9732502448042009}
episode index:67
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.031887 , 19.65662  ,  2.8920112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.229026707962765}
done in step count: 225
reward sum = 0.10421225282987544
running average episode reward sum: 0.2939024363460877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.804953, 13.524479,  0.811795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3313121725338823}
episode index:68
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.24029334, 17.56846559]), 'previousTarget': array([11.32050294, 17.45299804]), 'currentState': array([ 2.9831643, 23.209372 ,  4.289936 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 181
reward sum = 0.16216989001100654
running average episode reward sum: 0.2919932690078981
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.475172 , 14.508674 ,  0.7805094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.602030543879743}
episode index:69
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.30519475, 19.37273367]), 'previousTarget': array([10.3715414 , 19.24275371]), 'currentState': array([ 2.987583 , 26.188345 ,  1.5645304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.29871254679845816
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.772621 , 16.66936  ,  4.9496384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.072008444043722}
episode index:70
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.18227061, 12.36762333]), 'previousTarget': array([18.26042701, 12.3323779 ]), 'currentState': array([25.887672 ,  5.993709 ,  1.2535164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.296743957536505
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.465453, 15.31733 ,  4.739106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5633339955289477}
episode index:71
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.291006, 16.473152,  3.398647], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.461186802043122}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.3058307088193313
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.600593, 14.421877,  3.264677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.833629292498451}
episode index:72
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.8646815 , 17.06802081]), 'previousTarget': array([17.91263916, 17.11828302]), 'currentState': array([25.972704 , 22.921223 ,  1.3252033], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.30425030814735454
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.713783 , 13.673611 ,  3.9521031], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.16710906174039}
episode index:73
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.30952146, 13.4342806 ]), 'previousTarget': array([13.47409319, 13.64363839]), 'currentState': array([5.9729133, 6.639122 , 2.7833228], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 203
reward sum = 0.1300003445350054
running average episode reward sum: 0.3018955789093498
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.20106  , 14.066115 ,  2.1061647], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.521408566544595}
episode index:74
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64920772, 16.25215516]), 'previousTarget': array([14.63117406, 16.35236179]), 'currentState': array([11.951562  , 25.881418  ,  0.48486722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.3006782838065148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.614405 , 16.739687 ,  2.3031356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3733548417653765}
episode index:75
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.7430648 , 15.77553952]), 'previousTarget': array([13.57492926, 15.85504245]), 'currentState': array([ 5.2326555, 21.026533 ,  1.0281366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.30561316229669594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.639814 , 16.70128  ,  5.0940666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7389908976827066}
episode index:76
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.56775182, 15.41897046]), 'previousTarget': array([18.44220991, 16.05914151]), 'currentState': array([26.228714 , 18.000793 ,  3.5403874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.313508020545877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.831896, 13.074946,  3.72732 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.097112781525902}
episode index:77
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.67206508, 21.19548901]), 'previousTarget': array([21.67206508, 21.19548901]), 'currentState': array([29.       , 28.       ,  5.1190805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.31540178909552086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.588556, 16.73566 ,  2.6637  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3528766734839426}
episode index:78
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.140118, 16.922565,  0.612311], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.16720999805615}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.31595047907762686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.497396 , 15.299941 ,  5.6049423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5852990265554677}
episode index:79
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.59439944, 17.37371306]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.02349  , 27.074203 ,  1.1196003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.31447952629651793
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.925602 , 13.224147 ,  1.4880337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6194650867621636}
episode index:80
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.8717516 , 11.50670709]), 'previousTarget': array([10.3715414 , 10.75724629]), 'currentState': array([5.2006407, 4.0571246, 6.2329636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.31991458507314896
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.730754, 14.338756,  1.152278], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9855178368873252}
episode index:81
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.99791807,  8.90778919]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([28.013624 ,  1.7817898,  2.8878958], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.3200909542416461
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.22339 , 13.225615,  2.033187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5109337219120325}
episode index:82
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.35236179, 14.63117406]), 'currentState': array([24.45673  , 13.195034 ,  2.7168596], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.627442150189518}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.3177850432398149
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.638584 , 14.726621 ,  3.9057841], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6612327439939751}
episode index:83
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.55202182, 11.27397006]), 'previousTarget': array([17.45299804, 11.32050294]), 'currentState': array([23.202831 ,  3.0236154,  1.0307121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.32263260624836293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.90134  , 13.924364 ,  3.7089465], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.184510986332366}
episode index:84
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.3365597 , 13.62938911]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([17.721262 ,  3.917891 ,  1.7666862], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.3300250467619116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.302164 , 13.430231 ,  2.0780177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.039560173792011}
episode index:85
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.2156495 , 13.87352019]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([23.550613 ,  7.0765867,  4.5801544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.334449793046916
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.129833 , 13.750191 ,  2.6843786], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.249343677357404}
episode index:86
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.86344281, 12.72993181]), 'previousTarget': array([14.41421356, 10.89949494]), 'currentState': array([14.262973 ,  2.7479763,  0.7108394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 371
reward sum = 0.024024656984952455
running average episode reward sum: 0.3308816880347095
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.875077 , 15.37985  ,  0.3909312], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3998649909679889}
episode index:87
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.72615943, 12.74253001]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([13.521945 ,  2.8153014,  1.6980218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.3297151968085803
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.078674, 13.846721,  5.43635 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.476107777043272}
episode index:88
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.122572 , 10.174629 ,  5.5717854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.826927296839383}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.3298055249444266
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.500041 , 14.60029  ,  1.5546622], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.552303070665811}
episode index:89
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.51773027, 17.0155321 ]), 'previousTarget': array([11.63778901, 16.96128974]), 'currentState': array([ 2.8629084, 22.02493  ,  5.253437 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.33435991214935834
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.017764, 13.176664,  4.574307], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6932901961590914}
episode index:90
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.93782243, 11.09621678]), 'previousTarget': array([ 9.92623989, 11.09710761]), 'currentState': array([2.0189974, 4.989482 , 2.2817132], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.3362898663702397
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.300188  , 13.644535  ,  0.24763578], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.174085050463267}
episode index:91
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.647685 , 17.107346 ,  5.5751677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.64836194931985}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.34297139010425887
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.934294 , 14.167501 ,  6.2681327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8350875092666894}
episode index:92
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.88881339, 19.46197246]), 'previousTarget': array([21.40743398, 20.49208627]), 'currentState': array([27.274967, 26.203243,  3.543422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 260
reward sum = 0.07330786904388821
running average episode reward sum: 0.3400717823509215
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.035211 , 15.712597 ,  4.3309374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7134662706622505}
episode index:93
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.238998, 10.68553 ,  1.770019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.020336423826887}
done in step count: 290
reward sum = 0.05422585810406326
running average episode reward sum: 0.33703086826318895
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.846153 , 15.837829 ,  3.8342683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4259449935610524}
episode index:94
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.95488167, 20.19262161]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([11.981751 , 29.996027 ,  3.8718438], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.3404545648181447
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.518473, 16.642345,  5.558029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7114809594294844}
episode index:95
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.80862246, 20.63824708]), 'previousTarget': array([ 7.80868809, 20.75304952]), 'currentState': array([-0.06100044, 26.808258  ,  5.4929457 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.34201118677841613
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.45262  , 13.078501 ,  5.4776273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.467092545005459}
episode index:96
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.33471177, 22.75958076]), 'previousTarget': array([20.33471177, 22.75958076]), 'currentState': array([26.       , 31.       ,  0.7276344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.3436382573260531
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.773166 , 16.211798 ,  3.9863071], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7244060116338222}
episode index:97
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.85818091, 12.42183274]), 'previousTarget': array([15.90470911, 10.77802414]), 'currentState': array([19.016457 ,  2.9336667,  0.8037249], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.3422379438545363
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.962345, 15.739024,  5.661977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.096891770062569}
episode index:98
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.508171 ,  8.1799755,  2.6699996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.9847916261162855}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3481016484057826
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.73422  , 13.565529 ,  1.5677392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2506061812322313}
episode index:99
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02996487, 18.79390816]), 'previousTarget': array([15., 19.]), 'currentState': array([15.108944, 28.793596,  5.939521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.3473281745168448
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.277823, 14.989424,  5.710176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7222090269941066}
episode index:100
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.34449841, 13.59434387]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([ 2.0107806, 10.005234 ,  4.1070447], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.34610402104616955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.271632 , 13.122674 ,  1.3275256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8968755897912477}
episode index:101
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38125483, 18.5563288 ]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([27.145378 , 24.858582 ,  1.8191899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.34926939102189025
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.358077 , 15.482272 ,  3.7269049], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8029019239962363}
episode index:102
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.41793645, 16.35457773]), 'previousTarget': array([19.38476052, 16.25278872]), 'currentState': array([28.978634 , 19.28597  ,  4.7552776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.3476726451279743
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.156397, 16.608599,  4.749804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9811217827974201}
episode index:103
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.76551688, 16.20432904]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([25.026554  , 21.83951   ,  0.11536849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.3443296389248207
{'scaleFactor': 1.0, 'currentTarget': array([15.78218243, 16.94162879]), 'previousTarget': array([16.65213496, 18.16140934]), 'currentState': array([19.518856 , 26.217257 ,  3.8790038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
episode index:104
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.47213595, 16.05572809]), 'currentState': array([11.659149 , 24.308422 ,  0.0921809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.889793023557125}
done in step count: 186
reward sum = 0.1542219517938446
running average episode reward sum: 0.3425190895235733
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.612841, 16.321022,  4.877689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4562530270570808}
episode index:105
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.68237552, 16.11955927]), 'previousTarget': array([18.44220991, 16.05914151]), 'currentState': array([28.249956 , 19.028408 ,  3.4131284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.3438174137410059
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.982868 , 15.4690485,  3.1162314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0375899421833545}
episode index:106
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.80469734, 13.13896267]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([13.761    ,  3.193577 ,  6.1549964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.34888813764731547
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.035793 , 14.674702 ,  2.0971892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.017601858197361}
episode index:107
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.55939075, 18.88658866]), 'previousTarget': array([15.58578644, 19.10050506]), 'currentState': array([16.983995 , 28.784594 ,  2.3433757], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.34975998805452707
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.4816675 , 14.991378  ,  0.19066781], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5183569625859825}
episode index:108
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.51499382, 12.96383566]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([17.967016 ,  3.2691162,  3.1372056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.34762980450670966
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.524201 , 14.174681 ,  2.2376242], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9526469676716257}
episode index:109
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05572809, 17.52786405]), 'previousTarget': array([20.05572809, 17.52786405]), 'currentState': array([29.       , 22.       ,  0.1675708], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.3467868950175242
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.918728 , 14.946911 ,  3.4264834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.919462194039531}
episode index:110
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.858075 , 13.679583 ,  5.9490147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.745707450513701}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.35267169776511403
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.858075 , 13.679583 ,  5.9490147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.745707450513701}
episode index:111
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32050294, 10.54700196]), 'previousTarget': array([ 8.32050294, 10.54700196]), 'currentState': array([0.       , 5.       , 4.3349524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.35313652164826775
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.42461 , 15.940449,  5.334792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0318612509837168}
episode index:112
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.37979203, 12.74935113]), 'previousTarget': array([15.35601013, 12.86393924]), 'currentState': array([17.043745 ,  2.8887599,  3.7905498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.3544794364017085
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.033645, 14.312874,  3.111197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.687949361172191}
episode index:113
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.53316587, 11.25997378]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([7.0272055, 2.9122555, 3.995815 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 243
reward sum = 0.08696655909824688
running average episode reward sum: 0.352132832214836
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.948645 , 14.588122 ,  2.4754128], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9916975446347511}
episode index:114
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.84384608,  8.94834015]), 'previousTarget': array([19.82178448,  8.86318339]), 'currentState': array([26.092777 ,  1.141237 ,  3.5296428], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.3529233987314563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.262723 , 13.564098 ,  2.5458243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2538733343521438}
episode index:115
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.674215, 20.935423,  4.240408], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.510043973024679}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.3576773528372955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.901649 , 14.043287 ,  4.7417097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3146372160675301}
episode index:116
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.9705206 , 15.00625883]), 'previousTarget': array([14.80580676, 15.03883865]), 'currentState': array([ 5.1885586, 17.083088 ,  0.9515383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.35953784341113854
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.031033 , 15.62241  ,  0.7262299], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.065000425090047}
episode index:117
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.94405117, 11.01312356]), 'previousTarget': array([18., 11.]), 'currentState': array([23.884346 ,  2.9686909,  3.0548892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.36099011205134174
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.151257, 16.365145,  2.945899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3734987004448247}
episode index:118
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.160637 , 12.212508 ,  3.3713374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7921165341707876}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.3650401379448928
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.334927 , 13.164036 ,  1.8515854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.478554813646209}
episode index:119
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.817183 , 24.131039 ,  5.8782434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.555751387752267}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.3652056041194644
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.670341 , 15.5253725,  4.4811387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.429688971079867}
episode index:120
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7552327 , 13.10742758]), 'previousTarget': array([11.07959385, 13.19058177]), 'currentState': array([1.6219223, 9.035249 , 1.7135985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.36429406822340527
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0956   , 14.14875  ,  6.2386775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.085992549063842}
episode index:121
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.052077 , 10.940984 ,  4.1252556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.059350290551353}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.3632950729596444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.537478, 16.468842,  1.350257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.072791547682581}
episode index:122
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.097307 ,  8.758571 ,  2.0463386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.451307297557841}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.3670582725602618
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.169    , 16.907118 ,  4.3843045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.080302866105429}
episode index:123
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.33353832, 14.78271289]), 'previousTarget': array([18.25608804, 14.24859507]), 'currentState': array([26.203377 , 13.174519 ,  2.7700517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.3712463902953898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504338 , 16.92722  ,  4.1541367], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.989938475585429}
episode index:124
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.3266954 , 20.53596192]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 0.63025045, 26.920689  ,  3.121381  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.37215635013522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.603548 , 13.273384 ,  5.7767997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2206486743500697}
episode index:125
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.2064536 , 14.20116955]), 'previousTarget': array([14.07106781, 14.07106781]), 'currentState': array([7.158889 , 7.1066766, 4.7365584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.3756940532103153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.03839  , 14.258301 ,  1.0962343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.097148277309278}
episode index:126
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.9577465,  8.909181 ,  5.795169 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.165659642819877}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.37879916417562903
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.009113 , 13.984016 ,  1.2894493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.419182534729446}
episode index:127
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.71195 , 15.19345 ,  1.201544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.7169877505720232}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.382559001601142
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.653223 , 16.10017  ,  2.4590526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9858299875591139}
episode index:128
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05572809, 12.47213595]), 'previousTarget': array([20.05572809, 12.47213595]), 'currentState': array([29.       ,  8.       ,  1.0967677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.38063203007600105
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.291241 , 14.639309 ,  1.8719668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.46359377599004514}
episode index:129
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.90163263, 14.33663123]), 'previousTarget': array([11.55779009, 13.94085849]), 'currentState': array([ 1.0301096, 12.73881  ,  2.2255678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.38166668459406106
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.190187  , 13.221644  ,  0.55016863], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.139881961868165}
episode index:130
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.00195  , 10.507235 ,  2.6522052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.016688413156025}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.382725263510521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.273295 , 13.340988 ,  2.0638845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8111929383615117}
episode index:131
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.39478721, 12.37205427]), 'previousTarget': array([ 9.41178475, 11.40757591]), 'currentState': array([0.34050965, 8.12705   , 2.1309268 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.3858987887077068
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.339868  , 14.642135  ,  0.73672754], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6982659419318313}
episode index:132
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.82494959, 15.33393773]), 'previousTarget': array([17.51316702, 15.83772234]), 'currentState': array([26.661623 , 17.133898 ,  4.8302617], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.38546137493876015
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.865799 , 14.796132 ,  2.6432025], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8769037930092274}
episode index:133
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.0236426 ,  8.63998607]), 'previousTarget': array([18.85504245,  8.57492926]), 'currentState': array([22.317265  , -0.39133778,  3.630886  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.3852620236159477
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.332323 , 13.604777 ,  0.7159128], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5467510332501502}
episode index:134
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.066307 , 21.804276 ,  1.7065144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.463270347986951}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.3836462599966805
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.606396, 16.95146 ,  4.78336 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.527588355571268}
episode index:135
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9658619 ,  9.41176633]), 'previousTarget': array([10.81238194,  9.13733471]), 'currentState': array([5.112681 , 1.3037288, 4.9996204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.3863747381379081
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.981439 , 13.678079 ,  1.0382622], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6688149395468777}
episode index:136
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.251513 , 17.581179 ,  5.9437394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8685827350604503}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3902898473079082
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.381016, 16.086483,  4.955217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7571709908114381}
episode index:137
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.99136  , 21.155338 ,  1.7001184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.851284975988027}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.39167301083551836
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.045238 , 16.405132 ,  5.4824467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.405860305270628}
episode index:138
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.84545081, 17.07091767]), 'previousTarget': array([13.78852131, 17.22104427]), 'currentState': array([ 8.976008 , 25.805248 ,  1.4884654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.39194794610665773
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.780902, 16.95898 ,  4.284684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.108888955570651}
episode index:139
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.93266981, 20.2013486 ]), 'previousTarget': array([ 7.55689083, 21.45069462]), 'currentState': array([-0.12123854, 26.12879   ,  4.710795  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.3932579947057311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.234357 , 16.933641 ,  2.5358922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.079706368961264}
episode index:140
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.097721 ,  8.051795 ,  5.7880397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.617674457377733}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.393874189470736
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.619122, 13.338195,  2.914356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3201618402081325}
episode index:141
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.94963748, 14.502445  ]), 'previousTarget': array([16.05572809, 14.47213595]), 'currentState': array([24.80747 ,  9.861454,  6.218249], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.3934316465048929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.425816 , 15.851204 ,  1.8782871], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6605716441803755}
episode index:142
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.299343, 19.005527,  6.010865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.876104295700502}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.3913263354409136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.041948 , 14.237842 ,  1.5304687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7633119144586092}
episode index:143
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.70369481, 12.71828916]), 'previousTarget': array([14.7124705, 10.974587 ]), 'currentState': array([13.415898 ,  2.8015568,  1.9151694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.394521692632955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.795555, 14.37347 ,  1.365404], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9017249083549843}
episode index:144
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.026077 ,  6.055567 ,  2.5425162], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.442458850110304}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3982288902555346
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.977283 , 13.955198 ,  1.0911236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2363498315308528}
episode index:145
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.380033 , 15.708821 ,  5.6124077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.664491317780452}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4015724243751277
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.843367  , 13.662058  ,  0.95473737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5815676989198746}
episode index:146
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.83096706, 14.06835968]), 'previousTarget': array([14.78885438, 13.8386991 ]), 'currentState': array([13.045754 ,  4.228999 ,  3.9533424], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.4011382881610037
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.163216 , 13.080025 ,  1.5070546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.657081457941554}
episode index:147
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.43535882, 19.01545314]), 'previousTarget': array([14.41421356, 19.10050506]), 'currentState': array([13.042888 , 28.91803  ,  1.5389984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.40197922194049396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.018728 , 15.483699 ,  4.4964714], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4840612772658006}
episode index:148
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.134914 , 11.053401 ,  3.7837899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.4763407566377005}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.40284449926273974
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.23283  , 15.668714 ,  1.5082346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8894622194850434}
episode index:149
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.77611072,  7.46515703]), 'previousTarget': array([21.45069462,  7.55689083]), 'currentState': array([28.462896  ,  0.02964104,  1.3540782 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.4032961135448925
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.072511 , 14.24456  ,  1.5996422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1962130220977425}
episode index:150
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.58294437, 13.01078216]), 'previousTarget': array([11.63778901, 13.03871026]), 'currentState': array([2.9406846, 7.979744 , 1.5555267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.40533092886759053
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.863705, 16.924585,  5.523653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1095057517994453}
episode index:151
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.254856 , 18.658426 ,  1.2424431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.56570702066949}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4067664187756406
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.697758 , 13.163455 ,  5.7204113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.251384564135674}
episode index:152
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.26817864, 13.12351747]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([26.94036 ,  8.144232,  1.155066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.40677984201516293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.281704, 14.598751,  2.541756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.343043453562011}
episode index:153
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.40303433, 13.08134512]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([27.11392 ,  8.170086,  6.194541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.40635377850541543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.888039, 15.248632,  3.719616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9043392486559363}
episode index:154
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.210629, 21.888939,  2.111262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.073838719320733}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.40809168347673713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.825514, 16.96297 ,  4.786549], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6806258374030256}
episode index:155
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.999713 , 14.140697 ,  1.5261931], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.0915399290873475}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.40967865486838717
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.431155, 16.022009,  6.196063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8723718046255255}
episode index:156
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.1739361 , 13.72864134]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([28.740017 , 10.814864 ,  0.4843778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.41217516400641685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.1723   , 15.262628 ,  3.4885027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2013581236902036}
episode index:157
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42089759, 15.8874583 ]), 'previousTarget': array([15.52786405, 16.05572809]), 'currentState': array([19.706106, 24.922777,  2.487436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.41371620233912393
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.939014, 16.699854,  4.928347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.578619834308063}
episode index:158
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.67307984, 15.82451412]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([10.987235 , 25.120457 ,  1.3954688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.41413397123366663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.804832 , 14.658393 ,  5.4486995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2430289050567218}
episode index:159
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52776533, 14.62257543]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([23.661823,  8.805609,  4.859792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4167091878124286
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.955448 , 13.573705 ,  2.2893744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4203503252367278}
episode index:160
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.48871068, 15.54703343]), 'previousTarget': array([15.41495392, 15.47423305]), 'currentState': array([22.151052 , 23.00446  ,  2.5251274], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.4166858367771582
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.794653 , 15.832612 ,  4.5666566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1509631177694681}
episode index:161
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.77045611, 18.94103068]), 'previousTarget': array([16.80941823, 18.92040615]), 'currentState': array([20.868313 , 28.062849 ,  1.9234748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4159992515250047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.989405, 15.775335,  4.491314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1351524114411373}
episode index:162
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.80404246, 19.41590594]), 'previousTarget': array([19.6284586 , 19.24275371]), 'currentState': array([27.166264 , 26.183306 ,  1.4754168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.416276691499809
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.458452 , 16.972258 ,  6.0981936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4529335474774916}
episode index:163
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.02128657,  8.04747691]), 'previousTarget': array([20.08636336,  8.06404996]), 'currentState': array([25.876207  , -0.05930436,  4.924067  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 262
reward sum = 0.07184904244991483
running average episode reward sum: 0.41417652290804136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.842772 , 14.184032 ,  1.6171427], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.01534363596037}
episode index:164
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.07072802, 22.45620567]), 'previousTarget': array([22.1768175 , 22.68944732]), 'currentState': array([28.95174  , 29.712353 ,  1.1248263], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4154834750795274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.936832 , 14.480445 ,  5.6089745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.005307295040995}
episode index:165
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.86477341, 15.88293241]), 'previousTarget': array([15.92893219, 15.92893219]), 'currentState': array([22.861994 , 23.027084 ,  1.9509532], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4168908193764483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.953687  , 16.628223  ,  0.05337256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6288819549743763}
episode index:166
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.08564806, 17.62876151]), 'previousTarget': array([16.15384615, 17.76923077]), 'currentState': array([19.902815 , 26.871555 ,  0.9127725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.41580294997443606
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.410925 , 13.212035 ,  3.4242477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2776143893398535}
episode index:167
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.5880478 , 14.72425802]), 'previousTarget': array([13.8386991 , 14.78885438]), 'currentState': array([ 3.7734544, 12.807553 ,  6.1690426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4176868810439597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.1115885, 14.207914 ,  4.55013  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0478031519644553}
episode index:168
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.64327028, 15.27544733]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([ 0.66319656, 15.906422  ,  1.3484774 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.41758627614400456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.148401 , 13.0138035,  4.8258004], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7153995102878468}
episode index:169
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.95716258, 13.84502664]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([4.2521296, 8.923401 , 1.7057332], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.41807010410727036
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.041766, 15.550133,  5.361975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1049244877094468}
episode index:170
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.261114 , 14.846462 ,  0.8060442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.262737232712844}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4196571156965207
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.460913 , 13.146466 ,  2.2337854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.930337394177787}
episode index:171
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.278225, 18.924519,  5.818351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.161378409757567}
done in step count: 396
reward sum = 0.018686891355133923
running average episode reward sum: 0.41732589346197774
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6791    , 14.15097   ,  0.62759066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5702317901416973}
episode index:172
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.20414  , 21.94481  ,  2.3189828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.173249763238003}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.41649446720510047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.022334  , 13.160603  ,  0.57394904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0830778961367464}
episode index:173
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.55957664, 15.86715191]), 'previousTarget': array([14.47213595, 16.05572809]), 'currentState': array([10.031206, 24.783085,  4.429528], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.41481133253229163
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0589   , 15.014826 ,  5.2308826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9411567383762836}
episode index:174
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.7254659 , 19.49706347]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([22.908445, 28.049065,  2.302823], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.41720963212610757
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.866646 , 16.619768 ,  4.4006186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.976901755101511}
episode index:175
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.12399235, 15.69826726]), 'previousTarget': array([13.89352217, 15.86059386]), 'currentState': array([ 6.304261 , 21.931389 ,  1.0736196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4195806783154483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.732779, 16.528801,  5.171399], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5519792822022127}
episode index:176
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.353037, 22.237383,  4.295172], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.976375580484831}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.42263726211027636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.862162 , 16.582573 ,  4.2210855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.949157059538771}
episode index:177
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.09145961, 16.68848003]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([24.872282, 22.970104,  4.624719], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.425192788856842
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.267923 , 13.862407 ,  2.4024298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1687178319319003}
episode index:178
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.92866576, 15.4595892 ]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 2.038778 , 16.939495 ,  3.2953067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.42640741909833846
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.285751 , 15.021084 ,  0.0662594], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.714378309115552}
episode index:179
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.140064 , 21.947104 ,  5.7018557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.271603739915289}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.42814793551106034
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.526282 , 14.064745 ,  5.2904963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7454357158516207}
episode index:180
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.83887087, 20.31200006]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 2.8703885 , 27.484186  ,  0.03592697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.43034694483882013
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.448821 , 15.883739 ,  3.8980563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0415342858798378}
episode index:181
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.80748912, 14.83197222]), 'previousTarget': array([17.13606076, 14.64398987]), 'currentState': array([27.789627  , 14.234543  ,  0.94824183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.4294404575031546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.155754 , 16.386885 ,  2.3935921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.805330068592108}
episode index:182
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.58037714, 10.92813325]), 'previousTarget': array([13.96116135,  9.80580676]), 'currentState': array([13.555264 ,  0.9808148,  0.3062717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.42877982369513007
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.632615 , 15.722549 ,  3.9058027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5465507378467194}
episode index:183
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.848248 , 14.865315 ,  2.0669973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.849272526571392}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.43093954543502383
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.846495, 16.849478,  3.11342 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8558372501437617}
episode index:184
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.30329   ,  8.693003  ,  0.94870174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.310531926051937}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.4301962112517276
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.542334  , 13.417942  ,  0.06944292], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6469261966776907}
episode index:185
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.700351 , 12.82407  ,  1.2022699], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.165921364526507}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4332059090406968
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.031921 , 14.655461 ,  1.2764709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9980090930820038}
episode index:186
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.7268102 , 19.26268196]), 'previousTarget': array([12.74099823, 19.195289  ]), 'currentState': array([ 8.021316 , 28.086418 ,  2.2217834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.43273214580169583
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.516565 , 15.357746 ,  5.6640644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5259622310930663}
episode index:187
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.581934 , 15.531883 ,  2.4925816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.600567254180045}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.4316322309598463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.22183  , 13.162591 ,  5.9125605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9953996794958542}
episode index:188
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.6993105 , 20.11196774]), 'previousTarget': array([15.6783628 , 20.08772099]), 'currentState': array([17.054674 , 30.019691 ,  5.0144114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.4337638792693185
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.415908, 16.526678,  5.098639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5823164424550014}
episode index:189
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.28880548, 20.30241315]), 'previousTarget': array([15.33480989, 20.02214842]), 'currentState': array([15.832667 , 30.287613 ,  3.0130842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.43419215947013157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.458247, 16.423758,  5.430344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4956858133797355}
episode index:190
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72940186, 15.42862749]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 1.814188  , 16.728064  ,  0.02337283], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4346431980208131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.302139  , 16.325005  ,  0.16015404], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1536871053419975}
episode index:191
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.3894721 , 16.54781901]), 'previousTarget': array([16.05914151, 18.44220991]), 'currentState': array([17.82967 , 26.245522,  4.175748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.4342292272743769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.171711 , 15.557726 ,  4.7272515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5835604880182355}
episode index:192
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.52169106, 14.55355125]), 'previousTarget': array([15.47423305, 14.58504608]), 'currentState': array([23.1194   ,  8.051642 ,  0.4759436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4367603954979704
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.732853 , 15.4324875,  2.1095715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8509517334108814}
episode index:193
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42069178, 17.25212078]), 'previousTarget': array([14.42535625, 17.298575  ]), 'currentState': array([11.929509 , 26.936852 ,  5.1284747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4392179050442883
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.029948, 16.746582,  4.611989], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9978861878301057}
episode index:194
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1783353 , 13.96068048]), 'previousTarget': array([15.21114562, 13.8386991 ]), 'currentState': array([16.869505 ,  4.1047206,  3.208704 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4402942369587709
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.42972   , 15.915301  ,  0.25999436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.078422865018999}
episode index:195
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.79005591, 14.44431192]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([11.255793 ,  5.0896883,  2.3594773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4427086400736937
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.930505 , 13.973513 ,  1.0308678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3854657922222715}
episode index:196
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.904804, 20.43802 ,  5.221509], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.257180359959691}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4452404751464211
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0764475, 14.39094  ,  5.3377376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0176740828290267}
episode index:197
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.13145251, 16.75322377]), 'previousTarget': array([16.96128974, 18.36221099]), 'currentState': array([21.553873 , 25.155447 ,  4.3808637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.44570021125783915
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.693691 , 14.457148 ,  3.3710604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7785608768887409}
episode index:198
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.988543 , 16.958447 ,  3.8137538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.225103335830493}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4471775990072383
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.503081, 13.51243 ,  3.625517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5703358726046484}
episode index:199
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.127323 , 16.118208 ,  3.9893541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1254333222357369}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4499417110122021
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.127323 , 16.118208 ,  3.9893541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1254333222357369}
episode index:200
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.955366  , 12.035014  ,  0.87294936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.186326559827127}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4522480569647963
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.896002 , 13.208771 ,  1.0159179], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6083184438561022}
episode index:201
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.931984 , 16.088322 ,  1.3547707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.07982130211084}
done in step count: 481
reward sum = 0.00795295639758183
running average episode reward sum: 0.45004857626891903
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.550688 , 15.138429 ,  3.0633502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5568542396360363}
episode index:202
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.50151577, 13.31973661]), 'previousTarget': array([16.35636161, 13.47409319]), 'currentState': array([23.164835,  5.863184,  4.318749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.44859897613600175
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.957731, 15.455621,  2.322686], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0100502275818504}
episode index:203
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.165787 , 15.920012 ,  4.9495535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.895860627386417}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.45061593387377274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.618364 , 14.614608 ,  0.8207199], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4343793963353229}
episode index:204
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.60899274, 15.93227837]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([10.74129  , 25.15404  ,  2.4734168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4525297253835784
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.001533 , 14.691006 ,  5.4400573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0451864487778662}
episode index:205
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.60206577, 15.58256937]), 'currentState': array([24.188396, 18.046494,  3.489174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.680276453865229}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4549494356967649
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.445557 , 15.944061 ,  3.1650558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7265241673149807}
episode index:206
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.164064 , 16.056665 ,  5.7916565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.224782830667756}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4555591795538729
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.038795, 15.403444,  3.377098], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.40530529708922336}
episode index:207
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.688612  ,  8.205739  ,  0.38829213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.306888301358979}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4560800738370388
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.696415, 15.327194,  2.060344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7276804472247544}
episode index:208
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.81943155, 13.75273117]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([4.1391096, 8.787652 , 5.772271 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4557580826611402
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.361695 , 13.393719 ,  0.9397558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2943804994423442}
episode index:209
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.38412308, 15.01949744]), 'previousTarget': array([12., 15.]), 'currentState': array([ 2.3844008, 15.09403  ,  4.991257 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.45473062740839254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.866888 , 14.58623  ,  1.3558815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9121915071763256}
episode index:210
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.94977499, 12.83712732]), 'previousTarget': array([19.48341683, 12.24097426]), 'currentState': array([29.113152 ,  8.833063 ,  1.1057416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4563367110872326
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.261843 , 13.438036 ,  2.9709973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.007978763854724}
episode index:211
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.83006059, 13.93464891]), 'previousTarget': array([15.86059386, 13.89352217]), 'currentState': array([21.976173 ,  6.0463476,  4.1673183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.45836524014680285
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.091383 , 13.346378 ,  1.5749481], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9813080147057143}
episode index:212
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.8785074 , 11.77574278]), 'previousTarget': array([16.96128974, 11.63778901]), 'currentState': array([21.9126   ,  3.1352615,  5.8309536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.45744668309095043
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.034759, 16.864002,  3.533335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7086303165948498}
episode index:213
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.03929168, 18.58704139]), 'previousTarget': array([13.57662651, 20.3376506 ]), 'currentState': array([11.452198 , 28.246593 ,  5.1695657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4582819111422258
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.02641  , 14.191124 ,  2.6352699], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.306826761931132}
episode index:214
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.14411186, 14.78667955]), 'previousTarget': array([16.1613009 , 14.78885438]), 'currentState': array([25.974697 , 12.95376  ,  0.6974575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.45823187068930293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.77767  , 13.4453335,  2.6990557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.361588084891526}
episode index:215
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.089202 , 20.182884 ,  6.0779943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.49281374798275}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.45974782564355254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.9438   , 16.633823 ,  5.4201646], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5392394960970615}
episode index:216
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2567467 , 18.16263654]), 'previousTarget': array([15.23303501, 18.02945514]), 'currentState': array([16.065897  , 28.129847  ,  0.97208387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4611422721352739
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.366561 , 15.574596 ,  3.3147388], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7315554292480304}
episode index:217
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.24035175, 10.92578532]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([22.05877  ,  2.1632028,  3.5967872], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.462932710203896
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.647151, 14.142521,  2.080055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9272393545601094}
episode index:218
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.30233735, 14.50458301]), 'previousTarget': array([14.32050294, 14.54700196]), 'currentState': array([6.1489296, 8.714769 , 3.2440627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.4622136522848293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.02806  , 14.630562 ,  1.2424196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0062482613458537}
episode index:219
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.33558706, 18.17164728]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 2.0661955, 23.794561 ,  2.0101948], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4622303210252155
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.320334, 15.893882,  4.685839], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9495466383246289}
episode index:220
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.38439564, 16.67242177]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 0.9825555, 20.079092 ,  1.2665081], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.46069818850535643
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.330957, 16.347214,  5.407793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5041950866251894}
episode index:221
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.115229 ,  5.919563 ,  3.8415272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.148665169831183}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.46082968438147737
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.956642 , 14.309056 ,  1.1766781], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1800709411818557}
episode index:222
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.21233253, 15.03364358]), 'previousTarget': array([13., 15.]), 'currentState': array([ 3.214103, 15.221808,  5.495553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.46225117171339514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.227855, 13.487494,  5.849551], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.329843738288456}
episode index:223
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.67176357, 19.29068042]), 'previousTarget': array([15., 21.]), 'currentState': array([13.908994, 29.261547,  4.233782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4641446257312645
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.011853 , 16.527355 ,  3.8285627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5070982261139187}
episode index:224
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.129432  ,  5.8870974 ,  0.20828503], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.154391496542496}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.46471716715646555
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.651832 , 16.01589  ,  3.3055248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2070282551360274}
episode index:225
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.0142417, 15.193733 ,  1.6677735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.987846508888428}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4647017016708976
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.529746  , 15.797121  ,  0.75946444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.672437928934117}
episode index:226
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.067595, 12.157617,  4.549886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.617744792303195}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.46353683526202794
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.897488 , 15.684646 ,  2.5472393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.128815190584635}
episode index:227
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.47268451,  9.67018109]), 'previousTarget': array([10.45942241,  9.63386285]), 'currentState': array([3.9987166, 2.0486498, 4.80125  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.46509109009683147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.286524 , 14.5736685,  6.1615067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7657177543345641}
episode index:228
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.58668289, 14.42926804]), 'previousTarget': array([16.60206577, 14.41743063]), 'currentState': array([25.996452 , 11.044561 ,  6.0797796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.4645955360202428
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.091515 , 15.4451065,  4.825753 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1787806821055076}
episode index:229
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.22469614,  8.35622587]), 'previousTarget': array([21.19548901,  8.32793492]), 'currentState': array([28.061861  ,  1.0587485 ,  0.81268376], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.46572764384605847
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.052204, 16.148954,  2.554793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.557956908668016}
episode index:230
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.60613217, 18.59126316]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 2.8633478, 24.919714 ,  3.8078463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.46625279163000405
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.604295  , 15.068841  ,  0.11579432], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3974019287029962}
episode index:231
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.978815 , 20.040894 ,  4.1553955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.60897599792646}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4673995613628678
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.252034 , 15.1009245,  3.3806534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.2714903030279218}
episode index:232
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.30818188, 12.00240807]), 'previousTarget': array([18.598156 , 10.6822128]), 'currentState': array([23.409178 ,  4.0791607,  2.563712 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.46589814685634234
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.62374 , 13.00108 ,  2.847777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.093975826158036}
episode index:233
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.0129957, 15.097101 ,  3.8179796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.987476314656604}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4669436813880344
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.978728  , 13.930414  ,  0.03914635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.478854178288855}
episode index:234
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.963396 , 14.102862 ,  4.7837424], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.096219021705151}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.46813614271970916
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.758474 , 13.018847 ,  2.4765587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6489993213320866}
episode index:235
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.54636144, 15.44012675]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 3.975442 , 18.33797  ,  4.1699567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.46824928897767737
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.847689 , 15.738842 ,  5.0913615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7543780592132587}
episode index:236
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.9503719 , 14.99503719]), 'currentState': array([ 5.114834  , 14.112865  ,  0.55317485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.924893847620561}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4697246377060299
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.671562, 16.366306,  6.242351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5224285534674773}
episode index:237
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.239433 ,  7.30682  ,  4.2007713], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.347391678811594}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.4697684058525231
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.482    , 16.099976 ,  4.4200897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2158412421332991}
episode index:238
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.15757561, 10.5425525 ]), 'previousTarget': array([16.25278872, 10.61523948]), 'currentState': array([18.671146  ,  0.86360824,  4.1766005 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.4708978283108317
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.29765  , 13.984425 ,  1.3465872], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.234782758238409}
episode index:239
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.5475793 , 16.68174834]), 'previousTarget': array([17.56338512, 16.63124508]), 'currentState': array([25.893152 , 22.19096  ,  4.9838223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.47091633753456036
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.693264 , 14.8772335,  3.3100946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6977086356497992}
episode index:240
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.32050294, 10.54700196]), 'previousTarget': array([ 8.32050294, 10.54700196]), 'currentState': array([0.       , 5.       , 2.0508578], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.470954616036788
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.042142  , 14.949373  ,  0.41589898], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9585125355160413}
episode index:241
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.01681858, 18.3789526 ]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([26.669338, 24.816261,  5.487957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4718574857468387
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.441412 , 16.571486 ,  3.7982943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.667809138566638}
episode index:242
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.85642931, 14.74157276]), 'currentState': array([10.920103 ,  7.6318216,  1.6206028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.422328145527803}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4737513452619011
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.619206, 13.259189,  5.133161], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7819730338211646}
episode index:243
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.236853, 13.048266,  2.510147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3106424367931115}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4759081020436146
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.236853, 13.048266,  2.510147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3106424367931115}
episode index:244
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.897008 , 12.974489 ,  1.1299903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.325618390033618}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.477583517430033
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.008573  , 16.962254  ,  0.85233086], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9622722961564805}
episode index:245
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.42898126, 22.79271398]), 'previousTarget': array([ 6.83941129, 23.70462796]), 'currentState': array([ 1.9826124, 30.437603 ,  6.066383 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.47655142457047456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.05249  , 15.37542  ,  4.3244886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1174415340666959}
episode index:246
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.105103, 16.76761 ,  3.569979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.084633943419026}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4786706495722135
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.105103, 16.76761 ,  3.569979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.084633943419026}
episode index:247
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.50045  , 20.815224 ,  4.2919383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.005678747230831}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.4791800665785253
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.328327 , 16.686981 ,  0.6749205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3749517034515857}
episode index:248
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.44157023, 17.18060752]), 'previousTarget': array([14.42535625, 17.298575  ]), 'currentState': array([11.960736  , 26.867994  ,  0.63416684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4801966260286292
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.898437, 16.070038,  4.702857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3972004904991036}
episode index:249
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.1794491 , 15.73318803]), 'previousTarget': array([18.25608804, 15.75140493]), 'currentState': array([27.923717 , 17.980238 ,  1.2282797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.4803572016151159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.843352 , 16.999405 ,  3.1108387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0055319691735507}
episode index:250
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.66973463, 18.05327766]), 'previousTarget': array([14.3216372 , 20.08772099]), 'currentState': array([13.594333 , 27.995285 ,  4.8479824], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.4817020212803833
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.816801 , 14.22692  ,  5.5651536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4133690932966514}
episode index:251
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.17581839, 15.84609554]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([ 1.4119393, 18.006342 ,  0.9916792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4826386423570416
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.72418   , 13.277133  ,  0.29875505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.14382528396906}
episode index:252
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.897349, 12.578928,  6.081513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.76375187776121}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.48293756526688675
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.991813 , 13.340592 ,  1.5117502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9416681628722317}
episode index:253
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.981989 , 19.970783 ,  3.0614996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.351351790228302}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.4829649381319943
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.656181 , 14.770337 ,  5.1987267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.672029204686095}
episode index:254
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.21432519, 13.42768311]), 'previousTarget': array([18.05572809, 13.47213595]), 'currentState': array([27.19721 ,  9.033622,  5.35531 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.4812991076764834
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.151556  , 16.799778  ,  0.37784803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9897381711113549}
episode index:255
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.9198065,  9.750792 ,  1.4753957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.0861959772111955}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.48227959307483464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.164751  , 14.001941  ,  0.07558158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.301446547369057}
episode index:256
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.117398, 11.000519,  4.527661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.9602799298842815}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.4809402982110706
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.458855, 13.938229,  3.265249], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8043324123453213}
episode index:257
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.36425349, 18.44818945]), 'previousTarget': array([17.45299804, 18.67949706]), 'currentState': array([23.01918  , 26.695723 ,  2.9738648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 255
reward sum = 0.07708584232989273
running average episode reward sum: 0.4793749708626939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.94829 , 16.260061,  2.682452], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3202559797167677}
episode index:258
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.09545309, 18.91667274]), 'previousTarget': array([20.07376011, 18.90289239]), 'currentState': array([28.023878, 25.010939,  4.382689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 159
reward sum = 0.2023000271287771
running average episode reward sum: 0.4783051834351498
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.495978, 16.974876,  4.658615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.482381501244731}
episode index:259
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.86602808, 14.04815044]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([25.774044 ,  9.504226 ,  2.1725652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.4782937790450366
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.955578 , 13.30061  ,  3.5229557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.590793797087559}
episode index:260
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.50572519, 14.25753918]), 'previousTarget': array([14.54700196, 14.32050294]), 'currentState': array([8.96415  , 5.9334207, 2.9603677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.47847501547599647
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504954 , 16.064785 ,  6.1298523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1742390350417315}
episode index:261
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([15.397251 , 24.597588 ,  4.9213324], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.605805323864326}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.48031517194364537
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.76717  , 16.797337 ,  4.8377423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5205770007248094}
episode index:262
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.816896  , 18.018265  ,  0.07083058], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.997873352539225}
done in step count: 268
reward sum = 0.0676444472200646
running average episode reward sum: 0.47874608173557087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.24932  , 14.036558 ,  3.9523265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.577662305734089}
episode index:263
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.856066, 11.634188,  5.920432], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.472972936602668}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.48039294221188933
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6441145 , 13.069342  ,  0.44904137], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.035270279192152}
episode index:264
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.692106, 11.395358,  3.01233 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.670484217086493}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4804663161276905
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.656242, 13.038639,  0.889773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0682337276493605}
episode index:265
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.48472136, 20.9402761 ]), 'previousTarget': array([ 8.54930538, 22.44310917]), 'currentState': array([ 2.6806688, 28.268637 ,  4.9338684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.480260014651228
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.64133   , 16.01396   ,  0.38010317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.075527305516649}
episode index:266
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.163062 , 16.882147 ,  4.6242537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.13288845561878}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.482023048491111
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.069078  , 16.431696  ,  0.26459807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4037910285811153}
episode index:267
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.87240212, 16.04999041]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([13.666048, 25.97696 ,  4.563552], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4835990150079681
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.990123 , 16.450193 ,  5.1116405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7559624317538847}
episode index:268
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.51900814, 21.08346654]), 'previousTarget': array([ 8.32793492, 21.19548901]), 'currentState': array([ 1.2278681, 27.92739  ,  3.6268768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.48383528871571085
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.05919  , 16.591936 ,  4.3702793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5101802383796428}
episode index:269
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.660163, 10.821814,  6.048952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.230018536670175}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4854608791072376
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.525513 , 13.066204 ,  2.0212111], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4630784961918777}
episode index:270
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.6745074 , 11.34500829]), 'previousTarget': array([13.57662651,  9.6623494 ]), 'currentState': array([10.265245 ,  1.9441077,  1.8845115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.48560898098332006
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.340832 , 13.064834 ,  5.9701285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0443511464327657}
episode index:271
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.90115236, 20.96393218]), 'previousTarget': array([ 7.55689083, 21.45069462]), 'currentState': array([ 1.7514458, 27.955477 ,  6.1157045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4860036775473575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.203873 , 13.527477 ,  0.3859746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.322584025199337}
episode index:272
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.76773377, 20.56931088]), 'previousTarget': array([12.71390676, 20.71523309]), 'currentState': array([ 9.047302 , 29.851465 ,  1.0182115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.48541188946060315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7002125, 16.360388 ,  4.446283 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8815160304544298}
episode index:273
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.79077473, 13.90836773]), 'previousTarget': array([14.78885438, 13.8386991 ]), 'currentState': array([12.908409 ,  4.0871305,  2.3797398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.48481257991718774
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.878803, 15.444948,  4.128197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.46115891938763065}
episode index:274
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.47423305, 15.41495392]), 'currentState': array([23.561275, 20.071033,  4.698052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.9504180020005}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.48555671266610373
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.710572 , 13.410928 ,  2.5541656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6152148373564352}
episode index:275
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.934155, 19.904799,  2.162817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.272381062814239}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4864241895620884
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.195614, 16.927433,  3.298796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.640228657038178}
episode index:276
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.37452477, 21.93112133]), 'previousTarget': array([ 7.31055268, 22.1768175 ]), 'currentState': array([-2.5421213e-02,  2.8657249e+01,  3.6085324e+00], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.48566544399579636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.904568 , 14.225592 ,  3.9643722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0559879552515596}
episode index:277
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.67447352, 17.13305138]), 'previousTarget': array([14.64398987, 17.13606076]), 'currentState': array([13.165833 , 27.018597 ,  3.9363694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.48463166274613134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.070019 , 15.731116 ,  3.4543676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0638213564367875}
episode index:278
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.070612, 17.00851 ,  5.63601 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.290323783581519}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.484936213067748
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.028242 , 13.515807 ,  2.0413063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.805577539160818}
episode index:279
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.645855 , 22.945738 ,  1.1108097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.581316142317206}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.48381332455996884
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.157096 , 15.509369 ,  4.4895453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5330438981617984}
episode index:280
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.62790478, 11.10098602]), 'previousTarget': array([ 8.57492926, 11.14495755]), 'currentState': array([0.09803955, 5.8816566 , 3.4867826 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.4836368761693348
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.604728 , 14.222916 ,  6.1377573], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.871837280376311}
episode index:281
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.4211744 , 15.42544868]), 'previousTarget': array([18., 15.]), 'currentState': array([29.375193 , 16.383322 ,  0.6496181], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.48411082391675664
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.57147 , 13.052273,  2.512224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5026306237153}
episode index:282
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.68631308, 15.3036312 ]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([-0.29740113, 15.874114  ,  2.8765144 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4849878293787271
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.107803 , 15.9527   ,  4.7177286], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9587795396697838}
episode index:283
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.43087452,  8.77800856]), 'previousTarget': array([21.92893219,  8.07106781]), 'currentState': array([27.006748 ,  1.2442256,  3.1041508], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.48573228497263854
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.667898 , 16.645136 ,  1.7541802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.342724139668202}
episode index:284
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.98504262, 12.95898252]), 'previousTarget': array([18.60059927, 11.72672794]), 'currentState': array([23.957127 ,  5.7902975,  2.6174448], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.48581738483676806
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.602535 , 13.181543 ,  1.7036692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4238200856458523}
episode index:285
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.19056897, 12.34098098]), 'previousTarget': array([12., 11.]), 'currentState': array([7.564709  , 4.073593  , 0.28922862], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.48591990138427515
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.272308  , 13.404041  ,  0.25843734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.352020969708898}
episode index:286
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.86238912, 13.49372185]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([2.8474107 , 9.165886  , 0.37756526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4874097876076179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.209268 , 13.113977 ,  0.5495668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.600731357943546}
episode index:287
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.73670124, 13.48077016]), 'previousTarget': array([17.92893219, 12.07106781]), 'currentState': array([24.263292 ,  6.896666 ,  1.7528027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.4873188576764042
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.999441 , 16.115849 ,  1.7627212], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.289734234960628}
episode index:288
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.42687 , 17.54928 ,  0.930634], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.38931485447914}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.48719679813519345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.586401  , 14.751701  ,  0.01860445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4352401855780856}
episode index:289
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.99649018, 13.07860203]), 'previousTarget': array([13.07106781, 13.07106781]), 'currentState': array([5.77907  , 6.1569805, 5.8884077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.48709132554763734
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3189335 , 16.483652  ,  0.11174066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.242143666068168}
episode index:290
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.924273 ,  9.213681 ,  4.6725187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.9915234361747585}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.48831418419999567
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.949341  , 13.699629  ,  0.92853296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3013575728323363}
episode index:291
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.91363664, 8.06404996]), 'previousTarget': array([9.91363664, 8.06404996]), 'currentState': array([4.       , 0.       , 4.1030197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.4875506638765289
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.652406  , 15.64718   ,  0.04855746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7346177303084697}
episode index:292
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.32806  , 11.59174  ,  2.9064012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.081875039035998}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.48919826911927117
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.839603 , 13.875376 ,  2.1103735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.156135539979625}
episode index:293
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7878157 , 17.23398216]), 'previousTarget': array([10.804711  , 17.25900177]), 'currentState': array([ 1.95341  , 21.919415 ,  2.2695339], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.4880382239574627
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.467344  , 14.0209255 ,  0.74034876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1145891375254113}
episode index:294
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.65171484, 15.04733952]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 4.742828, 16.394173,  5.256118], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4890740750072464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.736157 , 14.0313   ,  1.1722227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0039887403348173}
episode index:295
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.51830152, 10.03750307]), 'previousTarget': array([18.59242409,  9.41178475]), 'currentState': array([22.043625 ,  1.1200231,  3.04191  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.48843321796773653
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.527379  , 15.29574   ,  0.99623555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5575238099947748}
episode index:296
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.917914, 12.206937,  4.735229], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.794269114458119}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.4879489720599244
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.410496 , 13.726848 ,  1.5283864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9001092582582348}
episode index:297
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.19501387, 7.25751544]), 'previousTarget': array([7.07106781, 7.07106781]), 'currentState': array([0.0955774 , 0.21493056, 3.7707324 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.4876162034223885
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.606545 , 13.643502 ,  1.7131478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4124066941152367}
episode index:298
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.015514, 11.309631,  5.492755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.057282336094168}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.4868043274879823
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.149566, 13.153311,  5.010047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6142624584583736}
episode index:299
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.97141119, 19.19034596]), 'previousTarget': array([13.58979079, 18.66654394]), 'currentState': array([11.587517 , 28.902042 ,  0.6117144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.48563731307954433
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.95524  , 15.667172 ,  4.2034626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2396136454289866}
episode index:300
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.652454, 19.493048,  5.654474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.069363689665923}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4866080241968852
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.499552, 16.914597,  4.382012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.97869450709761}
episode index:301
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.19673615, 18.3164475 ]), 'previousTarget': array([ 9.13733471, 19.18761806]), 'currentState': array([ 0.51449597, 23.278172  ,  4.211106  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.48596833776419757
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.448521 , 16.31478  ,  5.0544705], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0336500929539674}
episode index:302
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.68258054, 13.91801667]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([26.956638  , 10.177448  ,  0.04229229], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.4851486157081715
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.987942 , 16.561329 ,  3.1993704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.527777732648788}
episode index:303
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.4650195 , 10.07577826]), 'previousTarget': array([20.65196555,  9.78280103]), 'currentState': array([26.182232 ,  2.6677382,  3.2087152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.48638187142834627
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.707057 , 13.349821 ,  1.5800626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7952771465239847}
episode index:304
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.15113175, 15.9047456 ]), 'previousTarget': array([15.47942816, 18.11628302]), 'currentState': array([16.798737 , 25.768082 ,  4.6737905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.4868110788693756
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.273819 , 15.193767 ,  3.0428064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2884720643303362}
episode index:305
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.91861543, 20.1610515 ]), 'previousTarget': array([ 8.06404996, 20.08636336]), 'currentState': array([-0.1627951, 26.050941 ,  3.734156 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.48732023090602705
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.700437 , 13.889694 ,  6.1833878], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1500074669633393}
episode index:306
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.920319 , 11.049431 ,  6.0121264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.43507262749205}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.48856265951144384
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.942422 , 13.9513   ,  1.8881903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0502797835663888}
episode index:307
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.13681661, 10.17821552]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([29.      ,  4.      ,  5.226294], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.4884148845832449
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.184505, 13.890398,  2.526818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1248372360179575}
episode index:308
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.83246188, 11.09892303]), 'previousTarget': array([18., 11.]), 'currentState': array([23.70782  ,  3.006942 ,  0.6831143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.488872518067452
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.192032 , 13.601993 ,  1.7965393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4111346165219127}
episode index:309
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.29029409, 14.92960012]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([ 3.298761  , 14.518183  ,  0.92319554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.4891702403127767
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.111939 , 16.522434 ,  5.2942405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.425402794142624}
episode index:310
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.6751949 , 18.63318219]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 3.0184479, 25.065462 ,  1.90691  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.48870542340935147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.316807 , 16.522934 ,  6.1545744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2699046715174127}
episode index:311
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.56196931, 15.54037919]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([ 1.6832497, 17.093086 ,  0.8733026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.48965725904203694
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.109574, 16.73051 ,  3.907661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.562884426310737}
episode index:312
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.11074133, 15.35786247]), 'previousTarget': array([10.77802414, 15.90470911]), 'currentState': array([ 1.1528064, 16.274122 ,  4.923642 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.49050410258248933
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.763707  , 15.089143  ,  0.18130273], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.2525484992378581}
episode index:313
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.01190456, 15.12353997]), 'previousTarget': array([15.00496281, 15.0496281 ]), 'currentState': array([15.971082 , 25.077433 ,  1.2473665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.49028381633908896
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.192675, 14.922556,  5.944539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8089838455081069}
episode index:314
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.91878968,  9.0562464 ]), 'previousTarget': array([10.54700196,  8.32050294]), 'currentState': array([7.3164845 , 0.17825189, 0.35732478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 192
reward sum = 0.14519690621578263
running average episode reward sum: 0.4891883023386975
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.440365 , 13.69792  ,  2.1547244], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9416651601293275}
episode index:315
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.00644  , 17.884499 ,  0.5554972], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.663155718025434}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4896134197201928
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.922947 , 16.418484 ,  6.1660223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.389523174051122}
episode index:316
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.953197 , 12.242549 ,  2.9460058], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.669012400989571}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.49116069599867795
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6965475, 13.131191 ,  3.347539 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9943982956330457}
episode index:317
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.90957985, 10.5298849 ]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.15878  , 3.1524394, 2.690619 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.4921624795591263
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.057573 , 14.895142 ,  6.1248846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9452549182043113}
episode index:318
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.911899 , 12.515264 ,  0.2712369], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.783982456029921}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.49366134012477164
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.477479  , 13.651226  ,  0.36815342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4464506215104307}
episode index:319
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.31049429, 13.50381565]), 'previousTarget': array([13.47213595, 11.94427191]), 'currentState': array([10.125122,  4.421819,  1.179908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.49296475549785673
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.325422  , 16.904873  ,  0.49901074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5362869041787404}
episode index:320
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.46664863, 13.62159494]), 'previousTarget': array([14.41743063, 13.39793423]), 'currentState': array([10.85803  ,  4.295402 ,  5.4645333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.4927682578593086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.952955 , 16.139822 ,  1.5490029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.261244878080307}
episode index:321
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.6453038 , 10.28053658]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([10.886273  ,  0.66868186,  4.008698  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4931938956647245
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.062704 , 16.281845 ,  1.0149493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5879705512694824}
episode index:322
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.47134347, 12.93722541]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([27.06808  ,  7.8287907,  3.5887518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.4943030717496476
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.900194 , 14.609024 ,  2.8052888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9814335104443405}
episode index:323
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.10050506, 14.41421356]), 'previousTarget': array([19.10050506, 14.41421356]), 'currentState': array([29.       , 13.       ,  6.0856776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.4936047496668304
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.159282  , 16.34569   ,  0.19850844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.586722463154331}
episode index:324
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.48607109, 19.95131271]), 'previousTarget': array([12.47213595, 20.05572809]), 'currentState': array([ 7.958882 , 28.867846 ,  3.3652039], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.4940238846869424
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.439196, 16.140165,  5.121792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2706213108265463}
episode index:325
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.108309 , 13.217758 ,  3.7013707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7855298104487094}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49557595866029536
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.108309 , 13.217758 ,  3.7013707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7855298104487094}
episode index:326
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.27657785, 19.84448853]), 'previousTarget': array([13.36336397, 19.58258088]), 'currentState': array([ 9.924863 , 29.26606  ,  3.9602757], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.4962775010985142
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.153979, 14.885875,  4.193178], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8495450771897572}
episode index:327
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.28490412, 14.23052122]), 'previousTarget': array([13.10366477, 14.13802944]), 'currentState': array([ 4.1610875, 10.1371155,  3.3656733], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 212
reward sum = 0.11875755691154309
running average episode reward sum: 0.4951265256589198
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.864487 , 13.479152 ,  1.3968587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4060943677642888}
episode index:328
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.47117054, 17.81283327]), 'previousTarget': array([10.51658317, 17.75902574]), 'currentState': array([ 1.976312, 23.088947,  2.604333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.4938729627614047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.339338 , 16.858473 ,  3.7355897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.290796439396198}
episode index:329
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.88210797, 14.99341055]), 'previousTarget': array([13., 15.]), 'currentState': array([ 2.8821564, 14.962297 ,  4.257938 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.49410244227569494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.13474 , 15.592441,  5.344472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9570848733771333}
episode index:330
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.929878  , 15.856073  ,  0.15891522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.991353562545131}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.49388259871037493
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.425085 , 16.873976 ,  3.7357476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.354283877550076}
episode index:331
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.64763821, 14.63117406]), 'currentState': array([ 5.924991  , 12.0551405 ,  0.10277002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.540858634208721}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.493626386589026
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.006417 , 14.973231 ,  3.8534057], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9937624346885263}
episode index:332
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.155079,  8.909512,  1.588969], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.092462459810291}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4949712927836566
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.97303  , 13.03973  ,  2.6871696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9604554487578627}
episode index:333
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.97628515, 15.3015672 ]), 'previousTarget': array([19.025413 , 15.2875295]), 'currentState': array([28.947649, 16.05781 ,  4.374681], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4956382366154368
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.113777 , 13.766005 ,  1.7849436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5192549690777228}
episode index:334
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.354271, 6.84095 , 6.264951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.92191278826964}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.49664980534628655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.920332 , 13.641671 ,  2.1036003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6407522934879546}
episode index:335
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.807952 , 19.915302 ,  4.3657994], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.981263171837296}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4980886452113274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.556114 , 15.819399 ,  4.6556473], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7586659483725877}
episode index:336
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.943893 , 12.012922 ,  1.4758253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.559119609261469}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.498989363147018
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.976332 , 16.503677 ,  3.4291267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.483330960364213}
episode index:337
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.912981, 16.068134,  2.529737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1909831887744664}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5004716431377073
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.912981, 16.068134,  2.529737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1909831887744664}
episode index:338
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.094864 , 11.977123 ,  0.7097721], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.64086831318339}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5004846645113042
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.556947 , 13.297079 ,  1.8240451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2321161053539593}
episode index:339
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.762783 , 13.928255 ,  3.1515074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.370657232938662}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5008282100302188
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157856 , 15.888815 ,  6.2763734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0453573124401077}
episode index:340
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.66317128, 15.85049006]), 'previousTarget': array([15.86059386, 16.10647783]), 'currentState': array([21.812273 , 23.736462 ,  1.3117684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5003205694067806
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.513976 , 15.579327 ,  4.4724746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5949565460469588}
episode index:341
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.43502091, 18.10166781]), 'previousTarget': array([15., 17.]), 'currentState': array([16.823965 , 28.00474  ,  0.5404803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.4995881614741832
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.824356 , 16.9779   ,  5.8233557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.300918352341202}
episode index:342
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.72022601, 18.67357128]), 'previousTarget': array([16.80941823, 18.92040615]), 'currentState': array([20.961006 , 27.729828 ,  1.5133924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5001016917586908
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.695954 , 16.357534 ,  4.3762803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.391166217338754}
episode index:343
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.22773349, 19.92471151]), 'previousTarget': array([11.09710761, 20.07376011]), 'currentState': array([ 5.1468153, 27.863379 ,  5.7898583], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5013303051385433
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.161657 , 16.693542 ,  4.4036965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8896837195249896}
episode index:344
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.72729074, 15.22582658]), 'previousTarget': array([15.42173715, 15.12652114]), 'currentState': array([25.277502, 18.191204,  2.174911], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5023700965863773
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.80803  , 16.674698 ,  3.0249205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.464463932125063}
episode index:345
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.703154 , 16.87187  ,  4.5377665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.061971091522506}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5026147979891262
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.821718 , 15.949292 ,  2.6180618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.054218320153387}
episode index:346
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.06421816, 14.07956601]), 'previousTarget': array([14.07106781, 14.07106781]), 'currentState': array([6.934928 , 7.067204 , 4.5946455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5022318436365631
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.49383  , 16.44067  ,  1.2003454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0842454218105106}
episode index:347
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.038877 , 12.250437 ,  4.5116906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.098154983316463}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5013250603982848
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.057366, 15.104193,  5.295213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9483745483717121}
episode index:348
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.407808 , 11.73481  ,  5.4409785], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.95832935942963}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5006338294555325
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.000087 , 13.1795025,  2.4799275], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0770261512509216}
episode index:349
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.78360959, 11.43377526]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([17.92972  ,  1.6667793,  2.2472315], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5000761871597403
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.255553 , 14.029964 ,  2.7756324], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0031333088630983}
episode index:350
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.80714 , 21.539125,  5.750849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.541968847165276}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5006355519201102
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.125935, 13.964626,  4.600349], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1410557661207283}
episode index:351
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.9757649, 14.8789854]), 'previousTarget': array([14.96116135, 14.80580676]), 'currentState': array([13.012096 ,  5.0736804,  5.4030533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5004593642777517
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.511429 , 15.0073395,  4.000621 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5114466531035438}
episode index:352
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.7176285, 16.972927 ,  0.6127117], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.714991745499442}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5013125405532795
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.560394 , 16.303595 ,  5.7531676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9421182923977411}
episode index:353
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.06015081,  9.53939943]), 'previousTarget': array([13.16227766,  9.48683298]), 'currentState': array([9.712655  , 0.11632816, 4.423977  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5017485481239599
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.327951 , 13.319794 ,  2.1226535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.141622798357701}
episode index:354
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.74252436, 15.67478529]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.177559, 25.017752,  4.675935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5029607081233487
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.568796 , 15.555708 ,  5.1248717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5353031436739164}
episode index:355
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.44828467, 17.26501027]), 'previousTarget': array([14.42535625, 17.298575  ]), 'currentState': array([12.081663 , 26.98093  ,  2.0093138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5026847847091772
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.808013 , 16.789478 ,  3.5595965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1501315520874846}
episode index:356
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.203697 , 10.163446 ,  2.6402562], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.174670988548003}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5035909019056097
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.332213  , 16.20079   ,  0.47443366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3739856397137888}
episode index:357
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.72644622, 18.29163905]), 'previousTarget': array([14.76696499, 18.02945514]), 'currentState': array([13.898245 , 28.257284 ,  1.8855623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5028028038199484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.724997 , 16.17251  ,  4.6159935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.085759621916407}
episode index:358
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.47423305, 14.58504608]), 'currentState': array([22.012981 ,  9.606035 ,  2.0170648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.847415681249311}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5040779938093077
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.670557, 13.745482,  3.302219], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.089156542834299}
episode index:359
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28846662, 15.79641194]), 'previousTarget': array([14.58504608, 15.47423305]), 'currentState': array([ 7.625958, 23.253689,  3.708687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 208
reward sum = 0.12362903413636196
running average episode reward sum: 0.5030211911435496
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.642451  , 16.395893  ,  0.44070214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9471661583092754}
episode index:360
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.23093229, 16.17861953]), 'previousTarget': array([17.22104427, 16.21147869]), 'currentState': array([26.072844  , 20.849873  ,  0.41659656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5031434500112705
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.014908 , 16.593544 ,  4.4966555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.545579186769044}
episode index:361
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.266165 , 22.12827  ,  1.8161621], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.55695142078205}
done in step count: 334
reward sum = 0.03484616555067069
running average episode reward sum: 0.5018498111039208
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.420885 , 16.39954  ,  4.0691943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5146240946927618}
episode index:362
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.06404996, 9.91363664]), 'previousTarget': array([8.06404996, 9.91363664]), 'currentState': array([0.       , 4.       , 1.9337333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5014457091854667
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.5354   , 13.993571 ,  0.5926429], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.108490668696676}
episode index:363
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.706226, 15.201199,  2.172426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.708311429047509}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5010636386049037
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.708357 , 15.793407 ,  3.6976743], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.063609328383988}
episode index:364
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.067354 ,  9.8919   ,  1.7741942], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.195190308958654}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5021686206224486
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.783664 , 13.595858 ,  2.4424706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6080250916033345}
episode index:365
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80177819,  9.45659711]), 'previousTarget': array([16.83772234,  9.48683298]), 'currentState': array([19.892906 , -0.0536567,  5.2841187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5018803943541414
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.850963 , 13.27424  ,  2.2517047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7321839682813698}
episode index:366
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.11391212, 14.21350995]), 'previousTarget': array([14.52576695, 14.58504608]), 'currentState': array([6.635028 , 7.5752654, 3.4606616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5018748266035831
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.707616 , 13.808923 ,  1.6239722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.385418769886494}
episode index:367
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.16566848, 11.19146036]), 'previousTarget': array([9.34803445, 9.78280103]), 'currentState': array([2.310493 , 5.0030665, 1.5742899], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 221
reward sum = 0.10848707650771466
running average episode reward sum: 0.5008058381522356
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.051283 , 15.251175 ,  5.4848657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9648377152139347}
episode index:368
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.959337 , 17.916065 ,  0.5858112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.212964086941277}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5020518819783812
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.033066 , 15.830867 ,  4.7943196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2748730984307777}
episode index:369
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.37814017, 15.79332236]), 'previousTarget': array([16.42507074, 15.85504245]), 'currentState': array([25.044779 , 20.782248 ,  0.6552845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5019926646124165
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.143545 , 13.542921 ,  5.9092565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.359979581766946}
episode index:370
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.899914 ,  8.1256485,  3.999939 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.059011605559277}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5027787067122311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.3142605, 15.428574 ,  3.064527 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5314461297797343}
episode index:371
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.94280007, 14.71512279]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([-0.04137167, 14.152704  ,  4.950535  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5033760229198807
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.131089 , 16.954983 ,  1.3391685], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7045859427019088}
episode index:372
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.14357069, 14.74157276]), 'currentState': array([20.221613 ,  7.8837757,  1.2284325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.826431313070954}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5035535703181576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.998489, 15.296173,  3.420434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0414890994903094}
episode index:373
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.46100888, 14.29728652]), 'previousTarget': array([15.45299804, 14.32050294]), 'currentState': array([20.946352 ,  5.936006 ,  1.0093186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5031760795361853
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.395121, 13.987024,  1.034604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8978297015576302}
episode index:374
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.95746  , 21.138561 ,  2.7107682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.813846655373483}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5030519026514059
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.52951  , 15.239633 ,  4.7757382], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5280009989929408}
episode index:375
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.77863911, 14.27514826]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([ 2.022571 , 12.079895 ,  2.0763097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5029406528236576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.293931 , 16.053823 ,  5.5765095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.005296814872581}
episode index:376
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85335695, 14.93262502]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.766552, 10.757703,  4.447301], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5023927640559535
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3092    , 15.966269  ,  0.79582924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.947428704786468}
episode index:377
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.03099218, 15.26386521]), 'previousTarget': array([15.00496281, 15.0496281 ]), 'currentState': array([16.19752  , 25.195593 ,  2.9313223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5033389693220522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7333355, 15.486583 ,  4.5778193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3569088940371645}
episode index:378
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.34912407, 20.03256156]), 'previousTarget': array([15.33480989, 20.02214842]), 'currentState': array([16.041191 , 30.008585 ,  5.7061844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5037064432871252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.619754 , 16.847883 ,  1.0637853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8865999987736117}
episode index:379
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.094365, 11.065705,  4.826543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.543704412009561}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.503941338032163
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.486749  , 13.281334  ,  0.19980359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.289921962429312}
episode index:380
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.98104526, 19.68797115]), 'previousTarget': array([17.11828302, 17.91263916]), 'currentState': array([23.34697 , 28.126387,  1.012353], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5045024645270872
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.994123 , 15.6130085,  4.2114954], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6130366659184188}
episode index:381
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.148504, 14.098202,  4.129701], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.198253576845637}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5056712801956028
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.082664 , 15.474817 ,  2.3970637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4819592366423509}
episode index:382
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.89784837, 13.61079754]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([21.325897 ,  5.212209 ,  4.1802053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5055431821996454
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0412245, 14.252932 ,  1.8126844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0964047178440364}
episode index:383
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.26333471, 12.79368494]), 'previousTarget': array([13.24695048, 12.80868809]), 'currentState': array([7.0782247, 4.9359236, 2.5072415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5063566294793266
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.496155 , 15.419853 ,  5.9505963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6558480912875028}
episode index:384
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.780706, 18.097256,  0.947142], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.783495200775592}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5057237878111991
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.154607  , 16.949146  ,  0.37641007], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.124584857097767}
episode index:385
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.82290534, 11.19880265]), 'previousTarget': array([20.36613715, 10.45942241]), 'currentState': array([25.914078 ,  4.147897 ,  2.9243238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5053524101688398
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.735617, 15.280223,  2.577773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7871828093573443}
episode index:386
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.48189892, 19.91848793]), 'previousTarget': array([17.52786405, 20.05572809]), 'currentState': array([21.986902, 28.84625 ,  5.496433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.504992409990815
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.88977  , 16.814903 ,  2.0172465], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6201348081221774}
episode index:387
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.79192767, 20.51145487]), 'previousTarget': array([20.08636336, 21.93595004]), 'currentState': array([24.46006  , 28.749918 ,  3.5973623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.505234584208535
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.090389 , 14.610084 ,  5.3316584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9490120637289194}
episode index:388
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1016237 , 14.65220983]), 'previousTarget': array([15.12652114, 14.57826285]), 'currentState': array([17.906326 ,  5.0535827,  4.4693766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5052869284330004
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.97358  , 16.702635 ,  0.6073975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.606526611354121}
episode index:389
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.33094909, 13.56230793]), 'previousTarget': array([11.33345606, 13.58979079]), 'currentState': array([2.0202243, 9.913965 , 1.3865029], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5056225657602591
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.072683 , 13.362547 ,  6.2065864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8817993477122112}
episode index:390
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.75485571, 15.02871952]), 'previousTarget': array([13., 15.]), 'currentState': array([ 4.822782  , 16.192297  ,  0.69053966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5050430760488136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.413164, 15.391647,  4.22437 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6344527183284658}
episode index:391
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.06448721, 16.26408897]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 2.8798614, 20.219168 ,  4.567486 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 275
reward sum = 0.06304904523214554
running average episode reward sum: 0.5039155402559139
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.717456 , 13.4249935,  0.5527456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.031148662371957}
episode index:392
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.024824, 16.102322,  5.22227 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.173047129149336}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5048438615600184
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.806987 , 15.074755 ,  3.0032074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8104418405718414}
episode index:393
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.80405053, 10.18663068]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([20.313643  ,  0.82272315,  5.104941  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.5041417970893973
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.935941  , 13.145957  ,  0.93674517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.076887224882783}
episode index:394
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.36139159, 15.34026006]), 'previousTarget': array([19.025413 , 15.2875295]), 'currentState': array([27.259167 , 16.76646  ,  2.6778154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5051095010757941
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.483295, 16.387878,  4.242336], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4809423029584554}
episode index:395
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.4606667, 16.035772 ,  0.8474396], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.610149247606934}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5049754964020332
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.673559 , 16.83183  ,  0.0987746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8606884072069374}
episode index:396
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.03357182, 13.94448003]), 'previousTarget': array([19.22197586, 14.09529089]), 'currentState': array([27.478188, 10.658261,  3.423951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.504786361684456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.748826 , 13.431836 ,  2.0970097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.006134152949639}
episode index:397
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.19339521, 16.83584454]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([24.861809, 23.254211,  4.652663], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5047489343259629
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.977505 , 13.861698 ,  3.5876536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.281722169399469}
episode index:398
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.60115692, 13.92323661]), 'previousTarget': array([11.55779009, 13.94085849]), 'currentState': array([ 2.0681086, 10.903139 ,  3.4581633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5047116945732768
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.021149, 14.779337,  6.170795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.991116453383376}
episode index:399
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.181335, 15.83848 ,  5.390816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3369366797698263}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5054742350073967
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.25795  , 16.517555 ,  2.0652323], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.971144761128809}
episode index:400
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.99613765, 15.98333092]), 'previousTarget': array([16.89633523, 15.86197056]), 'currentState': array([25.966745 , 20.402403 ,  0.7709675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.504983138338138
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.843295 , 15.8355055,  3.0164645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1871040550747347}
episode index:401
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.12133096, 14.50938758]), 'previousTarget': array([17.298575  , 14.42535625]), 'currentState': array([26.86416  , 12.256107 ,  0.7836886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5052020022885444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.941542 , 13.55407  ,  3.2675729], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.725460204558574}
episode index:402
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.190955 , 15.410332 ,  2.8857822], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4525881122344636}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5064297888833619
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.190955 , 15.410332 ,  2.8857822], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4525881122344636}
episode index:403
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.71281389, 14.08053232]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([21.839746 ,  6.1773243,  2.9051032], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5069707555840413
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.2119465, 14.824494 ,  1.7297688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.27517910993632366}
episode index:404
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45866738, 14.61279494]), 'previousTarget': array([15.47423305, 14.58504608]), 'currentState': array([23.099903 ,  8.162096 ,  3.4611435], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5078640273301769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.619722, 15.967849,  2.747076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.886857652456117}
episode index:405
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.6212634, 15.6756575]), 'previousTarget': array([14.41743063, 16.60206577]), 'currentState': array([ 9.731609 , 24.39869  ,  4.2215962], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5087963446808812
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.543792 , 16.037737 ,  5.4716687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.788138771143758}
episode index:406
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.2257198 , 13.09441883]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([26.8356   ,  8.008166 ,  5.1755023], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5091093401142547
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.231512 , 15.676961 ,  3.9802403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4053106770577644}
episode index:407
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.677038 , 14.442705 ,  2.3220973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8769026510121153}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5103125034963276
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.677038 , 14.442705 ,  2.3220973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8769026510121153}
episode index:408
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.76205396, 17.51780188]), 'previousTarget': array([16.25278872, 19.38476052]), 'currentState': array([18.658937 , 27.089012 ,  4.7405925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5109475417415814
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.992866 , 13.822078 ,  3.7442923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1779438545670915}
episode index:409
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.79074951, 19.43702823]), 'previousTarget': array([18.29411765, 21.17647059]), 'currentState': array([23.114872, 27.90188 ,  4.268876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5102692903298898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.862847 , 15.838591 ,  3.5799813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8497323617500094}
episode index:410
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.720413 , 15.21816  ,  1.9130316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7341900848363827}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5114608492341965
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.720413 , 15.21816  ,  1.9130316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7341900848363827}
episode index:411
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.20443898, 16.61286839]), 'previousTarget': array([ 9.94427191, 17.52786405]), 'currentState': array([ 0.7261479, 19.800657 ,  4.679921 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 142
reward sum = 0.2399924795841344
running average episode reward sum: 0.5108019454243663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.432505 , 16.23355  ,  4.4154906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9946641538292114}
episode index:412
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 0.      , 15.      ,  4.264758], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.510478542764558
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.779375, 14.297565,  2.440544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7362673696811547}
episode index:413
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.0082318, 20.0115017]), 'previousTarget': array([17.77114535, 20.9381686 ]), 'currentState': array([23.154863, 28.585426,  5.603481], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5107668159250378
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7429285, 15.89777  ,  4.8955584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.544739326639048}
episode index:414
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.9922696 , 18.01409113]), 'previousTarget': array([19., 18.]), 'currentState': array([26.973145 , 24.039507 ,  0.5281393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5103664433164173
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.063053 , 13.182559 ,  3.5628512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.105510318056792}
episode index:415
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.51575986, 18.06358758]), 'previousTarget': array([17.06080701, 19.8085497 ]), 'currentState': array([20.950329 , 27.026545 ,  4.3070683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5113801426062985
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.917848 , 16.360361 ,  3.8344805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6410443621066302}
episode index:416
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.828454, 14.387636,  6.120726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3219339737989664}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5125518928638373
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.828454, 14.387636,  6.120726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3219339737989664}
episode index:417
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.96115169, 17.86740429]), 'previousTarget': array([ 9.0618314 , 17.77114535]), 'currentState': array([-0.07223121, 22.156693  ,  4.1527643 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5128630404935522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.625244 , 15.588274 ,  5.3509974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6975014377429353}
episode index:418
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.25549  ,  9.907504 ,  5.2093115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.066266898488946}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5126445946263952
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.619553 , 14.134327 ,  1.5216355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9455845126650115}
episode index:419
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.811233 ,  8.224575 ,  2.2988904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.878920799211714}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.51371114561538
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.057681 , 13.599138 ,  0.8121342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4020487588970936}
episode index:420
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.88364533, 14.50562251]), 'previousTarget': array([14.90535746, 13.95893206]), 'currentState': array([12.592682 ,  4.771585 ,  2.8345065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5137145327218133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.73973  , 13.816376 ,  0.3141707], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.728943949039901}
episode index:421
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.2124729 , 16.54244029]), 'previousTarget': array([18.44220991, 16.05914151]), 'currentState': array([27.227207, 20.870785,  2.042373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5134659678917203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.966433 , 16.101557 ,  3.6847508], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1020681046644487}
episode index:422
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.80868809, 9.24695048]), 'previousTarget': array([7.80868809, 9.24695048]), 'currentState': array([0.       , 3.       , 2.7083313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5130917240307589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.990151 , 15.143181 ,  3.1584504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0004501789102584}
episode index:423
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.85839937, 14.68584288]), 'previousTarget': array([13.64763821, 14.63117406]), 'currentState': array([ 4.216814 , 12.032575 ,  4.4390945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5134125044655174
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.176018  , 15.177219  ,  0.85752773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8325713955660066}
episode index:424
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.79883539, 14.84967123]), 'previousTarget': array([15., 15.]), 'currentState': array([6.7884364, 8.863562 , 3.1094365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5135719254294059
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.351759 , 13.559052 ,  0.7544581], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9757484833463224}
episode index:425
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.73102367, 20.91400354]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 4.8781443, 29.022259 ,  0.2894532], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5143852738547859
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.521246 , 16.906189 ,  3.9456663], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.976171479598171}
episode index:426
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.96798214, 19.12359514]), 'previousTarget': array([18., 19.]), 'currentState': array([23.809725 , 27.239878 ,  0.6299016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5139876787950499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.534445 , 16.886175 ,  3.3284988], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9427810874250808}
episode index:427
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.05420548, 18.81988052]), 'previousTarget': array([18.03861062, 20.31756858]), 'currentState': array([21.79046  , 27.627144 ,  4.1810684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5137516086836921
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.084255 , 13.715487 ,  3.8186998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6809474240700752}
episode index:428
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.783247 , 11.996521 ,  6.2450657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.732945220122236}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5147266989849119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.404648 , 13.016528 ,  0.9338082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.430472418907405}
episode index:429
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.39559782, 18.74311129]), 'previousTarget': array([11.96138938, 20.31756858]), 'currentState': array([ 6.684216 , 26.95165  ,  4.5506244], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5143723857729946
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.09994  , 15.576206 ,  4.9509077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5848090772759436}
episode index:430
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([24.83023  , 16.043392 ,  6.0381927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.885449059747232}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5145686354729787
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.872004 , 16.66951  ,  3.1500683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5083182764872984}
episode index:431
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.0324494, 20.5099231]), 'previousTarget': array([17.99181166, 21.83842665]), 'currentState': array([20.493217 , 29.891985 ,  3.5038118], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5154709813978302
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.696142 , 15.833034 ,  4.2762856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8867211960830594}
episode index:432
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.85642931, 14.74157276]), 'currentState': array([10.266502 ,  6.419231 ,  5.3960905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.799775375544774}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.5146261093983054
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.009123 , 13.060415 ,  1.4014893], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.77949289712088}
episode index:433
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.191072 , 11.537178 ,  1.0944378], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.147724245173807}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5150289273164409
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.1637   , 13.32221  ,  1.2820641], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.041856059819085}
episode index:434
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.155214, 16.099644,  1.855963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0499216230469517}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.5142827952071273
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.459066 , 15.20417  ,  1.0128558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5544008070593696}
episode index:435
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96764291, 14.91831851]), 'previousTarget': array([13.95893206, 14.90535746]), 'currentState': array([ 3.9987977, 14.12957  ,  2.4726825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5139427712302147
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.213989 , 13.882404 ,  1.4086543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1068589080999183}
episode index:436
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.324295 , 21.942474 ,  3.2046113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.967250250780346}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.5130890907763795
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.010962 , 14.751979 ,  5.1949553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0044421926475064}
episode index:437
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.78664644, 12.81304104]), 'previousTarget': array([13.78852131, 12.77895573]), 'currentState': array([8.935179 , 4.0687137, 1.7897764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5137850219335047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.914125 , 14.824665 ,  1.7155042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9221390551856212}
episode index:438
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.18178966, 12.6211758 ]), 'previousTarget': array([17.31035268, 12.43294146]), 'currentState': array([23.941057 ,  5.2514877,  2.9948158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5135366095206227
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.301586 , 14.302421 ,  2.0118341], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9871164577068915}
episode index:439
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.03898863, 13.13387892]), 'previousTarget': array([ 9.28476691, 12.71390676]), 'currentState': array([1.9926641 , 8.871953  , 0.10274696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5136640290500695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.605914  , 13.387404  ,  0.27337828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.660050637257995}
episode index:440
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.808132 , 18.843115 ,  3.1995926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.964476369100159}
done in step count: 169
reward sum = 0.18295651830906084
running average episode reward sum: 0.5129141253975955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.924108 , 15.971249 ,  3.1702585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3406336791836493}
episode index:441
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.329819  , 15.307165  ,  0.65433073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.3439562156377836}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.51320755860277
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.343195 , 14.977618 ,  3.0675669], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3433814234672266}
episode index:442
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.51063502, 16.08185823]), 'previousTarget': array([11.33345606, 16.41020921]), 'currentState': array([ 0.78893363, 18.424618  ,  3.9462566 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5134708003016424
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.263564 , 13.979007 ,  0.7479586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0544634755411324}
episode index:443
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25950669, 21.11462058]), 'previousTarget': array([18.29411765, 21.17647059]), 'currentState': array([22.963564 , 29.939123 ,  6.2176757], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5131387317002271
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.180814 , 13.046483 ,  2.3294873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1183235261075133}
episode index:444
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.37375249, 20.8028455 ]), 'previousTarget': array([13.19058177, 18.92040615]), 'currentState': array([ 8.250574 , 29.913246 ,  1.9544922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5132528136300072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.29054  , 16.64037  ,  5.4237885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6659007053233417}
episode index:445
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.215118 , 18.086546 ,  1.5831609], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.79914667104383}
done in step count: 128
reward sum = 0.2762516676992083
running average episode reward sum: 0.5127214209261265
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.688883, 16.049887,  3.776011], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2557155890362226}
episode index:446
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.09464254, 16.04106794]), 'currentState': array([14.809729, 24.56655 ,  4.398579], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.568441290429956}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5137018876576117
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.160583 , 15.346421 ,  4.7689075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.38183036766685474}
episode index:447
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.10196981, 16.8826862 ]), 'previousTarget': array([13.03871026, 18.36221099]), 'currentState': array([ 9.796722 , 25.908474 ,  5.8332386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5136597822378407
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.451973  , 16.71183   ,  0.25338674], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7974137142977151}
episode index:448
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.00812433, 17.3247464 ]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 2.3667123, 22.35724  ,  4.597796 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5139044717983159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.539899  , 14.887794  ,  0.59642196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.47358561748944245}
episode index:449
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.61954542, 14.6215902 ]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([ 1.6816161, 13.509133 ,  5.56282  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5148546399707663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.322415  , 13.280197  ,  0.49177885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.40250122095426}
episode index:450
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.01665011, 13.45188433]), 'previousTarget': array([10.77802414, 14.09529089]), 'currentState': array([1.695841 , 9.829383 , 4.8676996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.5141707217826214
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.512062 , 13.008232 ,  0.0275132], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.050663970960062}
episode index:451
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.473897 ,  8.862711 ,  0.7009305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.625740973469682}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5150542317952342
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.994017 , 13.032417 ,  6.1385045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9675918002025428}
episode index:452
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.55907666, 18.78916557]), 'previousTarget': array([17.52786405, 20.05572809]), 'currentState': array([23.155893, 27.076242,  5.18555 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.515778048487483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.837357 , 14.560871 ,  3.5726843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8891038418273285}
episode index:453
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.95040792, 13.80247882]), 'previousTarget': array([15.86059386, 13.89352217]), 'currentState': array([22.166964 ,  5.9695716,  5.323167 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5162388464775058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.320936 , 14.155736 ,  0.7179593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5676907238121056}
episode index:454
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.47977687, 11.16936663]), 'previousTarget': array([14.41421356, 10.89949494]), 'currentState': array([13.134069 ,  1.2603267,  3.9421918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5164746410894041
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157309 , 13.172484 ,  5.4678817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5952504601414095}
episode index:455
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.05931841, 10.73808566]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([11.904013 ,  0.9731147,  5.375913 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5166293826263516
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.509295  , 16.726194  ,  0.35313419], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2807777310420487}
episode index:456
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.60202311, 21.46769693]), 'previousTarget': array([20.49208627, 21.40743398]), 'currentState': array([25.39958 , 29.6156  ,  3.174288], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.5158609651494116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.081958 , 14.726705 ,  5.9389906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9374148220017056}
episode index:457
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.80393589, 14.98722176]), 'previousTarget': array([14., 15.]), 'currentState': array([ 3.8045065 , 14.880392  ,  0.10643196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.5151632152561225
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.563795 , 16.125666 ,  3.2070057], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.926805198057398}
episode index:458
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.58420749, 10.20919634]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([13.719561  ,  0.24664718,  5.9244967 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5148068884397868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.302925 , 13.411382 ,  1.8628575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.324601302917926}
episode index:459
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.76184039, 17.10672113]), 'previousTarget': array([14.7124705, 19.025413 ]), 'currentState': array([13.63852  , 27.043428 ,  4.7346554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.514818918079375
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.162651 , 15.469887 ,  4.4560795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4972413436052999}
episode index:460
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.24284674, 11.24013507]), 'previousTarget': array([15.2875295, 10.974587 ]), 'currentState': array([15.887396 ,  1.2609289,  5.17573  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5149755728816702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6035595, 16.74064  ,  6.212939 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.231563560660158}
episode index:461
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.83323  , 19.019926 ,  5.8873606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.92797736321239}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5150814811448102
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.03133  , 15.244451 ,  2.6813507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0599046531419019}
episode index:462
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.35544583, 20.72972209]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 0.7822809, 27.260202 ,  0.3667616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5148974801348299
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.936892 , 14.51959  ,  4.8641033], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.052881376223546}
episode index:463
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.74291297, 10.91120075]), 'previousTarget': array([20.86266529, 10.81238194]), 'currentState': array([28.889149 ,  5.1113005,  3.9410806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5152736689402917
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.44455  , 13.803432 ,  1.6454123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2764788202538202}
episode index:464
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.49084623, 14.5692536 ]), 'previousTarget': array([16.35636161, 13.47409319]), 'currentState': array([23.007074 ,  7.973322 ,  1.3299633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5153304676650801
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.548601 , 14.742586 ,  1.2655789], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6059910228651472}
episode index:465
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.09278067, 11.54842614]), 'previousTarget': array([11.39940073, 11.72672794]), 'currentState': array([3.5982358 , 4.9278674 , 0.00692623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5154843438759651
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.47539  , 13.687032 ,  2.2088046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.012043728196954}
episode index:466
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.104801 , 11.807476 ,  0.7190577], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.200154028706816}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.515827480289636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.566139 , 13.725338 ,  2.2912552], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3464763031955407}
episode index:467
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.952627, 14.756902,  4.691986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.9563418641678085}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5158827315624506
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.881695 , 13.187801 ,  1.2362431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6124393205452936}
episode index:468
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.44704706, 13.65190612]), 'previousTarget': array([13.78852131, 12.77895573]), 'currentState': array([10.652137  ,  4.399952  ,  0.08131255], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5155324681829102
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.036447, 13.378808,  3.719931], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8859211650725196}
episode index:469
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.001627 , 21.800556 ,  1.4077077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.888887836457636}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.515917320842201
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.60809 , 16.563135,  5.587638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.093036985033477}
episode index:470
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.37377711, 12.20724735]), 'previousTarget': array([18.26042701, 12.3323779 ]), 'currentState': array([26.076979 ,  5.830674 ,  0.9632462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5165760284918686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.45132  , 15.228978 ,  2.2855024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4692728202329925}
episode index:471
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.24536001, 15.13005179]), 'previousTarget': array([12.96545758, 15.1695452 ]), 'currentState': array([ 3.2727153, 15.869212 ,  5.1906557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5168989008013555
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.766203  , 15.581097  ,  0.45857802], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6263660169503159}
episode index:472
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.028376 ,  7.9848547,  0.9039262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.089503927104717}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.517401692315948
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.22276  , 13.042573 ,  2.8405228], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.307955964328177}
episode index:473
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.73146759, 11.73230488]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([17.91589  ,  1.973806 ,  6.1261196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.518035669626668
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.560534, 13.831496,  1.91854 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.29599339164461}
episode index:474
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.47213595, 22.05572809]), 'previousTarget': array([11.47213595, 22.05572809]), 'currentState': array([ 7.        , 31.        ,  0.41810262], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5181442286431954
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.55565  , 14.094459 ,  4.3382363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8000151105268012}
episode index:475
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.191524 , 15.592801 ,  5.7289233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.87035766001653}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.519033589821258
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.252907  , 15.674758  ,  0.29253644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0067008253007534}
episode index:476
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00309863, 14.95531198]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([15.694829,  4.979265,  2.23699 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.5182408242512014
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.055492 , 13.004822 ,  5.5614805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2571664424650315}
episode index:477
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.343792  , 15.0019865 ,  0.21685952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.656208296042283}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5178491762680839
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.164035 , 15.022679 ,  4.9819403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8361052279335361}
episode index:478
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.51029741, 18.21404727]), 'previousTarget': array([18.60059927, 18.27327206]), 'currentState': array([25.885736 , 24.96704  ,  1.5847335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5178221130374346
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.935528 , 13.809686 ,  3.1912925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2722491030546674}
episode index:479
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.1352  , 20.314657,  4.633316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.117035570628861}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5177951625702464
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.889032 , 16.78915  ,  5.9726896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6018266364418166}
episode index:480
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.325073 , 10.511602 ,  3.0924704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.755789097130071}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5178910254140762
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.952066 , 14.967865 ,  5.9974346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9523309076165256}
episode index:481
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.9229764 , 15.03277884]), 'previousTarget': array([14.80580676, 15.03883865]), 'currentState': array([ 5.7215524, 18.948618 ,  1.1400193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.5171430217972581
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.122211 , 15.9528   ,  3.6155658], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1056868874857284}
episode index:482
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.69995848, 11.60395461]), 'previousTarget': array([20.36613715, 10.45942241]), 'currentState': array([26.067118 ,  4.841931 ,  2.4431453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5168150824098557
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.424658 , 13.489187 ,  0.5273886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5693595698427667}
episode index:483
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.73848351, 18.09836251]), 'previousTarget': array([19.77807808, 18.30790021]), 'currentState': array([28.10808  , 23.571009 ,  1.0999403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5162846493085497
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.712054 , 15.507455 ,  5.0239167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.384310187578076}
episode index:484
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.714958,  8.165424,  1.816232], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.90910609890588}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.516975727910171
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.743816 , 13.051499 ,  3.3266466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9652696423337341}
episode index:485
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.02834083, 14.98916828]), 'previousTarget': array([17., 15.]), 'currentState': array([27.028198 , 14.935767 ,  3.5491009], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.517261084890961
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.796487 , 13.196503 ,  2.4843361], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.16818967216958}
episode index:486
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.79819421, 19.70175253]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([22.912401 , 28.295057 ,  2.3339489], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5170053352882236
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.160191  , 13.695883  ,  0.64064515], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3139188335590357}
episode index:487
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.040235,  7.117389,  2.022535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.313153394732549}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5176219369322994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.390664  , 16.89003   ,  0.23400295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.482372874319502}
episode index:488
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.53436903, 17.85607745]), 'previousTarget': array([12.93919299, 19.8085497 ]), 'currentState': array([ 8.968796 , 26.753017 ,  4.3426485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 409
reward sum = 0.016398140018627688
running average episode reward sum: 0.5165969393925987
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.795723 , 16.495396 ,  3.0894775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3368417009579616}
episode index:489
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.71523309, 14.71390676]), 'currentState': array([22.876404 , 10.82037  ,  3.6353037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.916672374248492}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5168673591660187
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.449816 , 13.043625 ,  2.0987878], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.007420690167736}
episode index:490
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.605103, 10.162821,  4.390072], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.013095311123168}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5166814584821543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.364574 , 16.91294  ,  3.8217204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.349765791169742}
episode index:491
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.00062514, 18.78235871]), 'previousTarget': array([17.11828302, 17.91263916]), 'currentState': array([21.676216 , 27.621977 ,  1.6175959], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5166888956044473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.554893, 14.166768,  3.943925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.764077441341128}
episode index:492
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.69883863, 12.3919928 ]), 'previousTarget': array([16.63124508, 12.43661488]), 'currentState': array([22.156927 ,  4.0128956,  2.3803866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5169443169157663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.454336 , 13.302288 ,  2.1567452], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7832484027999973}
episode index:493
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.18381  , 11.209841 ,  3.7735536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.9499442914341385}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5175535529900203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.809277, 14.28669 ,  2.217229], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3880323346086791}
episode index:494
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.59597762, 15.50360306]), 'previousTarget': array([18.44220991, 16.05914151]), 'currentState': array([26.132473, 18.512798,  2.93176 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5184099703565072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8610935, 15.642096 ,  3.1570194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.074136289176766}
episode index:495
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.55931598, 14.96758951]), 'previousTarget': array([12.86393924, 14.64398987]), 'currentState': array([ 4.5862517, 14.234112 ,  0.5503381], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5184903255343123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.516316, 16.010992,  5.867287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.120738478411108}
episode index:496
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.52880197, 15.53410056]), 'previousTarget': array([13.39793423, 15.58256937]), 'currentState': array([ 4.129062  , 18.946562  ,  0.44669783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5183844713082267
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.189928, 16.051529,  0.944302], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3273769805176348}
episode index:497
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.68438001, 16.74531635]), 'previousTarget': array([13.07106781, 16.92893219]), 'currentState': array([ 7.6649756 , 24.730726  ,  0.12978667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5179095239673276
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.737904 , 16.827684 ,  0.5853837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2211072938074645}
episode index:498
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.04914551, 14.79732967]), 'previousTarget': array([15.09464254, 13.95893206]), 'currentState': array([17.405748  ,  5.0789747 ,  0.34934804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5181855754635336
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.74424  , 13.760382 ,  3.9093223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1398658925077716}
episode index:499
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.348246 , 14.043069 ,  1.3759223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1578000380975622}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5191492043126066
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.348246 , 14.043069 ,  1.3759223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1578000380975622}
episode index:500
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8704747 , 15.02589531]), 'previousTarget': array([14.80580676, 15.03883865]), 'currentState': array([ 5.064526, 16.986347,  4.039648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 429
reward sum = 0.013412152484926368
running average episode reward sum: 0.5181397491193378
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.225992 , 14.415505 ,  0.9823266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3581939507493777}
episode index:501
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.0496281 , 14.99503719]), 'currentState': array([24.83278  , 15.905482 ,  1.1049995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.874384901847439}
done in step count: 166
reward sum = 0.1885568451673771
running average episode reward sum: 0.5174832094700311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.653032 , 16.732262 ,  3.6266026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7666683428561458}
episode index:502
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.23949699, 7.80057177]), 'previousTarget': array([7.31055268, 7.8231825 ]), 'currentState': array([-0.09161813,  0.9994872 ,  1.7942014 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5168902751954714
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.96937   , 13.382929  ,  0.85495543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6173612192428712}
episode index:503
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.49216268, 10.56754957]), 'previousTarget': array([16.63663603, 10.41741912]), 'currentState': array([19.682674 ,  1.0901747,  3.8229744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5165084403833047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.225346 , 14.59598  ,  2.0309505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8200633529667227}
episode index:504
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.37971361, 14.15856009]), 'previousTarget': array([12.48683298, 14.16227766]), 'currentState': array([ 2.8585868, 11.101087 ,  4.185291 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 202
reward sum = 0.13131347932828827
running average episode reward sum: 0.5157456780841858
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.51653  , 15.420416 ,  2.0993266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.573724871466522}
episode index:505
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.704852  , 18.84606714]), 'previousTarget': array([18.90289239, 20.07376011]), 'currentState': array([25.642466 , 26.048117 ,  4.7491813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5164606491215669
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.2710085, 16.468246 ,  3.5872812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4930483145771485}
episode index:506
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([ 7.086293 , 11.7024355,  0.8906082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.57325414684056}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5163426000064235
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.046747 , 13.910479 ,  0.8166133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4476697775411378}
episode index:507
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.17818782, 17.4247535 ]), 'previousTarget': array([17.31035268, 17.56705854]), 'currentState': array([23.860899 , 24.863932 ,  4.6530523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5163818236780786
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.371691 , 16.835564 ,  5.2287545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.453708410127758}
episode index:508
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.714796, 11.928298,  5.229275], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.240293484251414}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5158834558265505
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.021278 , 14.636498 ,  3.5158346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0840400860504014}
episode index:509
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.82332178, 13.90025343]), 'previousTarget': array([12.68964732, 12.43294146]), 'currentState': array([6.5174584, 7.07205  , 1.4925922], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5158228831097811
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.807337, 15.620752,  5.261054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9109682874786167}
episode index:510
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.48191579, 13.5845553 ]), 'previousTarget': array([14.41743063, 13.39793423]), 'currentState': array([11.044704  ,  4.1938376 ,  0.36454266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5158114601413658
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.888351, 13.606648,  6.081935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.782467692681998}
episode index:511
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.03920813, 17.85998812]), 'previousTarget': array([12.07106781, 17.92893219]), 'currentState': array([ 4.8467627, 24.807558 ,  2.5288796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5155261440818119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.075263 , 16.935184 ,  4.3529916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9366474871593105}
episode index:512
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.914455, 13.018733,  5.98055 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9831128823258837}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5164705375631339
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.914455, 13.018733,  5.98055 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9831128823258837}
episode index:513
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.129505 , 15.990264 ,  0.9062041], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3484920447784385}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5173918011087308
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.9838   , 16.435013 ,  1.0140344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.448413312642832}
episode index:514
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.054755 , 12.827963 ,  2.5921087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3688041249019753}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5182902636308498
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.149689 , 13.121351 ,  2.1525474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0621227963893403}
episode index:515
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.00962997, 21.70055988]), 'previousTarget': array([12.98274993, 21.45520022]), 'currentState': array([10.162147  , 31.286583  ,  0.28191885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5181796661575694
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.490221, 16.776642,  4.848183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.31888226241919}
episode index:516
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.42199841, 19.98003578]), 'previousTarget': array([11.14495755, 21.42507074]), 'currentState': array([ 5.5871425, 28.10127  ,  4.7817106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.518775389479964
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.512051, 15.842524,  6.246621], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7099237719289222}
episode index:517
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.68128773, 20.73869745]), 'previousTarget': array([21.40743398, 20.49208627]), 'currentState': array([29.267185 , 27.254383 ,  2.6020741], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5184876563683226
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.890307 , 16.952192 ,  0.1014451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9552716594687496}
episode index:518
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.937221 , 11.90862  ,  2.1419768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.575520525483512}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5190645721317695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.656086 , 14.672357 ,  0.5555568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3832769496016388}
episode index:519
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.4116285 , 15.81928319]), 'previousTarget': array([11.74391196, 15.75140493]), 'currentState': array([ 1.6625031, 18.045166 ,  1.069809 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5193789760219207
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.031232 , 13.085597 ,  5.1213565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.74608568866166}
episode index:520
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.12090025, 13.85087715]), 'previousTarget': array([17.22104427, 13.78852131]), 'currentState': array([25.913296 ,  9.087078 ,  3.8776903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5195202187673709
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.834303, 14.048961,  2.578745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.965365892032315}
episode index:521
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.095783 , 10.963818 ,  2.4845526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.951082404857111}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5202750023472872
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.617816 , 13.502873 ,  2.0620522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6195939518552305}
episode index:522
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.04106794, 15.09464254]), 'currentState': array([24.122057 , 15.70196  ,  3.2107763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.14902565822537}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.5197725350517167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.220379 , 13.0808525,  3.7776258], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0714575041169834}
episode index:523
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.00750646, 22.89287011]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 2.9605684, 30.85745  ,  3.9325795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5197441254214407
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.43918  , 16.853115 ,  5.6071615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.936118322595605}
episode index:524
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.73606259, 22.46561115]), 'previousTarget': array([19.7000106 , 22.52001696]), 'currentState': array([25.092913 , 30.90979  ,  1.3083583], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5194946774074459
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.38006 , 15.506433,  2.330316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4700476370237034}
episode index:525
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.7025293 , 16.20164631]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([24.872522 , 21.968033 ,  1.9681222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5198992566702729
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.385145 , 15.789143 ,  3.4745054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5941685127022969}
episode index:526
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 7.922071, 19.310333,  5.988572], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.287101541192278}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5206813555151243
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.742149, 14.080471,  6.177083], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9549976264871052}
episode index:527
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.91430414, 14.37799422]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([24.182396 ,  8.753169 ,  4.5786777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.520784903610696
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.018652 , 14.402572 ,  1.8773941], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.18092015817839}
episode index:528
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.84586334, 16.50123544]), 'previousTarget': array([18.66654394, 16.41020921]), 'currentState': array([28.161303 , 20.137524 ,  5.3608394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5212856469702357
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.545915 , 13.892259 ,  3.5899565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.234954945172875}
episode index:529
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.707302 , 11.011708 ,  0.7068075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.8034846306140935}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5220784667861428
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.307745 , 15.850555 ,  2.8336544], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5600132867073024}
episode index:530
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.6927487 , 15.80815564]), 'previousTarget': array([14.71390676, 15.71523309]), 'currentState': array([11.139033 , 25.155407 ,  1.9611661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5219212898276188
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.977148 , 14.229549 ,  3.8331606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.244352216287693}
episode index:531
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.963074  , 12.981563  ,  0.10789814], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.187533955161285}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5218427563064605
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.787066 , 13.033331 ,  2.9259236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3106265500747556}
episode index:532
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.97891635, 17.91100014]), 'previousTarget': array([19., 18.]), 'currentState': array([27.049608 , 23.815569 ,  1.3638413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5220453470661167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.677351 , 16.01767  ,  3.8829534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0675925962276556}
episode index:533
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02811806, 10.8792422 ]), 'previousTarget': array([15., 11.]), 'currentState': array([15.096352  ,  0.879475  ,  0.35925072], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5219489824491121
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.876694 , 15.707911 ,  2.3773766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1268226204227456}
episode index:534
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.36729908, 20.73136008]), 'previousTarget': array([15., 20.]), 'currentState': array([13.270037, 30.670979,  2.731134], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5219757473888469
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.170669 , 16.696388 ,  5.7421103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.888259421859397}
episode index:535
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.794736 , 12.910856 ,  1.3565204], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.099203412766637}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.522848926964614
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.862717 , 13.017825 ,  1.3764868], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.986923234532452}
episode index:536
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.32001137, 10.4668341 ]), 'previousTarget': array([20.36613715, 10.45942241]), 'currentState': array([27.931522 ,  3.9810882,  4.2739973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5229253818314441
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.711923 , 14.257903 ,  0.3780852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.796050483650828}
episode index:537
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.65378998,  8.19647285]), 'previousTarget': array([20.49208627,  8.59256602]), 'currentState': array([27.045076 ,  0.5054743,  1.157955 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5229501826555626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7650795 , 14.547294  ,  0.08046138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3152838754727363}
episode index:538
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.218473 , 11.115389 ,  5.6166325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.8907498340320776}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5225028923269631
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.85291  , 14.386191 ,  3.8087435], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9519306149255515}
episode index:539
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.04691991, 12.37877058]), 'previousTarget': array([18.27327206, 11.39940073]), 'currentState': array([23.20165  ,  4.4971914,  2.6674268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5223313851439946
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.381098 , 14.999505 ,  2.2731671], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3810978822702844}
episode index:540
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.26605107,  9.16261484]), 'previousTarget': array([19.18761806,  9.13733471]), 'currentState': array([25.16646  ,  1.0888813,  0.6024333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5222270402087372
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.738787 , 15.165178 ,  1.9167166], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7466147398486276}
episode index:541
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96386094, 15.08234496]), 'previousTarget': array([15., 15.]), 'currentState': array([10.945115  , 24.239292  ,  0.86468285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5225355679682582
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.110088 , 15.242362 ,  4.9202104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9053885170216673}
episode index:542
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.78153414, 16.19244636]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([25.091782, 21.7548  ,  5.761328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5224312313332702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.671893 , 14.26046  ,  3.5921953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9991796255444613}
episode index:543
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.04975502, 15.0189042 ]), 'previousTarget': array([17., 15.]), 'currentState': array([27.04933  , 15.111127 ,  1.6532431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.522803564246183
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.692587 , 14.682774 ,  4.1690035], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7617802882329441}
episode index:544
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.00925344, 15.16137517]), 'previousTarget': array([18.02945514, 15.23303501]), 'currentState': array([27.994905, 15.696869,  5.252631], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5231745307997825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.6520815, 13.657673 ,  2.3622682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4923311832612391}
episode index:545
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.06739988, 14.60605764]), 'previousTarget': array([10.974587 , 14.7124705]), 'currentState': array([ 1.1171991, 13.609311 ,  2.7706919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5234293064594605
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5045595, 16.375505 ,  6.1429224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.03183598606468}
episode index:546
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.04250853, 10.2416019 ]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([4.648144  , 2.5531626 , 0.44757855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5237584077182176
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.185022 , 13.724574 ,  1.3637068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5135718739446287}
episode index:547
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.34934268, 10.77777731]), 'previousTarget': array([10.3715414 , 10.75724629]), 'currentState': array([2.9454684 , 4.0559735 , 0.26558226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.5229816839219502
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.931866 , 15.151621 ,  0.8601661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9378064761801679}
episode index:548
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.84549262, 13.59356893]), 'previousTarget': array([14.52057184, 11.88371698]), 'currentState': array([13.753485 ,  3.6533718,  1.4738808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5237098496970066
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.557866, 13.099413,  2.100925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.38578738607489}
episode index:549
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.95420815, 12.52198823]), 'previousTarget': array([ 9.94427191, 12.47213595]), 'currentState': array([0.9782265, 8.113842 , 2.9565012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5241161446872513
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.384592  , 13.689288  ,  0.47224835], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.080266426763162}
episode index:550
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.32838512, 11.47313168]), 'previousTarget': array([18.27327206, 11.39940073]), 'currentState': array([25.191845 ,  4.20038  ,  1.8002639], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5240904996810124
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.911487 , 16.86362  ,  3.1089194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8657215621026213}
episode index:551
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.29333379, 15.83130154]), 'previousTarget': array([18.25608804, 15.75140493]), 'currentState': array([27.989214 , 18.27873  ,  2.5865057], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5240375072170979
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.445975 , 14.101375 ,  1.0023502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0556850508913604}
episode index:552
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.67949706, 14.54700196]), 'currentState': array([23.375927 , 11.146985 ,  2.0305257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219646240761989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.524826943930991
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.651112 , 14.227699 ,  2.9585173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8228049613959483}
episode index:553
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.22967  , 15.058417 ,  5.3723664], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2304347062410628}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5256487364509711
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.956385, 14.511097,  4.839939], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.016548318610484}
episode index:554
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.38330962, 22.62465649]), 'previousTarget': array([20.33471177, 22.75958076]), 'currentState': array([26.151     , 30.79373   ,  0.06863277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5252745748786615
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.829315 , 15.9971485,  5.6393085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2969459647726738}
episode index:555
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.755279  , 16.967415  ,  0.25985816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.601591194637556}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.525924053829808
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.772535 , 16.729025 ,  5.6917677], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.476168115320461}
episode index:556
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.242226 , 20.104895 ,  1.7939049], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.04747686325434}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5255337852782906
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.854218, 14.15415 ,  6.03696 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0380345563713664}
episode index:557
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.93175078, 10.48179995]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([4.240405 , 3.0503874, 1.0680333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5252887138316884
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.5287   , 15.122572 ,  4.7560883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4869781203379109}
episode index:558
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.07926188, 13.55217924]), 'previousTarget': array([14.14495755, 13.57492926]), 'currentState': array([8.713006 , 5.1139746, 2.4976435], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.525966877268499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.788848 , 13.228689 ,  1.8006433], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.145793868331851}
episode index:559
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.5441093 , 15.25340346]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 5.8036017 , 20.11175   ,  0.51268476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5259568302066808
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.936237 , 16.118773 ,  3.3982682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4588325022464834}
episode index:560
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.8469353, 11.005873 ,  4.6252217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.986573315626405}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5255529506364592
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.987446 , 16.785583 ,  6.0547485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6717502416710786}
episode index:561
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.89023997, 16.22083954]), 'previousTarget': array([17.76923077, 16.15384615]), 'currentState': array([27.102146 , 20.111956 ,  3.3937168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.526259697511533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.59332  , 15.722984 ,  5.0414143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7496792958466125}
episode index:562
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.01286  , 13.968991 ,  1.9491098], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.143189483976869}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5268993514621628
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.249432 , 15.620099 ,  2.4160178], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.394848451296103}
episode index:563
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.08287403, 15.79468932]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 3.8450837, 19.62395  ,  5.577044 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5275686470712881
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.234122 , 15.212018 ,  0.4887511], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7785600279087688}
episode index:564
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.34922235, 16.67188147]), 'previousTarget': array([13.47213595, 18.05572809]), 'currentState': array([10.721848 , 25.990795 ,  5.8586445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5274349745105716
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.658325 , 16.422764 ,  5.6027565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1850170144894783}
episode index:565
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.76667353,  7.83035944]), 'previousTarget': array([20.75304952,  7.80868809]), 'currentState': array([27.03412  ,  0.0381128,  1.6732594], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 297
reward sum = 0.050542043299318794
running average episode reward sum: 0.5265924074942974
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.622519 , 15.919144 ,  4.6711774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1101146023975748}
episode index:566
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85965936, 13.08229385]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([14.129796 ,  3.1089644,  4.6821113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5262133398998614
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.316658, 13.044752,  0.943793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9807237490592764}
episode index:567
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.892266 , 17.214441 ,  2.5698435], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2170604016320037}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.52672688496623
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.752848 , 16.93369  ,  1.5440173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.949419882003922}
episode index:568
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.218249  , 18.53711905]), 'previousTarget': array([11.39940073, 18.27327206]), 'currentState': array([ 3.9149106, 25.368023 ,  2.2733574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5264129416086311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.877005 , 15.082048 ,  5.4458756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1259887026192978}
episode index:569
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.96538346, 14.16081278]), 'previousTarget': array([19.10050506, 14.41421356]), 'currentState': array([27.587505 , 11.437805 ,  3.7505908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5260093866012461
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.188331 , 13.624076 ,  1.9844992], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.388753250749846}
episode index:570
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.87173632, 22.34785456]), 'previousTarget': array([10.9026124 , 22.28424463]), 'currentState': array([ 5.973545 , 31.066095 ,  4.4988985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5261162647016597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.982874 , 16.6715   ,  5.5107794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5933952108654283}
episode index:571
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.66148019, 16.05312757]), 'previousTarget': array([12.43294146, 17.31035268]), 'currentState': array([ 5.802387 , 22.236546 ,  4.9742475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5267152674080711
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.856208, 13.811285,  5.758752], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.649637473551283}
episode index:572
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.135023 , 12.974704 ,  0.7304678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.732317602751483}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.526987238311391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.575289, 15.414626,  3.428613], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7091346786807197}
episode index:573
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.057246 , 9.87416  , 1.0492401], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.629951786486957}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.527644720605289
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.127266, 13.047956,  6.071501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1382559979171476}
episode index:574
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.045263, 17.08287 ,  4.572561], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2912598358239964}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5284488167433667
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.852712 , 15.316672 ,  4.9433794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1901898404595677}
episode index:575
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17779545, 17.14099005]), 'previousTarget': array([10.1914503 , 17.06080701]), 'currentState': array([ 1.0381247, 21.198874 ,  2.3679597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 219
reward sum = 0.11068980359934157
running average episode reward sum: 0.5277235406788806
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.858211 , 13.641108 ,  1.4106745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3662697061378268}
episode index:576
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.25665046,  9.60040051]), 'previousTarget': array([13.16227766,  9.48683298]), 'currentState': array([10.184159  ,  0.08410932,  5.4638705 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5275391571112478
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.391564  , 13.436484  ,  0.81181735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.243133167542126}
episode index:577
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.85043114, 14.07574526]), 'previousTarget': array([14.78885438, 13.8386991 ]), 'currentState': array([13.252949 ,  4.2041674,  1.4546807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 224
reward sum = 0.10526490184835903
running average episode reward sum: 0.5268085788149452
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.120984 , 13.254766 ,  2.8939514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5644767364020367}
episode index:578
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.52369405, 14.81051317]), 'previousTarget': array([14.28476691, 14.71390676]), 'currentState': array([ 5.2319775, 11.114028 ,  4.189367 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5266794511317873
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.448196 , 15.583057 ,  5.5004363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6577244385821197}
episode index:579
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.876289 , 16.448774 ,  3.8889568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.454046561265069}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5274955210436291
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.876289 , 16.448774 ,  3.8889568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.454046561265069}
episode index:580
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.62224964, 15.16745411]), 'previousTarget': array([14.28476691, 15.28609324]), 'currentState': array([ 5.480232, 19.220047,  2.599372], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5271294515980822
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.975912 , 16.734716 ,  4.8856864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.990388267981604}
episode index:581
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36766182, 10.34991549]), 'previousTarget': array([20.36613715, 10.45942241]), 'currentState': array([27.925869 ,  3.8021293,  1.7786185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5268278704210374
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.317651, 13.455988,  1.897726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.688068040386218}
episode index:582
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.7356806, 12.1297   ,  1.5126548], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.748576984018914}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.5266469207670643
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.286295 , 13.055346 ,  1.5339329], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5920003915229732}
episode index:583
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.84935096, 10.38067286]), 'previousTarget': array([17.75902574, 10.51658317]), 'currentState': array([23.09926  ,  1.8695947,  5.4639444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5261729312733852
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.920721 , 16.441723 ,  3.6424928], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8009463836070512}
episode index:584
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.27103034, 20.86234561]), 'previousTarget': array([21.45069462, 22.44310917]), 'currentState': array([28.576128 , 27.691368 ,  4.9043765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.5255649902470881
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.5614805, 15.381293 ,  3.9900808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6787082988704124}
episode index:585
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.101231  , 9.90911   , 0.89527625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.57380774374631}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5262909715775538
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.980349 , 15.013201 ,  0.8980993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0197368601854224}
episode index:586
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.94427191, 15.52786405]), 'currentState': array([ 6.5881014, 18.762463 ,  5.2072487], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.214996647396923}
done in step count: 145
reward sum = 0.232864462948006
running average episode reward sum: 0.5257910967757998
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.987843 , 14.363731 ,  5.8886776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1955335357389956}
episode index:587
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.19730666, 15.31460011]), 'previousTarget': array([14.28476691, 15.28609324]), 'currentState': array([ 4.886859, 18.96365 ,  3.469749], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5256062834818499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.859266 , 15.160233 ,  1.0138417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8661579708798686}
episode index:588
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.28024919, 14.07510913]), 'previousTarget': array([19.025413 , 14.7124705]), 'currentState': array([29.054659 , 11.963021 ,  4.4923725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5260613055534319
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.890071 , 13.972948 ,  2.2449687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3590665517643559}
episode index:589
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.02844726, 16.80870243]), 'previousTarget': array([18.92040615, 16.80941823]), 'currentState': array([28.151133, 20.904629,  2.669973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5266421267521023
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.88266  , 15.36849  ,  5.7979517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9183830134381383}
episode index:590
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.20914138, 11.78326312]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([19.727684 ,  2.4227152,  2.9403927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.526079836518821
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.566503 , 16.933767 ,  0.6221893], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4071499582736786}
episode index:591
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.118259  , 20.0331    ,  0.21871068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.7412511284553664}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.52562177895831
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.5504265, 15.559159 ,  4.5802193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7846199159488694}
episode index:592
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.593668 , 22.137632 ,  4.7840037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.313383176920019}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5263716562281947
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.088034, 16.54769 ,  4.31153 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5501911250612501}
episode index:593
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.079274  ,  9.59961122]), 'previousTarget': array([16.03883865,  9.80580676]), 'currentState': array([18.039032 , -0.2064761,  4.546357 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5261205871889612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.713289 , 15.668709 ,  6.0416265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4501020609262583}
episode index:594
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.8383   , 10.101933 ,  3.8459995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.676462908697095}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5257761846467357
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.305825, 13.010005,  6.279267], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1075954824059493}
episode index:595
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.703617  , 18.920555  ,  0.78760207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.545873609085094}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.525552920961687
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.395719 , 16.78275  ,  4.2628007], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.882378991009676}
episode index:596
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.07533511,  9.67371644]), 'previousTarget': array([9.78280103, 9.34803445]), 'currentState': array([5.1433105, 1.6231833, 6.2722454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5259116269121503
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.375343 , 14.70575  ,  1.6533659], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6904916748553365}
episode index:597
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.00974698, 12.06728648]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([22.662626 ,  3.8183498,  4.2957683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.525462752296518
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.0931425, 14.666721 ,  1.5296171], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3460494034349168}
episode index:598
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.953105, 13.053438,  5.354536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.615615051377316}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5261572721581281
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.065416 , 13.054169 ,  3.9543936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.743879260514997}
episode index:599
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.85177784, 15.65572264]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([-0.06808193, 16.919203  ,  3.1934757 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.5254168055196192
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.571379 , 14.139641 ,  1.1325815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9612149349292836}
episode index:600
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.45416443, 18.71468788]), 'previousTarget': array([19.92893219, 19.92893219]), 'currentState': array([25.263767 , 26.037891 ,  3.6147816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5260473633723467
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.347818 , 16.724566 ,  5.0595636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1887760404896404}
episode index:601
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.16314166, 15.60943267]), 'previousTarget': array([15.12652114, 15.42173715]), 'currentState': array([17.749035  , 25.269306  ,  0.61614937], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5263657739524562
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.863592 , 15.797463 ,  5.3594623], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0270480001670426}
episode index:602
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.35042616, 18.17476507]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 3.8056226 , 24.737991  ,  0.45031846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.526131164176239
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.967392 , 14.055553 ,  6.0934496], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3993779546436544}
episode index:603
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.26725518, 15.93318846]), 'previousTarget': array([14.13940614, 16.10647783]), 'currentState': array([ 8.091514 , 23.798315 ,  1.9881115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 268
reward sum = 0.0676444472200646
running average episode reward sum: 0.5253720802077685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.008081 , 14.884245 ,  2.1343935], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.99527912929487}
episode index:604
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.25842724, 14.85642931]), 'currentState': array([22.633152, 11.421012,  2.308652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.430549520697108}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5250292983624981
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.864701 , 16.687325 ,  4.3616776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0337077599745403}
episode index:605
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.65867094, 17.65601854]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([24.733267 , 24.723557 ,  1.8426298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.5245210732772017
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.489226 , 15.169472 ,  4.6458373], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5381546259012577}
episode index:606
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.73381399, 15.11840632]), 'previousTarget': array([17.298575  , 15.57464375]), 'currentState': array([25.606121, 16.711376,  3.33283 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5251319747285715
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.071396 , 16.05949  ,  3.2729208], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5067875135347675}
episode index:607
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.09012418, 19.47936795]), 'previousTarget': array([19.24275371, 19.6284586 ]), 'currentState': array([25.833057 , 26.864004 ,  3.6736238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5257115619790163
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.81069  , 16.977715 ,  4.4439745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9867544136480202}
episode index:608
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.35637659, 12.37324473]), 'previousTarget': array([17.31035268, 12.43294146]), 'currentState': array([24.033955 ,  4.9294586,  4.3153815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5259691039051761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.17953  , 13.38438  ,  2.2851048], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0003789626289734}
episode index:609
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.62107729, 21.76076422]), 'previousTarget': array([20.49208627, 21.40743398]), 'currentState': array([27.014263 , 29.450184 ,  1.6284873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5260038375748246
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.270975 , 16.333815 ,  2.3920386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8424003855750848}
episode index:610
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.76353483, 15.2382192 ]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 3.9441152, 17.130043 ,  5.992061 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.526699396023802
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.901274  , 15.447753  ,  0.25898725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1864578073426768}
episode index:611
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.0015545, 20.032051 ,  1.5465338], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.0320513265303015}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5270115384038256
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.970104 , 15.834423 ,  5.8300133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.139526227845867}
episode index:612
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.61312906, 13.78241133]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([3.7052042, 9.238308 , 3.904801 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5274335067601117
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.471154  , 15.930331  ,  0.29006046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0701374044002234}
episode index:613
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.865357 , 20.314476 ,  2.2862933], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.384468285976343}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5278670259407038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.119483, 15.08995 ,  3.886425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8826670288471286}
episode index:614
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.01181 , 15.881392,  5.836155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.107489857070971}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5280964645303444
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.255804, 13.614498,  3.015293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4089184243594293}
episode index:615
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.29124931, 20.63370225]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 0.6332741, 27.06452  ,  3.1270537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 189
reward sum = 0.14964140560361563
running average episode reward sum: 0.5274820894346842
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.19699  , 15.26775  ,  6.1026335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.846472082900442}
episode index:616
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.68836044, 13.87257805]), 'previousTarget': array([14.63117406, 13.64763821]), 'currentState': array([12.024093 ,  4.2340264,  4.068201 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5271857038494537
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.962943 , 16.931543 ,  1.3738244], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1582676116479624}
episode index:617
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.132641, 17.941168,  2.816211], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0663952986217615}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5266141894880084
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.700964  , 14.549233  ,  0.30405873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5409371863163979}
episode index:618
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.821852 , 16.113907 ,  2.7272305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.912195709351137}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5273309662416628
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.747612 , 15.117966 ,  3.5256314], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7568616833845382}
episode index:619
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.74848196,  9.41387058]), 'previousTarget': array([16.42337349,  9.6623494 ]), 'currentState': array([17.076508  , -0.49755454,  3.519803  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5270362585273174
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.949965 , 15.215738 ,  1.8236082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9618625473690263}
episode index:620
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.948771 ,  9.179062 ,  5.5984635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.572162200194424}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5275725259928794
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.214062 , 13.505848 ,  0.7759981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5094081067904321}
episode index:621
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.60022274, 22.50115957]), 'previousTarget': array([ 8.54930538, 22.44310917]), 'currentState': array([ 2.1097414 , 30.108633  ,  0.09419347], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5280261519450151
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.543115 , 15.705965 ,  4.8040385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6189198646845446}
episode index:622
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.008718, 15.991342,  0.485981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9913799197387559}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5287837343656492
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.008718, 15.991342,  0.485981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9913799197387559}
episode index:623
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.16845948, 20.96085282]), 'previousTarget': array([ 8.80451099, 21.67206508]), 'currentState': array([ 3.871685 , 28.72942  ,  5.9108334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5287135542949897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.830135 , 16.253395 ,  5.5988636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.218196251616692}
episode index:624
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.47818459, 18.28292016]), 'previousTarget': array([14.52057184, 18.11628302]), 'currentState': array([12.908405 , 28.158941 ,  4.7375035], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5291502215513803
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.277634 , 14.071549 ,  5.0099797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9566722445961817}
episode index:625
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.60998491, 19.99032777]), 'previousTarget': array([11.69209979, 19.77807808]), 'currentState': array([ 5.990748 , 28.262218 ,  2.9287033], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5294400027106788
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.842731, 16.449396,  4.672022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.457903474265597}
episode index:626
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.97785158, 14.66519011]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([ 0.       , 14.       ,  3.2590477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 266
reward sum = 0.06901790349970881
running average episode reward sum: 0.528705677193596
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.390742 , 16.261913 ,  4.1639786], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.32102411867985}
episode index:627
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.88040109, 13.06181167]), 'previousTarget': array([16.92893219, 13.07106781]), 'currentState': array([23.843664 ,  5.884558 ,  1.0450478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5289616380354358
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.893518 , 15.967051 ,  5.1143103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3166480120101445}
episode index:628
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.51694819, 17.73754967]), 'previousTarget': array([19.48341683, 17.75902574]), 'currentState': array([28.068922 , 22.920574 ,  4.9117694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 146
reward sum = 0.23053581831852593
running average episode reward sum: 0.528487193171021
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.199198 , 13.239184 ,  2.7611167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7720472342360745}
episode index:629
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.20402519, 15.28640295]), 'previousTarget': array([15., 15.]), 'currentState': array([21.006077 , 23.431107 ,  1.7415966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 188
reward sum = 0.1511529349531471
running average episode reward sum: 0.5278882499040085
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.59603 , 14.273975,  4.397804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7534027568651465}
episode index:630
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.199966 ,  6.091363 ,  4.0515394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.17625557228628}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.52766779929889
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.047249 , 16.795212 ,  2.5336847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.078344416039985}
episode index:631
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.61833186, 14.96336503]), 'previousTarget': array([17.03454242, 15.1695452 ]), 'currentState': array([25.600826 , 14.371922 ,  4.3838205], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.5270115800831849
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.174084 , 13.969721 ,  1.2055981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3204593386961112}
episode index:632
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.891682  , 20.079699  ,  0.57660425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.561460958473576}
done in step count: 217
reward sum = 0.11293725497331045
running average episode reward sum: 0.5263574342299308
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.318517 , 14.398968 ,  0.3032834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9086579222239357}
episode index:633
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.99003 , 19.878338,  4.837229], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.9784369710891445}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.526191782475869
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.01506 , 15.973722,  5.087093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4065856145275066}
episode index:634
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.50288607, 13.79284079]), 'previousTarget': array([13.84615385, 12.23076923]), 'currentState': array([10.695071 ,  4.5461903,  1.2732724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5261992057207181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.96945   , 13.161068  ,  0.42626518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6945137462044206}
episode index:635
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.099651 , 19.047106 ,  3.3783717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.172154818108273}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5267655353842329
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.861289, 14.806764,  5.030823], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8826998758490228}
episode index:636
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.13837317, 13.95027674]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([4.427737, 9.038574, 1.889905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5265017720597394
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.035543 , 15.060671 ,  5.6460247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9663629779077525}
episode index:637
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.00428527, 15.16114623]), 'previousTarget': array([13.8386991 , 15.21114562]), 'currentState': array([ 4.132728 , 16.758757 ,  4.3311877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5260133279776803
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.118773, 16.22472 ,  5.040125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6587920313194384}
episode index:638
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13199899, 15.41333942]), 'previousTarget': array([13.94427191, 15.52786405]), 'currentState': array([ 5.1034164, 19.712723 ,  3.968354 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5265093058578152
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.079879 , 14.100887 ,  5.6651115], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1202049536294143}
episode index:639
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.95152924,  8.939062  ]), 'previousTarget': array([19.54057759,  9.63386285]), 'currentState': array([24.412989  ,  0.56216156,  4.328249  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5268898274827329
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.632303 , 13.89294  ,  1.6373743], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.972307446005581}
episode index:640
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.207224 , 18.504993 ,  3.5021136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.142078743315522}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5273828904560577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.949312, 16.934856,  5.236221], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1551944269800214}
episode index:641
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.20064764, 19.18685395]), 'previousTarget': array([ 9.13733471, 19.18761806]), 'currentState': array([ 1.0928155, 25.04032  ,  2.1473844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.5268733797650942
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.298075, 15.047677,  3.568436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2989499931872157}
episode index:642
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.16842713, 14.56630674]), 'previousTarget': array([13.94427191, 14.47213595]), 'currentState': array([5.3018317, 9.942079 , 5.4462967], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5269049244970159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.450037  , 13.677434  ,  0.43962514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.432354777502144}
episode index:643
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.44594404, 20.41302866]), 'previousTarget': array([12.51123442, 21.63670822]), 'currentState': array([10.686461 , 30.024754 ,  5.4636207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5275195825248589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.857255, 15.972157,  4.07934 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5003181278021043}
episode index:644
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.09629408, 21.00993935]), 'previousTarget': array([ 9.07106781, 20.92893219]), 'currentState': array([ 2.0885572, 28.143776 ,  1.2452455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5271566416550919
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.364374 , 16.302723 ,  5.8850718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8864262204712983}
episode index:645
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.58814685, 13.40918334]), 'previousTarget': array([13.64363839, 13.47409319]), 'currentState': array([6.9503045, 5.929942 , 3.5079193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.5266209919824463
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.120426 , 13.302831 ,  1.3170217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.033651510554287}
episode index:646
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.12239   ,  9.729642  ,  0.62946355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.691097056710577}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5265127829496201
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.655514 , 15.8024645,  2.9002628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8397486433899979}
episode index:647
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.93841328, 17.49304953]), 'previousTarget': array([13.84615385, 17.76923077]), 'currentState': array([10.020629 , 26.693645 ,  1.9593534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5264872782633546
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.443787, 15.038758,  4.663468], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.44547589066866883}
episode index:648
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.74186979, 16.26635247]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 4.0197744, 21.157677 ,  6.197597 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.52632525506442
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.09457  , 16.518568 ,  0.3987001], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4365368382991353}
episode index:649
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.958927, 15.947412,  4.313227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.106901756838119}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5266535244772259
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.957687 , 15.98002  ,  3.0937378], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1892871502172215}
episode index:650
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.8173555 , 18.86406504]), 'previousTarget': array([17.75902574, 19.48341683]), 'currentState': array([21.073355, 27.91318 ,  3.291318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5267832690472039
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.617039 , 14.977175 ,  6.0185113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3836408846781632}
episode index:651
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.100319 , 15.945823 ,  1.9244938], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.303458297398265}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5274338929442173
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.908167  , 16.195387  ,  0.74829197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2516773017225975}
episode index:652
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.199874, 10.915656,  5.282499], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.95202698064076}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.527878721496519
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.205029 , 13.569843 ,  1.4301496], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8701449078508003}
episode index:653
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.1788017 , 16.63059062]), 'previousTarget': array([18.05572809, 16.52786405]), 'currentState': array([27.07648  , 21.194723 ,  1.6651169], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.5274135990997026
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.53328  , 16.855947 ,  4.1459384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.913731515377294}
episode index:654
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.03966083, 20.45572813]), 'previousTarget': array([12.00818834, 21.83842665]), 'currentState': array([ 7.270412, 29.245169,  4.528763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5279481142506939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.083661 , 13.00696  ,  5.7100587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.764880402179282}
episode index:655
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.024246 , 18.894949 ,  3.6502628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.901605653253426}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.527875238248134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.023597, 16.984678,  1.805877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2118569368790375}
episode index:656
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.98339 , 21.864838,  1.391661], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.792535069700852}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.527279842154844
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.769822 , 13.743208 ,  2.3493414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.277696426985822}
episode index:657
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.41898216,  9.68736634]), 'previousTarget': array([10.45942241,  9.63386285]), 'currentState': array([3.8886364, 2.1140854, 2.9558437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5271124273186409
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.589389 , 14.642752 ,  1.3232808], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6892064753887367}
episode index:658
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.14068393, 19.85602987]), 'previousTarget': array([10.07106781, 19.92893219]), 'currentState': array([ 3.0672247, 26.924706 ,  1.278599 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 183
reward sum = 0.1589427091997875
running average episode reward sum: 0.5265537479284758
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.445683  , 16.92606   ,  0.46618867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4749967408657834}
episode index:659
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.230415, 13.015642,  3.647998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.259981747585885}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.526710217448589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.571013  , 13.8599205 ,  0.83046633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2181176962716935}
episode index:660
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.98922864, 14.91157246]), 'previousTarget': array([14.99503719, 14.9503719 ]), 'currentState': array([13.7800665,  4.9849453,  1.9271721], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.527295402062863
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.737103 , 13.966242 ,  1.5161972], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6320427707761196}
episode index:661
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.80014912, 22.06385   ]), 'previousTarget': array([ 8.32793492, 21.19548901]), 'currentState': array([ 2.203672, 29.5796  ,  0.466574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5272026307231453
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.135041, 16.28884 ,  4.697366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2669755254568744}
episode index:662
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.23059985, 16.29645154]), 'previousTarget': array([15.21114562, 16.1613009 ]), 'currentState': array([16.981813 , 26.14192  ,  3.0434136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.527545793100944
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.395746 , 16.468103 ,  4.8085704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.174616695895315}
episode index:663
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.41691641, 21.00730206]), 'previousTarget': array([10.17821552, 21.13681661]), 'currentState': array([ 4.3513846, 28.957731 ,  2.8217285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5270716664595705
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.060526 , 16.045599 ,  0.2587301], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.20336947470323}
episode index:664
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.50279862, 18.3872364 ]), 'previousTarget': array([18.60059927, 18.27327206]), 'currentState': array([25.691454 , 25.338728 ,  4.6157913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5272454107236685
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.248343 , 13.899291 ,  4.1195984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6643074386219376}
episode index:665
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.663639, 15.62934 ,  0.630413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7135879259269604}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5279552524493086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.663639, 15.62934 ,  0.630413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7135879259269604}
episode index:666
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.68934468, 10.09786781]), 'previousTarget': array([12.71390676,  9.28476691]), 'currentState': array([11.106426  ,  0.43719846,  0.42192286], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.527473171916577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.309468 , 13.642576 ,  2.8505106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3922535453785005}
episode index:667
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.95356967, 14.4764193 ]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([ 2.0980675, 12.782584 ,  5.0693655], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5269894448055437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.69042   , 16.162752  ,  0.06146735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2032590115392106}
episode index:668
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.61180053, 18.67803124]), 'previousTarget': array([20.58821525, 18.59242409]), 'currentState': array([28.9755   , 24.159685 ,  2.1692085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.526658308155503
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.60595 , 14.884565,  4.710306], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3988207779714623}
episode index:669
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.93142  , 17.053822 ,  1.4023466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.193021193613617}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.5263236008831351
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.1493225, 16.93842  ,  4.823806 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6800143131071015}
episode index:670
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.1296663 , 20.39559849]), 'previousTarget': array([ 9.34803445, 20.21719897]), 'currentState': array([ 1.7671646, 27.162693 ,  4.2479906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5259635176825241
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.495907 , 16.04774  ,  5.8935075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1626998726083362}
episode index:671
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.875544, 14.203656,  2.160746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.224343543105385}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5256218853457929
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.487865 , 15.5082445,  4.9855375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7045035002286617}
episode index:672
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.91380208, 18.70460842]), 'previousTarget': array([19.24275371, 19.6284586 ]), 'currentState': array([24.096003 , 26.56466  ,  2.8873978], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5252812682611768
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.793049 , 15.404025 ,  2.7962747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.838004481220535}
episode index:673
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.18359156, 11.45413569]), 'previousTarget': array([18.27327206, 11.39940073]), 'currentState': array([24.864323 ,  4.0131793,  3.7701797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5252896128823844
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.000443, 13.045553,  5.810445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1956200651958317}
episode index:674
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.18503096, 12.1362181 ]), 'previousTarget': array([11.63778901, 13.03871026]), 'currentState': array([3.1876028, 6.1327906, 4.9557223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5250018254385898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.585172 , 16.644339 ,  5.7054157], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6958572982236237}
episode index:675
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.94872778, 12.19989205]), 'previousTarget': array([12.88171698, 12.08736084]), 'currentState': array([7.0391016, 4.1329026, 3.8585658], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5251568872814368
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.868446 , 13.522303 ,  1.0744003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3821589551705102}
episode index:676
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.95129436, 19.42486684]), 'previousTarget': array([15., 21.]), 'currentState': array([14.8412285, 29.424261 ,  4.7503304], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5256773660638852
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.650984 , 15.391484 ,  4.8532295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6967638222658739}
episode index:677
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.9347377, 12.0234375,  5.3663135], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.596998346459758}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5262225886128451
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.660155 , 16.351267 ,  0.7951759], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3933472475120297}
episode index:678
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.8146  , 13.454729,  5.909847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5404287076433767}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5269056186737983
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.864555 , 13.060253 ,  0.1689288], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.247632621464078}
episode index:679
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.868977  , 11.0996475 ,  0.93057233], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.325022885269122}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.527572080999278
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.940233 , 13.200895 ,  1.8259047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6459937030639784}
episode index:680
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.108734 , 11.146643 ,  5.4110727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.009694997189399}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5278619609625065
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.80925  , 13.068214 ,  2.8237543], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2692909451663104}
episode index:681
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.467644 , 10.1844845,  2.1171536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.568792147248591}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5277183056143561
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.636974 , 13.112808 ,  5.9100695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4982349286177543}
episode index:682
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.938457  , 22.011967  ,  0.04583312], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.042333272708603}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5279652967013769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.487524 , 16.982956 ,  3.8637683], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.042007324437701}
episode index:683
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.97449333, 20.40475483]), 'previousTarget': array([11.09710761, 20.07376011]), 'currentState': array([ 5.001174 , 28.424696 ,  3.9891272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5282968083833978
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.356893 , 16.960117 ,  5.7437925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0629219891142023}
episode index:684
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.72266471, 19.62784008]), 'previousTarget': array([10.75724629, 19.6284586 ]), 'currentState': array([ 3.9351673, 26.971537 ,  0.7099202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5283405592303532
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.43819  , 16.795206 ,  3.9698763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3002514283001534}
episode index:685
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.08545953, 10.88458912]), 'previousTarget': array([19.54057759,  9.63386285]), 'currentState': array([26.130655 ,  3.7877436,  2.0443876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5288495686527563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.84177 , 16.159298,  2.493322], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.176255749634905}
episode index:686
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.228617 , 23.108639 ,  5.5101805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.942782256215402}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5292349612509963
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.931019 , 15.483887 ,  5.4013305], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9907235056306265}
episode index:687
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.83772234,  9.48683298]), 'previousTarget': array([16.83772234,  9.48683298]), 'currentState': array([20.      ,  0.      ,  4.434968], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5289977481405636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.996829 , 13.134136 ,  2.4774377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1154469601389}
episode index:688
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.60206577, 15.58256937]), 'currentState': array([23.851194 , 19.344837 ,  2.8501256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.860083781559542}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5291911941388849
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.953058 , 13.9304905,  2.8501325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.226721150855564}
episode index:689
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.15458454, 15.1138876 ]), 'previousTarget': array([15., 15.]), 'currentState': array([23.20556  , 21.045311 ,  2.1751218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.5285946416565713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.61956  , 15.444594 ,  4.8984866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6794759724691395}
episode index:690
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.3789786 , 13.05248031]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([27.042933  ,  8.058894  ,  0.53777546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5287690381641137
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.899344 , 13.23761  ,  3.3744342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7652621717189385}
episode index:691
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.66345587, 21.90694836]), 'previousTarget': array([19.45299804, 21.67949706]), 'currentState': array([25.259228 , 30.19473  ,  2.7578259], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5284552983422712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.134669, 15.428093,  4.506039], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9138239692502637}
episode index:692
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.645414 , 14.637539 ,  0.6368291], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5070591911202903}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.529135738027203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.645414 , 14.637539 ,  0.6368291], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5070591911202903}
episode index:693
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.274966, 21.969503,  4.375627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.179813309116731}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5297163282431682
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.313798 , 14.322747 ,  5.1858416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8171264855997475}
episode index:694
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.94687 , 14.116034,  4.450027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.00288587959742}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5296449974925613
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.246471 , 13.527661 ,  4.0317492], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4928259549175242}
episode index:695
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.32196218, 12.90378285]), 'previousTarget': array([13.24695048, 12.80868809]), 'currentState': array([7.0725956, 5.0970283, 1.0559819], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5301704188384897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.567795 , 13.130676 ,  1.1516969], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3549061435140253}
episode index:696
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.06906153, 14.60314367]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([ 2.1594892, 13.261363 ,  4.460076 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.53047103570298
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.426262 , 13.981138 ,  2.4170308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.169296674464425}
episode index:697
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.035199 , 17.075272 ,  1.4921684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.381077918811124}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5310198125106886
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.69931 , 15.92325 ,  5.772185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5950500360028157}
episode index:698
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00135098, 14.52269935]), 'previousTarget': array([15.1695452 , 12.96545758]), 'currentState': array([15.029655 ,  4.5227394,  1.9297619], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5315935543352899
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.262505 , 14.751811 ,  2.3050451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7551319348671859}
episode index:699
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.105551 ,  6.4228063,  2.8560345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.783916569662058}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5317802521733594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.527649 , 13.46654  ,  0.6928359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6217003200732538}
episode index:700
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.995764 , 16.511086 ,  2.2389405], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.13729794365763}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5320663051226905
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.285299 , 13.722031 ,  2.9237657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.30942788164626}
episode index:701
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.59855141, 15.80307377]), 'previousTarget': array([11.74391196, 15.75140493]), 'currentState': array([ 1.866126, 18.100876,  2.191295], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5323727236258371
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.40979  , 15.284813 ,  5.9146967], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6553367085066532}
episode index:702
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.96256845, 11.61769495]), 'previousTarget': array([16.96128974, 11.63778901]), 'currentState': array([21.98134 ,  2.968306,  3.095805], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5318783165708196
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.992153, 15.460345,  3.331028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0446496055986976}
episode index:703
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.89056977, 19.68458966]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 0.9549409, 25.769472 ,  3.5478587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5318186744634807
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.519043 , 14.555349 ,  6.2345796], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6550067682672809}
episode index:704
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.058508 , 16.874603 ,  6.2719827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.285118864223098}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5324545344997027
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.43589   , 16.44386   ,  0.37127274], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.128654169921018}
episode index:705
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.84239605, 19.84825109]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 0.9854983, 26.034458 ,  3.0801034], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5328705601219915
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.749182 , 15.6021   ,  4.0936337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6522535198526713}
episode index:706
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.28169 , 16.991848,  2.210659], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.8388737412101785}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5334485086216789
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.326736 , 16.82692  ,  2.6785069], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2578451388757013}
episode index:707
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92620625, 15.33863018]), 'previousTarget': array([14.42535625, 17.298575  ]), 'currentState': array([12.796992, 25.109324,  4.866904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5334524912722234
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.538258 , 15.4977045,  6.07884  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6789078142195422}
episode index:708
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.88194641,  9.83056469]), 'previousTarget': array([13.96116135,  9.80580676]), 'currentState': array([11.768008 ,  0.056555 ,  4.3465242], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 122
reward sum = 0.2934227215252159
running average episode reward sum: 0.5331139443473334
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.548009 , 14.419223 ,  1.7998977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.563835099741138}
episode index:709
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.22885465,  9.0618314 ]), 'previousTarget': array([12.22885465,  9.0618314 ]), 'currentState': array([8.       , 0.       , 5.3402367], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5332865433279345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.039931 , 13.55972  ,  2.7207952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4323395495477813}
episode index:710
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.96324747, 17.0551627 ]), 'previousTarget': array([15., 19.]), 'currentState': array([14.784446 , 27.053564 ,  4.5781884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5338343044405927
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.279816, 15.057222,  4.905595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7211358212576182}
episode index:711
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.89352217, 15.86059386]), 'currentState': array([ 6.829955, 20.070234,  4.958472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.61545160121124}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5343046857725146
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.568476  , 15.806562  ,  0.04877919], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9147437590630876}
episode index:712
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.85504245, 13.57492926]), 'currentState': array([20.141209 ,  6.7363186,  1.979096 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.73244351866829}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5340386373820166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8191185, 15.187809 ,  2.71027  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8403733298640843}
episode index:713
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.98390499, 15.32763972]), 'previousTarget': array([14.64398987, 17.13606076]), 'currentState': array([14.493256 , 25.315596 ,  5.5382257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5339978072019116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.439291 , 16.072767 ,  4.3459024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2104644445182886}
episode index:714
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13625254, 13.09201446]), 'previousTarget': array([14.13802944, 13.10366477]), 'currentState': array([10.012152 ,  3.9820313,  5.8926067], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5337680615102303
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.705573 , 13.031395 ,  1.4593383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9905006956781286}
episode index:715
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.09089  , 19.935469 ,  2.4690046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.054591373276335}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5333858232418889
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.741829 , 16.55396  ,  6.2523017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9994470832954752}
episode index:716
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99445   , 15.11358539]), 'previousTarget': array([15., 17.]), 'currentState': array([14.506413 , 25.10167  ,  4.1166463], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5338535498660549
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.614012, 16.67397 ,  5.868764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7830265168694677}
episode index:717
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.16066182, 10.36589359]), 'previousTarget': array([12.24097426, 10.51658317]), 'currentState': array([6.9362726, 1.8391265, 3.9679906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5339276212199148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.530086, 13.78039 ,  2.110102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.307007518013377}
episode index:718
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.84526 , 23.053074,  2.185949], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.398326949134475}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5340435635282909
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.010138, 13.635799,  3.963026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6974749310000783}
episode index:719
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.758958 , 21.193087 ,  5.3248887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.9898981443885075}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5344264583959201
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.143723 , 15.351879 ,  5.3837776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1966286601084883}
episode index:720
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.8108  ,  9.048197,  3.824925], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.0694447039291335}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5343249265082949
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.308296 , 14.0051775,  0.7674068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0414981324606167}
episode index:721
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13444109, 12.46834546]), 'previousTarget': array([14.16227766, 12.48683298]), 'currentState': array([10.899349 ,  3.0060952,  2.8900037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5347063710259028
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.212214 , 13.702909 ,  1.1281434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2087600470947653}
episode index:722
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.994205 , 21.025888 ,  4.0380735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.733953686713264}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5349300319484805
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.793465 , 16.381594 ,  4.3934507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.263916220298618}
episode index:723
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.08362083,  9.62304199]), 'previousTarget': array([16.03883865,  9.80580676]), 'currentState': array([18.059206  , -0.17986897,  4.312416  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5344290925814129
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.951696 , 13.082238 ,  1.5147725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1409195131327343}
episode index:724
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.15126813, 15.38332113]), 'previousTarget': array([11.97054486, 15.23303501]), 'currentState': array([ 3.3595316 , 17.413567  ,  0.63038826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5343281172363601
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.232424, 15.387191,  4.841576], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.809486782311318}
episode index:725
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.81065466, 15.27038565]), 'previousTarget': array([11.97054486, 15.23303501]), 'currentState': array([ 1.8463985, 16.115133 ,  4.25361  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5346421869293501
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.215424  , 13.463159  ,  0.46795765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9593713160754627}
episode index:726
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.150143 ,  8.127162 ,  6.1838975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.22280636850908}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5350205716353912
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.138964 , 16.019905 ,  2.2674022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.528870400557652}
episode index:727
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.99050158, 15.31673542]), 'previousTarget': array([11.97054486, 15.23303501]), 'currentState': array([ 2.0454285, 16.363407 ,  2.771626 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.534945188235578
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.29219 , 16.690453,  4.033345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.832655307387753}
episode index:728
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.867289 ,  5.059328 ,  2.3662496], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.941557752937644}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5354644914718579
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.893473 , 14.477966 ,  1.0320368], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9641175964574693}
episode index:729
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2299869 , 18.30693526]), 'previousTarget': array([15.6783628 , 20.08772099]), 'currentState': array([15.9237795, 28.282839 ,  4.7102556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5359210412270594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.972902, 15.031993,  4.875493], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9731616821199918}
episode index:730
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.27776894, 15.34376879]), 'previousTarget': array([16.35236179, 15.36882594]), 'currentState': array([25.934393  , 17.94177   ,  0.16328147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5362100303557933
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.533802, 16.485878,  2.455926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1355051107701097}
episode index:731
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.39945417, 21.4052999 ]), 'previousTarget': array([10.9026124 , 22.28424463]), 'currentState': array([ 4.5658226, 29.527414 ,  3.8836334], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5362101098569565
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.892189 , 16.868145 ,  5.0472517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6590120363837126}
episode index:732
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.955326, 14.041095,  4.914692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.006517885726106}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5366401885216739
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.494467 , 14.304934 ,  3.0772743], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6481954781174586}
episode index:733
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.04073894, 16.4230846 ]), 'previousTarget': array([10.77802414, 15.90470911]), 'currentState': array([ 1.6301621, 19.805546 ,  1.3350197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5364178780202197
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5190115, 14.894021 ,  0.7109769], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4847755674605607}
episode index:734
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.89483054, 11.31801794]), 'previousTarget': array([20.36613715, 10.45942241]), 'currentState': array([26.161673 ,  4.4483013,  2.810485 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5363347789236014
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.65784  , 13.944571 ,  2.5673008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9652906207616576}
episode index:735
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00036806, 15.19790662]), 'previousTarget': array([15., 15.]), 'currentState': array([15.018966 , 25.19789  ,  6.1836443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5359851964639023
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.547166 , 16.71737  ,  4.9574313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.802428097193755}
episode index:736
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.63005845, 14.43028417]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([ 0.7139739, 13.137509 ,  5.8401656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 222
reward sum = 0.10740220574263752
running average episode reward sum: 0.5354036727315804
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.166564 , 13.139429 ,  3.1613889], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8680116873803978}
episode index:737
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.01563468, 12.12918118]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.3501866, 7.1381884, 5.217013 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.5351945175085085
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.520704, 14.970734,  1.658413], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.479585204785778}
episode index:738
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.7919918 , 16.16026355]), 'previousTarget': array([19.58258088, 16.63663603]), 'currentState': array([27.026358 , 19.997774 ,  3.1307797], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5351744173801484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.570936 , 16.720564 ,  4.1368513], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8128177631745082}
episode index:739
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.090899 ,  9.8885565,  4.195435 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.741149614719287}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5351758954988336
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.830518 , 13.541214 ,  1.6773787], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6786352759098897}
episode index:740
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.29494827, 8.83315927]), 'previousTarget': array([8.32793492, 8.80451099]), 'currentState': array([0.93464994, 2.0636683 , 4.9946904 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5352701332473339
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.714132, 13.112304,  5.27409 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.549832693717151}
episode index:741
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.006487 ,  5.982096 ,  5.1671724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.017906137902823}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5357554002567831
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.644508, 14.250823,  1.110656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8071175661934538}
episode index:742
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.9669988 , 18.25129068]), 'previousTarget': array([13.19058177, 18.92040615]), 'currentState': array([10.938956  , 27.781818  ,  0.03621834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5358820062203719
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.929821, 16.933964,  5.805723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.732109996523549}
episode index:743
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.959911 , 23.27339  ,  2.1922774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.502366231707592}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.5355681922814588
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.528902 , 16.09511  ,  4.2781215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1921405347689282}
episode index:744
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.155792, 16.01598 ,  1.365497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.902371922899746}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5352981367977117
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.586686  , 13.924542  ,  0.16484547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1521446709227623}
episode index:745
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.80655  ,  8.90951  ,  1.8873895], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.876932116351776}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5356880436167972
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.764941 , 13.959777 ,  1.7401949], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6147551962864444}
episode index:746
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.85642931, 15.25842724]), 'currentState': array([11.1069975, 22.293037 ,  5.260394 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.267034732808078}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5355640274695541
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.044608 , 15.210992 ,  4.0933847], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9667422261636398}
episode index:747
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.45741937,  8.52525259]), 'previousTarget': array([18.29411765,  8.82352941]), 'currentState': array([23.167778, -0.295888,  4.63785 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5355722106894704
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.891072 , 14.441775 ,  1.7564076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.241505333986744}
episode index:748
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.913811 , 7.910978 , 3.2388973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.724881458760759}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5354486803435915
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.7927885, 13.430028 ,  1.0619113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.758785323877111}
episode index:749
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.147266, 23.290358,  5.644751], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.76744649409598}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5357013892177439
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.022421 , 13.993409 ,  3.1475892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.219018767136306}
episode index:750
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.83796081, 19.92959879]), 'previousTarget': array([19.92893219, 19.92893219]), 'currentState': array([26.84238 , 27.066692,  6.017745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5355662759521967
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.561186 , 15.427774 ,  5.3939357], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.618731657619961}
episode index:751
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.21400561, 11.98690208]), 'previousTarget': array([15.23303501, 11.97054486]), 'currentState': array([15.922472,  2.01203 ,  4.010379], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5356505708066036
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.012078  , 16.35354   ,  0.77433753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4049749300396472}
episode index:752
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17906085, 18.3178424 ]), 'previousTarget': array([10.22192192, 18.30790021]), 'currentState': array([ 1.9413921, 23.98713  ,  2.0914397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5353024148706349
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.861557 , 15.2603   ,  5.4059563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.879667634043472}
episode index:753
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.4068804 , 17.50558536]), 'previousTarget': array([15.35601013, 17.13606076]), 'currentState': array([17.009777 , 27.376286 ,  3.5189426], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5352165849325912
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.44876 , 15.436509,  4.466951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5130905761325837}
episode index:754
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.42280263, 14.77621973]), 'previousTarget': array([15.25842724, 14.85642931]), 'currentState': array([24.26117  , 10.098264 ,  3.8879397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5356019518715355
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.939413 , 14.220731 ,  1.9785682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2205561978776773}
episode index:755
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.57258241, 11.35459756]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([7.0300875, 3.0310917, 2.1532104], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5355748819846998
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.996708 , 13.8651085,  0.4840111], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.29669789966958}
episode index:756
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.81018698, 15.14617014]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 3.8848057, 16.365517 ,  5.8286467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.536062077748272
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.734211 , 14.292155 ,  0.5148377], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0198577907142266}
episode index:757
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.42704047, 19.88172614]), 'previousTarget': array([17.52786405, 20.05572809]), 'currentState': array([21.878881 , 28.836117 ,  1.2303729], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 168
reward sum = 0.1848045639485463
running average episode reward sum: 0.5355986773342882
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.363375 , 16.637177 ,  2.6358519], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1305251786854624}
episode index:758
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.70420347, 16.90811779]), 'previousTarget': array([14.64398987, 17.13606076]), 'currentState': array([13.1723   , 26.790085 ,  2.2077289], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 170
reward sum = 0.18112695312597024
running average episode reward sum: 0.5351316526647121
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.90771  , 14.163348 ,  3.3714232], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0831083339395193}
episode index:759
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.86205239, 14.52059217]), 'previousTarget': array([14.87347886, 14.57826285]), 'currentState': array([12.096796 ,  4.9105268,  3.8111143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5354929634746549
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.335098  , 15.079915  ,  0.27253312], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6696870386710606}
episode index:760
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.43252068, 19.33324813]), 'previousTarget': array([16.63663603, 19.58258088]), 'currentState': array([19.571331, 28.827871,  6.031902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.535089201972238
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.668221 , 15.904773 ,  3.8946726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1247816154259096}
episode index:761
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.227192 , 19.873465 ,  1.8142782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.716820531387027}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5351496707545816
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.891415 , 16.513157 ,  4.4565206], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4222083159010417}
episode index:762
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.173172,  5.870599,  5.032474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.384489486737134}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5352990193228828
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.570311 , 13.461803 ,  1.7837615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5970853625995696}
episode index:763
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.1161753 , 12.88959892]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([4.0462527, 6.9839787, 1.924042 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 143
reward sum = 0.23759255478829303
running average episode reward sum: 0.5349093511755862
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.736851, 16.665693,  4.458586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8213959273498872}
episode index:764
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.268739 , 17.878277 ,  6.2501698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8907953914101716}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.5344590924940038
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.4361315, 14.116556 ,  4.982118 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.796150818032265}
episode index:765
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.22632084, 22.660788  ]), 'previousTarget': array([21.69407375, 23.23886   ]), 'currentState': array([29.088104 , 29.935122 ,  5.0364776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.5339908003386477
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.602477 , 13.84316  ,  3.0043519], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3043228743142836}
episode index:766
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.61225895,  9.46095145]), 'previousTarget': array([19.54057759,  9.63386285]), 'currentState': array([26.011143 ,  1.7762728,  1.122947 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5343087019801868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.961084, 15.622932,  3.168547], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0576434840350695}
episode index:767
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.76438985, 19.54587431]), 'previousTarget': array([10.75724629, 19.6284586 ]), 'currentState': array([ 3.947408 , 26.86221  ,  0.0787726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5345761390523328
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.705423 , 15.626594 ,  5.6087074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8168897453852255}
episode index:768
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.12652114, 15.42173715]), 'currentState': array([16.626244 , 23.483747 ,  3.9107704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.63820812215708}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.535080909605487
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.14813  , 16.543552 ,  5.4139085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7630189432157917}
episode index:769
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.81398603, 13.76006363]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([4.1158047, 8.826339 , 3.8641636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5350483184842456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.239749 , 13.110178 ,  3.4699442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.582617075262499}
episode index:770
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.52620127, 16.49320571]), 'previousTarget': array([18.92040615, 16.80941823]), 'currentState': array([26.134794 , 21.581636 ,  3.0830653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5351469811574604
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.881797 , 14.3604   ,  3.9573138], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.089336296000726}
episode index:771
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.786259 , 19.885313 ,  4.9561825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.904230792114201}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5354412761486387
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.563961, 15.991604,  5.9695  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0832396697564917}
episode index:772
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.95893206, 14.90535746]), 'currentState': array([ 5.6822906, 13.181182 ,  5.9360876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.493566693938295}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5359423154995822
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.505505  , 13.146088  ,  0.33079764], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9215946303482099}
episode index:773
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.51235844, 16.38459585]), 'previousTarget': array([13.47409319, 16.35636161]), 'currentState': array([ 6.192315 , 23.197596 ,  1.3110001], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5355859112952904
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.220638 , 14.001779 ,  4.8347507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0402387375257565}
episode index:774
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.3776396 , 17.05507732]), 'previousTarget': array([18.36221099, 16.96128974]), 'currentState': array([26.92061   , 22.252928  ,  0.16758803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5352659083250603
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.87482 , 15.181565,  3.534775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.883591952725122}
episode index:775
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([8.016787, 9.07098 , 4.538308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.160706688953308}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.5347728849023164
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.992761 , 14.470861 ,  5.6679716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1377691824697498}
episode index:776
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9026124 , 22.28424463]), 'previousTarget': array([10.9026124 , 22.28424463]), 'currentState': array([ 6.       , 31.       ,  2.3862891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5351479115933503
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.447048, 16.860378,  6.216182], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.356895361814343}
episode index:777
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.91819666, 15.45528653]), 'previousTarget': array([14.96116135, 15.19419324]), 'currentState': array([13.149771 , 25.297678 ,  3.3932652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5349750796413686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.183295, 16.28907 ,  5.734091], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2275811876094576}
episode index:778
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.761956, 18.011116,  4.875983], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.926760425901273}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5347582083469293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.948416 , 14.007554 ,  1.0954838], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.445952411723797}
episode index:779
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.95987492, 15.19731389]), 'previousTarget': array([14.96116135, 15.19419324]), 'currentState': array([12.967096 , 24.996744 ,  1.9051791], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5345418931327323
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.91171  , 15.236767 ,  0.6698884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9263168158171784}
episode index:780
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.20746  , 19.756697 ,  2.6284404], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.24395317852325}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5346243695902655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.889453  , 16.942726  ,  0.82889616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2377443155577215}
episode index:781
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.75297523, 10.26715329]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([11.205102  ,  0.59718215,  5.604193  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5350186391859095
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.578734  , 13.139686  ,  0.89413357], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9074155970267206}
episode index:782
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.24608125, 13.57351176]), 'previousTarget': array([16.35636161, 13.47409319]), 'currentState': array([22.824858 ,  6.0422635,  5.034421 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.534775463635669
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.917506 , 16.399794 ,  3.5390027], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.673690488781547}
episode index:783
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.6623494 , 13.57662651]), 'previousTarget': array([ 9.6623494 , 13.57662651]), 'currentState': array([ 0.      , 11.      ,  3.344643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.5344713961914898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.234793 , 15.043998 ,  5.7504253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7664711351066688}
episode index:784
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.94994319, 9.2567959 ]), 'previousTarget': array([7.80868809, 9.24695048]), 'currentState': array([0.1969005, 2.9409168, 5.5379133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5346174232388489
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.000517 , 16.103775 ,  0.5941641], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2839115582217926}
episode index:785
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.802955 , 13.965969 ,  1.1677333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4282150022730074}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5351967903848555
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.028173  , 15.674043  ,  0.80921924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.182700474984894}
episode index:786
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.61479222, 13.59489398]), 'previousTarget': array([14.63117406, 13.64763821]), 'currentState': array([11.970862 ,  3.9507437,  2.221571 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5355986467771172
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.001299 , 14.235573 ,  1.3948792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1398960233463478}
episode index:787
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.404446 , 16.855537 ,  4.6615896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.447205096915031}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.536187988595928
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.404446 , 16.855537 ,  4.6615896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.447205096915031}
episode index:788
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.03914369, 16.66402936]), 'previousTarget': array([17.92893219, 17.92893219]), 'currentState': array([24.78683  , 22.986477 ,  4.2309113], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.536314728136445
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.895172 , 14.741949 ,  3.3116415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.912659833255115}
episode index:789
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8012352 , 15.06115464]), 'previousTarget': array([14.57826285, 15.12652114]), 'currentState': array([ 5.2433944, 18.001848 ,  1.4728107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5360225057285866
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.032131 , 16.925764 ,  5.5778255], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1849169573526726}
episode index:790
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.26630066, 19.26062015]), 'previousTarget': array([17.25900177, 19.195289  ]), 'currentState': array([21.962452 , 28.089333 ,  1.9550478], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 162
reward sum = 0.19629151402302528
running average episode reward sum: 0.5355930101638513
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.68768  , 14.323494 ,  3.3715668], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9646577341801713}
episode index:791
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.895913, 20.26603 ,  2.075776], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.061695962208935}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5359087742176939
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.77988  , 15.79247  ,  4.5897985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8224728994067249}
episode index:792
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.06700382, 13.78379189]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([2.8647787e+01, 1.0918728e+01, 1.7253041e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5355122332505075
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.3633175, 15.963068 ,  4.6894875], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1544975564574458}
episode index:793
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.846015 , 19.828272 ,  1.9205567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.062278020083529}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5356152280964669
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.273933  , 15.2849    ,  0.40307635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7799618823077893}
episode index:794
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.354872 , 13.834961 ,  2.3593316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5514054225446974}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5360790857655391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.453749 , 16.47894  ,  2.5967913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0737998997052784}
episode index:795
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.296671,  6.336122,  5.668768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.075834370694691}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5359792499137531
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.746199 , 14.433931 ,  1.0199397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6203618612180644}
episode index:796
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([7.185409, 9.118601, 2.694921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.780525928311901}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5362442346620816
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.507385, 14.522746,  1.229969], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5811330121173266}
episode index:797
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.03614326, 14.91386753]), 'previousTarget': array([16.04106794, 14.90535746]), 'currentState': array([26.00177  , 14.085445 ,  2.9640565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5366285691968208
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.543512, 15.653571,  4.274358], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8500358162642877}
episode index:798
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.29987993, 10.51088209]), 'previousTarget': array([ 8.32050294, 10.54700196]), 'currentState': array([-0.00779837,  4.944691  ,  2.8901997 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5364059405716457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.310387, 13.615697,  5.63264 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.184282101651952}
episode index:799
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.035338 , 13.129291 ,  3.7293143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.5055344989241344}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5365550071716488
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.039469 , 13.906998 ,  5.3447065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.244623975740622}
episode index:800
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.711853, 11.940326,  4.911912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.8103492068942915}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5369481442052608
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.132599 , 13.140452 ,  2.5477858], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1773142971831936}
episode index:801
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.12234875, 20.32152411]), 'previousTarget': array([13.16227766, 20.51316702]), 'currentState': array([ 9.794989 , 29.751724 ,  1.7341342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.5363997458081097
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.654539 , 15.991305 ,  0.9474904], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0497759413998278}
episode index:802
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.1647567 , 18.41562259]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([26.89696  , 24.756998 ,  1.0075445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.536225745883594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.79166  , 14.838531 ,  2.6220536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7989216349322135}
episode index:803
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.264748 , 21.621357 ,  3.2721176], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.997960404775122}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5365458808808079
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.326485 , 16.635317 ,  3.7931745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7685825063760865}
episode index:804
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.767335 , 18.88472  ,  4.3300676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.51704165652799}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5363205578172356
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.990381, 13.357849,  5.193381], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9276902105708797}
episode index:805
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.167404 , 13.824829 ,  1.6749817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.27836769359144}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.536835036095254
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.799456 , 16.460768 ,  1.9307103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3177323007018638}
episode index:806
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.028511 , 12.853934 ,  2.9444942], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.146255092000234}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5373721661620504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.963158, 13.059348,  3.298185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9410015792541688}
episode index:807
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.12198133, 19.90950203]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([12.361506 , 29.753319 ,  1.0051926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 185
reward sum = 0.15577974928671173
running average episode reward sum: 0.5368998983193829
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.644935, 15.036991,  1.504832], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3555701509712734}
episode index:808
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.95656666, 11.05806152]), 'previousTarget': array([18.92893219, 11.07106781]), 'currentState': array([26.040718 ,  4.0001016,  4.205476 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5364513073801455
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.730582 , 15.76501  ,  4.9780574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0578234833725382}
episode index:809
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.70956621, 14.97268353]), 'previousTarget': array([14.9503719 , 14.99503719]), 'currentState': array([ 4.7535057, 14.036276 ,  4.263079 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5368615475102552
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.635961  , 16.586569  ,  0.09462017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6277977024300123}
episode index:810
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.87401762, 11.6287935 ]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([24.361624 ,  4.018868 ,  2.7004585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5369761740006288
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.555037, 13.012261,  5.455902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0637757496929554}
episode index:811
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.78472307, 20.64430105]), 'previousTarget': array([10.3715414 , 19.24275371]), 'currentState': array([ 2.9983025, 27.988993 ,  1.6471052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5368234320020244
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.251599  , 13.244956  ,  0.08433168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9079525607133683}
episode index:812
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.52796666, 15.37006767]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 3.8297424, 17.808191 ,  2.3048568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.536930076482823
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.317366 , 13.228409 ,  1.1722525], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.443316127767601}
episode index:813
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.98935113, 11.66900633]), 'previousTarget': array([11.72672794, 11.39940073]), 'currentState': array([5.2840242, 4.2502065, 3.6796641], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.536638246402761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.853715  , 13.657429  ,  0.16915584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5910143896447018}
episode index:814
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.43857236, 17.35284179]), 'previousTarget': array([13.74721128, 19.38476052]), 'currentState': array([12.117566 , 27.07976  ,  5.0816054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5365748760026032
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.73424 , 15.805572,  5.145756], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9122061726555402}
episode index:815
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.04089941, 15.48608776]), 'previousTarget': array([17.51316702, 15.83772234]), 'currentState': array([25.101614 , 19.717335 ,  2.6067748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5361460725721046
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.529466, 15.668192,  4.70781 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.852534063651964}
episode index:816
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.86344  , 19.8758   ,  2.0771403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.9516608838199705}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5365531713973148
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.219041, 15.823502,  5.521584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1349237971508535}
episode index:817
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.273719 , 18.119505 ,  3.5559971], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.142936156398569}
done in step count: 301
reward sum = 0.04855048513057287
running average episode reward sum: 0.5359565910962553
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.32125 , 16.260326,  4.175963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0991961053129446}
episode index:818
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.36947496, 14.56179519]), 'previousTarget': array([15.41495392, 14.52576695]), 'currentState': array([21.81553 ,  6.916642,  4.343095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5359635855832767
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.883009, 13.473733,  3.052817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5307442690595485}
episode index:819
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.918931 ,  6.7102933,  3.562721 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.843767490461572}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5360404056087437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.515767 , 13.039282 ,  1.5003107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0274198829592924}
episode index:820
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.310291 , 16.88619  ,  1.9400032], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.635326775541983}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5365227746005807
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.500651 , 16.414978 ,  3.3263445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5005038762285086}
episode index:821
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.08224962, 12.32185863]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([19.828941 ,  3.050273 ,  3.9243164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5364660440633588
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.662914 , 15.357854 ,  2.6542392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7533357443352927}
episode index:822
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.32050294, 15.45299804]), 'currentState': array([ 6.1112766, 19.178091 ,  4.6581287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.821702908580464}
done in step count: 184
reward sum = 0.15735328210778962
running average episode reward sum: 0.5360053967219791
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.808419  , 16.90259   ,  0.39156073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9122110583580183}
episode index:823
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13151101, 14.97194184]), 'previousTarget': array([15.19419324, 14.96116135]), 'currentState': array([24.911402, 12.885381,  5.712202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5360531507914633
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.376    , 16.906307 ,  2.5151143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.351038431641561}
episode index:824
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.46575236, 12.69328357]), 'previousTarget': array([17.6676221 , 11.73957299]), 'currentState': array([24.7684   ,  5.8616414,  0.9527748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.5356100893127944
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.68372 , 13.303615,  4.422542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3901119595176596}
episode index:825
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.928114 , 11.801132 ,  3.8522966], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.875292517646275}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.535848216770835
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.096815 , 13.344962 ,  2.6344392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9854857763113143}
episode index:826
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.96229567, 15.07989074]), 'previousTarget': array([13.95893206, 15.09464254]), 'currentState': array([ 3.9918003, 15.847499 ,  4.43272  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 165
reward sum = 0.1904614597650274
running average episode reward sum: 0.5354305786124242
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.470324, 15.247192,  5.291985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5495205959230607}
episode index:827
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.95601931, 17.14328084]), 'previousTarget': array([17.91263916, 17.11828302]), 'currentState': array([26.051907 , 23.013256 ,  1.1559068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5355073001436484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.017117 , 14.009482 ,  3.2445643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4197363197352515}
episode index:828
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.12944179, 14.17317813]), 'previousTarget': array([15., 15.]), 'currentState': array([ 3.9831111, 10.130328 ,  3.6893234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 161
reward sum = 0.19827425658891443
running average episode reward sum: 0.5351005051574544
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.266287 , 13.4834585,  0.3706457], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6847056271548506}
episode index:829
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.767579 , 20.780191 ,  3.3067722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.258429956127186}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5353651060996787
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.702676 , 14.369766 ,  5.9265504], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6968473767434328}
episode index:830
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42529793, 13.88963863]), 'previousTarget': array([13.84615385, 12.23076923]), 'currentState': array([9.828689 , 5.0086937, 1.609313 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5356199881553128
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.894299 , 15.287642 ,  1.8875784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.916012697433193}
episode index:831
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.58132113, 17.54648441]), 'previousTarget': array([16.63124508, 17.56338512]), 'currentState': array([21.856743 , 26.041773 ,  1.5769517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5353904114668419
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.112673 , 16.001516 ,  4.6506014], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.338052515117094}
episode index:832
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.056189, 17.474707,  3.921489], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6485756535580096}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5358666118707316
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.579532, 15.378338,  4.312416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4699897326214852}
episode index:833
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.5347828 , 11.28557559]), 'previousTarget': array([10.6822128, 11.401844 ]), 'currentState': array([2.846993 , 4.89043  , 2.3506556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5358542975729556
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.085463  , 15.710102  ,  0.28951842], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1578530461106116}
episode index:834
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.63366773, 14.20759584]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 1.8997062, 11.916309 ,  5.818963 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5363400770362227
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.027625  , 14.55004   ,  0.21909112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.023048835455422}
episode index:835
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.313442 , 17.090078 ,  3.9430099], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1177624152510437}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5368827324464664
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.809893 , 15.392518 ,  4.4308224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.851967017796728}
episode index:836
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.072603 , 19.239262 ,  1.8282692], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.11801853384487}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5366613781741983
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.837389 , 14.933563 ,  3.6192565], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8400203381629919}
episode index:837
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.25729044, 10.60700599]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([4.7720942, 2.9950266, 3.088732 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5366419022129527
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.555843, 16.984602,  6.078572], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0609723022192696}
episode index:838
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33153161, 14.99379498]), 'previousTarget': array([17.03454242, 15.1695452 ]), 'currentState': array([25.32978 , 14.806665,  4.192716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.536514663966601
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.284669 , 14.626737 ,  2.4881315], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8068606359029279}
episode index:839
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.85718771, 12.40708867]), 'previousTarget': array([15.83772234, 12.48683298]), 'currentState': array([18.996004 ,  2.9124675,  4.274554 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 144
reward sum = 0.23521662924041012
running average episode reward sum: 0.5361559758300222
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.352261 , 14.396896 ,  1.2612773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8850425648448578}
episode index:840
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.02858084, 19.87417357]), 'previousTarget': array([12.00818834, 21.83842665]), 'currentState': array([ 9.27904 , 29.144608,  5.036337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5366156532599841
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.168162, 16.436993,  5.13068 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6603920456757748}
episode index:841
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.8052741 , 10.46163654]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.017698, 3.118013, 4.417638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.5362091365683247
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.984598 , 14.977192 ,  1.5752484], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9847292168265644}
episode index:842
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.52826855,  8.24550501]), 'previousTarget': array([10.54700196,  8.32050294]), 'currentState': array([ 5.0080156 , -0.09276865,  5.463077  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5362286790838573
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.118949 , 14.139064 ,  1.2366133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8691144512907492}
episode index:843
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.42228306, 16.85590275]), 'previousTarget': array([17.91263916, 17.11828302]), 'currentState': array([27.212872, 21.623035,  5.564165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 300
reward sum = 0.04904089407128572
running average episode reward sum: 0.5356514423717571
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.202642, 16.935965,  4.03004 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9465412481299893}
episode index:844
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.96140361, 14.63151294]), 'previousTarget': array([17.76923077, 13.84615385]), 'currentState': array([25.29903  , 11.052585 ,  2.7373242], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5361429673510805
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.660881 , 15.736308 ,  2.0946193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.816776115049628}
episode index:845
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.302891 , 13.882441 ,  2.5040064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.032023326403122}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5366912617159136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.302891 , 13.882441 ,  2.5040064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.032023326403122}
episode index:846
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.65296133, 18.7303743 ]), 'previousTarget': array([13.58979079, 18.66654394]), 'currentState': array([10.256608 , 28.135946 ,  1.3035853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5370330295578495
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.253808 , 15.785689 ,  4.8775535], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0835636712298642}
episode index:847
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.73569801, 18.31290882]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([24.103056 , 26.02373  ,  2.7411351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5374769967959695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.977384 , 16.943396 ,  4.4656725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.943527209964989}
episode index:848
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.68719805,  9.79580912]), 'previousTarget': array([14.66519011,  9.97785158]), 'currentState': array([14.087223  , -0.18617615,  3.5052352 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.5370388172893368
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.2802305, 13.037545 ,  5.869009 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9823617155435864}
episode index:849
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.773922 , 13.756929 ,  3.1364264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.906217012473681}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5375485351513494
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.718906, 14.317294,  3.670771], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9914200589012248}
episode index:850
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.55980996, 17.96799186]), 'previousTarget': array([14.52057184, 18.11628302]), 'currentState': array([13.092733 , 27.85979  ,  3.1668282], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.537948032786893
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.140516, 16.7629  ,  4.443037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5623219920714084}
episode index:851
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.981614  , 14.12445422]), 'previousTarget': array([13.10366477, 14.13802944]), 'currentState': array([ 3.8075705, 10.144891 ,  4.430507 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5381848313087256
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.619101, 14.351393,  5.677462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5256390804767752}
episode index:852
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.64363839, 16.52590681]), 'currentState': array([ 8.130319, 22.067568,  5.35342 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.856116726710876}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5384936774496756
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.718136 , 16.883938 ,  2.3421266], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9049066058414237}
episode index:853
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.5820478 , 12.59218568]), 'previousTarget': array([10.6822128, 11.401844 ]), 'currentState': array([3.4068987, 6.8331113, 1.658472 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5385234333197022
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.169568  , 13.335959  ,  0.51596195], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4737647592033776}
episode index:854
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.28912602, 19.20376909]), 'previousTarget': array([10.3715414 , 19.24275371]), 'currentState': array([ 2.827886, 25.86184 ,  1.868726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.538812503153021
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175871, 14.408671,  4.779017], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.917580905257689}
episode index:855
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.191    , 14.131378 ,  1.1899238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4741047590167151}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5393512735932628
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.191    , 14.131378 ,  1.1899238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4741047590167151}
episode index:856
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.58821525, 11.40757591]), 'previousTarget': array([20.58821525, 11.40757591]), 'currentState': array([29.       ,  6.       ,  5.5559897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5392765813743743
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.892578, 13.143366,  5.790028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.651215247598061}
episode index:857
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.41949722, 12.3242503 ]), 'previousTarget': array([12.24097426, 10.51658317]), 'currentState': array([8.333688  , 3.7141085 , 0.95732915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5391057590515111
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.90223  , 14.688685 ,  2.0589457], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.954429786056947}
episode index:858
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.66221159, 20.79955992]), 'previousTarget': array([17.77114535, 20.9381686 ]), 'currentState': array([21.834042, 29.887785,  5.635778], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5391754333791184
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.704623, 15.455463,  4.346627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8390118013475073}
episode index:859
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.90480619, 19.54549851]), 'previousTarget': array([21.13681661, 19.82178448]), 'currentState': array([27.239422 , 26.342808 ,  3.3572335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5390585055517123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.980959, 15.253891,  4.506512], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.997162725426222}
episode index:860
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.79118573, 18.10162223]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([28.185734 , 23.535917 ,  5.3675585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5391566087913633
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.878202 , 15.752361 ,  3.4669857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0232874045914198}
episode index:861
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.07743889, 20.67706114]), 'previousTarget': array([ 9.07106781, 20.92893219]), 'currentState': array([ 1.8583323, 27.596924 ,  1.8544476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 498
reward sum = 0.0067038904626207565
running average episode reward sum: 0.5385389142225364
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.419627 , 16.908945 ,  0.0842607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.995220271368427}
episode index:862
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.94918823, 16.5245738 ]), 'previousTarget': array([18.67949706, 17.45299804]), 'currentState': array([26.832432 , 21.11674  ,  4.4608545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.5380097582258159
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.942064 , 16.013153 ,  2.8254082], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1904549399405884}
episode index:863
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.195161, 21.798586,  2.066272], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.846059900344846}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5380873002500193
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.431932 , 15.402016 ,  4.6593094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6959291299624101}
episode index:864
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.81545 , 18.287786,  3.675546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.901094602248126}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5377511746842897
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.002222 , 15.292522 ,  5.7207327], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.039774104091899}
episode index:865
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.825727  , 21.048193  ,  0.82717294], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.675730221618236}
done in step count: 270
reward sum = 0.06629832272038531
running average episode reward sum: 0.5372067718529226
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.089949 , 13.261358 ,  3.3641834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0520387825169095}
episode index:866
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.956557, 16.92254 ,  2.639156], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.341873744517297}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5371995039995227
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.284838 , 13.065399 ,  4.9264593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.585432654110957}
episode index:867
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.80155938, 14.26268057]), 'previousTarget': array([14.87347886, 14.57826285]), 'currentState': array([12.202661 ,  4.606298 ,  2.9338217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.537374906743612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.165309 , 14.818704 ,  1.0391477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.24534754069770445}
episode index:868
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.27656494, 19.05354552]), 'previousTarget': array([17.25900177, 19.195289  ]), 'currentState': array([22.173368 , 27.772566 ,  1.3152956], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5370808742796268
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.170949 , 16.303493 ,  4.3056345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5448044840550892}
episode index:869
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.463079 , 19.94541  ,  4.1202936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.967043428422339}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5369104180081263
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.15328   , 13.789333  ,  0.25079125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2081864862323535}
episode index:870
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.303421 , 13.099584 ,  2.3870885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.583669110544067}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5373533965114785
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.722157, 16.643036,  2.251652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.380207979754416}
episode index:871
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.28932904, 15.52640865]), 'previousTarget': array([18.11628302, 15.47942816]), 'currentState': array([28.163681 , 17.106653 ,  6.1618037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5373046410792408
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.842419 , 14.461512 ,  2.7835746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.919498971620627}
episode index:872
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.39071164, 12.93898474]), 'previousTarget': array([11.07959385, 13.19058177]), 'currentState': array([4.2634010e-03, 9.4901314e+00, 2.7459042e+00], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5370645701930535
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.914059, 15.603785,  5.577469], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6098702369059497}
episode index:873
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.81011165, 12.75068182]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([25.617134,  6.50165 ,  3.999235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5369178374747806
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.347958 , 13.435766 ,  2.2990036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6024674153673526}
episode index:874
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.68863329, 17.39059598]), 'previousTarget': array([13.78852131, 17.22104427]), 'currentState': array([ 8.879194, 26.15811 ,  4.09148 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5368420303937644
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.803011, 16.21487 ,  5.106638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7054890869547457}
episode index:875
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.803848 , 15.790189 ,  5.4821424], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.85739302288794}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5368664871382326
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.65654  , 16.909752 ,  3.0271807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5280974636926525}
episode index:876
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.24535536, 9.21863865]), 'previousTarget': array([9.07106781, 9.07106781]), 'currentState': array([2.1906826, 2.1312137, 3.9200265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5367251908828115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.158631  , 16.344273  ,  0.02871738], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.279848121608058}
episode index:877
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.65196555, 20.21719897]), 'previousTarget': array([20.65196555, 20.21719897]), 'currentState': array([28.        , 27.        ,  0.40297526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.536187894522009
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.099996, 16.666073,  4.149293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8936226189022938}
episode index:878
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.74677046, 22.81334698]), 'previousTarget': array([ 7.8231825 , 22.68944732]), 'currentState': array([ 0.9432776, 30.142227 ,  3.2199197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.5356719860326512
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.014447, 16.965145,  5.808408], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.198433443413677}
episode index:879
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.3286389 ,  8.40346272]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([24.814903  ,  0.04278671,  0.20418853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5360020958483364
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.099778 , 13.278666 ,  2.3321662], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7242238829400276}
episode index:880
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175853, 14.016388,  2.074626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0724395725895923}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.536528767703219
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175853, 14.016388,  2.074626], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0724395725895923}
episode index:881
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.812124  ,  8.289624  ,  0.12611598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.877495790560559}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5365793772796531
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.534632 , 13.0762005,  2.852407 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9967062024292588}
episode index:882
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.98381268, 17.15419039]), 'previousTarget': array([19.77807808, 18.30790021]), 'currentState': array([27.780159 , 21.91069  ,  4.1866302], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5366778438907647
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.194277 , 14.562991 ,  2.9583507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2717208183799293}
episode index:883
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.41184297, 13.81585785]), 'previousTarget': array([11.73957299, 12.3323779 ]), 'currentState': array([5.394973 , 7.8384175, 0.9530101], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5371145710972547
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.764659  , 13.859922  ,  0.58317554], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.372763704871635}
episode index:884
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.08546926, 11.05629825]), 'previousTarget': array([20.07376011, 11.09710761]), 'currentState': array([27.987766 ,  4.9281907,  5.9902077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5372193270973744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.233972 , 13.964826 ,  1.6072884], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.287783169641459}
episode index:885
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.895538 , 10.997262 ,  0.8086351], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.75461499343909}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.536845950359248
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.5534525, 15.673031 ,  5.4716406], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8076974721827898}
episode index:886
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.30611606, 11.87468097]), 'previousTarget': array([13.58979079, 11.33345606]), 'currentState': array([12.138691,  2.112393,  6.247519], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5368893650149612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.735435, 15.287931,  2.727529], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7591591289461352}
episode index:887
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.2492666 , 16.82392349]), 'previousTarget': array([17.19131191, 16.75304952]), 'currentState': array([25.016497, 23.122347,  3.246901], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 328
reward sum = 0.03701210861730961
running average episode reward sum: 0.5363264401766755
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.440414 , 16.992214 ,  1.6018732], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0403142648075074}
episode index:888
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.888639, 21.931446,  4.950384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.2458807655461746}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5364909262900993
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.991488 , 16.635958 ,  5.6149716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9129576614663821}
episode index:889
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.239496 , 15.093017 ,  3.2803187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2414271037537}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5369674488560655
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.76844 , 14.288914,  2.747056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9060494730345106}
episode index:890
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.20964079, 15.72270237]), 'previousTarget': array([18.25608804, 15.75140493]), 'currentState': array([27.965391 , 17.919367 ,  4.5311184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5371464003366418
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.233401, 14.389032,  2.459837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.65403181766951}
episode index:891
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.72673821, 17.29125869]), 'previousTarget': array([16.75304952, 17.19131191]), 'currentState': array([22.745224 , 25.27736  ,  0.4057858], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5369504649302784
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.692833 , 16.738457 ,  3.4966981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.76538477024647}
episode index:892
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.876574, 10.208399,  3.067724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.5824507063982525}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5369089045327072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.031103 , 13.05168  ,  1.3882726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2043425347833345}
episode index:893
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.910042 ,  8.791932 ,  4.5469446], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.274414958429716}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.536671249751198
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.231723 , 13.713828 ,  2.5825667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4981615391736456}
episode index:894
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.9425745, 12.755823 ,  3.323955 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.480822139043748}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5366829652737003
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.265436 , 14.419794 ,  0.7226634], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8290299555392653}
episode index:895
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.94070098, 15.03142929]), 'previousTarget': array([11., 15.]), 'currentState': array([ 0.9410007, 15.108852 ,  2.9751148], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5365495254463114
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.110844 , 13.325722 ,  1.2421443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0092738297706654}
episode index:896
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.0702554, 11.6109001]), 'previousTarget': array([13.03871026, 11.63778901]), 'currentState': array([8.122178 , 2.9208753, 2.9775574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5367435095063181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.29361  , 16.024052 ,  0.6415237], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9900879244542955}
episode index:897
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.85449413, 18.86479546]), 'previousTarget': array([16.80941823, 18.92040615]), 'currentState': array([21.180653, 27.880579,  5.565765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5363779738183765
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.332223, 16.32839 ,  3.483344], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3693036170561173}
episode index:898
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.38743122, 15.58402331]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([20.915476 , 23.917133 ,  2.7027445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5367189807366919
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.826437 , 15.686313 ,  4.4443727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9511271589921582}
episode index:899
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.851081, 19.143984,  5.524152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.26314991098433}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5366780007913169
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.163559  , 13.017443  ,  0.21325624], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.702415424846454}
episode index:900
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.35951191, 14.71863006]), 'previousTarget': array([18.11628302, 14.52057184]), 'currentState': array([26.151985 , 12.691941 ,  2.8631392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5370962463481341
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.987316 , 14.335626 ,  3.7216523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0954280435791577}
episode index:901
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.84959769, 15.8469541 ]), 'previousTarget': array([16.89633523, 15.86197056]), 'currentState': array([25.941694 , 20.01034  ,  4.0821652], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5373631256308957
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.029774, 15.371898,  2.466085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.094870675294463}
episode index:902
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.918428 ,  9.85114  ,  2.6911864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.918444349557523}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5376381145735051
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.210138, 13.568401,  1.68593 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8745424744759573}
episode index:903
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.04106794, 15.09464254]), 'currentState': array([24.830729 , 14.202262 ,  4.5987415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.863042607869351}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5380238963845035
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.419103 , 13.170589 ,  3.7221894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.876803137868424}
episode index:904
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.172926  , 23.863308  ,  0.05425918], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.940580694209155}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5382145365291309
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.9319725, 15.318582 ,  4.578762 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.32576366292788933}
episode index:905
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.55195153, 11.31327634]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([28.882536 ,  5.7814264,  1.567112 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 257
reward sum = 0.07555183406752786
running average episode reward sum: 0.5377038712946258
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.71219  , 14.298042 ,  1.8616887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.466696986794171}
episode index:906
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.91086906, 16.67403306]), 'previousTarget': array([11.07959385, 16.80941823]), 'currentState': array([ 0.41159832, 19.79875   ,  3.8597956 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.537994859958622
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5427065, 13.483584 ,  5.4929633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.103145414701285}
episode index:907
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.445262 , 11.901916 ,  4.9634843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.147357869098349}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5380488708859115
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.544756 , 13.358913 ,  0.4003475], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7291397245884677}
episode index:908
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9084546, 13.8998034]), 'previousTarget': array([10.61523948, 13.74721128]), 'currentState': array([ 1.2514838, 11.303091 ,  4.5641294], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.538041012439343
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.112501 , 14.693138 ,  3.1414924], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9122803512256044}
episode index:909
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.911804,  7.957417,  6.199782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.143176288083268}
done in step count: 121
reward sum = 0.296386587399208
running average episode reward sum: 0.537775458126112
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.300693 , 14.257858 ,  1.4470116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8542977429238576}
episode index:910
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.16788812, 13.45440167]), 'previousTarget': array([ 9.48683298, 13.16227766]), 'currentState': array([1.8938024, 9.713902 , 5.801648 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5381104391746936
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.043043  , 13.56696   ,  0.44572824], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4255479478504283}
episode index:911
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.769981 , 19.940151 ,  1.7514906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.867154900645711}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.538197259023123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.054445, 14.167689,  3.151505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.834089533209028}
episode index:912
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.55025197,  9.79533018]), 'previousTarget': array([19.54057759,  9.63386285]), 'currentState': array([26.132156 ,  2.2668154,  3.442263 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5381180514833056
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.871521 , 14.5385475,  2.0428066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.92757081165136}
episode index:913
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.64763821, 15.36882594]), 'currentState': array([ 5.846342  , 17.351261  ,  0.23645467], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.450813834554792}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5384152175847695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.987652 , 16.512138 ,  5.7039366], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8197283499381092}
episode index:914
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.586845 ,  8.910341 ,  2.0225985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.623850355301611}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5385579023290153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.795909 , 13.465725 ,  2.2074604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.950342342783523}
episode index:915
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.168356, 16.042591,  5.571774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.243778233715936}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5387456701510058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.021238 , 14.684619 ,  3.2725518], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0688278286777593}
episode index:916
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.20207103, 12.854637  ]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([1.0731235, 8.772687 , 1.9269861], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5385049323033156
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.74679 , 16.686419,  5.633509], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4280202917101157}
episode index:917
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.42535625, 21.298575  ]), 'previousTarget': array([13.42535625, 21.298575  ]), 'currentState': array([11.    , 31.    ,  2.2212], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5384309472371744
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.15467  , 16.896746 ,  4.5843425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.076590328673727}
episode index:918
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.03308659, 15.51287016]), 'previousTarget': array([16.05572809, 15.52786405]), 'currentState': array([24.99006 , 19.95951 ,  4.407852], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.538819312097916
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.066935, 15.330387,  3.627442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1169176585417533}
episode index:919
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.92385083, 17.05247113]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([29.154043, 20.90001 ,  1.9706  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.5384513313530894
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.368261, 15.682787,  5.31137 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.768832685964379}
episode index:920
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.22122061,  8.97683473]), 'previousTarget': array([10.17821552,  8.86318339]), 'currentState': array([4.0058475, 1.1429892, 4.6927342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 191
reward sum = 0.14666354163210368
running average episode reward sum: 0.5380259374445976
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.709178 , 13.166887 ,  3.5873177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.241991022033824}
episode index:921
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38476052, 13.74721128]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([29.       , 11.       ,  1.1028507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5379039897070098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.457338 , 16.672668 ,  3.5951476], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7584940863811729}
episode index:922
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.1543   , 12.914281 ,  5.4855714], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.2066444860458665}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5380104702014377
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.828594 , 14.095023 ,  6.2347097], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9210662490906281}
episode index:923
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 6.92615755, 23.03189234]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([-0.163304 , 30.084518 ,  1.8455832], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5377758279983677
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.54185 , 16.469772,  5.725667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0703699882035123}
episode index:924
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([16.1613009 , 15.21114562]), 'currentState': array([24.184332 , 15.904666 ,  3.6212654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.228779595010517}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5378617894177665
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.993611 , 13.067004 ,  4.0201497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9330063536294142}
episode index:925
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.300639 , 22.939728 ,  2.4596207], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.119550761051515}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5379819199133935
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.778145 , 16.297314 ,  6.1742945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3161468475619977}
episode index:926
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.2609582 , 17.68924954]), 'previousTarget': array([15., 19.]), 'currentState': array([16.226797 , 27.642498 ,  5.2358065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5383969822375732
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.38673  , 15.61625  ,  3.8289196], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7269636869868565}
episode index:927
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.59817332, 15.8524583 ]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 1.8980927, 18.283186 ,  4.0283947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5386634490032731
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.023707, 15.058722,  6.116569], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9771648137025404}
episode index:928
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.020149 , 22.040901 ,  6.2378354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.108754955236665}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5387823286366048
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.148088 , 16.719236 ,  2.4434888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.526925024433515}
episode index:929
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.26431962, 10.1741915 ]), 'previousTarget': array([15.6783628 ,  9.91227901]), 'currentState': array([18.798697  ,  0.50067437,  0.21255717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5384317301146612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1983385, 14.601999 ,  2.0424745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8950227469994558}
episode index:930
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13068672, 14.53452486]), 'previousTarget': array([15.57464375, 12.701425  ]), 'currentState': array([17.833769 ,  4.9067864,  2.0414698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5386722360053512
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.978231 , 13.702635 ,  1.0443709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6248363497080236}
episode index:931
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.05304  ,  9.87319  ,  1.8798859], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.1270844275575325}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5391458709452596
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.067328 , 13.16171  ,  2.4853263], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6673076727675054}
episode index:932
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.6305895, 19.134645 ,  4.179988 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.151115178503735}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5396184905905487
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.541105 , 15.347549 ,  4.5195417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4997214555648766}
episode index:933
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72917405, 14.97262093]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([ 1.7295244, 14.888917 ,  0.5377575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5397016508157647
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.65667 , 16.024378,  5.185628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.947794687048525}
episode index:934
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.75205 , 13.058465,  5.80295 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.070884774267435}
done in step count: 138
reward sum = 0.2498370564584527
running average episode reward sum: 0.5393916352068263
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.90049  , 13.479417 ,  4.2821198], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4339339825664643}
episode index:935
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57225847, 13.9959207 ]), 'previousTarget': array([17.91263916, 12.88171698]), 'currentState': array([25.00024  ,  8.613624 ,  1.8939215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5390932311749577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.963753, 14.30716 ,  3.08269 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.186948229303607}
episode index:936
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.98106644, 10.87183171]), 'previousTarget': array([11.09710761,  9.92623989]), 'currentState': array([4.005433 , 3.7066004, 1.896687 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5392994319630894
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.832855 , 13.714613 ,  1.2827531], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2962087858227387}
episode index:937
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.191986 , 18.466536 ,  5.4977636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.4718478512599007}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5397693686027876
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.257254 , 16.826958 ,  5.7814817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2177603972798177}
episode index:938
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.2486715 , 14.05802241]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([8.013132, 6.240219, 4.306521], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 130
reward sum = 0.27075425951199406
running average episode reward sum: 0.5394828775387931
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.631521  , 13.293608  ,  0.93641174], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8195036957506916}
episode index:939
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.16590185, 14.9945791 ]), 'previousTarget': array([11., 15.]), 'currentState': array([ 3.1659455 , 14.965023  ,  0.27881974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5394353836899223
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.502141 , 13.582891 ,  0.9463203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.061984042008473}
episode index:940
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.42535625, 21.298575  ]), 'previousTarget': array([13.42535625, 21.298575  ]), 'currentState': array([11.       , 31.       ,  0.8120454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5393009674172804
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.268486 , 14.70608  ,  3.6436691], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3020927386726886}
episode index:941
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.13671126, 11.11745312]), 'previousTarget': array([11.07106781, 11.07106781]), 'currentState': array([4.0832458, 4.0288267, 5.4712896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5395137056401795
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.094166, 15.024411,  4.551183], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9059905290993475}
episode index:942
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.52918113, 14.11070083]), 'previousTarget': array([11.88371698, 14.52057184]), 'currentState': array([-0.34126306, 12.506228  ,  3.2025518 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5393751122878808
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.746281 , 13.947789 ,  1.4149647], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0823682748414014}
episode index:943
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.42672034, 11.25836691]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([6.760076 , 3.0188801, 4.6538277], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5393718210939394
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.697385  , 13.8553295 ,  0.43085742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3403790992279725}
episode index:944
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.790142 , 21.8017   ,  2.5361524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.847441523495276}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.5388988055841637
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.051656 , 16.679525 ,  3.6426036], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9287722173210788}
episode index:945
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.26287828, 21.19078381]), 'previousTarget': array([20.33471177, 22.75958076]), 'currentState': array([24.93423  , 29.42703  ,  4.3362765], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5392112949666857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.408545, 16.996035,  4.51208 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.081819822577136}
episode index:946
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.77058823, 12.05951535]), 'previousTarget': array([12.24097426, 10.51658317]), 'currentState': array([6.728961 , 4.0909057, 1.748946 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.539249461233856
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.9856615, 13.321415 ,  2.3673975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.961257340975285}
episode index:947
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.040455 ,  6.0506268,  3.8057184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.451753660865153}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5394609073437235
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.175385 , 14.065684 ,  0.3642114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.24616808946333}
episode index:948
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33645804, 13.29845643]), 'previousTarget': array([15.23303501, 11.97054486]), 'currentState': array([17.276268  ,  3.4884036 ,  0.64682865], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5398746106530631
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.41976  , 13.108339 ,  1.4839742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.365184541640465}
episode index:949
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.100278 , 11.8319435,  3.7220752], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.024381988184773}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5399059018023517
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.39676 , 16.992449,  5.636118], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.081766259934624}
episode index:950
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.83721  , 18.721535 ,  1.5235485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.345465277481482}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5400853416819205
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.078896 , 14.774339 ,  4.0190153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1022425598589207}
episode index:951
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.18849313, 12.05679099]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([5.281062 , 4.8257885, 1.2041837], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.540506974883306
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.203794  , 13.411937  ,  0.60668737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3975617933088724}
episode index:952
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.31247552, 19.23468485]), 'previousTarget': array([15.2875295, 19.025413 ]), 'currentState': array([16.04837  , 29.207571 ,  0.9359708], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.540723832301405
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.320929 , 14.480915 ,  5.0084524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.757478312786187}
episode index:953
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.71291222, 18.3418366 ]), 'previousTarget': array([17.6676221 , 18.26042701]), 'currentState': array([24.015577 , 26.105625 ,  5.9337125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5404937245888928
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.577041 , 15.1149025,  5.1460085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.43828891903767647}
episode index:954
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.006842 , 10.959648 ,  3.8144999], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.95628359574062}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5407260271959695
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.938112 , 14.07025  ,  0.8537388], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.320791617045389}
episode index:955
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.44467231, 21.85464736]), 'previousTarget': array([11.47213595, 22.05572809]), 'currentState': array([ 6.8404126, 30.731628 ,  3.8780198], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5408672437460367
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.767456, 13.503081,  6.168431], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9390539204779986}
episode index:956
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.034455 , 22.755915 ,  2.4848795], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.710898772335812}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.540966844835188
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.2212105, 15.979452 ,  4.7512684], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0041217842141976}
episode index:957
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.63830133, 12.2876428 ]), 'previousTarget': array([17.92893219, 12.07106781]), 'currentState': array([24.610834 ,  5.1193943,  5.3504963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5408740231289576
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.508623, 13.117695,  1.936986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.945385281883857}
episode index:958
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.91883   , 13.114347  ,  0.29148352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.21194368905373}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5411129898887868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.882885 , 16.715727 ,  2.1859846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9295607061418263}
episode index:959
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.764206 ,  9.7843685,  4.8708453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.505927202556549}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5413354964484898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.098901 , 14.32703  ,  1.4474628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6801983117755462}
episode index:960
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.028338,  9.000251,  5.47357 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.999816108308001}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5416147809144344
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.199665, 14.745634,  2.066012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2263353134225503}
episode index:961
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.48332435, 10.48060513]), 'previousTarget': array([16.63663603, 10.41741912]), 'currentState': array([19.601784  ,  0.9792783 ,  0.67771053], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.54191924970917
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.232985 , 15.127501 ,  2.5890384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.239559304383543}
episode index:962
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.59426835, 11.39035422]), 'previousTarget': array([12.54700196, 11.32050294]), 'currentState': array([7.048382 , 3.0691078, 3.813886 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.541581893163961
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.937782 , 13.258057 ,  2.9591713], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6056413531792635}
episode index:963
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.9785957, 12.982951 ,  2.140732 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.244145225107081}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5417352823680119
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.941533 , 16.325363 ,  5.5155807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.326652133541263}
episode index:964
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.246158,  8.190424,  2.510911], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.304778317043889}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5418602012888575
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.213649 , 14.230932 ,  1.8188438], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.099915197623012}
episode index:965
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.48755606, 15.83798212]), 'previousTarget': array([ 9.6623494 , 16.42337349]), 'currentState': array([ 1.7605469, 18.158606 ,  5.6377892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5419382861125153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.742001 , 13.789178 ,  0.9915265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4200897969557913}
episode index:966
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.95848751, 10.60041881]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.193456 , 3.2360213, 4.2255864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 117
reward sum = 0.30854447063465107
running average episode reward sum: 0.5416969274615557
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.386328 , 13.19939  ,  0.4892462], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9023113648932197}
episode index:967
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.64185069, 13.02267193]), 'previousTarget': array([16.75304952, 12.80868809]), 'currentState': array([23.030085 ,  5.329138 ,  1.6417855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 104
reward sum = 0.35160920655802225
running average episode reward sum: 0.5415005558490521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.459206 , 14.394153 ,  2.7284422], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8121019489197078}
episode index:968
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9517864 , 20.38379916]), 'previousTarget': array([11.09710761, 20.07376011]), 'currentState': array([ 4.9419436, 28.376408 ,  1.9310713], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5412765568542268
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.977947 , 16.186495 ,  5.1923428], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3065223259289658}
episode index:969
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.69307754, 12.92121772]), 'previousTarget': array([16.75304952, 12.80868809]), 'currentState': array([23.008137,  5.167507,  3.34816 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5413057575198175
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.650015 , 13.082645 ,  3.0288892], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3449325003608985}
episode index:970
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.89949494, 14.41421356]), 'previousTarget': array([10.89949494, 14.41421356]), 'currentState': array([ 1.       , 13.       ,  4.0223308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5413232244415765
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.336963, 13.876867,  1.330775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3042413641362918}
episode index:971
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.38462842, 15.58993045]), 'previousTarget': array([15.45299804, 15.67949706]), 'currentState': array([20.846222 , 23.966743 ,  3.3214607], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5416335330515996
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.078836, 14.530328,  5.451774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0339894973773862}
episode index:972
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.93679234, 11.91362313]), 'previousTarget': array([12.07106781, 12.07106781]), 'currentState': array([4.8924155, 4.8159647, 2.4280987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.5412521290411556
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.272509 , 13.4080925,  5.878024 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7502608944520384}
episode index:973
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.07786725, 21.85400788]), 'previousTarget': array([ 8.07106781, 21.92893219]), 'currentState': array([ 0.97191894, 28.890022  ,  2.1507754 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5414333183671896
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.574881 , 16.756124 ,  5.9516134], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8478260067698584}
episode index:974
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.98907708, 12.92408357]), 'previousTarget': array([15., 13.]), 'currentState': array([14.9364605,  2.924222 ,  1.4737817], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5417871148321628
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.5990715, 14.52148  ,  1.5803667], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7667257868380409}
episode index:975
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.47809895, 12.0952487 ]), 'previousTarget': array([16.52786405, 11.94427191]), 'currentState': array([21.013264 ,  3.1827698,  1.0861902], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5420616442926024
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.736633 , 15.580844 ,  2.6721308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.831194934228695}
episode index:976
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.92470425, 11.9084864 ]), 'previousTarget': array([12.07106781, 12.07106781]), 'currentState': array([4.8722568, 4.818847 , 4.4219856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.54202359336578
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.392845, 13.003351,  6.080985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.434465856592677}
episode index:977
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.072767, 22.93506 ,  3.340152], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.417969675402007}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.541900189100738
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.717913, 15.968552,  4.48246 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.972134941229321}
episode index:978
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.254364, 14.015499,  5.379528], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.306583438089943}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5419055583073672
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.769665, 14.123016,  2.637052], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9750477684663001}
episode index:979
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.027463 , 17.192238 ,  5.0036755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.434338177714475}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5423426944723597
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.122262 , 14.537597 ,  5.8770757], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9338347401513418}
episode index:980
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.79391097, 10.38797582]), 'previousTarget': array([10.75724629, 10.3715414 ]), 'currentState': array([4.0554905 , 2.9992216 , 0.36335915], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5422993655584217
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.840218 , 13.89387  ,  0.7990923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6026908723149988}
episode index:981
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.714449 , 18.00126  ,  2.2846951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.046702252628643}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5423213674167658
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.326371 , 13.445791 ,  3.4790452], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5881067379831717}
episode index:982
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.24608505, 11.51843062]), 'previousTarget': array([11.39940073, 11.72672794]), 'currentState': array([3.9140525, 4.718335 , 3.7042003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 211
reward sum = 0.11995712819347787
running average episode reward sum: 0.5418916988112489
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.009176 , 15.869835 ,  3.9464755], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1725542431987166}
episode index:983
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.00496281, 14.9503719 ]), 'currentState': array([15.782036 ,  5.0922694,  4.366391 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.938546436436976}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5419316121398128
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.180927 , 14.062683 ,  2.9802194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0463598250919572}
episode index:984
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.876609 , 11.979856 ,  1.8737501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.144791829085017}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5423865039041378
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.041197 , 13.722333 ,  2.6132252], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2783310438341877}
episode index:985
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.62424763, 18.65213569]), 'previousTarget': array([13.58979079, 18.66654394]), 'currentState': array([10.099086  , 28.010193  ,  0.18121827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5424751825322301
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.294048 , 15.885977 ,  5.0211754], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9222970753190427}
episode index:986
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.7391158 , 8.49898869]), 'previousTarget': array([8.80451099, 8.32793492]), 'currentState': array([1.8023247, 1.296147 , 4.074795 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5425832143922465
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.674415 , 16.211338 ,  5.8227243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2543307717780747}
episode index:987
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.952152, 11.986684,  1.159445], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.53643705704438}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5427902881573673
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.014826, 15.120819,  5.248897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9888473481434545}
episode index:988
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 14.85642931]), 'currentState': array([7.752758, 8.772506, 5.207582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.555323162235535}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5429456197346092
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.810366 , 13.679397 ,  1.2504175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.240851879674444}
episode index:989
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.236925  , 22.413702  ,  0.17562836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.45286929807458}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5427315666725752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.965208, 16.190674,  4.953997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5327526714035282}
episode index:990
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.76574306, 15.0641482 ]), 'previousTarget': array([13.39793423, 15.58256937]), 'currentState': array([ 5.1208267, 17.705282 ,  5.703418 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5428068023681046
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.345165, 14.97165 ,  5.879669], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.655077568095043}
episode index:991
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.9608324 ,  8.52876272]), 'previousTarget': array([11.14495755,  8.57492926]), 'currentState': array([5.6658926 , 0.04562577, 5.590817  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.542540910519533
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.649439 , 13.492916 ,  2.1056752], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6410584055195954}
episode index:992
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.77302117, 11.47189698]), 'previousTarget': array([16.63663603, 10.41741912]), 'currentState': array([21.26332  ,  2.53673  ,  0.9053944], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5428434304418536
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.730545 , 14.506814 ,  3.1551936], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7994495204838947}
episode index:993
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.766668  , 20.794365  ,  0.38832453], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.209865966907489}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5430642546711346
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.012204, 15.020073,  5.136864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9878971763305575}
episode index:994
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.96116135, 14.80580676]), 'currentState': array([13.442808 ,  6.875474 ,  1.5035284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.27241014253308}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5434838845759877
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.611983 , 14.120265 ,  2.0275614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6433271188114704}
episode index:995
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.49746517, 19.6581791 ]), 'previousTarget': array([11.96138938, 20.31756858]), 'currentState': array([ 5.4876957, 27.650843 ,  3.8488941], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5433698335006342
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.763822  , 13.108473  ,  0.11524365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.906214912575479}
episode index:996
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.15441013, 15.52916045]), 'previousTarget': array([15.12652114, 15.42173715]), 'currentState': array([17.955608 , 25.12881  ,  0.4797091], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5431502504478386
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.856894 , 14.869777 ,  4.8986425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.861454194419921}
episode index:997
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.48933527, 8.85772316]), 'previousTarget': array([9.50791373, 8.59256602]), 'currentState': array([1.215477, 1.995436, 2.364637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5430499475732677
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.476803, 13.174609,  4.370085], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.347977473664799}
episode index:998
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.13374636, 14.9315482 ]), 'previousTarget': array([13.95893206, 14.90535746]), 'currentState': array([ 4.164822 , 14.143799 ,  1.1329691], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5433936261760133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.793994 , 13.589503 ,  5.7352953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8557886643433936}
episode index:999
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.60381136, 10.56449687]), 'previousTarget': array([ 9.63386285, 10.45942241]), 'currentState': array([1.8785993, 4.2146072, 3.118642 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5434613497893701
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.100866, 14.380681,  6.090201], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0917863139695942}
episode index:1000
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.77921946, 12.81998379]), 'previousTarget': array([12.77895573, 13.78852131]), 'currentState': array([3.4978807, 7.21468  , 3.9963748], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5432952911451485
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.059983  , 13.039141  ,  0.87090415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7583752765176666}
episode index:1001
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.09848222, 13.82603467]), 'previousTarget': array([14.13940614, 13.89352217]), 'currentState': array([8.007881 , 5.894794 , 4.1653876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 154
reward sum = 0.2127257032290187
running average episode reward sum: 0.5429653813767692
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.261053, 15.746289,  3.424718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4653335912026892}
episode index:1002
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.48360645, 16.72634908]), 'previousTarget': array([12.43661488, 16.63124508]), 'currentState': array([ 4.2375746, 22.383465 ,  2.5492582], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5429376363479028
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.999752 , 16.525219 ,  1.0079542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.823948699695727}
episode index:1003
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.49645965, 16.61111778]), 'previousTarget': array([12.07106781, 17.92893219]), 'currentState': array([ 6.6736884, 23.922054 ,  5.836252 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5432197389250817
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.920672 , 16.318787 ,  3.7579873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.321170321135829}
episode index:1004
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.36882594, 16.35236179]), 'currentState': array([17.674751, 24.064789,  4.741278], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.451174040330319}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5436066499787952
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.603348, 15.022629,  4.916153], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6035074555854527}
episode index:1005
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.959005, 12.180894,  5.366162], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.146686344554682}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5434726673987194
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.07157 , 14.359526,  2.00675 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.032005927608488}
episode index:1006
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.071408 , 17.70458  ,  5.7285066], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7055228290164477}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5439062595860096
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.842846, 16.274857,  5.42653 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.240834741848233}
episode index:1007
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.41743063, 16.60206577]), 'currentState': array([11.325872, 23.857574,  4.683541], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.589360710207737}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5441699714993381
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.699247, 15.244483,  4.7885  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7167450380417644}
episode index:1008
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.85197 ,  6.929668,  4.630065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.15157855441049}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5442302649538854
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.07795  , 13.871584 ,  1.7620952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5605441306673749}
episode index:1009
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.66153498, 11.94051404]), 'previousTarget': array([16.63124508, 12.43661488]), 'currentState': array([24.224884  ,  4.3958173 ,  0.11757384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5443471479004499
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.751303 , 13.57622  ,  1.6639363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.257036100501542}
episode index:1010
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.585953 , 18.662914 ,  1.7260311], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.334641169878118}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5444192972506396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.427021 , 14.890138 ,  3.9713163], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4312437742785802}
episode index:1011
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.872362 , 16.094267 ,  1.1405048], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.2430992824370435}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.544821047006222
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.897586  , 16.65413   ,  0.32098168], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9878294868847055}
episode index:1012
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.9043255, 14.089328 ,  2.9930964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.153874520370452}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.544623407456707
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.554461, 16.773983,  5.951238], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.829076574033696}
episode index:1013
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.17344492, 10.0631472 ]), 'previousTarget': array([10.07106781, 10.07106781]), 'currentState': array([3.1827054, 2.9126534, 4.693912 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5443699165315691
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.805591, 13.13565 ,  5.389546], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.03095503989582}
episode index:1014
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.673567 , 23.071024 ,  1.2382891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.099081411831332}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.5439655911703143
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.577443 , 13.284924 ,  0.2155878], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.228262840452298}
episode index:1015
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.859658 ,  9.214377 ,  1.8281322], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.114482322065602}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.544285256742754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.306349 , 15.775517 ,  0.3782321], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0404707217533562}
episode index:1016
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.45382103, 13.94485524]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([9.85684  , 5.0641026, 5.50663  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5444348417587882
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.111668 , 13.086852 ,  1.1619519], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.212677092214022}
episode index:1017
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.317679 , 10.88218  ,  4.7754483], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.524122079874874}
done in step count: 137
reward sum = 0.2523606630893462
running average episode reward sum: 0.5441479319565589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.660421 , 14.695757 ,  1.2154703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3736938372490999}
episode index:1018
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.05460193, 20.25753481]), 'previousTarget': array([11.09710761, 20.07376011]), 'currentState': array([ 5.0524144, 28.255894 ,  6.2635045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 246
reward sum = 0.08438356532646984
running average episode reward sum: 0.5436967402326824
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.659183 , 13.2541895,  1.6961362], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7787666703692548}
episode index:1019
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.69765574, 9.5809828 ]), 'previousTarget': array([8.59256602, 9.50791373]), 'currentState': array([1.1152097 , 3.061281  , 0.43133974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5436841998431946
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.945876 , 14.3816   ,  1.6799474], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6207636299810125}
episode index:1020
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.87199719,  9.05095515]), 'previousTarget': array([20.92893219,  9.07106781]), 'currentState': array([27.896828 ,  1.9339504,  5.0882673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.5432468329772268
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.86118  , 14.811963 ,  3.0924382], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8814700229275819}
episode index:1021
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.8130541 , 16.30121023]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([24.937288 , 22.13189  ,  0.5704996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5432782497257587
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.564011 , 16.08671  ,  5.0107703], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.224355565989518}
episode index:1022
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.76515385, 17.94384414]), 'previousTarget': array([12.88171698, 17.91263916]), 'currentState': array([ 6.7185674, 25.908691 ,  2.7388375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.5429593510424225
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.263374 , 14.910976 ,  5.9916687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.738905955510967}
episode index:1023
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.999601, 19.029001,  4.361121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.2275607744704775}
done in step count: 439
reward sum = 0.012129710294652202
running average episode reward sum: 0.5424409627213798
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.606377 , 15.763105 ,  5.3532567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5888725208073697}
episode index:1024
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.46862167, 21.92779536]), 'previousTarget': array([18.52786405, 22.05572809]), 'currentState': array([22.945635 , 30.869627 ,  4.2054024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5424349405384391
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.859247, 14.311945,  4.458709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9824782256212754}
episode index:1025
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.68843763, 10.46507434]), 'previousTarget': array([18.90289239,  9.92623989]), 'currentState': array([22.78797  ,  1.8630528,  3.620323 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5422524121506873
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.816692 , 13.132678 ,  0.6013854], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.876297691246289}
episode index:1026
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.63436463, 22.29427943]), 'previousTarget': array([19.7000106 , 22.52001696]), 'currentState': array([24.996983 , 30.734797 ,  2.8697715], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 173
reward sum = 0.1757473014911758
running average episode reward sum: 0.5418955425200548
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.4511866e+01, 1.3875465e+01, 7.7154934e-03], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2259091561718347}
episode index:1027
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.33176309, 20.82448299]), 'previousTarget': array([19.45299804, 21.67949706]), 'currentState': array([23.29707  , 29.504675 ,  3.9004211], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5421174759862855
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.96974  , 16.999998 ,  2.4957545], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.249761813067459}
episode index:1028
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.9526851 , 14.87310273]), 'previousTarget': array([11., 15.]), 'currentState': array([ 2.971839, 14.254467,  5.877122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5422278178177606
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.36384  , 13.450697 ,  1.4377754], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2532996169405406}
episode index:1029
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.93988 , 16.872454,  2.989428], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.362194420029166}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5426529364412385
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.896441 , 15.50343  ,  3.5115633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.028129089424232}
episode index:1030
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.74095432, 12.30087992]), 'previousTarget': array([17.92893219, 12.07106781]), 'currentState': array([24.866188 ,  5.284396 ,  5.5687213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5423788651754154
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.131441 , 14.402549 ,  2.5705826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2794948018986252}
episode index:1031
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.19735908, 19.73057742]), 'previousTarget': array([11.70588235, 21.17647059]), 'currentState': array([ 7.1002293, 28.334023 ,  4.5245214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5427122043290402
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.04972 , 14.017431,  6.273242], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1838118389862666}
episode index:1032
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.150887,  9.278889,  2.905178], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.112072264692902}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5427904358784711
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.003229 , 15.560459 ,  1.3736614], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0739354634647866}
episode index:1033
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.50919563, 11.87871364]), 'previousTarget': array([15.6783628 ,  9.91227901]), 'currentState': array([17.119276 ,  2.0091827,  1.5493083], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 214
reward sum = 0.11639428152900338
running average episode reward sum: 0.5423780604874174
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.98702  , 13.059113 ,  2.7578993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7776419020357626}
episode index:1034
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.19844524, 15.26206784]), 'previousTarget': array([12.701425  , 15.57464375]), 'currentState': array([ 4.693566 , 18.369682 ,  0.2987011], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 204
reward sum = 0.12870034108965533
running average episode reward sum: 0.5419783718696419
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.290223 , 16.611328 ,  4.6761317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3494074359676307}
episode index:1035
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.77787444, 15.23574314]), 'previousTarget': array([13.64363839, 16.52590681]), 'currentState': array([ 7.920141 , 22.513895 ,  4.8886695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5422607419368045
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.141004 , 16.967524 ,  5.1939425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9725696528071521}
episode index:1036
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.46836055, 15.92816406]), 'previousTarget': array([13.57492926, 15.85504245]), 'currentState': array([ 4.916129 , 21.110764 ,  2.6725473], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 499
reward sum = 0.0
running average episode reward sum: 0.541737828974474
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.45156  , 15.154614 ,  1.7379248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.551806852320559}
episode index:1037
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.897982  , 16.931717  ,  0.76148343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.360039960182252}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5419004642329497
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.315739 , 16.625292 ,  5.3309593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3405789273439743}
episode index:1038
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.24120701, 12.73123294]), 'previousTarget': array([15.75140493, 11.74391196]), 'currentState': array([16.298412 ,  2.7872741,  2.6936812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5422320180418844
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.884411 , 13.065704 ,  0.7923536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.232943978615632}
episode index:1039
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.013854,  6.563298,  4.401509], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.43671314953575}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5421723155789321
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.510588 , 14.195079 ,  1.9696121], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6929994331058051}
episode index:1040
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.42173715, 14.87347886]), 'currentState': array([23.295046, 12.542061,  2.788812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.651546136545305}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5422444748732292
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.733015 , 15.876077 ,  2.3972542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.941868064768836}
episode index:1041
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.6727061 , 16.06017735]), 'previousTarget': array([14.13802944, 16.89633523]), 'currentState': array([11.722912 , 25.615213 ,  6.1996665], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5425494785966151
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.416341 , 16.53739  ,  4.1775002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6444528842968427}
episode index:1042
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.96990677, 11.53039011]), 'previousTarget': array([10.6822128, 11.401844 ]), 'currentState': array([3.3915231, 5.0059667, 1.7709014], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5427750508696759
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.890081 , 16.907318 ,  5.0127654], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.206758183129286}
episode index:1043
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.38184165, 19.46684183]), 'previousTarget': array([13.84615385, 17.76923077]), 'currentState': array([ 9.975842, 28.868925,  1.569381], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 210
reward sum = 0.12116881635704835
running average episode reward sum: 0.542371213480296
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.093141 , 14.12924  ,  1.8984388], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0962670817911206}
episode index:1044
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.37153471, 11.84396664]), 'previousTarget': array([18.60059927, 11.72672794]), 'currentState': array([25.67207  ,  5.0100665,  4.7762227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 248
reward sum = 0.0827043323764731
running average episode reward sum: 0.5419313408667995
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.135208 , 16.492508 ,  3.2719316], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8751739740374416}
episode index:1045
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.84788  , 14.052685 ,  1.6426466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.924111723290973}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.5415990246698741
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.942905 , 16.734617 ,  3.0594885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6045687628585013}
episode index:1046
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.794027 ,  9.933753 ,  0.8132055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.584608161506349}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5419900380655094
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.311246  , 14.777282  ,  0.27590555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.703377163887747}
episode index:1047
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.33083858, 14.78798256]), 'previousTarget': array([15.71523309, 14.71390676]), 'currentState': array([23.750307,  9.392378,  3.806789], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5417855845534599
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.55916  , 13.797376 ,  2.1920948], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9690824990762688}
episode index:1048
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.81606841, 16.2842121 ]), 'previousTarget': array([12.68964732, 17.56705854]), 'currentState': array([ 7.03789 , 23.63651 ,  5.364068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5416360235375782
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1637125, 16.99872  ,  4.728986 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.166623893143628}
episode index:1049
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.862291 , 19.83651  ,  1.8168356], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.599887224915304}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5418106371684547
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.953892, 13.98479 ,  4.296028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2018956921242916}
episode index:1050
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.994963 , 22.968908 ,  3.2432587], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.978174819049611}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5418053637032203
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.564777, 15.801834,  5.70022 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7582565698096198}
episode index:1051
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.53014411,  8.12388169]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([25.031717  , -0.22672862,  3.8112657 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.542029713509015
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.480825, 13.905763,  1.429232], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8412493763747237}
episode index:1052
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.2331505, 13.800165 ,  5.8243337], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.8903443842571965}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5424272123660814
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.233103  , 14.235948  ,  0.28909045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9250199419943095}
episode index:1053
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.68007104, 17.06096446]), 'previousTarget': array([11.73957299, 17.6676221 ]), 'currentState': array([ 3.184035, 22.335182,  4.241873], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5423329246708822
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.035429 , 14.051174 ,  2.6357775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4044157800367816}
episode index:1054
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.4731924 , 17.49454115]), 'previousTarget': array([15.35601013, 17.13606076]), 'currentState': array([17.33687  , 27.319342 ,  2.2874246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 195
reward sum = 0.14088441290426768
running average episode reward sum: 0.5419524047545158
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.847031, 15.904074,  4.642226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4651578851588087}
episode index:1055
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.98930886, 19.04255709]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([26.013344, 26.160347,  4.142774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5420790871828356
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.744736, 16.769936,  2.444595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9202352318077625}
episode index:1056
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.898779, 12.042754,  6.137335], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.05621570587967}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5424842148203163
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.030243 , 14.949584 ,  1.0370107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9710667158775909}
episode index:1057
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.63366136,  9.98993984]), 'previousTarget': array([15.6783628 ,  9.91227901]), 'currentState': array([16.888443  ,  0.06897602,  2.9699259 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5424439055718087
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.2466345, 13.302183 ,  1.0232985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.440670540432989}
episode index:1058
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.03875531, 7.98617162]), 'previousTarget': array([8.07106781, 8.07106781]), 'currentState': array([0.9943435, 0.8885479, 3.516045 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.5420262114912513
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3362665, 13.004728 ,  5.389965 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.597906501063823}
episode index:1059
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.0241744 , 23.66231982]), 'previousTarget': array([20.75304952, 22.19131191]), 'currentState': array([28.322557  , 31.429583  ,  0.85114706], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5417482043916311
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.478804 , 14.481937 ,  3.9659648], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5669234309950577}
episode index:1060
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.16339435, 18.39489672]), 'previousTarget': array([19.3177872, 18.598156 ]), 'currentState': array([26.913464, 24.714424,  1.896943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5415791410678505
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.636341 , 16.488703 ,  3.5834923], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.532476345221966}
episode index:1061
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.33042863, 11.71225842]), 'previousTarget': array([19., 12.]), 'currentState': array([25.446957,  4.686945,  3.830731], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5416055271897049
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.924046, 13.358541,  2.435365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9626683689245168}
episode index:1062
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.992599 ,  6.2601542,  2.0981257], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.79771342427751}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5419996856871746
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.311873, 14.009589,  1.26714 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.038353794344175}
episode index:1063
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.883489 , 17.108862 ,  3.6027143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.987794986997786}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.541979329330937
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.503554 , 16.939333 ,  1.0047555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0036415136395074}
episode index:1064
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.014208 , 10.012577 ,  3.0311913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.827506886827646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5423368554953473
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.705905  , 14.463616  ,  0.97234154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8865716049696633}
episode index:1065
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.78885438, 16.1613009 ]), 'currentState': array([14.080118 , 24.241076 ,  5.0221643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.28674642981896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5427202074600797
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.578419, 16.74362 ,  5.327026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3519387989200125}
episode index:1066
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.37933259, 20.07238598]), 'previousTarget': array([14.3216372 , 20.08772099]), 'currentState': array([13.164771 , 29.998354 ,  1.9827908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 152
reward sum = 0.21704489667280757
running average episode reward sum: 0.5424149822390981
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.460358, 16.357494,  4.681636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9938494011599586}
episode index:1067
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.21463338, 14.65359502]), 'previousTarget': array([16.35236179, 14.63117406]), 'currentState': array([25.831196 , 11.911018 ,  2.2131443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5422937600376888
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.562912 , 13.995568 ,  2.9061077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.85784201947847}
episode index:1068
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.78522172, 14.07578228]), 'previousTarget': array([ 9.6623494 , 13.57662651]), 'currentState': array([ 2.174504  , 11.312794  ,  0.19078076], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5424060035184618
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.117397 , 16.998287 ,  0.9213131], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7454224869773496}
episode index:1069
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.93449355, 15.17357612]), 'previousTarget': array([13.8386991 , 15.21114562]), 'currentState': array([ 4.0645995, 16.78143  ,  2.9607856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5422310080148979
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.447414 , 13.3177805,  5.954313 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.770653357201693}
episode index:1070
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.50973106, 20.43672861]), 'previousTarget': array([ 8.59256602, 20.49208627]), 'currentState': array([ 0.8439075, 26.858189 ,  2.521213 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5420951039984714
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.277446, 16.832302,  4.901595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.514860623850991}
episode index:1071
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.956297 , 20.919857 ,  3.1857028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.23472572295007}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5423297300988866
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.364105 , 15.601436 ,  3.8609598], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4908078073679079}
episode index:1072
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.904168  , 21.709398  ,  0.53621787], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.860780104836562}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5423043874962072
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.287491, 15.931994,  5.349384], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9753279543743177}
episode index:1073
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.13733471, 10.81238194]), 'previousTarget': array([ 9.13733471, 10.81238194]), 'currentState': array([1.       , 5.       , 3.8859522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 125
reward sum = 0.28470777327319546
running average episode reward sum: 0.5420645396244912
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.152165 , 13.1887455,  1.0483563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5874959961142006}
episode index:1074
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.642717 , 19.905384 ,  4.4540753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.918378157117868}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5424720144713522
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.485025, 16.313808,  4.511012], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0053031294261694}
episode index:1075
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.014331 , 11.90696  ,  3.2571895], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.298967261583681}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5423837720915125
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.424881, 16.064724,  5.200225], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7787419544148115}
episode index:1076
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.29482395, 15.92036187]), 'previousTarget': array([19.22197586, 15.90470911]), 'currentState': array([29.072828 , 18.015745 ,  5.9315534], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 187
reward sum = 0.15267973227590617
running average episode reward sum: 0.5420219299004116
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.645447, 14.958878,  2.98528 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6467554384534054}
episode index:1077
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02329572, 14.88540441]), 'previousTarget': array([15.03883865, 14.80580676]), 'currentState': array([17.015413 ,  5.0858397,  5.9839725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5418621040263386
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.552935, 16.572426,  4.7576  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2100065715098833}
episode index:1078
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.84978147, 15.01535956]), 'previousTarget': array([14.9503719 , 15.00496281]), 'currentState': array([ 4.9016485, 16.032537 ,  5.448409 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5416430094219845
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.753711, 14.162194,  5.233753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5017174747890585}
episode index:1079
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.53128586, 10.17305971]), 'previousTarget': array([12.47213595,  9.94427191]), 'currentState': array([7.977819 , 1.2699176, 4.311875 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 164
reward sum = 0.19238531289396707
running average episode reward sum: 0.54131962266594
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.610788 , 13.556937 ,  0.7874975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.00308242842759}
episode index:1080
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.568466 ,  6.9713836,  0.7768037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.388744747580338}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5415679189152973
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.461629  , 13.024462  ,  0.14230841], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5038644114201825}
episode index:1081
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.97495549, 10.88543821]), 'previousTarget': array([ 8.82352941, 11.70588235]), 'currentState': array([0.7168801, 5.245918 , 5.494365 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5412265899053862
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.575134 , 14.021699 ,  0.8124883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1348358374844942}
episode index:1082
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.388567, 17.357744,  4.598268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.736252185907101}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5416409697854365
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.991116, 15.387568,  5.305727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.028484788028899}
episode index:1083
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.005175 , 16.612654 ,  2.9961143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.258557331205502}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5417522622865422
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.8255625, 15.975891 ,  4.4840593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2782475771916473}
episode index:1084
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.45391772, 14.83726401]), 'previousTarget': array([14.57826285, 14.87347886]), 'currentState': array([ 4.8704123, 11.981318 ,  3.8395584], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5418695152785081
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.461781  , 15.591758  ,  0.22177343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7999108962349558}
episode index:1085
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.82071281, 13.14922525]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([13.856512,  3.195818,  6.067971], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 388
reward sum = 0.020251421078849283
running average episode reward sum: 0.5413892039578823
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.18127  , 14.4932165,  3.7342563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2853908860777101}
episode index:1086
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58374765, 17.24850527]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.096603  , 26.927635  ,  0.28611112], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5412524255074682
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.788628 , 16.632643 ,  6.1381927], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.42171656374985}
episode index:1087
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.98973588, 15.98282408]), 'previousTarget': array([15.92893219, 15.92893219]), 'currentState': array([23.085537, 23.029072,  5.378788], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5411158984880292
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.215078, 16.99135 ,  4.040508], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.140461984207223}
episode index:1088
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.36006321, 14.44455988]), 'previousTarget': array([18.02945514, 14.76696499]), 'currentState': array([27.094114 , 12.153654 ,  4.2536774], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5410779931908861
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.103421 , 13.069971 ,  1.8216593], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.932797858348192}
episode index:1089
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.38499807, 14.02757982]), 'previousTarget': array([ 9.80580676, 13.96116135]), 'currentState': array([ 1.728271 , 11.429961 ,  5.4837093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5412269562200684
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.343085  , 15.390866  ,  0.06989223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7644039416802071}
episode index:1090
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.45371174, 14.77023826]), 'previousTarget': array([14.28476691, 14.71390676]), 'currentState': array([ 5.2358236, 10.893314 ,  3.4934952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 258
reward sum = 0.07479631572685258
running average episode reward sum: 0.5407994304267657
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.355427 , 14.358994 ,  3.6068954], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.732951223543521}
episode index:1091
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.744896, 14.049224,  4.394655], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.317138114438068}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.5407394859318746
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.024413  , 15.004255  ,  0.45619482], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9755961714587654}
episode index:1092
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.78972354, 16.13840205]), 'previousTarget': array([19.8085497 , 17.06080701]), 'currentState': array([28.366953 , 19.015324 ,  4.5610313], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5405186633384441
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.551224 , 15.191625 ,  4.7539997], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4879755335877345}
episode index:1093
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.99181166, 21.83842665]), 'previousTarget': array([17.99181166, 21.83842665]), 'currentState': array([22.       , 31.       ,  1.7651529], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 317
reward sum = 0.04133868785485247
running average episode reward sum: 0.5400623745125906
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.128675, 15.716813,  5.495973], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3370599456291046}
episode index:1094
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.29480684, 16.23538072]), 'previousTarget': array([15.36882594, 16.35236179]), 'currentState': array([17.615993 , 25.962255 ,  1.3318908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5402866811484762
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.21121 , 16.810637,  4.480834], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5452256040678667}
episode index:1095
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.90451423, 18.7027741 ]), 'previousTarget': array([21.13681661, 19.82178448]), 'currentState': array([29.376457 , 24.015606 ,  4.7925563], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.5399710259639272
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.032751 , 13.964953 ,  1.0536159], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.416648115875133}
episode index:1096
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.961391 , 16.037374 ,  4.8609886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2188290294694353}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.540390377808992
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.961391 , 16.037374 ,  4.8609886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2188290294694353}
episode index:1097
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.86542004, 16.57574943]), 'previousTarget': array([13.73462344, 16.80768079]), 'currentState': array([ 8.022236 , 24.690994 ,  1.9286766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5403915569512122
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.906644 , 16.991934 ,  5.0159855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2722737480849555}
episode index:1098
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.84725975, 10.69614547]), 'previousTarget': array([19.6284586 , 10.75724629]), 'currentState': array([27.325045  ,  4.0566626 ,  0.39906424], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 213
reward sum = 0.11756998134242766
running average episode reward sum: 0.540006823943379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.322723, 16.750023,  2.832041], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7795309760614746}
episode index:1099
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.244051 ,  7.796657 ,  1.7781868], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.9001275877915305}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.540389177748885
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.633325 , 14.647335 ,  2.1210637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7248949188929676}
episode index:1100
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.27976364, 10.62552974]), 'previousTarget': array([21.40743398,  9.50791373]), 'currentState': array([29.485176 ,  4.9096565,  1.480942 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5406717105312155
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.8569   , 14.620768 ,  4.4223557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8952287686432139}
episode index:1101
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.09836  ,  6.9523363,  1.7742934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.098014906395717}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5407356356936489
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.06992  , 14.636294 ,  1.3985984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.96404994661223}
episode index:1102
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.174038 , 12.159185 ,  3.4937763], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.271884055535235}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5407124276081822
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.982448 , 13.055106 ,  1.1870599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.178948225959671}
episode index:1103
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.65616448, 14.18292282]), 'previousTarget': array([14.71390676, 14.28476691]), 'currentState': array([10.777482 ,  4.9657745,  3.8654234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5408106071378562
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.741289 , 14.677473 ,  2.0984905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4134669653288359}
episode index:1104
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.06933521, 11.98737624]), 'previousTarget': array([15., 10.]), 'currentState': array([15.299423 ,  1.9900236,  1.1285784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5409327052753425
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.28586  , 13.3186245,  1.9320375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4011037695540076}
episode index:1105
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.91462995, 15.82102192]), 'previousTarget': array([14.90535746, 16.04106794]), 'currentState': array([13.8804035, 25.767397 ,  3.0693464], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5410305080991157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.374514 , 16.291273 ,  4.8284845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0759557352205}
episode index:1106
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.946486  , 17.74507   ,  0.19594318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.469207966844098}
done in step count: 237
reward sum = 0.09237216435585796
running average episode reward sum: 0.5406252160090134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.106416 , 14.57609  ,  1.4408454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9890362086355047}
episode index:1107
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.26334574, 13.24421422]), 'previousTarget': array([14.13802944, 13.10366477]), 'currentState': array([10.3944845,  4.022939 ,  5.8552766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.54079821073252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.116482 , 13.936687 ,  1.7371773], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3824753093094553}
episode index:1108
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.60600168, 11.69377928]), 'previousTarget': array([19.6284586 , 10.75724629]), 'currentState': array([27.729784 ,  5.8624706,  1.0994436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.5405143062643518
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.47514  , 13.1231365,  2.6788568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9488701705826854}
episode index:1109
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.684511, 10.669485,  4.763001], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.704333768170015}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5404868030571313
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.747581, 13.05871 ,  4.289686], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6120198158837318}
episode index:1110
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([7.77545719, 7.30899084]), 'previousTarget': array([7.8231825 , 7.31055268]), 'currentState': array([0.92887604, 0.02034716, 3.6644623 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 180
reward sum = 0.16380796970808742
running average episode reward sum: 0.5401477582026316
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.746179 , 15.757117 ,  3.4063985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0630188644141085}
episode index:1111
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.25041052, 10.28926169]), 'previousTarget': array([18.30790021, 10.22192192]), 'currentState': array([23.929665 ,  2.0584614,  0.2700073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 175
reward sum = 0.1722499301915014
running average episode reward sum: 0.5398169148321179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.894987 , 13.733851 ,  2.4630952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2790586493886473}
episode index:1112
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.92051312,  8.79058143]), 'previousTarget': array([18.85504245,  8.57492926]), 'currentState': array([22.176615  , -0.25848475,  3.4988174 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.539763298068182
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.906967 , 13.210859 ,  1.6972369], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.09660326041886}
episode index:1113
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.67916756, 19.43062526]), 'previousTarget': array([18.598156 , 19.3177872]), 'currentState': array([25.06766  , 27.123945 ,  4.5929794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5397365677703198
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.96293   , 13.864268  ,  0.33863276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2678146616464665}
episode index:1114
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.46242969, 9.44415037]), 'previousTarget': array([8.59256602, 9.50791373]), 'currentState': array([0.8424124, 2.9684005, 4.446667 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5396538652106726
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.485463  , 13.233978  ,  0.19965047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8315313833547497}
episode index:1115
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.8513195 , 14.27092824]), 'previousTarget': array([14.8304548 , 12.96545758]), 'currentState': array([12.853134 ,  4.472599 ,  2.2821982], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5399409660076535
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.963872 , 14.1110325,  0.842705 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8897013412306398}
episode index:1116
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.62360997, 18.73344771]), 'previousTarget': array([10.6822128, 18.598156 ]), 'currentState': array([ 3.0158236, 25.223562 ,  2.5220537], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 197
reward sum = 0.13808081308747275
running average episode reward sum: 0.5395811986370893
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.675047, 15.178793,  5.365571], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6983231812475977}
episode index:1117
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.739046 , 12.588268 ,  5.1265597], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.106126348480844}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.5394006737732806
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.502058 , 16.756596 ,  3.9618278], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3112348356676007}
episode index:1118
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.97749373, 10.96419027]), 'previousTarget': array([12., 11.]), 'currentState': array([5.983018 , 2.96005  , 0.7073111], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5394125440175962
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.515547 , 16.22346  ,  0.2969642], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9236563695236484}
episode index:1119
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.50047103, 11.78483631]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([25.865303 ,  5.020278 ,  1.5072502], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.539683553525959
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.627266, 13.213515,  2.727967], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.824954337951625}
episode index:1120
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.05002379,  8.20141847]), 'previousTarget': array([20.08636336,  8.06404996]), 'currentState': array([26.012997  ,  0.17378156,  3.362303  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5397205587539626
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.050483 , 13.209513 ,  2.1745346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0266789932718114}
episode index:1121
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.674913 ,  9.182461 ,  0.2144618], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.053849803053452}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5396674579498785
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.342023, 13.079572,  4.731699], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0300193676743175}
episode index:1122
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.08019386, 15.56112447]), 'previousTarget': array([10.89949494, 15.58578644]), 'currentState': array([ 1.1811067, 16.97819  ,  1.6952178], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5396644310284692
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.559689 , 13.38364  ,  1.0858191], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2461627598778477}
episode index:1123
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.26290749, 11.53288584]), 'previousTarget': array([18.27327206, 11.39940073]), 'currentState': array([25.116266 ,  4.2506146,  6.1444874], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 119
reward sum = 0.30240443566902153
running average episode reward sum: 0.539453345623345
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.538137 , 16.3552   ,  3.3448477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0499837333980895}
episode index:1124
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.49780128, 13.37281034]), 'previousTarget': array([17.56338512, 13.36875492]), 'currentState': array([25.876686,  7.914396,  6.25701 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5397617292020942
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.102526 , 16.036854 ,  3.3230696], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5134823176938854}
episode index:1125
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.899865 , 22.216795 ,  3.8732247], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.5161623180587}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.5394307986566349
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.138824 , 14.940759 ,  6.0835414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8621181248651115}
episode index:1126
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.50710617, 21.96293637]), 'previousTarget': array([ 7.31055268, 22.1768175 ]), 'currentState': array([ 0.18173069, 28.770203  ,  4.348469  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5395223521645569
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.226226 , 14.594374 ,  5.9455523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8736470245089102}
episode index:1127
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.82187748, 16.37363975]), 'previousTarget': array([11.33345606, 16.41020921]), 'currentState': array([ 0.1561935, 18.937729 ,  3.5385456], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5393784818584921
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.311488 , 16.814056 ,  0.6329227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.47828022466809}
episode index:1128
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.83499577, 22.61764193]), 'previousTarget': array([19.7000106 , 22.52001696]), 'currentState': array([25.193811 , 31.060574 ,  1.5741423], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5393757269810331
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.360153, 16.588882,  5.60793 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.09154587585689}
episode index:1129
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.66325655, 17.59312439]), 'previousTarget': array([16.63124508, 17.56338512]), 'currentState': array([22.062212 , 26.010445 ,  5.1744585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.539553005429181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.262693 , 13.92235  ,  3.2829711], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.109205796455941}
episode index:1130
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.88246805, 14.97272057]), 'previousTarget': array([17., 15.]), 'currentState': array([26.881418 , 14.827823 ,  4.5273075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.539540665448718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.004272, 16.589108,  3.732328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.875296248937735}
episode index:1131
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.70194299, 19.99304905]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 0.865778, 26.205498,  5.532038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5396432436776274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.742203, 16.91099 ,  5.824974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2877796589689487}
episode index:1132
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.24199272, 14.48316466]), 'previousTarget': array([14.32050294, 14.54700196]), 'currentState': array([5.979781, 8.849706, 2.628671], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5399260460703579
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.930942  , 13.608228  ,  0.71239245], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6744199833648803}
episode index:1133
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.060886  , 14.161835  ,  0.21695441], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.099570489238943}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5400892332748441
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.16131  , 14.235079 ,  3.8544807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3905918867690912}
episode index:1134
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.61919397, 12.72697206]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([5.386307 , 5.8215146, 5.289438 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5402521329247851
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.662225 , 15.069028 ,  6.0455284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3447563739098295}
episode index:1135
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.89571246, 15.39185809]), 'previousTarget': array([16.35236179, 15.36882594]), 'currentState': array([25.057346 , 19.399908 ,  2.2965903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5403145141806022
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.919019 , 15.414315 ,  3.0894248], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0080935269220697}
episode index:1136
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.60077941, 15.54605355]), 'previousTarget': array([14.54700196, 15.67949706]), 'currentState': array([ 8.698865  , 23.618687  ,  0.99210894], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5404579910326857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.777066, 16.880796,  5.035843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2434264472899157}
episode index:1137
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.967003 , 13.169937 ,  3.0599127], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.241334258697373}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5408357072092825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.403622 , 15.781824 ,  2.7458441], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.606674436394347}
episode index:1138
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.40602083, 13.71029796]), 'previousTarget': array([17.22104427, 13.78852131]), 'currentState': array([26.219658 ,  8.985914 ,  2.8188663], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5411710092173762
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.409454 , 14.367067 ,  2.6570687], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.545045406603947}
episode index:1139
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.84850557, 17.97176538]), 'previousTarget': array([ 9.68243142, 18.03861062]), 'currentState': array([ 1.1864673, 22.968674 ,  3.2580638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5412831151378606
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.887142  , 14.314587  ,  0.35265398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.306998086957311}
episode index:1140
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.086983, 15.094557,  4.538752], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.087717118053607}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.541617437293242
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.666689, 14.205973,  3.466916], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0367996494974931}
episode index:1141
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.12174036, 17.33640121]), 'previousTarget': array([ 8.82352941, 18.29411765]), 'currentState': array([-0.17112827, 21.02999   ,  4.633596  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5415011524746162
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.558419 , 14.865764 ,  6.1907105], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4478171559937623}
episode index:1142
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.64263217, 15.9269586 ]), 'previousTarget': array([10.77802414, 15.90470911]), 'currentState': array([ 0.8615085, 18.007732 ,  3.4368694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5417502053804437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.593621 , 15.543068 ,  0.3202625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5075887888278148}
episode index:1143
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.589205, 18.131174,  5.901209], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.1580063129157256}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5421333782778385
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.750631 , 16.359882 ,  5.4535666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5532956627442207}
episode index:1144
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.83848324, 10.83414932]), 'previousTarget': array([20.86266529, 10.81238194]), 'currentState': array([28.97878  ,  5.0259175,  4.355063 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 134
reward sum = 0.26008546137772603
running average episode reward sum: 0.5418870482194104
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.674069 , 13.335986 ,  5.6711264], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1276827956183118}
episode index:1145
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.549742 ,  6.329988 ,  3.1430202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.037161667147478}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5418331689945867
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.820885 , 14.952562 ,  1.6107957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.180069155685764}
episode index:1146
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.848237 , 11.84961  ,  4.4813375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.03867673429761}
done in step count: 167
reward sum = 0.18667127671570335
running average episode reward sum: 0.5415235247990515
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.797463 , 16.747803 ,  3.0276258], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9211356797027455}
episode index:1147
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.23292315, 12.62261156]), 'previousTarget': array([17.25900177, 10.804711  ]), 'currentState': array([20.836693 ,  3.7453763,  2.6222093], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 118
reward sum = 0.3054590259283046
running average episode reward sum: 0.5413178937024742
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.301073 , 13.950048 ,  1.2786391], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2613076990894794}
episode index:1148
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.368092 , 19.040281 ,  2.4302309], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.950134202577023}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.541434874690601
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.481196 , 14.983805 ,  4.9304075], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.518889940839587}
episode index:1149
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.846354 , 15.79338  ,  3.8774943], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.251912903698763}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5412888133043086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.812806 , 14.268706 ,  3.1865988], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0933637308682917}
episode index:1150
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.95805954, 15.35789958]), 'previousTarget': array([12.86393924, 15.35601013]), 'currentState': array([ 3.108214 , 17.084324 ,  2.3729303], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5415291418223737
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.620142, 14.166902,  6.125074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.611850213367895}
episode index:1151
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.45629912, 10.70679565]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([5.090538 , 2.9946568, 6.1883965], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5415017604026056
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.942486 , 13.843294 ,  2.8848822], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.26080064523302}
episode index:1152
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.149511 , 12.451695 ,  3.3306522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.745548114156196}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5416181760909469
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.831266  , 13.988323  ,  0.75526893], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.092134451589689}
episode index:1153
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.55353027, 18.10259418]), 'previousTarget': array([18.30790021, 19.77807808]), 'currentState': array([23.908308, 25.823786,  5.076324], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5419404456502127
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.549262 , 15.992749 ,  4.3200827], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.757894084139955}
episode index:1154
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.48358347, 14.43941886]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([27.238186 , 12.237662 ,  2.7124155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.542158345077047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.146775 , 14.149905 ,  2.6137328], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8626726688684324}
episode index:1155
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.858263, 23.15079 ,  4.41868 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.735323511653293}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5424112476863663
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.27364 , 14.029565,  5.295415], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9804202392195676}
episode index:1156
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.18658278, 20.07432024]), 'previousTarget': array([ 8.06404996, 20.08636336]), 'currentState': array([ 0.166433 , 26.04736  ,  2.4376392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 190
reward sum = 0.14814499154757946
running average episode reward sum: 0.5420704816914322
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.465712 , 15.394623 ,  4.1417217], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5842247537409835}
episode index:1157
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.11583192, 13.02223491]), 'previousTarget': array([14.13802944, 13.10366477]), 'currentState': array([10.034562 ,  3.8929832,  2.1735148], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5421573047660379
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.070331 , 13.123067 ,  5.0044527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0945555638903346}
episode index:1158
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([21.85576  , 15.0758095,  1.9649074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.856179703723918}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5424698369232792
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.175003 , 16.12263  ,  3.3766081], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6250933558435565}
episode index:1159
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.21103914, 11.98229214]), 'previousTarget': array([20.31756858, 11.96138938]), 'currentState': array([2.8864733e+01, 6.9709463e+00, 1.3012584e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5423177356339257
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.7105  , 16.506762,  4.005482], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9832148726860381}
episode index:1160
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.94129839, 17.08563061]), 'previousTarget': array([17.91263916, 17.11828302]), 'currentState': array([26.09865  , 22.869886 ,  4.9806495], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5426140897563049
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.745575 , 15.549661 ,  3.8015897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.830070702525174}
episode index:1161
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.03052 , 11.0495  ,  1.530349], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.06440641390016}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5426373144660475
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.587449 , 14.128501 ,  2.5058646], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6597622520936834}
episode index:1162
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.99232149, 15.32817932]), 'previousTarget': array([15., 15.]), 'currentState': array([14.758412, 25.325443,  3.632856], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5424791983725099
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.000832 , 16.747442 ,  4.9733768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.013757259669867}
episode index:1163
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.28611968, 10.16613662]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([29.213343 ,  4.0703073,  3.4784698], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5427901114967679
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.738668 , 15.008259 ,  1.7206202], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7386880567014698}
episode index:1164
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.741426 ,  7.056249 ,  5.3581643], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.780743118524788}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5426480055185934
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.712716 , 15.185203 ,  3.0160089], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.736385935106865}
episode index:1165
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.70586253, 17.50872332]), 'previousTarget': array([18.67949706, 17.45299804]), 'currentState': array([26.986813 , 23.114601 ,  6.0918574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5429352036467926
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.498323 , 14.677262 ,  3.8114784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5937052049578546}
episode index:1166
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.990368 ,  9.935429 ,  6.2770815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.844385936435878}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5432219095759718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.943987, 13.09967 ,  6.102885], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.718517535169835}
episode index:1167
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.82369796, 14.61683304]), 'previousTarget': array([15., 15.]), 'currentState': array([24.890692 , 10.399058 ,  5.5096636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.543043103880179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.868751  , 13.188128  ,  0.73464143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.816619070840193}
episode index:1168
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.0496281 , 14.99503719]), 'currentState': array([24.889656 , 14.196503 ,  5.2685905], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.922242944808254}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5431338305906052
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.21737  , 16.905373 ,  1.7058363], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2610688391937437}
episode index:1169
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.072178 , 14.740509 ,  0.2567144], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.078811286004287}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5431867128440641
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.359171, 13.914198,  2.444477], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9675584473112682}
episode index:1170
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.070595, 21.142628,  3.004016], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.875970480278771}
done in step count: 232
reward sum = 0.09713262969004904
running average episode reward sum: 0.5428057956082365
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.076721  , 16.487373  ,  0.18906711], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.431312580475117}
episode index:1171
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.52977095, 14.2070448 ]), 'previousTarget': array([19.58258088, 13.36336397]), 'currentState': array([28.286604, 12.015194,  1.654063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5429309178695513
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.45826  , 14.024558 ,  3.7055287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0777239020259324}
episode index:1172
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.45397333, 15.1215661 ]), 'previousTarget': array([15.42173715, 15.12652114]), 'currentState': array([25.113634  , 17.708254  ,  0.45938095], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5429028316192358
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.449367, 16.339388,  5.02634 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4127597968264722}
episode index:1173
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.90233908, 15.34951747]), 'previousTarget': array([14.87347886, 15.42173715]), 'currentState': array([12.211252 , 24.980616 ,  2.0953414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 228
reward sum = 0.10111704470857531
running average episode reward sum: 0.5425265234532131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.605637 , 14.636189 ,  0.6757998], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6463374468994223}
episode index:1174
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.03569952, 15.46364147]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 2.155818 , 17.008938 ,  4.0940804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5424988291747419
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.140639, 14.872789,  5.944524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.868725126267126}
episode index:1175
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([22.1768175 ,  7.31055268]), 'previousTarget': array([22.1768175 ,  7.31055268]), 'currentState': array([29.      ,  0.      ,  6.059674], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5425468369785612
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.728882 , 16.991018 ,  2.2179782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1202411615029777}
episode index:1176
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.030357, 24.010708,  5.447465], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.069426241127017}
done in step count: 163
reward sum = 0.19432859888279502
running average episode reward sum: 0.5422509846097457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.94589 , 15.229376,  5.390606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9733046670108864}
episode index:1177
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.1199771 , 14.78340833]), 'previousTarget': array([15.14357069, 14.74157276]), 'currentState': array([19.965551,  6.035814,  5.023673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.5421013932317011
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.519571 , 13.023783 ,  1.1758437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0337763961341775}
episode index:1178
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.23656902, 18.34780532]), 'previousTarget': array([20.58821525, 18.59242409]), 'currentState': array([28.661911 , 23.734234 ,  0.6651136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5421811931408039
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.362676, 15.473031,  2.953753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5960637615969852}
episode index:1179
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.967873  , 19.869999  ,  0.11133783], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.28179136251432}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.541915133197579
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.015774, 14.214956,  2.871281], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.28377957415389}
episode index:1180
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.916155 , 17.124548 ,  3.1574054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.7448370947196747}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.541842901711166
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.44362 , 15.495447,  5.003276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.526272767230463}
episode index:1181
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.58024609, 10.91716332]), 'previousTarget': array([15.58578644, 10.89949494]), 'currentState': array([16.987291,  1.016647,  5.048074], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5415316892972617
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.018393 , 14.646448 ,  4.7270517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.012900135596167}
episode index:1182
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.79401496, 15.9894766 ]), 'previousTarget': array([14.78885438, 16.1613009 ]), 'currentState': array([12.755951, 25.779589,  1.566038], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5415506018088044
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.949451 , 13.389851 ,  4.1015887], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.528426779584588}
episode index:1183
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.919577 , 20.043034 ,  6.1272926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.827187629953427}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5418964121534761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.567122 , 16.12882  ,  4.0030365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9313480661959925}
episode index:1184
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.13358495, 16.18862282]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 2.8962884, 20.019073 ,  2.082668 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.54177051731483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.087095 , 15.016874 ,  0.3736226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.912979164652141}
episode index:1185
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.30782123, 14.63985154]), 'previousTarget': array([17.13606076, 14.64398987]), 'currentState': array([27.188234 , 13.097958 ,  1.7307981], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5420536121762837
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.310896 , 13.818461 ,  2.5721397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.22175680613774}
episode index:1186
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.899348 , 22.328066 ,  0.1459926], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.928448779628013}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.5419212132434422
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.522379 , 15.559098 ,  4.6986127], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6217978994861375}
episode index:1187
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.64851216, 11.41359098]), 'previousTarget': array([16.05914151, 11.55779009]), 'currentState': array([20.824984  ,  2.327498  ,  0.11873575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5419591893113666
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.843816 , 14.8142   ,  1.9267464], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.24272411108592326}
episode index:1188
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.608011 ,  8.847436 ,  3.1166434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.324892462151558}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5422563962625417
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.215563 , 15.066751 ,  2.1030674], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.217394185628177}
episode index:1189
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.015842,  5.988955,  3.837636], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.22690700327789}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5423040429938053
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.381307 , 13.164759 ,  1.6408045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4470960057502023}
episode index:1190
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.68331571, 15.69011555]), 'previousTarget': array([11.94427191, 16.52786405]), 'currentState': array([ 4.8261724, 20.332422 ,  6.017773 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5423566895296104
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.202752 , 15.1211405,  5.0872736], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.801325895555912}
episode index:1191
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.813272 , 19.94544  ,  5.0649514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.407324379824335}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5427157015350385
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.031449, 15.684349,  4.286089], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.084112622646137}
episode index:1192
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.18826715, 19.4026459 ]), 'previousTarget': array([10.17821552, 21.13681661]), 'currentState': array([ 4.642781 , 26.962845 ,  5.1148734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5430265159071664
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.511028, 16.391346,  6.252268], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4822258744416184}
episode index:1193
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.09926681, 10.87454193]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([11.966165 ,  1.1046968,  4.557918 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 115
reward sum = 0.31480917318095203
running average episode reward sum: 0.5428353791042131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.607353 , 14.048537 ,  1.9591236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0292972640281222}
episode index:1194
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.23658229, 17.26994231]), 'previousTarget': array([16.21147869, 17.22104427]), 'currentState': array([21.020428 , 26.051447 ,  3.2181666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5430452359280955
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.961287 , 15.401066 ,  5.4810886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0018737992020172}
episode index:1195
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57025154, 18.02282536]), 'previousTarget': array([17.06080701, 19.8085497 ]), 'currentState': array([21.18004 , 26.896936,  4.656429], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5433473570310058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.552166 , 14.6260195,  3.3223991], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.596584064894921}
episode index:1196
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.309949 , 15.098991 ,  3.7758536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.690912103995852}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5434411848200978
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.805087 , 14.05097  ,  1.2760242], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2445171816870364}
episode index:1197
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.29258229, 9.35187302]), 'previousTarget': array([9.07106781, 9.07106781]), 'currentState': array([2.1846912, 2.3178213, 0.5905716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5437500963916033
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.295498  , 14.061018  ,  0.41200054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9843811357501989}
episode index:1198
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.918127, 15.674248,  4.265418], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.964129711728569}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5441058502728446
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.547398 , 15.782718 ,  2.5120163], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7340952625819346}
episode index:1199
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.48293129, 11.37941072]), 'previousTarget': array([19.3177872, 11.401844 ]), 'currentState': array([27.262547 ,  5.0962915,  0.8386531], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5441465674362852
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.61438  , 13.703472 ,  2.5299046], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3526593707009662}
episode index:1200
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.71523309, 14.71390676]), 'currentState': array([23.271032 , 12.15452  ,  2.4615536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.74681268189332}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.54449331967822
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.719633 , 15.128599 ,  2.2240064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.724434908208477}
episode index:1201
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.073203 , 12.925571 ,  3.7228963], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.831218852051028}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.544439782354504
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.089296 , 14.196937 ,  2.3527567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2142042526766452}
episode index:1202
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.950909 , 13.060655 ,  5.6498694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.613588025609756}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5445377393442209
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.351218  , 14.033191  ,  0.36558628], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1643186691502896}
episode index:1203
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.34422464, 15.04662966]), 'previousTarget': array([17.03454242, 15.1695452 ]), 'currentState': array([25.253717 , 16.388998 ,  2.8241308], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5448596061287415
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.62431  , 15.555062 ,  4.6296744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7165301137167328}
episode index:1204
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.95406247, 15.84544332]), 'previousTarget': array([16.35236179, 15.36882594]), 'currentState': array([26.131874 , 19.816309 ,  1.3817843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.544767798428047
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.765852 , 15.383022 ,  3.5749495], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8069142991559328}
episode index:1205
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36928673, 18.06413586]), 'previousTarget': array([20.31756858, 18.03861062]), 'currentState': array([29.054523 , 23.020613 ,  5.7997055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5448077641394679
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.975733 , 14.984155 ,  2.7382226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.975796341657062}
episode index:1206
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.09464254, 16.04106794]), 'currentState': array([14.300844 , 24.870573 ,  3.9279165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.89530348481562}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5446883580821457
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.588218  , 14.49589   ,  0.11826485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4990851857437748}
episode index:1207
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.733492, 18.120356,  5.35285 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.274184591301669}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5448316049153543
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.1480465, 16.774271 ,  3.671004 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9682129963273853}
episode index:1208
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.44383897, 15.13526108]), 'previousTarget': array([15.42173715, 15.12652114]), 'currentState': array([25.009499, 18.05042 ,  4.383996], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5449989667759136
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.640808 , 14.5748625,  2.7709985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6949906046183902}
episode index:1209
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.9926942, 14.7419096]), 'previousTarget': array([ 9.97785158, 14.66519011]), 'currentState': array([5.9510749e-03, 1.4227165e+01, 4.0542731e+00], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.545069069804366
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.366745  , 16.078032  ,  0.83006907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.956955265720338}
episode index:1210
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.11638886, 13.65838701]), 'previousTarget': array([13.19231921, 13.73462344]), 'currentState': array([4.9712415, 7.8569584, 2.9545605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 100
reward sum = 0.3660323412732292
running average episode reward sum: 0.544921227749427
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.231012 , 14.541248 ,  1.1467159], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8275038791481573}
episode index:1211
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.99509667, 14.40303342]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([ 0.06548078, 13.218665  ,  0.15532222], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5447797616212956
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.206511  , 15.125934  ,  0.17940319], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8034206871556386}
episode index:1212
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.809229 , 14.865224 ,  5.6977553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.192520509737391}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5451386406306763
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.69849   , 15.738679  ,  0.99609566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4965207894286263}
episode index:1213
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.74157276, 15.14357069]), 'currentState': array([ 7.6085234, 18.553429 ,  5.708538 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.201267094723745}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.545480862516483
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.788787  , 15.450855  ,  0.19203061], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4978769322880329}
episode index:1214
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.37679819, 11.89710055]), 'previousTarget': array([19.24275371, 10.3715414 ]), 'currentState': array([25.740187 ,  5.130971 ,  1.3723055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5455608055120124
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.23691 , 16.69097 ,  4.796599], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0950713010085718}
episode index:1215
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.53744369, 19.15693236]), 'previousTarget': array([10.3715414 , 19.24275371]), 'currentState': array([ 3.220253, 25.972996,  4.1897  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 116
reward sum = 0.3116610814491425
running average episode reward sum: 0.5453684537652501
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.135916  , 14.971316  ,  0.81860065], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8643049161490566}
episode index:1216
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.01656512, 18.35403876]), 'previousTarget': array([13.57662651, 20.3376506 ]), 'currentState': array([11.202927, 27.95005 ,  4.839889], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5456129687526114
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.893763 , 14.779098 ,  6.1700644], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.128077612530912}
episode index:1217
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.62638683, 18.04134233]), 'previousTarget': array([18.92893219, 18.92893219]), 'currentState': array([24.162262 , 25.609852 ,  3.6270034], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 133
reward sum = 0.2627125872502283
running average episode reward sum: 0.5453807024295388
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.937824 , 15.433922 ,  3.2879853], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.985812418554178}
episode index:1218
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.101147 ,  8.037744 ,  3.3319845], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.979603479472078}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5454604652676481
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.410583 , 14.189911 ,  2.1792789], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0018269848597263}
episode index:1219
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.94062592, 11.82962825]), 'previousTarget': array([15., 12.]), 'currentState': array([14.753381 ,  1.8313814,  3.4855762], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5456573650016969
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.759132 , 13.295697 ,  0.7245302], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4493253645210906}
episode index:1220
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.90052 , 13.970366,  2.274564], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.989681732433475}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5459972000917857
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.970638 , 15.814637 ,  3.1098983], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1323810525468603}
episode index:1221
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.97079826, 13.99226189]), 'previousTarget': array([19.22197586, 14.09529089]), 'currentState': array([27.44079  , 10.779902 ,  3.7882793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5461557133268892
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.929128 , 14.558261 ,  0.7632481], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.44738831090892905}
episode index:1222
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.904007 , 16.172363 ,  1.7822934], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.990479438397872}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5464266579791149
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.155634 , 16.580366 ,  3.5921235], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7917899390802132}
episode index:1223
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.13962628, 15.09921711]), 'previousTarget': array([16.42507074, 15.85504245]), 'currentState': array([23.291183, 20.891636,  3.200013], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5466689100505241
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.724042 , 14.214267 ,  4.1778784], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.068463114375114}
episode index:1224
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.20743312, 18.92314928]), 'previousTarget': array([11.07106781, 18.92893219]), 'currentState': array([ 4.257033 , 26.11286  ,  2.7973907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5466068020762671
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.923925 , 14.461078 ,  0.8307365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2034840262004214}
episode index:1225
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.61391514, 11.91885933]), 'previousTarget': array([12.93919299, 10.1914503 ]), 'currentState': array([9.511323 , 2.7991698, 1.4408876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5467827693782825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.45325575e+01, 1.51829786e+01, 1.43763423e-02], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5019797621065264}
episode index:1226
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.87347886, 14.57826285]), 'currentState': array([13.285121 ,  6.521617 ,  0.9250809], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.650074536814149}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5471121966647713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.807847 , 13.652751 ,  1.3328148], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5708904367578331}
episode index:1227
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.78262108, 13.14304063]), 'previousTarget': array([15.86197056, 13.10366477]), 'currentState': array([19.666325,  3.928007,  5.390604], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5471305101874199
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.532821, 15.366538,  2.679551], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6467209912556345}
episode index:1228
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.60535302, 13.58781549]), 'previousTarget': array([16.52590681, 13.64363839]), 'currentState': array([24.113708 ,  6.9829235,  5.5711246], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5470179712649098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.138739 , 13.037969 ,  2.4373794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7044150854309734}
episode index:1229
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.28904672, 8.11647801]), 'previousTarget': array([9.24695048, 7.80868809]), 'currentState': array([1.3083017, 0.9562267, 2.553428 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5470852929396564
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.095686  , 14.599692  ,  0.23987293], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9889540454026022}
episode index:1230
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.50934634,  9.95898696]), 'previousTarget': array([17.52786405,  9.94427191]), 'currentState': array([21.96562  ,  1.0068015,  4.037519 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 139
reward sum = 0.24733868589386818
running average episode reward sum: 0.5468417944773933
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.909563 , 16.918766 ,  2.9732165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.7070452792167488}
episode index:1231
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.214306 , 16.888773 ,  3.4429274], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.457457516348953}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5471698368925091
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.919075 , 14.114249 ,  3.4061177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1136232706044673}
episode index:1232
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.242023 , 11.250519 ,  4.0281143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.444953011247507}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5473762121987917
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.85477  , 15.047234 ,  1.8361472], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8553719880141224}
episode index:1233
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.92715079, 15.0535904 ]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.8719497, 20.979275 ,  3.5735364], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5474481808161864
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.357908, 14.334788,  5.026261], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7717143943019564}
episode index:1234
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.2699969 , 20.58517176]), 'previousTarget': array([18.03861062, 20.31756858]), 'currentState': array([23.322512 , 29.214893 ,  1.5533984], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5476671757609484
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.433447 , 15.944952 ,  3.3109043], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7168879616889008}
episode index:1235
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.57733058, 13.38960327]), 'previousTarget': array([17.56338512, 13.36875492]), 'currentState': array([26.057957,  8.090643,  4.4543  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.5475418067096514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.849902 , 14.1480255,  3.0132375], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2034094052806177}
episode index:1236
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.83325397, 14.32394344]), 'previousTarget': array([ 9.91227901, 14.3216372 ]), 'currentState': array([-0.08222429, 13.026526  ,  5.444412  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 200
reward sum = 0.13397967485796172
running average episode reward sum: 0.5472074800064568
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.513097  , 16.937208  ,  0.09464175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.997460946519277}
episode index:1237
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.42152527,  9.21094762]), 'previousTarget': array([11.40757591,  9.41178475]), 'currentState': array([6.16353  , 0.7048625, 5.0075383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 274
reward sum = 0.06368590427489448
running average episode reward sum: 0.5468169133055427
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175072, 13.230129,  6.135383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5422048974936793}
episode index:1238
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.5547964 , 17.01930009]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.204092, 26.661978,  2.691265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5466322257756909
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.404558, 15.298978,  4.799068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4360262678182243}
episode index:1239
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.70793763, 16.9131953 ]), 'previousTarget': array([12.54700196, 18.67949706]), 'currentState': array([ 8.1112585, 25.200365 ,  5.166094 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5468576583547714
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.990276, 16.353605,  4.962332], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6887241054431512}
episode index:1240
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.6497973 , 15.69045012]), 'previousTarget': array([ 9.91227901, 15.6783628 ]), 'currentState': array([-0.2679575, 16.970348 ,  4.886176 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5468713952863569
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.319798, 14.012898,  5.941909], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1987675344550295}
episode index:1241
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.78180625, 19.52626204]), 'previousTarget': array([13.42535625, 21.298575  ]), 'currentState': array([11.182898 , 29.182642 ,  4.5968566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5471519644159643
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.1      , 16.059246 ,  3.9085627], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1753162466383063}
episode index:1242
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.06563997, 15.2053589 ]), 'previousTarget': array([15.57464375, 17.298575  ]), 'currentState': array([18.110247, 24.730608,  4.323921], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5471346229220863
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.793333 , 16.99958  ,  3.7694716], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.335458590693157}
episode index:1243
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.028453 , 23.938408 ,  4.8212886], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.93845318363955}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5474516209337252
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.131502 , 16.437067 ,  5.1066265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8290595323511862}
episode index:1244
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.121892 , 10.686033 ,  6.1176414], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.315688448353529}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5477238564765223
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.78732  , 13.021834 ,  1.9772949], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3202869857059043}
episode index:1245
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.8768693 , 12.59603901]), 'previousTarget': array([13.94085849, 11.55779009]), 'currentState': array([9.644047 , 3.5360603, 2.3107276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5479607901337514
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.019281 , 15.213815 ,  1.1586345], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9922256307106603}
episode index:1246
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.719966  , 14.909493  ,  0.05438298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2832297707134421}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5483232915049352
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.719966  , 14.909493  ,  0.05438298], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2832297707134421}
episode index:1247
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.27937963, 16.14860265]), 'previousTarget': array([12.23076923, 16.15384615]), 'currentState': array([ 3.0667582, 20.038025 ,  3.1775281], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 214
reward sum = 0.11639428152900338
running average episode reward sum: 0.5479771945418134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.2154   , 14.247653 ,  3.3449285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4294141626784722}
episode index:1248
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.78625471, 15.18524336]), 'previousTarget': array([14.52576695, 15.41495392]), 'currentState': array([ 7.229319 , 21.734497 ,  3.3083897], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5478592661658405
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.91847 , 16.634817,  5.954891], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5205466939267365}
episode index:1249
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.936362 ,  9.917646 ,  5.6429224], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.485336701447702}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5475601706156888
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.742289 , 13.56977  ,  1.9486444], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6113815828025806}
episode index:1250
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.95663319, 14.87608263]), 'previousTarget': array([10., 15.]), 'currentState': array([-0.04034966, 14.630453  ,  5.519769  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5478600783085843
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.870562 , 15.505908 ,  5.8265896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2375677827907974}
episode index:1251
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.001276 , 18.07652  ,  6.2709928], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.869601556820511}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5480375408225592
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.5096245, 15.384519 ,  5.109086 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5391795732595441}
episode index:1252
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.41331029,  8.46125851]), 'previousTarget': array([22.1768175 ,  7.31055268]), 'currentState': array([26.790342 ,  0.7584368,  3.18508  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5482529194313179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.229658 , 13.602018 ,  1.1908122], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8618302245207075}
episode index:1253
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.72560976, 20.52277492]), 'previousTarget': array([16.83772234, 20.51316702]), 'currentState': array([19.707954 , 30.067701 ,  1.5023828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5482024716249726
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.411067  , 16.963531  ,  0.40991226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.006098705036081}
episode index:1254
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.15609   , 11.04954   ,  0.96345234], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.495490704150037}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5481972784810218
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.83663  , 13.519452 ,  2.6988442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.700579793537736}
episode index:1255
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.11131721, 12.46790166]), 'previousTarget': array([18.26042701, 12.3323779 ]), 'currentState': array([25.867388 ,  6.155741 ,  2.8777215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5482376914810101
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.256168 , 15.591788 ,  2.3094296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.388586455679212}
episode index:1256
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.7316432 , 10.36617181]), 'previousTarget': array([11.69209979, 10.22192192]), 'currentState': array([5.9678493, 2.1943495, 5.4894876], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5482456695614133
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.787275 , 13.0300455,  2.0066977], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3133140423446035}
episode index:1257
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.185854 , 15.365783 ,  5.4500785], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.8316454741972343}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5485507726443589
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.388012 , 16.814444 ,  0.6560645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9148720400810375}
episode index:1258
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.79181277, 16.04258972]), 'previousTarget': array([13.47409319, 16.35636161]), 'currentState': array([ 6.220965, 22.575756,  4.375818], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.5484365400788577
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.243865, 16.68889 ,  4.467732], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7064059074232067}
episode index:1259
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.23616328, 19.8427934 ]), 'previousTarget': array([20.49208627, 21.40743398]), 'currentState': array([25.820082, 27.369547,  4.209507], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5487190365351512
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.535709 , 16.885662 ,  4.2555084], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9419802404855138}
episode index:1260
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.350805 , 13.080588 ,  4.3765116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.56855238563718}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5487221804214535
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.938597 , 13.438147 ,  2.3199434], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8221827251057374}
episode index:1261
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.96064  , 16.857866 ,  1.1478481], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.151214975046853}
done in step count: 124
reward sum = 0.2875836093668641
running average episode reward sum: 0.5485152560386843
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.07476  , 16.479984 ,  5.5391045], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8290608180748218}
episode index:1262
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.925806 , 15.250815 ,  5.2849226], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.928974469408093}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5487483739621564
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.318663, 15.973718,  4.016271], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.63920630369014}
episode index:1263
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.38476052, 13.74721128]), 'previousTarget': array([19.38476052, 13.74721128]), 'currentState': array([29.       , 11.       ,  0.7362908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5487018090088669
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.145872 , 13.697039 ,  3.9248354], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.735145868417914}
episode index:1264
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.85206835, 15.83453516]), 'previousTarget': array([11.74391196, 15.75140493]), 'currentState': array([ 2.1859732, 18.397074 ,  1.6441586], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 106
reward sum = 0.3446121833475176
running average episode reward sum: 0.5485404733364073
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.6814995, 13.876402 ,  5.3830276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7323153020158792}
episode index:1265
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.43135664, 10.73045085]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([5.0181866, 3.0576906, 6.171088 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5488003315904851
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.595085 , 13.5879345,  0.769064 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1303111533128707}
episode index:1266
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.57274963, 16.36944711]), 'previousTarget': array([12.68964732, 17.56705854]), 'currentState': array([ 6.357075, 23.292889,  5.546978], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 102
reward sum = 0.3587482976818919
running average episode reward sum: 0.5486503299851904
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.088976 , 15.5825815,  2.7781436], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9978524254723702}
episode index:1267
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.867088 , 10.798907 ,  5.1631317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.811953693288086}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5485400538372702
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.898895 , 14.556262 ,  2.9123154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9500529789154009}
episode index:1268
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.36300886, 15.96807208]), 'previousTarget': array([13.24695048, 17.19131191]), 'currentState': array([ 8.866232 , 24.32184  ,  6.1819625], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5486733796676572
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.962526, 15.965861,  5.131851], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.187326599449892}
episode index:1269
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.939175, 17.075901,  5.785849], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.9251266993255527}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5490130856679188
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.875614  , 16.783901  ,  0.50669813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.788232473445557}
episode index:1270
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.86266529, 10.81238194]), 'previousTarget': array([20.86266529, 10.81238194]), 'currentState': array([29.        ,  5.        ,  0.39595735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5489823796573615
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.135564 , 14.602427 ,  1.6932154], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.906354327197028}
episode index:1271
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.17059199, 10.86928697]), 'previousTarget': array([21.40743398,  9.50791373]), 'currentState': array([27.983524 ,  4.6276455,  2.241345 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 283
reward sum = 0.05817817197670824
running average episode reward sum: 0.5485965272928327
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.553273, 14.398977,  3.78596 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8169085293308816}
episode index:1272
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.7537496 , 17.25453347]), 'previousTarget': array([12.74099823, 19.195289  ]), 'currentState': array([ 8.915925, 26.006416,  5.227724], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5484771881562492
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.175086 , 16.889235 ,  3.4215078], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0614776811637516}
episode index:1273
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.788352 , 24.114553 ,  1.7413484], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.379044271191159}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5484551028615036
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.3845215, 16.738678 ,  6.039768 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.373346149937074}
episode index:1274
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.848047 ,  7.0981097,  0.9957528], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.903351162617351}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5485766656788668
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.697777 , 13.159904 ,  5.9724913], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9679551537955726}
episode index:1275
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42840756, 15.49768573]), 'previousTarget': array([14.52576695, 15.41495392]), 'currentState': array([ 6.886593, 22.064346,  3.598701], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 262
reward sum = 0.07184904244991483
running average episode reward sum: 0.5482030546888754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.440981, 13.279479,  2.196374], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7761352942080217}
episode index:1276
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.77848931, 18.91738553]), 'previousTarget': array([ 9.34803445, 20.21719897]), 'currentState': array([ 3.4483135, 25.719482 ,  5.633463 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5480278334478218
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.281757  , 16.783218  ,  0.04967087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.476333093968979}
episode index:1277
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.713945 , 19.495674 ,  4.0232816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.831335718664596}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5483659180851865
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.894386 , 16.016977 ,  4.5326056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.150102851228336}
episode index:1278
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.46691548, 18.00090162]), 'previousTarget': array([16.52786405, 18.05572809]), 'currentState': array([20.858553  , 26.984972  ,  0.11526077], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5483564593730067
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.186022, 16.877895,  5.73023 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.610940033359727}
episode index:1279
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.019214 , 17.331457 ,  5.8481655], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.613279976368197}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5486937590141215
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.26746 , 16.381205,  5.713102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.215721432253774}
episode index:1280
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.59868   , 14.79340195]), 'previousTarget': array([16.1613009 , 14.78885438]), 'currentState': array([25.051647 , 11.531284 ,  3.9774158], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.548710080203398
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.741228 , 15.615475 ,  3.5774417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9634460073633697}
episode index:1281
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.54282546, 11.28709307]), 'previousTarget': array([13.58979079, 11.33345606]), 'currentState': array([9.889489 , 1.9783266, 3.9161682], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5487446015498864
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.126636, 16.31287 ,  5.088031], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5768300999780127}
episode index:1282
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.99171527, 15.77689508]), 'previousTarget': array([12.43661488, 16.63124508]), 'currentState': array([ 6.070381 , 21.880375 ,  5.9522333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5490361059091053
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.475926, 15.393695,  4.99972 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6176581531500442}
episode index:1283
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.40747212, 18.97025493]), 'previousTarget': array([13.74721128, 19.38476052]), 'currentState': array([12.931402 , 28.860716 ,  6.1874666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.5488964591269719
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.923914 , 15.009272 ,  3.5273008], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9239362962067912}
episode index:1284
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.37659103, 17.43444604]), 'previousTarget': array([11.32050294, 17.45299804]), 'currentState': array([ 3.0760612, 23.011292 ,  2.2332993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5489496837820811
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.246734 , 16.675909 ,  5.7153163], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8374115731105178}
episode index:1285
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.40095153, 19.59936863]), 'previousTarget': array([20.36613715, 19.54057759]), 'currentState': array([28.014387 , 26.082855 ,  4.0178003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5490753475017468
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.0350685, 16.977324 ,  2.2983527], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.787609029848683}
episode index:1286
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.02223401, 15.11542995]), 'previousTarget': array([15.03883865, 15.19419324]), 'currentState': array([16.913656 , 24.934927 ,  3.2691705], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5493036830463328
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.338878, 16.630995,  4.337073], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3279758151570116}
episode index:1287
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.08636336,  8.06404996]), 'previousTarget': array([20.08636336,  8.06404996]), 'currentState': array([26.       ,  0.       ,  3.0968187], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 257
reward sum = 0.07555183406752786
running average episode reward sum: 0.5489358632878089
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.818767, 16.877201,  5.13474 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8859293315068009}
episode index:1288
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.697746  , 15.961264  ,  0.94950867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.8638894517454254}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5491256836294348
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.953768 , 15.518509 ,  2.2596147], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.021400508403002}
episode index:1289
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.98529272, 21.92117562]), 'previousTarget': array([20.08636336, 21.93595004]), 'currentState': array([25.82992 , 30.035381,  5.892146], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5491931718483761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.609453 , 15.261732 ,  4.3624735], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6305959953586764}
episode index:1290
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.335918, 22.849266,  5.967125], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.491206615249249}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5494612935233648
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.449669, 15.222118,  4.233745], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4665866656108588}
episode index:1291
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.03388828, 14.66592274]), 'previousTarget': array([10.974587 , 14.7124705]), 'currentState': array([ 1.0691766, 13.826566 ,  4.0849056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5496884466966315
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.132086  , 16.465797  ,  0.20830888], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.703477956240585}
episode index:1292
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.945652, 18.02422 ,  2.946492], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.221702194022201}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5498019229312432
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.592934 , 16.719896 ,  6.1462793], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.344244221098749}
episode index:1293
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.99348024, 19.39734925]), 'previousTarget': array([15.90470911, 19.22197586]), 'currentState': array([18.197208, 29.151506,  4.329239], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5500689525535982
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.858589, 16.825697,  4.232107], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8311653005260253}
episode index:1294
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.98839509, 18.34718782]), 'previousTarget': array([16.96128974, 18.36221099]), 'currentState': array([22.095686 , 26.944605 ,  1.0955734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5502211557518825
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.398691 , 15.574768 ,  4.0639167], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6995091029226083}
episode index:1295
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.65167753, 21.20062907]), 'previousTarget': array([16.57464375, 21.298575  ]), 'currentState': array([19.22565  , 30.863686 ,  1.2073269], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5501316574270676
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.062319 , 13.053432 ,  1.6076362], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.21757669691541}
episode index:1296
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.60421009, 20.21196669]), 'previousTarget': array([20.65196555, 20.21719897]), 'currentState': array([27.926893 , 27.02213  ,  4.3571177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.550344484694923
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.899984 , 13.929334 ,  2.7045615], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1808867282418016}
episode index:1297
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.973495 , 11.886684 ,  5.4959054], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.6861115471229007}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.5500492531466333
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.65774  , 14.556896 ,  2.7455866], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7159375514892456}
episode index:1298
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.84155876, 20.85056602]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 5.0481358, 29.00141  ,  4.0862317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5501728127879926
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.9401245, 16.536053 ,  5.0815673], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5372192371697067}
episode index:1299
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.68609279, 14.10491607]), 'previousTarget': array([10.77802414, 14.09529089]), 'currentState': array([ 0.89463913, 12.073307  ,  5.8384123 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5498834412615989
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.673305 , 15.323279 ,  4.9193454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4596079525023693}
episode index:1300
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.88563087,  9.88908269]), 'previousTarget': array([17.06080701, 10.1914503 ]), 'currentState': array([22.802137 ,  1.1811574,  0.2161368], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 296
reward sum = 0.0510525689892109
running average episode reward sum: 0.5495000201453251
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.469636 , 16.89037  ,  4.5907197], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9633600122021533}
episode index:1301
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.880516 ,  8.257219 ,  1.1791979], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.332289144529506}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5495426515178229
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.42493  , 13.142317 ,  2.5772116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.341241965341682}
episode index:1302
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.790044, 13.436195,  5.969087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.707284835258677}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5498434477556456
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.509195, 13.016813,  5.53163 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0430170826071437}
episode index:1303
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.90826708, 16.3790891 ]), 'previousTarget': array([16.80768079, 16.26537656]), 'currentState': array([25.013252  , 22.236496  ,  0.91168666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 236
reward sum = 0.09330521652106866
running average episode reward sum: 0.5494933417500977
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.072874 , 16.722881 ,  3.8307118], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0296252859424584}
episode index:1304
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.9687834, 13.931965 ,  4.3038635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.10192196022849}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5497117482019751
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.117183, 13.428383,  5.838747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4525458728178435}
episode index:1305
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.98900431, 20.09847487]), 'previousTarget': array([15., 20.]), 'currentState': array([14.967438 , 30.098452 ,  1.8076141], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 157
reward sum = 0.2064075371174136
running average episode reward sum: 0.5494488812715888
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.348147, 16.406567,  4.412505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5502714208620518}
episode index:1306
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.72386879, 16.59109045]), 'previousTarget': array([11.94427191, 16.52786405]), 'currentState': array([ 4.5278273 , 22.320393  ,  0.69002026], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 127
reward sum = 0.27904208858505886
running average episode reward sum: 0.5492419900759602
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.006884 , 16.119858 ,  5.5587993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4967837543700595}
episode index:1307
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.57533525, 18.1440046 ]), 'previousTarget': array([16.52786405, 18.05572809]), 'currentState': array([21.055052  , 27.084482  ,  0.99832165], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5494473914119856
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.133085, 15.549818,  4.519896], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9461938640642495}
episode index:1308
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.56491682, 12.77693478]), 'previousTarget': array([12.43294146, 12.68964732]), 'currentState': array([5.1796517, 6.0346904, 1.8339843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5495984416052015
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.590127, 13.534352,  1.39588 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5218801057640943}
episode index:1309
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.39378573, 15.70754258]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.242334 , 19.738787 ,  3.2404666], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.54961371088831
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.479401 , 13.732844 ,  6.0121346], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3699296089619113}
episode index:1310
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.53990384, 11.92744876]), 'previousTarget': array([11.401844 , 10.6822128]), 'currentState': array([6.28977   , 4.1213083 , 0.12747633], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5497474764299344
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.329935 , 13.76503  ,  1.6377318], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0770816034987063}
episode index:1311
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.731585, 18.902243,  4.775116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.5136688447606454}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5499973038282339
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.346227, 16.273703,  6.209455], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0874109613362433}
episode index:1312
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.7910347, 21.256145 ,  6.173691 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.545079200902725}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5501828460824726
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.60597  , 16.477306 ,  5.7748256], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0311948879569113}
episode index:1313
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.12418433, 10.71603379]), 'previousTarget': array([14.09529089, 10.77802414]), 'currentState': array([12.12121  ,  0.9186824,  1.6362689], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.550332761796513
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.070312 , 14.441206 ,  1.1685517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0089669700687165}
episode index:1314
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.64543967, 18.4222089 ]), 'previousTarget': array([11.69209979, 19.77807808]), 'currentState': array([ 6.977223, 26.660614,  5.583089], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5504178943282145
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.324896, 16.706326,  5.165469], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.73698132885902}
episode index:1315
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.14042789,  9.19867709]), 'previousTarget': array([19.18761806,  9.13733471]), 'currentState': array([24.949675 ,  1.0591038,  4.659657 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 445
reward sum = 0.011419881460400001
running average episode reward sum: 0.5500083213701082
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.527719, 13.312201,  2.176392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7683760509907194}
episode index:1316
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.48817605, 21.81071473]), 'previousTarget': array([10.54700196, 21.67949706]), 'currentState': array([ 4.9654784, 30.14737  ,  2.7550225], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.549635320237921
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.158328 , 13.038536 ,  0.7672833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1344208587977014}
episode index:1317
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.076924 , 24.97469  ,  2.4841568], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.97498609757017}
done in step count: 220
reward sum = 0.10958290556334815
running average episode reward sum: 0.5493014413193515
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.664579 , 16.158125 ,  3.9210813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3352599396703277}
episode index:1318
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.4696429 , 13.09862249]), 'previousTarget': array([ 9.48683298, 13.16227766]), 'currentState': array([0.0129437, 9.84734  , 4.5847397], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5492311671013261
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.069984 , 13.0328665,  5.583375 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2393036834842537}
episode index:1319
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.12604  , 22.129307 ,  2.9314766], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.784544838242159}
done in step count: 120
reward sum = 0.2993803913123313
running average episode reward sum: 0.5490418862105769
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.145548 , 14.091725 ,  4.0796022], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4619311735008726}
episode index:1320
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.52174043, 18.67085305]), 'previousTarget': array([20.58821525, 18.59242409]), 'currentState': array([28.849411 , 24.207088 ,  1.5989999], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5488271431095452
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.114691, 13.211559,  2.746694], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5986363742481733}
episode index:1321
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.30049633,  8.51446513]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([24.826847  ,  0.18023166,  5.2134213 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5488650620682114
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.828767 , 14.694458 ,  1.5489688], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8832951690056268}
episode index:1322
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.396996 , 10.0011   ,  1.0789149], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.356396281526596}
done in step count: 153
reward sum = 0.2148744477060795
running average episode reward sum: 0.5486126126242491
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.001051 , 13.32837  ,  2.6348753], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6716302360650457}
episode index:1323
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.30193955, 13.32803113]), 'previousTarget': array([19., 12.]), 'currentState': array([25.392925 ,  7.4513016,  2.2521505], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5485795863977859
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.125155, 14.139111,  2.015702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8699393687883695}
episode index:1324
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.994938 , 20.000217 ,  2.8163092], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.100227863159947}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5486130859147699
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.076236 , 14.723428 ,  4.8151064], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9435434147717536}
episode index:1325
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.36421751, 21.16416392]), 'previousTarget': array([ 8.32793492, 21.19548901]), 'currentState': array([ 1.0375853, 27.970078 ,  3.5071044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5487628287567131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.812701, 14.776311,  6.159131], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2081867341768933}
episode index:1326
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.179539 , 22.033302 ,  1.9039059], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.797246211185834}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5485889223777096
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.259928, 16.60796 ,  5.685637], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.042780576391117}
episode index:1327
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.199992 , 14.131848 ,  3.2713325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.855201942087566}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.548744141025922
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.35236   , 13.402399  ,  0.25512993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7238812658725144}
episode index:1328
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.46086148,  9.97450641]), 'previousTarget': array([12.47213595,  9.94427191]), 'currentState': array([7.951267 , 1.0490621, 5.615318 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 176
reward sum = 0.17052743088958636
running average episode reward sum: 0.5484595535841339
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.96985  , 13.695278 ,  2.1059465], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3627547926497066}
episode index:1329
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.46327384, 16.97540135]), 'previousTarget': array([18.67949706, 17.45299804]), 'currentState': array([25.264568, 23.231583,  3.332028], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5487271645024984
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.91693  , 15.66866  ,  3.4601955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0302029812500937}
episode index:1330
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.72977222, 11.63204051]), 'previousTarget': array([17.6676221 , 11.73957299]), 'currentState': array([24.026392 ,  3.8633482,  0.6483709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5486579553238669
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.783123 , 13.838511 ,  1.6220711], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6822143756537264}
episode index:1331
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.99810487, 16.13394651]), 'previousTarget': array([17.22104427, 16.21147869]), 'currentState': array([25.695175 , 21.06963  ,  2.4102695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5486651686746356
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.930489 , 16.39238  ,  4.5338945], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3941137811317832}
episode index:1332
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.65585412, 18.28326945]), 'previousTarget': array([19.24275371, 19.6284586 ]), 'currentState': array([23.944931 , 26.05807  ,  3.7776816], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5489252385062815
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.895014, 15.495596,  3.689786], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5065939479302503}
episode index:1333
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.8679433 , 17.96347531]), 'previousTarget': array([12.88171698, 17.91263916]), 'currentState': array([ 7.027865 , 26.080956 ,  4.3368497], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5492054629859828
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.245728, 16.841621,  5.294114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.5434309985106855}
episode index:1334
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.78051693,  8.03060079]), 'previousTarget': array([20.49208627,  8.59256602]), 'currentState': array([25.437008  , -0.21585965,  3.6998584 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5491537296478446
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.074805  , 13.08472   ,  0.95444906], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9167406229644166}
episode index:1335
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.57905979, 12.44390992]), 'previousTarget': array([16.15384615, 12.23076923]), 'currentState': array([17.788486 ,  2.691042 ,  3.3879359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 299
reward sum = 0.04953625663766235
running average episode reward sum: 0.5487797644734358
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.603632 , 13.3338995,  2.1983595], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3124719216662437}
episode index:1336
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.006174, 23.734917,  4.287449], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.60465589952067}
done in step count: 110
reward sum = 0.33103308832101386
running average episode reward sum: 0.5486169023371961
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.552153 , 16.723417 ,  4.5654826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3193414855203653}
episode index:1337
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.951217 , 15.187052 ,  1.5927163], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.0545159993419375}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5487541866924406
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.282089 , 16.488674 ,  4.9111214], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5151650410498554}
episode index:1338
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.95946708, 21.99251472]), 'previousTarget': array([11.96138938, 20.31756858]), 'currentState': array([ 5.9563074, 30.650944 ,  2.2754533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5488007610410891
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.041968 , 16.916662 ,  4.2268653], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.917121642721966}
episode index:1339
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.5949103 , 18.05120494]), 'previousTarget': array([14.09529089, 19.22197586]), 'currentState': array([13.27882 , 27.964222,  5.34727 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5489658672983758
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.46613  , 15.642569 ,  3.9427762], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8354108530992125}
episode index:1340
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.688796 , 10.936641 ,  4.4191055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.1213260572016415}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.5488613589517123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.409807 , 13.467512 ,  2.0492501], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5863357194715473}
episode index:1341
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.99503719, 15.0496281 ]), 'currentState': array([13.907816 , 24.938885 ,  1.4059479], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.99871470807438}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5488285903450322
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.154926 , 15.8497505,  3.9740632], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4338517017960237}
episode index:1342
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.25493242, 15.82142487]), 'previousTarget': array([13.10366477, 15.86197056]), 'currentState': array([ 4.207178 , 20.080315 ,  2.5107343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5490049489082952
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.2548065 , 13.887961  ,  0.05175656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0693791733443856}
episode index:1343
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.05939  , 13.680044 ,  3.3290396], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.228738049738576}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5493257041546432
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.965473 , 14.295022 ,  2.6423912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1954632645927674}
episode index:1344
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.065703 , 12.063384 ,  3.8489985], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.460306343863502}
done in step count: 272
reward sum = 0.06497898609824965
running average episode reward sum: 0.5489655950705864
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.652075 , 16.171528 ,  0.5293676], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7858834900981697}
episode index:1345
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.03287123, 11.58679375]), 'previousTarget': array([13.57662651,  9.6623494 ]), 'currentState': array([11.306705 ,  1.9655662,  1.5026697], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 132
reward sum = 0.26536624974770534
running average episode reward sum: 0.5487548971914461
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.683148 , 16.180758 ,  4.0631747], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.222531502104733}
episode index:1346
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.236367, 19.234737,  5.026775], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.84347186743555}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5490325436630396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.631928 , 14.457388 ,  5.6057343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.471749858976119}
episode index:1347
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.704867 ,  7.9400067,  1.7717333], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.791110577222176}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5491523661286251
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.694166 , 15.81312  ,  1.0341771], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5382996811893106}
episode index:1348
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.01120491, 11.6137165 ]), 'previousTarget': array([16.42337349,  9.6623494 ]), 'currentState': array([18.872532 ,  2.0318165,  1.7486355], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5494023531601948
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.232212 , 14.553045 ,  1.2252914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.823415121533942}
episode index:1349
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.2116347 , 15.49637968]), 'previousTarget': array([17.298575  , 15.57464375]), 'currentState': array([26.9689   , 17.686302 ,  2.1269042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 149
reward sum = 0.2236886739786474
running average episode reward sum: 0.5491610837682086
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.68826 , 15.347255,  4.096678], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7236022557077508}
episode index:1350
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.17068191, 15.44110969]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 2.2900448, 16.981567 ,  3.9686806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5493132363984348
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.690821  , 14.385879  ,  0.54922265], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4460620992233402}
episode index:1351
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.42114997, 14.52003808]), 'previousTarget': array([14.52576695, 14.58504608]), 'currentState': array([6.723178, 8.137153, 3.247655], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5494880625111631
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.838983 , 16.135487 ,  0.3882243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1468462991917476}
episode index:1352
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.32014354, 14.99414616]), 'previousTarget': array([13., 15.]), 'currentState': array([ 3.3202043, 14.959299 ,  3.559939 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5494743282025482
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.278547 , 16.916399 ,  5.6328826], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9365365285072194}
episode index:1353
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.2549442, 12.7684418]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([21.156645 ,  4.0521736,  2.9658928], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5497166078885131
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.109258 , 14.330189 ,  2.0149417], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0058797474774313}
episode index:1354
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.909927, 23.14089 ,  2.039987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.031147435071732}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5498513582662
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.07733 , 16.253561,  4.506398], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2952290867516982}
episode index:1355
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.986223 ,  5.881439 ,  3.0680728], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.595086243700111}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5497130991656058
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.70298  , 14.68048  ,  1.9819746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7326956022381517}
episode index:1356
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.8525748 , 10.63891623]), 'previousTarget': array([15.90470911, 10.77802414]), 'currentState': array([17.771215  ,  0.82470113,  3.9262187 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5497815579002551
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.149742 , 14.216631 ,  2.4182506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3912490964305466}
episode index:1357
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.80823834, 15.49705264]), 'previousTarget': array([18., 15.]), 'currentState': array([27.655184 , 17.239943 ,  2.1528726], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 131
reward sum = 0.2680467169168741
running average episode reward sum: 0.5495740948362025
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.983097 , 15.835299 ,  4.6432695], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.151835881851165}
episode index:1358
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.90652714, 13.25328298]), 'previousTarget': array([18.92040615, 13.19058177]), 'currentState': array([28.035528 ,  9.1714525,  5.91363  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 160
reward sum = 0.2002770268574893
running average episode reward sum: 0.5493170697677855
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.7234955, 16.962202 ,  5.1630836], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9815881811379286}
episode index:1359
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.67670267, 13.01107091]), 'previousTarget': array([14.64398987, 12.86393924]), 'currentState': array([13.072276 ,  3.1406193,  3.2080498], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5495025944146762
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.218214 , 13.9995165,  1.5838442], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2697072712082411}
episode index:1360
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.9376826, 20.96002  ,  3.971338 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.24111280273114}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5497371596008293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.428514 , 16.972492 ,  5.2312403], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.053612022524537}
episode index:1361
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.27172816, 20.96911834]), 'previousTarget': array([12.22885465, 20.9381686 ]), 'currentState': array([ 8.11472  , 30.064133 ,  4.1359015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5496654316204075
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.236477, 15.460397,  4.541968], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8915899897070447}
episode index:1362
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([8.5563322 , 7.44130882]), 'previousTarget': array([7.07106781, 7.07106781]), 'currentState': array([ 2.0688777 , -0.16874598,  6.0712647 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 151
reward sum = 0.2192372693664723
running average episode reward sum: 0.5494230045020994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.809717 , 14.335841 ,  1.5527409], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3630407679818288}
episode index:1363
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.934547 , 20.155436 ,  1.7091539], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.227539389848615}
done in step count: 150
reward sum = 0.22145178723886091
running average episode reward sum: 0.5491825563956014
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.787547 , 16.241209 ,  5.3210387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4699762955160127}
episode index:1364
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.90366403, 15.29694206]), 'previousTarget': array([ 9.97785158, 15.33480989]), 'currentState': array([-0.07940456, 15.878613  ,  3.0241215 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 109
reward sum = 0.334376856889913
running average episode reward sum: 0.5490251895827767
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.061445, 15.344271,  5.470056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9688872215948894}
episode index:1365
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.20851085, 22.94378608]), 'previousTarget': array([ 7.07106781, 22.92893219]), 'currentState': array([ 0.20620972, 30.082958  ,  1.9959009 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 215
reward sum = 0.11523033871371334
running average episode reward sum: 0.5487076238061521
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.018078, 16.405485,  2.844794], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4296921457597214}
episode index:1366
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.70972111, 20.85157365]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 4.796869 , 28.916199 ,  1.6573709], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 135
reward sum = 0.25748460676394874
running average episode reward sum: 0.5484945857541826
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.212737, 16.349419,  5.504201], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5622783254422912}
episode index:1367
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.69763084, 17.3724478 ]), 'previousTarget': array([15.90470911, 19.22197586]), 'currentState': array([18.518744, 26.966265,  4.763702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5485925828369723
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.574683, 16.453167,  4.299702], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0354906605264733}
episode index:1368
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.19296763, 13.53025151]), 'previousTarget': array([20.05572809, 12.47213595]), 'currentState': array([27.27681  ,  9.3488865,  2.2065995], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5486904367538266
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.510629 , 16.402199 ,  5.2742844], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.045577747091298}
episode index:1369
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.25842724, 14.85642931]), 'currentState': array([21.916737 ,  9.364448 ,  3.3952143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.921922223184813}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5486735798565797
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.181418 , 13.339694 ,  2.1443062], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8511325565081622}
episode index:1370
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.59798719, 17.07969862]), 'previousTarget': array([12.43661488, 16.63124508]), 'currentState': array([ 5.0379105 , 23.625326  ,  0.83952343], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.5485933055472823
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.078902 , 15.598005 ,  4.7502365], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6031881106308791}
episode index:1371
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.33579371, 20.12024449]), 'previousTarget': array([14.3216372 , 20.08772099]), 'currentState': array([13.049356 , 30.037153 ,  1.9485971], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5486344227204529
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.007301 , 14.508107 ,  4.4597325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0525122480153843}
episode index:1372
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.692186, 15.91706 ,  4.742531], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.8043710310143615}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5486138153642475
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.22017 , 16.957848,  1.311725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3069416462248}
episode index:1373
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.82225482, 20.09343858]), 'previousTarget': array([13.96116135, 20.19419324]), 'currentState': array([11.569417 , 29.83637  ,  1.2948024], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 229
reward sum = 0.10010587426148955
running average episode reward sum: 0.5482873903707229
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.607521, 14.574826,  5.445705], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5786298007629257}
episode index:1374
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.69599771, 18.8608084 ]), 'previousTarget': array([18.90289239, 20.07376011]), 'currentState': array([25.611204, 26.084375,  5.095042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.548198155994736
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.640749 , 13.258925 ,  3.4081993], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.855235802551961}
episode index:1375
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.79303342, 14.64621076]), 'previousTarget': array([12.86393924, 14.64398987]), 'currentState': array([ 2.9190984, 13.063363 ,  5.1720037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5480476966237471
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.45067  , 13.599159 ,  1.7769088], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4715496934035532}
episode index:1376
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.655566 , 13.050513 ,  2.0927746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.688890137528593}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.548293402633255
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.839181 , 16.693718 ,  1.1552889], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0533342133302956}
episode index:1377
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.969976 , 23.026068 ,  1.4920912], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.264294911406969}
done in step count: 282
reward sum = 0.05876583027950327
running average episode reward sum: 0.5479381576605745
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.760425 , 16.204475 ,  3.0007296], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.424432023485993}
episode index:1378
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.28034616, 12.49968061]), 'previousTarget': array([11.94427191, 13.47213595]), 'currentState': array([2.981066 , 6.9209757, 4.8510585], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5479106359699211
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.254737, 13.415462,  5.526635], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.3572665139067905}
episode index:1379
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.03883865,  9.80580676]), 'previousTarget': array([16.03883865,  9.80580676]), 'currentState': array([18.       ,  0.       ,  5.1494718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5477761876959287
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.625568, 15.52631 ,  4.913514], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8175194137386022}
episode index:1380
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.430625 , 15.204078 ,  5.2192974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4765349660133998}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5481036488199723
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.430625 , 15.204078 ,  5.2192974], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.4765349660133998}
episode index:1381
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.12398391, 12.89008303]), 'previousTarget': array([17.11828302, 12.08736084]), 'currentState': array([24.218506 ,  5.8425474,  1.2391908], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.5479883668150911
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.910092 , 14.075727 ,  1.8708342], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9286350779082909}
episode index:1382
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.69125125, 18.05588733]), 'previousTarget': array([16.03883865, 20.19419324]), 'currentState': array([17.897541 , 27.809465 ,  4.5522594], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5482016385624293
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.44596 , 16.11799 ,  3.963114], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8277583093929337}
episode index:1383
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.82927296, 19.66878417]), 'previousTarget': array([11.69209979, 19.77807808]), 'currentState': array([ 6.2110763, 27.941381 ,  2.649663 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5481703410553662
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.33638 , 15.018877,  5.338044], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6638884244803637}
episode index:1384
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.22115646, 21.7374011 ]), 'previousTarget': array([12.98274993, 21.45520022]), 'currentState': array([ 8.408239, 30.981949,  2.919581], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5481282615838491
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.113528 , 16.900938 ,  5.4285526], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.097474092796662}
episode index:1385
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.21762516, 11.77857536]), 'previousTarget': array([14.24859507, 11.74391196]), 'currentState': array([11.85757  ,  2.0610583,  1.2911557], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5483918900008042
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.63963  , 13.000406 ,  1.0205485], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.099405165117477}
episode index:1386
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 8.116364 , 12.059586 ,  1.7879813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.485351570042923}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.548629185698712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.067589 , 14.784753 ,  1.2087768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0890717131706316}
episode index:1387
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.25962395, 18.02760871]), 'previousTarget': array([15.23303501, 18.02945514]), 'currentState': array([16.11401  , 27.991043 ,  1.9857037], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 174
reward sum = 0.173989828476264
running average episode reward sum: 0.5483592726171396
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.842067 , 16.896038 ,  3.5999877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.902604324175506}
episode index:1388
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.46676971, 15.1653267 ]), 'previousTarget': array([14.57826285, 15.12652114]), 'currentState': array([ 4.9153247, 18.126728 ,  0.8022047], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5482004270338571
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.411684  , 15.777073  ,  0.17805737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.974657875977308}
episode index:1389
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.87918265, 19.9308721 ]), 'previousTarget': array([ 8.86318339, 19.82178448]), 'currentState': array([ 1.0917764, 26.204332 ,  3.4811952], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.5480943005776827
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.526322 , 16.679596 ,  4.5900645], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.234450339242017}
episode index:1390
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.54210206, 14.58602602]), 'previousTarget': array([16.35236179, 14.63117406]), 'currentState': array([26.200155 , 11.993342 ,  6.0957184], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5479607834801146
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.977762, 16.117916,  4.072287], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.514828867187584}
episode index:1391
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.67397 , 18.418194,  6.21101 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.328557668428709}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.548223395882416
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.950013 , 16.613434 ,  4.7134843], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9250041849154473}
episode index:1392
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.074303 , 15.854691 ,  5.0613556], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.999298960167424}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5484725809925209
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.417141  , 14.697473  ,  0.27556914], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6566943765528283}
episode index:1393
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.69427871, 14.24479843]), 'previousTarget': array([11.74391196, 14.24859507]), 'currentState': array([ 1.9454427, 12.017649 ,  5.0465903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5485992006158821
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.109735 , 14.436917 ,  5.4901867], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.053392006425668}
episode index:1394
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.46684173, 19.25036733]), 'previousTarget': array([10.07106781, 19.92893219]), 'currentState': array([ 1.5365113, 25.342154 ,  4.0733213], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5487579417952292
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.858183 , 16.940435 ,  5.7311325], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9456108723115453}
episode index:1395
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.30208152, 11.65583872]), 'previousTarget': array([15.90470911, 10.77802414]), 'currentState': array([16.201729 ,  1.6963892,  2.6475136], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5490392614281846
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.597657 , 13.240025 ,  1.9253558], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8053789773200848}
episode index:1396
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.41458677, 15.48595905]), 'previousTarget': array([10.77802414, 15.90470911]), 'currentState': array([ 2.586688 , 17.33323  ,  6.1373506], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.5489698300672957
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.103013, 15.359398,  5.171194], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9307320822990122}
episode index:1397
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.04387907, 14.85559252]), 'previousTarget': array([17.03454242, 14.8304548 ]), 'currentState': array([27.019012 , 14.150813 ,  4.6635575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5486749303354569
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.273077 , 13.527172 ,  1.8893946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.946778655303086}
episode index:1398
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.41077546,  8.17359147]), 'previousTarget': array([19.45299804,  8.32050294]), 'currentState': array([24.837812  , -0.22565074,  5.1341357 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5487329351252123
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.119789 , 13.61466  ,  1.8106605], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3905091228963902}
episode index:1399
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.16759178, 12.83088622]), 'previousTarget': array([12.43294146, 12.68964732]), 'currentState': array([6.7143197, 5.1918235, 6.0052123], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5488384210415868
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.059832, 13.98376 ,  1.464781], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.190204859591096}
episode index:1400
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.34119918, 19.77688744]), 'previousTarget': array([18.30790021, 19.77807808]), 'currentState': array([24.072803  , 27.97132   ,  0.73114175], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 172
reward sum = 0.17752252675876343
running average episode reward sum: 0.5485733847144755
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.535484 , 13.144641 ,  5.5023193], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.408333320934129}
episode index:1401
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.39423678, 16.45108796]), 'previousTarget': array([15.36882594, 16.35236179]), 'currentState': array([18.016035 , 26.101278 ,  2.6840744], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5487654913855758
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.407447, 15.226053,  4.374425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.608516573138094}
episode index:1402
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.0986144 , 15.83242487]), 'previousTarget': array([11.33345606, 16.41020921]), 'currentState': array([ 3.9380474, 19.842913 ,  6.0628185], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5488608506896563
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.143245, 13.727655,  5.892173], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5339134015822946}
episode index:1403
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.009287 , 24.04172  ,  3.3087227], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.041724205988725}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5490763755617397
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.452187 , 15.647113 ,  5.0158124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5898430463618984}
episode index:1404
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.07106781, 15.92893219]), 'currentState': array([ 8.270877 , 21.697224 ,  5.0348215], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.4938876494699}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.549355666503974
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175331  , 16.301657  ,  0.35551804], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2413671635498496}
episode index:1405
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.59001325, 10.503115  ]), 'previousTarget': array([16.63663603, 10.41741912]), 'currentState': array([19.923578 ,  1.0751065,  3.2152386], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5492706973340025
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.344996 , 16.104235 ,  6.0671387], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.989565531817602}
episode index:1406
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.068548 , 10.7806225,  0.5878879], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.490174378843404}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5495769015292165
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.055272  , 13.3049965 ,  0.14185303], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.579729344943994}
episode index:1407
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.56430135, 17.42275129]), 'previousTarget': array([16.63124508, 17.56338512]), 'currentState': array([21.988596 , 25.823765 ,  2.6653352], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 107
reward sum = 0.34116606151404244
running average episode reward sum: 0.5494288824667056
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.879452, 16.496075,  5.319015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.73541776563728}
episode index:1408
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.07376011, 18.90289239]), 'previousTarget': array([20.07376011, 18.90289239]), 'currentState': array([28.       , 25.       ,  6.1476164], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5492961238686884
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.291853, 16.618122,  2.46588 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6442315160071668}
episode index:1409
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.28718744,  9.00095594]), 'previousTarget': array([10.17821552,  8.86318339]), 'currentState': array([4.109557 , 1.1373128, 3.5041509], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 141
reward sum = 0.2424166460445802
running average episode reward sum: 0.5490784788489551
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.222383 , 13.257881 ,  1.0970169], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7562551162508222}
episode index:1410
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.87347886, 15.42173715]), 'currentState': array([13.750162 , 23.601467 ,  5.5036864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.691796796252987}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5492988756425713
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.285803 , 15.438474 ,  5.4520416], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3585095263892417}
episode index:1411
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.65674436, 17.33529788]), 'previousTarget': array([17.56705854, 17.31035268]), 'currentState': array([25.167576 , 23.937374 ,  5.7398734], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5492431304343157
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.929003 , 15.21078  ,  2.7493951], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.940484456053085}
episode index:1412
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.207167 , 21.972147 ,  2.5000124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.460619654224425}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5492081650411558
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.114068 , 14.128584 ,  5.5342717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0775238616628027}
episode index:1413
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.82942426, 15.0650457 ]), 'previousTarget': array([16.04106794, 15.09464254]), 'currentState': array([25.798815  , 15.846872  ,  0.18054771], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 148
reward sum = 0.22594815553398728
running average episode reward sum: 0.548979551173046
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.755833, 15.584141,  3.355538], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9552504782954814}
episode index:1414
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.16773852, 22.40774869]), 'previousTarget': array([ 9.24695048, 22.19131191]), 'currentState': array([ 2.9817302, 30.264803 ,  5.7208223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.5488375819598442
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.876626  , 14.339659  ,  0.11643522], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.6717676113253177}
episode index:1415
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.89615316, 14.85524917]), 'previousTarget': array([13.64363839, 13.47409319]), 'currentState': array([9.066932  , 6.729969  , 0.05166018], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.54910164065509
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.540229  , 13.5024395 ,  0.44998032], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.566549383398549}
episode index:1416
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.37369141, 16.01771976]), 'previousTarget': array([10.41741912, 16.63663603]), 'currentState': array([ 3.0493069, 19.631008 ,  5.8736186], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5493785485652847
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.584775  , 16.244808  ,  0.96650517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.884783629506696}
episode index:1417
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.89494801, 15.07997311]), 'previousTarget': array([16.04106794, 15.09464254]), 'currentState': array([25.855259 , 15.970033 ,  5.9763613], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 103
reward sum = 0.355160814705073
running average episode reward sum: 0.5492415826034651
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.607802 , 13.903371 ,  1.7807803], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2538019075284814}
episode index:1418
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.30452326, 11.71936655]), 'previousTarget': array([10.22192192, 11.69209979]), 'currentState': array([2.107122 , 5.99201  , 1.1364955], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5492175484490044
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.429237 , 15.376526 ,  0.4892133], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6152606572092396}
episode index:1419
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.7659235, 16.901644 ,  1.1947931], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.9338280063937474}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.549520986795167
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.122421 , 16.932491 ,  0.7650068], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6944061575131495}
episode index:1420
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.00598666, 19.95917146]), 'previousTarget': array([19.92893219, 19.92893219]), 'currentState': array([27.110195 , 26.996943 ,  3.4252903], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5494556023904863
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.070524, 16.778019,  4.410087], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0063092047832196}
episode index:1421
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 6.950779 , 13.918601 ,  3.2811096], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.121538219508738}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5496863094373278
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.729536 , 14.673488 ,  5.9610524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.42398237081398127}
episode index:1422
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.66899654, 10.33587561]), 'previousTarget': array([13.74721128, 10.61523948]), 'currentState': array([10.924842  ,  0.71976334,  3.6869235 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 156
reward sum = 0.20849246173476124
running average episode reward sum: 0.5494465386378179
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.515368 , 13.459257 ,  2.2377486], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.139630671171014}
episode index:1423
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.31593396, 18.90997574]), 'previousTarget': array([20.65196555, 20.21719897]), 'currentState': array([26.726948, 25.623907,  4.170675], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 300
reward sum = 0.04904089407128572
running average episode reward sum: 0.5490951301795549
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.346282 , 13.887913 ,  1.0726807], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2899942897745669}
episode index:1424
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([18.991772 , 22.991985 ,  1.6643575], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.933424352438191}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5491792541293024
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.881283 , 16.810368 ,  5.2833223], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.6108726103868842}
episode index:1425
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.92322493, 11.13578061]), 'previousTarget': array([11.96138938,  9.68243142]), 'currentState': array([8.189223  , 2.3273058 , 0.82402825], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.549403354100298
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.990954 , 13.471405 ,  1.7969317], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.821700635552434}
episode index:1426
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 7.3780725, 22.0833169]), 'previousTarget': array([ 7.31055268, 22.1768175 ]), 'currentState': array([ 0.05292377, 28.890827  ,  3.622606  ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5493582160597751
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.663954 , 15.076246 ,  4.5110393], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3382200833083568}
episode index:1427
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.39997646, 13.66691137]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([10.295569 ,  4.5480385,  4.3490577], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5495405477489638
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.363759 , 13.531555 ,  1.9511276], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.600353948318056}
episode index:1428
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.00320668, 14.72701657]), 'previousTarget': array([15.1695452 , 12.96545758]), 'currentState': array([15.1206665,  4.7277064,  2.6418824], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5497518264217041
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.773337 , 14.188822 ,  0.7388759], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1207411668263896}
episode index:1429
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.10824  , 22.206322 ,  1.2702454], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.295101543963378}
done in step count: 123
reward sum = 0.29048849430996376
running average episode reward sum: 0.5495705233922553
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.209469 , 16.419552 ,  3.9493284], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.284979055135246}
episode index:1430
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.93913  , 21.011497 ,  4.8228517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.011805663926711}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5496310509692445
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.913085 , 14.149176 ,  3.7991002], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2480489864526019}
episode index:1431
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.75710843, 18.5102351 ]), 'previousTarget': array([15.2875295, 19.025413 ]), 'currentState': array([17.865484, 28.285446,  5.651722], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5496959815216994
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.478961 , 14.990095 ,  4.3457437], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5211331454419813}
episode index:1432
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.28650671, 13.10315918]), 'previousTarget': array([18.36221099, 13.03871026]), 'currentState': array([26.947474 ,  8.104394 ,  5.1061015], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5498774413170237
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.27908  , 14.228175 ,  2.3814094], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0561432958344983}
episode index:1433
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.13488664, 14.18957552]), 'previousTarget': array([13.10366477, 14.13802944]), 'currentState': array([ 3.9632974, 10.204359 ,  1.787185 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5498060645893018
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.157393 , 15.578229 ,  5.8712983], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9312036653370697}
episode index:1434
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.7350839 , 20.83937242]), 'previousTarget': array([10.81238194, 20.86266529]), 'currentState': array([ 4.8370075, 28.91481  ,  3.048909 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 259
reward sum = 0.07404835256958406
running average episode reward sum: 0.5494745261140267
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.674274 , 16.77024  ,  3.5997727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2116285132313305}
episode index:1435
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.20915814, 18.72857795]), 'previousTarget': array([17.06080701, 19.8085497 ]), 'currentState': array([22.306551 , 27.331867 ,  5.4565606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.549667210026089
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.809141 , 16.660011 ,  2.4648986], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.455326703580769}
episode index:1436
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.24119639, 12.73290777]), 'previousTarget': array([16.21147869, 12.77895573]), 'currentState': array([21.04343  ,  3.9614456,  3.9684536], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 178
reward sum = 0.1671339350148836
running average episode reward sum: 0.549401007329491
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.124508, 13.079456,  2.457583], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9245753321348285}
episode index:1437
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([20.36613715, 19.54057759]), 'previousTarget': array([20.36613715, 19.54057759]), 'currentState': array([28.       , 26.       ,  5.9648056], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5494032899927963
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.728338 , 15.906727 ,  3.2091768], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1630262900582304}
episode index:1438
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.88270041, 21.9585094 ]), 'previousTarget': array([21.92893219, 21.92893219]), 'currentState': array([28.914934 , 29.0682   ,  0.7587975], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5494419298657253
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.757611 , 16.943169 ,  3.8880777], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.958227938252852}
episode index:1439
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.71119863, 15.97250381]), 'previousTarget': array([16.42507074, 15.85504245]), 'currentState': array([25.405258 , 20.913488 ,  4.8565392], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5494721552244306
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.69893 , 14.041746,  3.215359], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.950542519823836}
episode index:1440
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.257915, 18.945623,  3.20303 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.80489090928599}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.549332405716636
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.472495 , 15.954462 ,  4.1845846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0905316360303923}
episode index:1441
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.04427  , 19.236744 ,  2.7172964], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.704151054564186}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5493838571654395
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.215424 , 15.87236  ,  4.8678656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4960838407338957}
episode index:1442
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.044032 , 10.342331 ,  3.4619443], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.219072969502092}
done in step count: 198
reward sum = 0.136700004956598
running average episode reward sum: 0.5490978669698686
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.432405 , 14.019884 ,  1.1774907], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.0712617097891868}
episode index:1443
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.42952  , 17.554115 ,  5.3056555], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.2357230199499085}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5493963449013299
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.644279 , 15.913352 ,  0.1607725], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6346842548999188}
episode index:1444
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([9.832864 , 7.001649 , 2.9045746], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.522232758265956}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5494837723782565
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.737771  , 14.959707  ,  0.44943243], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2628719123045766}
episode index:1445
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.55393517, 10.96388389]), 'previousTarget': array([15.58578644, 10.89949494]), 'currentState': array([16.913635 ,  1.0567544,  3.4075944], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5495617794796437
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.979449 , 13.564124 ,  2.5815737], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7616081764018099}
episode index:1446
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.019411 , 14.789063 ,  2.8970704], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.986173894925643}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5496830086133536
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.835417 , 13.272656 ,  0.1940116], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9187592321958107}
episode index:1447
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.64010883, 20.87902091]), 'previousTarget': array([13.84288535, 21.17127813]), 'currentState': array([14.029091  , 30.860336  ,  0.17363042], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 216
reward sum = 0.1140780353265762
running average episode reward sum: 0.5493821764494815
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.346876, 14.38114 ,  5.895018], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8997548406510839}
episode index:1448
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([19.65176004, 11.56918841]), 'previousTarget': array([21.13681661, 10.17821552]), 'currentState': array([27.699675 ,  5.6336117,  2.2434855], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 231
reward sum = 0.0981137673636859
running average episode reward sum: 0.5490707420746811
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.694279 , 13.176226 ,  2.1275792], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.489323845167856}
episode index:1449
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.64929086, 16.69563094]), 'previousTarget': array([15.58256937, 16.60206577]), 'currentState': array([19.225285, 26.034382,  2.663441], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5490849010128898
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.447733 , 16.677309 ,  3.9819288], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7360387022192196}
episode index:1450
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.961403, 19.276093,  5.598516], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.382837304977793}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5493618859535425
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.982182, 15.529307,  4.80134 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.051635927879925}
episode index:1451
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.72106972,  8.88586628]), 'previousTarget': array([11.70588235,  8.82352941]), 'currentState': array([6.9949346 , 0.07316826, 4.9007287 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 286
reward sum = 0.05645022209082803
running average episode reward sum: 0.5490224151106619
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.862274 , 13.461661 ,  0.5845815], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.913349393328526}
episode index:1452
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.64444174, 22.44271513]), 'previousTarget': array([ 8.54930538, 22.44310917]), 'currentState': array([ 2.1506236, 30.04734  ,  3.0957532], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5489041867774291
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.404202  , 15.533113  ,  0.29212523], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.682491827622945}
episode index:1453
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.86059386, 16.10647783]), 'currentState': array([20.055136 , 23.547691 ,  3.5518253], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.930630622006431}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.549112270398005
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.175653, 16.309742,  4.579561], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.245810398219347}
episode index:1454
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.730923 , 13.8166895,  5.135226 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.820958351608008}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.5490215546657268
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.824286 , 13.492511 ,  1.4746394], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7181303416758313}
episode index:1455
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.290712 , 13.882386 ,  1.4754574], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.407466858792659}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.548862878504431
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.472776 , 13.083235 ,  1.3188143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.4172420740170644}
episode index:1456
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([21.19548901,  8.32793492]), 'previousTarget': array([21.19548901,  8.32793492]), 'currentState': array([28.      ,  1.      ,  0.649729], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5489141225101872
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.056713 , 15.538183 ,  3.1060517], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1858683546961937}
episode index:1457
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([14.99503719, 14.9503719 ]), 'currentState': array([13.367882 ,  6.841676 ,  1.7524718], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.319979360071338}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5491334858094045
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.340122 , 14.179609 ,  1.2581911], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8515493883984602}
episode index:1458
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.3076997 , 13.27869772]), 'previousTarget': array([13.94085849, 11.55779009]), 'currentState': array([10.576239 ,  4.000972 ,  1.7398319], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5493895592902945
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.025221 , 13.862975 ,  1.6333828], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2787229285164963}
episode index:1459
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.88852474, 20.87481098]), 'previousTarget': array([11.70588235, 21.17647059]), 'currentState': array([ 7.2081423, 29.711893 ,  3.853015 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 158
reward sum = 0.2043434617462395
running average episode reward sum: 0.5491532263467712
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.795858, 16.00401 ,  5.528542], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0574600604011026}
episode index:1460
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.25699231, 13.88579492]), 'previousTarget': array([ 9.48683298, 13.16227766]), 'currentState': array([ 1.6726218, 11.032754 ,  0.9759019], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 111
reward sum = 0.3277227574378037
running average episode reward sum: 0.5490016654508718
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.237286 , 15.822389 ,  5.0798864], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.121631100038182}
episode index:1461
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.33360003, 21.97117293]), 'previousTarget': array([12.51123442, 21.63670822]), 'currentState': array([ 8.761113 , 31.311266 ,  3.8263218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 234
reward sum = 0.09519969035921708
running average episode reward sum: 0.5486912673830936
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.031001, 13.342667,  5.534156], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.57365705102324}
episode index:1462
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.919866 , 11.79591  ,  3.4337177], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.444482124875271}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5485714950065188
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.77754  , 13.339616 ,  1.1394571], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.432390759369919}
episode index:1463
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.94322 , 17.920334,  4.214361], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.621949522805756}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5488463710686046
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.179545 , 13.235646 ,  3.4282727], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7734657352894432}
episode index:1464
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.12328117, 15.39209319]), 'previousTarget': array([15.12652114, 15.42173715]), 'currentState': array([18.122696, 24.93167 ,  1.635947], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5490244471758761
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.523567 , 16.43395  ,  3.5059466], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.5110268138726446}
episode index:1465
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.868373, 16.983227,  2.465348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9875900317883353}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5493320703360562
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.868373, 16.983227,  2.465348], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9875900317883353}
episode index:1466
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.21696181, 10.5998179 ]), 'previousTarget': array([16.25278872, 10.61523948]), 'currentState': array([18.882599 ,  0.9616449,  4.3305717], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 179
reward sum = 0.16546259566473476
running average episode reward sum: 0.5490704006191706
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.194283 , 13.812976 ,  2.4084425], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6838459011459626}
episode index:1467
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.55764706, 16.08652359]), 'previousTarget': array([11.55779009, 16.05914151]), 'currentState': array([ 2.0213947, 19.096489 ,  2.3744946], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5491520772935238
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.333336  , 14.5127945 ,  0.10348957], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7364154183671356}
episode index:1468
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([20.282766 ,  6.8230796,  2.7481656], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.734970348301323}
done in step count: 249
reward sum = 0.08187728905270836
running average episode reward sum: 0.5488339868998949
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.55916 , 15.631196,  4.36931 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7699013695285032}
episode index:1469
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 5.671592 , 15.8545475,  2.0027218], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.367467259103025}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5488181790771913
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.324425  , 15.323675  ,  0.70519835], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.706551507005536}
episode index:1470
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.75731864, 14.22207357]), 'previousTarget': array([16.89633523, 14.13802944]), 'currentState': array([25.901419 , 10.174181 ,  1.4400581], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.548691431176976
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.910578, 16.456305,  4.616742], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.7175490751179143}
episode index:1471
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([13.86782193, 17.46525654]), 'previousTarget': array([13.94085849, 18.44220991]), 'currentState': array([ 9.694365 , 26.552734 ,  3.7728982], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 112
reward sum = 0.3244455298634257
running average episode reward sum: 0.5485390902114098
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.511718, 16.211687,  5.975953], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.3153101164109926}
episode index:1472
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.54218371, 10.77731507]), 'previousTarget': array([18.29411765,  8.82352941]), 'currentState': array([22.699924 ,  2.2100687,  1.6890738], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 260
reward sum = 0.07330786904388821
running average episode reward sum: 0.5482164620911332
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.630943 , 16.928314 ,  2.2631638], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.525543848554343}
episode index:1473
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.53437974, 14.03822353]), 'previousTarget': array([14.47213595, 13.94427191]), 'currentState': array([10.176916 ,  5.0375285,  3.1702833], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5483122779824343
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.667326 , 15.983838 ,  1.0426877], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6564894902210963}
episode index:1474
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.26793954, 14.51787559]), 'previousTarget': array([14.32050294, 14.54700196]), 'currentState': array([5.9164186 , 9.017685  , 0.33595103], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5485852798617005
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.89222  , 15.240517 ,  0.4447883], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.133589786865453}
episode index:1475
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.717637, 12.038081,  2.426533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.861664780462726}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.548555673228181
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.836081  , 14.122606  ,  0.90086806], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2119613988693811}
episode index:1476
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.70184441, 16.55867728]), 'previousTarget': array([18.26042701, 17.6676221 ]), 'currentState': array([24.076292 , 23.312752 ,  3.7468524], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.5484638614461267
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.515103, 16.637505,  5.453616], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.2105065781892814}
episode index:1477
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([17.90733803, 15.74989786]), 'previousTarget': array([16.1613009 , 15.21114562]), 'currentState': array([27.59042  , 18.247482 ,  0.7535987], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 398
reward sum = 0.018315022217166757
running average episode reward sum: 0.5481051680501666
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.85352 , 16.030584,  5.140385], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.040942048040964}
episode index:1478
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 8.6448737 , 11.43030516]), 'previousTarget': array([ 8.57492926, 11.14495755]), 'currentState': array([-0.07384852,  6.532971  ,  4.1670713 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.548093538824274
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.149618 , 13.022319 ,  0.1192254], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.152759174491367}
episode index:1479
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([11.85701421, 15.51883089]), 'previousTarget': array([11.88371698, 15.47942816]), 'currentState': array([ 1.9905415, 17.147547 ,  3.2183106], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5479912308969752
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.281477  , 15.472443  ,  0.17675956], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8599287028319772}
episode index:1480
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.28441231, 15.95398543]), 'previousTarget': array([14.13940614, 16.10647783]), 'currentState': array([ 8.283883 , 23.953588 ,  5.5071034], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5482633435364102
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.06222  , 15.592867 ,  5.4282846], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0264461404951652}
episode index:1481
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.87255917, 15.57544803]), 'previousTarget': array([15.67949706, 15.45299804]), 'currentState': array([24.220592 , 21.08093  ,  2.2837124], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5483226702182775
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.817396, 15.937037,  4.826553], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.044741556544445}
episode index:1482
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([18.29095971, 16.94215922]), 'previousTarget': array([19., 18.]), 'currentState': array([26.903088 , 22.024605 ,  4.7057567], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5484721782935216
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.920343, 14.748936,  3.72261 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.936685842688701}
episode index:1483
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15.0496281 , 15.00496281]), 'currentState': array([23.463469 , 16.952417 ,  2.6265383], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.685748875340904}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5483698909809397
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.695263 , 16.23422  ,  2.9791782], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.0969541208105804}
episode index:1484
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([22.247004  , 20.81738   ,  0.38158143], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.293060912644416}
done in step count: 155
reward sum = 0.21059844619672852
running average episode reward sum: 0.5481424354625665
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.773746 , 16.898531 ,  2.8844566], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.598190989684661}
episode index:1485
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.39805033, 19.21288927]), 'previousTarget': array([14.41421356, 19.10050506]), 'currentState': array([12.983587 , 29.112349 ,  2.0568767], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 126
reward sum = 0.2818606955404635
running average episode reward sum: 0.5479632418287023
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.734983 , 16.685577 ,  4.5747657], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.838850676275523}
episode index:1486
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.985826, 18.845972,  4.646813], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 3.9774427630434155}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5482605093190662
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.493042 , 16.676954 ,  4.4560285], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.254572699870176}
episode index:1487
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([14.75568031, 14.28262018]), 'previousTarget': array([14.16227766, 12.48683298]), 'currentState': array([11.531797 ,  4.816545 ,  1.5508533], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 316
reward sum = 0.04175625035843684
running average episode reward sum: 0.5479201166719153
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.760123  , 13.387445  ,  0.76301956], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.6302984484722989}
episode index:1488
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([13.95893206, 15.09464254]), 'currentState': array([ 5.7817755, 15.757719 ,  5.889323 ], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.249313571562553}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5480590012726754
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.132764, 15.050845,  5.939171], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8687253575701287}
episode index:1489
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.21238658, 16.12594624]), 'previousTarget': array([15.21114562, 16.1613009 ]), 'currentState': array([17.065992, 25.952652,  2.523003], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 129
reward sum = 0.2734891510222162
running average episode reward sum: 0.5478747262053931
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.299086, 15.625976,  4.251055], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9397480699896301}
episode index:1490
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.902142 ,  5.979998 ,  3.4429088], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.06500377709213}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5478597843954134
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.211657  , 14.175023  ,  0.15642461], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1410838195576019}
episode index:1491
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([12.60128986, 13.72714172]), 'previousTarget': array([12.08736084, 12.88171698]), 'currentState': array([3.7679114, 9.039773 , 1.8744588], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 101
reward sum = 0.3623720178604969
running average episode reward sum: 0.5477354628360737
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.78181 , 16.94177 ,  4.903063], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.635397971656547}
episode index:1492
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 7.0291615, 13.89053  ,  2.5048451], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.047682376598324}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.547636969326439
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.042691  , 14.192292  ,  0.49547058], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.1174157566616523}
episode index:1493
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.27323734, 15.61382545]), 'previousTarget': array([15.36882594, 16.35236179]), 'currentState': array([19.339916, 24.749586,  6.010102], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 136
reward sum = 0.2549097606963093
running average episode reward sum: 0.5474410341131659
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.467986 , 16.072311 ,  4.6449733], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1970340529753856}
episode index:1494
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([ 9.93323008, 12.9111334 ]), 'previousTarget': array([10.1914503 , 12.93919299]), 'currentState': array([0.6880878, 9.099658 , 1.7302723], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 84
reward sum = 0.4298890135238935
running average episode reward sum: 0.5473624039990593
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.856202 , 16.803972 ,  5.6680155], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.9968469991126894}
episode index:1495
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.504217  ,  9.159779  ,  0.09075236], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.861227405268543}
done in step count: 114
reward sum = 0.3179890638191435
running average episode reward sum: 0.5472090795738054
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.813723 , 14.607414 ,  1.2908669], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2495509671297989}
episode index:1496
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([10.64977149, 19.97712273]), 'previousTarget': array([ 9.07106781, 20.92893219]), 'currentState': array([ 4.0687995, 27.506453 ,  5.7767606], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5472642663150408
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.5457   , 16.88942  ,  4.9991283], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.441125759510699}
episode index:1497
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([9.64442854, 8.872237  ]), 'previousTarget': array([9.50791373, 8.59256602]), 'currentState': array([3.063713, 1.342683, 4.547851], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5473110125597853
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.153963  , 14.675702  ,  0.69776857], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.8743055802032087}
episode index:1498
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([15.49857413, 15.45423475]), 'previousTarget': array([15.47423305, 15.41495392]), 'currentState': array([22.890697 , 22.18896  ,  1.8749148], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.547405834489945
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.050806 , 15.987367 ,  3.0677812], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.4419037065127285}
episode index:1499
at step 0:
{'scaleFactor': 1.0, 'currentTarget': array([16.66017099, 12.20613663]), 'previousTarget': array([17.75902574, 10.51658317]), 'currentState': array([21.768549 ,  3.6093655,  2.5806203], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 113
reward sum = 0.3212010745647914
running average episode reward sum: 0.5472550313166615
{'scaleFactor': 1.0, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.510218 , 13.12992  ,  1.9788729], dtype=float32), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 2.403738044234956}
episode index:0
done in step count: 152
reward sum = 1.0
episode index:1
done in step count: 345
reward sum = 1.0
episode index:2
done in step count: 351
reward sum = 1.0
episode index:3
done in step count: 92
reward sum = 1.0
episode index:4
done in step count: 201
reward sum = 1.0
episode index:5
done in step count: 180
reward sum = 1.0
episode index:6
done in step count: 163
reward sum = 1.0
episode index:7
done in step count: 55
reward sum = 1.0
episode index:8
done in step count: 60
reward sum = 1.0
episode index:9
done in step count: 129
reward sum = 1.0
episode index:10
done in step count: 31
reward sum = 1.0
episode index:11
done in step count: 228
reward sum = 1.0
episode index:12
done in step count: 168
reward sum = 1.0
episode index:13
done in step count: 171
reward sum = 1.0
episode index:14
reward sum = 0.0
episode index:15
done in step count: 380
reward sum = 1.0
episode index:16
done in step count: 0
reward sum = 1.0
episode index:17
done in step count: 204
reward sum = 1.0
episode index:18
done in step count: 97
reward sum = 1.0
episode index:19
done in step count: 246
reward sum = 1.0
episode index:20
done in step count: 63
reward sum = 1.0
episode index:21
done in step count: 106
reward sum = 1.0
episode index:22
done in step count: 128
reward sum = 1.0
episode index:23
done in step count: 106
reward sum = 1.0
episode index:24
done in step count: 283
reward sum = 1.0
episode index:25
done in step count: 20
reward sum = 1.0
episode index:26
done in step count: 121
reward sum = 1.0
episode index:27
done in step count: 103
reward sum = 1.0
episode index:28
done in step count: 288
reward sum = 1.0
episode index:29
done in step count: 5
reward sum = 1.0
episode index:30
done in step count: 18
reward sum = 1.0
episode index:31
done in step count: 178
reward sum = 1.0
episode index:32
done in step count: 56
reward sum = 1.0
episode index:33
done in step count: 178
reward sum = 1.0
episode index:34
done in step count: 110
reward sum = 1.0
episode index:35
done in step count: 182
reward sum = 1.0
episode index:36
done in step count: 228
reward sum = 1.0
episode index:37
done in step count: 463
reward sum = 1.0
episode index:38
done in step count: 245
reward sum = 1.0
episode index:39
done in step count: 60
reward sum = 1.0
episode index:40
done in step count: 64
reward sum = 1.0
episode index:41
done in step count: 279
reward sum = 1.0
episode index:42
done in step count: 48
reward sum = 1.0
episode index:43
done in step count: 1
reward sum = 1.0
episode index:44
done in step count: 118
reward sum = 1.0
episode index:45
done in step count: 191
reward sum = 1.0
episode index:46
done in step count: 139
reward sum = 1.0
episode index:47
done in step count: 57
reward sum = 1.0
episode index:48
done in step count: 149
reward sum = 1.0
episode index:49
done in step count: 322
reward sum = 1.0
episode index:50
done in step count: 105
reward sum = 1.0
episode index:51
done in step count: 58
reward sum = 1.0
episode index:52
done in step count: 107
reward sum = 1.0
episode index:53
done in step count: 72
reward sum = 1.0
episode index:54
done in step count: 14
reward sum = 1.0
episode index:55
done in step count: 29
reward sum = 1.0
episode index:56
done in step count: 62
reward sum = 1.0
episode index:57
done in step count: 86
reward sum = 1.0
episode index:58
done in step count: 44
reward sum = 1.0
episode index:59
done in step count: 251
reward sum = 1.0
episode index:60
done in step count: 55
reward sum = 1.0
episode index:61
done in step count: 235
reward sum = 1.0
episode index:62
done in step count: 182
reward sum = 1.0
episode index:63
done in step count: 114
reward sum = 1.0
episode index:64
done in step count: 331
reward sum = 1.0
episode index:65
done in step count: 152
reward sum = 1.0
episode index:66
done in step count: 76
reward sum = 1.0
episode index:67
done in step count: 304
reward sum = 1.0
episode index:68
done in step count: 203
reward sum = 1.0
episode index:69
done in step count: 38
reward sum = 1.0
episode index:70
done in step count: 127
reward sum = 1.0
episode index:71
done in step count: 106
reward sum = 1.0
episode index:72
done in step count: 245
reward sum = 1.0
episode index:73
done in step count: 49
reward sum = 1.0
episode index:74
done in step count: 160
reward sum = 1.0
episode index:75
done in step count: 46
reward sum = 1.0
episode index:76
done in step count: 107
reward sum = 1.0
episode index:77
done in step count: 159
reward sum = 1.0
episode index:78
done in step count: 12
reward sum = 1.0
episode index:79
done in step count: 41
reward sum = 1.0
episode index:80
done in step count: 16
reward sum = 1.0
episode index:81
done in step count: 444
reward sum = 1.0
episode index:82
done in step count: 384
reward sum = 1.0
episode index:83
done in step count: 109
reward sum = 1.0
episode index:84
done in step count: 81
reward sum = 1.0
episode index:85
done in step count: 318
reward sum = 1.0
episode index:86
done in step count: 69
reward sum = 1.0
episode index:87
done in step count: 249
reward sum = 1.0
episode index:88
done in step count: 144
reward sum = 1.0
episode index:89
done in step count: 236
reward sum = 1.0
episode index:90
done in step count: 70
reward sum = 1.0
episode index:91
done in step count: 199
reward sum = 1.0
episode index:92
done in step count: 128
reward sum = 1.0
episode index:93
done in step count: 237
reward sum = 1.0
episode index:94
done in step count: 57
reward sum = 1.0
episode index:95
done in step count: 88
reward sum = 1.0
episode index:96
done in step count: 63
reward sum = 1.0
episode index:97
done in step count: 71
reward sum = 1.0
episode index:98
done in step count: 302
reward sum = 1.0
episode index:99
done in step count: 12
reward sum = 1.0

Process finished with exit code 0
